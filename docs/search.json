[
  {
    "objectID": "releaseNotes/index.html",
    "href": "releaseNotes/index.html",
    "title": "Release Notes",
    "section": "",
    "text": "Welcome to our section on FACTS Release Notes — your go-to source for detailed information on what’s new in each version of FACTS. Whether you’re curious about the latest capabilities, need to verify that a known issue has been resolved, or just want to keep a pulse on how our solution is evolving, this is the place to start.",
    "crumbs": [
      "Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/index.html#what-youll-find-here",
    "href": "releaseNotes/index.html#what-youll-find-here",
    "title": "Release Notes",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nNew Features: Understand the core functionalities, enhancements, and integrations introduced with each update.\nImprovements & Optimizations: Learn about refinements in performance, stability, and efficiency that help ensure you’re always getting the best experience.\nFixes & Patches: Stay informed about resolved bugs, security patches, and other maintenance updates that keep our product reliable and secure.\nVersion & Update History: Trace the growth of our product through a chronological record of releases, so you’ll know exactly when key changes were introduced.",
    "crumbs": [
      "Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/index.html#how-to-use-these-notes",
    "href": "releaseNotes/index.html#how-to-use-these-notes",
    "title": "Release Notes",
    "section": "How to Use These Notes",
    "text": "How to Use These Notes\nExplore the latest release notes or browse previous versions to see how our product has evolved over time by selecting the major version (FACTS 6, 7 …) and then the release of interest from the sidebar. With every iteration, we’re committed to delivering meaningful enhancements that align with your needs. Check back often to stay current on all the ways we’re working to make our solution faster, more powerful, and easier to use.",
    "crumbs": [
      "Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v7/facts710.html",
    "href": "releaseNotes/v7/facts710.html",
    "title": "FACTS 7.1.0 Release Notes",
    "section": "",
    "text": "1 Introduction\nFACTS 7.1.0 is now available for download via App Center. FACTS users can now:\n\nPerform concurrent control analyses using posterior probabilities, predictive probabilities and p-values in Platform Trial designs.\nMake interim and final decisions based on conditional power as well as Bayesian predictive probabilities.\nExport the data associated with the in-built graphs FACTS provides into a CSV file.\nExplore, via integration with AIRSHIP, simulation results graphically in a much more generic, versatile way; namely, by allowing the user to view the impact of simulation input dimensions through dynamic filtering of the simulation results.\nCreate designs which use an independent Beta Binomial dose response model for analyzing (simulated) data for dichotomous endpoints in Core and Staged designs.\n\nCreate designs where frequentist (p-value) calculations are performed using the Fisher exact test rather than a normal approximation for dichotomous endpoints.\nProvide three separate patient queue lengths for Continual Reassessment Methods (CRM) designs, based on dose clearance and the current model estimate of the MTD. In addition, the ability to backfill to the current escalation dose (“frontfill”) is now available.\nView explanations of some of the most important inputs in Core Continuous/Dichotomous designs through informative tooltips.\nSet different random number seeds when simulating multiple design scenarios.\nCreate designs where decision QOIs can be used in multiple stopping criteria simultaneously.\n\n\n\n2 FACTS Core and Staged Improvements\n\nIn Core and Staged designs making use of dichotomous endpoints, FACTS users can specify a beta binomial model to independently model the response rate on each arm. With this model, the user specifies a Beta distribution prior on the response rate.\nIn Core and Staged designs which perform frequentist analyses on dichotomous endpoints, users can now specify whether p-value calculations (including Current Trial Predictive Probability QOIs) should be performed using a normal approximation as currently, or using a Fisher exact test. The latter is of particular use when sample sizes are small.\nIn Staged designs, the reported Early Success Time will now correctly only be reported for simulations which have stopped for early success. Previously, simulations which have graduated early to stage 2 in stage 1 were being reported as having an Early Success Time.\nWhen running analyses in FACTS Core and Staged designs, the user can now specify the number of MCMC samples to generate per imputation of missing data modelled by a longitudinal model.\nIn Core and Staged designs with a dichotomous endpoint, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Core and Staged designs, a new Operating Characteristics graph displaying the cumulative proportion of simulations having stopped at a given duration, sample size or number of events (in Time-to-Event) has been added.\nIn Core designs, two new summary columns “Ppn Overall Success” and “Ppn Overall Futility” have been added, which simply sum up the existing columns “Ppn Early Success” and “Ppn Late Success”, and “Ppn Early Futility” and “Ppn Late Futility”, respectively. This allows users to quickly view the estimated type I error and power of the design.\nIn Core and Staged designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Staged designs, the Stage 2 “Explore” Final Success/Futility graphs will now have the option to include/exclude simulations which have stopped in Stage 1.\nIn Core and Staged designs, the criteria for selecting a dose at the end of the trial have moved from the “Variants” tab to the Success/Futility tab. These criteria will be used when reporting the proportion of times the correct/incorrect arm was selected at the end of the trial (as reported in the “Ppn Correct/Incorrect Arm” columns, which are now reported in the summary file).\nIn Core and Staged Multiple Endpoint designs, designs not using a control arm will no longer crash when adding a new endpoint, and no longer crash when adding a new treatment to a design which uses Virtual Subject Response external data files.\nIn Core and Staged designs, the “Legacy Second Order NDLM” dose response model and the “Legacy Adaptation” allocation option have been removed. Older designs making use of these features will be migrated over to the “Second Order NDLM” dose response model and the “Fixed Allocation” allocation option, respectively, when loaded in FACTS 7.1. A warning prompt will appear for such designs.\nUsers can now choose a decision QOI in multiple stopping criteria simultaneously. Previously, after a stopping rule was created based on a certain decision QOI, any additional stopping rule could not use this decision QOI.\nIn Core and Staged Designs, a new class of QOI as a sub-category of Predictive Probabilities was introduced: Conditional Power. Contrary to the existing Bayesian Predictive Probabilities, Conditional Power assumes the observed treatment effect estimates to be the truth and then calculates the probability of being successful either: 1) at a later final analysis, 2) after the currently enrolled subjects are followed up or 3) in a future trial.\nFACTS will now correctly calculate the Bayesian Predictive Probabilities of success at the final analysis when a continuous endpoint and change from baseline as the final endpoint are used.\nIn Core and Staged Time-to-Event designs, minimum information required to trigger an interim will now be available when interims are based on predictor events.\nIn Core and Staged Time-to-Event designs, the complete information columns report at interims with the weeks files will now display the correct information when interims are based on predictor events.\nIn Core and Staged Time-to-Event designs with a Time-to-Event predictor, when interim information events and the event max cap differ (e.g. max cap on final events but interims based on predictor events), simulations will correctly stop at the specified max event cap.\nIn Core and Staged Time-to-Event designs with a Time-to-Event predictor, current trial predictive probability QOIs are now handled correctly when max event caps refer to predictor events.\nIn Staged Time-to-Event designs with a Time-to-Event predictor, current trial predictive probability QOIs are now handled correctly when interim information events and the event max cap differ (e.g. max cap on final events but interims based on predictor events).\nIn Staged Time-to-Event designs, the censoring and event time of subjects in Stage 2 which have not observed their event in Stage 1 will now be handled correctly for both their final and predictor endpoints.\nIn Core and Staged design, (Continuous, Dichotomous, and TTE) when a trial stops for early success or early futility and the option to continue follow-up after that decision is not selected, the final analysis model is no longer run. The data at the final analysis would be identical to the data at the interim analysis that the stopping threshold was hit at, so the model output for the final is now identical to the interim.\nTime Course Hierarchical longitudinal model has improved performance with informative priors on the variance components.\nIn Staged designs, FACTS will now correctly handle the Stage 1, Stage 2 and overall sample size caps (maximum number of subject caps and maximum number of event caps) specified in the Variants tab.\nIn Core and Staged Time-to-Event designs, the Cox Proportional Hazards current trial predictive probability calculation has been corrected.\nIn Core and Staged designs, when using predictive probabilities that predict success at trial maximum, but the trial is stopped at an interim, the predictive probability is now calculated based on the originally specified number of subjects at trial maximum rather that assigning the predictive probability a value of 0 or 1 depending on the p-value of the interim data.\nIn Core and Staged designs, when using predictive probabilities, visit values are now correctly imputed when there is only baseline visit data available.\nIn Core and Staged designs, when using a dichotomous endpoint and predictive probabilities of treatment versus control, FACTS now performs a Farrington and Manning Test when there is a superiority or non-inferiority margin.\nIn Staged design, the arm selection logic has been updated to properly account for arm-dropping in stage 1 when selecting a single arm from each group.\n\n\n\n3 FACTS Enrichment Design Improvements\n\nIn Enrichment Dichotomous designs, users can now perform frequentist calculations using a normal approximation as currently, or using a Fisher exact test. The latter is of particular use when sample sizes are small.\nIn Enrichment Time-to-Event designs, the GUI will now correctly calculate the mean frequentist estimated treatment effect and its standard error.\nWhen running analyses in Enrichment designs, the user can now specify the number of MCMC samples to generate per imputation of missing data modelled by a longitudinal model.\nIn Enrichment Dichotomous designs, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Enrichment designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration, sample size or number of events (in Time-to-Event) has been added.\nIn Enrichment designs, two new summary columns “Ppn Overall Success” and “Ppn Overall Futility” have been added, which simply sum up the existing columns “Ppn Early Success” and “Ppn Late Success”, and “Ppn Early Futility” and “Ppn Late Futility”, respectively. This allows users to quickly view the estimated type I error and power of the design.\nIn Enrichment designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Enrichment designs, the frequentist test reported in the frequentist simulations file will now make it clear whether the performed test is a one-sided or two-sided test. In the associated frequentist summary file, the proportion of frequentist results for each test type that are significant will now correctly take into account the correct alpha level depending on whether the test is one-sided or two-sided.\n\n\n\n4 FACTS Platform Trial Improvements\n\nBREAKING CHANGE: in Platform trial designs, the numerical value representing the outcomes for late futility and early futility have been swapped in FACTS 7.1.0. Any older designs with futility criteria will need to be re-simulated. Decision numeric values in FACTS Platform Trials and FACTS Core now match.\nIn Platform Trial designs, users can now specify whether Posterior Probability QOIs, Predictive Probability QOIs or p-value QOIs should be calculated based on the entire control population in the trial (as previously) or based on the given treatment’s concurrent control population. These concurrent control QOIs can be used as treatment stopping criteria like any other QOI.\nA time window allowing a treatment’s concurrent controls to additionally include control patients a certain number of weeks from the treatment entering the trial can also be specified.\nPr(Max) Target Dose QOIs can now be calculated based on all arms in the trial (as previously), only active arms or only randomizing arms.\nIn Platform Trial Dichotomous designs, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Platform Trial designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration or sample size has been added.\nIn Platform trial designs, users can now specify different variants of the design by modifying both the maximum number of participants per treatment as well as the maximum number of concurrent treatments. These variants will display as separate scenarios on the Simulations tab.\nIn Platform trial designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Platform trial designs, the “Per Sim: Arm and Participant Arrival” graph will now correctly display the accrual period for the control arm to end when the last treatment’s accrual period ends.\nUsers can now choose a decision QOI in multiple stopping criteria simultaneously. Previously, after a stopping rule was created based on a certain decision QOI, any additional stopping rule could not use this decision QOI.\nFACTS will now correctly calculate the Bayesian Predictive Probabilities of success at the final analysis when a continuous endpoint and change from baseline as the final endpoint are used.\n\n\n\n5 FACTS Dose Escalation Improvements\n\nThe Dose Escalation design types CRM(Toxicity), CRM(Ordinal), bCRM and CRM(Efficacy) have all been deprecated. Users can still create new designs or open existing designs with these design types. We recommend using the more general and versatile “Continual Reassessment Methods (CRM)” design type (formerly known as “N-CRM”) for any 1D model based CRM designs.\nIn CRM with open enrollment, users can now specify whether they want to allow backfilling to the highest dose (“frontfilling”) and conditions under which to do so. Users can choose whether to count these patients towards the backfill subject cap or regular allocation cap.\nIn CRM with open enrollment, previously two queues (maximum number of subjects on uncleared doses and maximum number of subjects on cleared doses) determined the allocation behavior. In order to give users more flexibility and control, there are now three queue concepts (maximum number of subjects on uncleared doses, maximum number of subjects on cleared doses at MTD and maximum number of subjects below MTD). These queues are now used in the same way in the MTD and MED phase of the trial. The concept of max cleared dose and the new queues are now harmonized.\nIn CRM, the stopping rule checker design was updated. In case of regular dosing, a concept of near doses like that of fine spaced dosing was introduced and both concepts aligned for both stopping in the MTD and MED phase. The stopping rule checker now also correctly checks the hierarchies and join conditions.\nIn CRM with open enrollment, backfill will now correctly evaluate all cleared doses for eligibility.\nIn CRM with open enrollment and fine spaced dosing, near doses are now correctly evaluated in regular allocation, backfill and for the purposes of stopping.\nIn CRM designs, FACTS will now correctly apply study size constraints and queue size constraints when the underlying toxicity model considers all doses to be toxic. This also includes preventing an expansion cohort from being allocated in this situation.\nIn Dose Escalation designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration or sample size has been added.\nIn 2D-CRM designs, the dose response model’s eta parameter can now be specified in log-normal space.\nIn 2D-CRM designs with custom run-in, FACTS now proceeds to the chosen escalation scheme more promptly, without first allocating more than one full-size cohort unless they are specified in the run-in.\nThe “Per Sim Allocation History Grouped” plot will now be displayed correctly if, previously, the “Per Sim Allocation History Group 1” plot had been set to space interims equally.\nIn CRM with an initial run-in, escalation will now be performed correctly and maximum cleared dose tracked correctly in all circumstances.\nIn CRM with small-cohort pre-escalation, we now only switch to regular escalation based on observed toxicities, not MTD estimate or overdose control rules.\nIn CRM, introduce the notion of “Selected MTD”, “Selected OSD” and “Selected MED” at the end of trial as a function of the respective model estimates and the max cleared dose.\nIn CRM with two groups that enroll consecutively, there are now several options regarding what dose level to start escalation at in group 2.\nIn CRM with fine grained dosing, the concept of near doses is now correctly applied in the escalation phase, when calculating the max cleared dose and adjusted for the new queue concepts.\nIn CRM, cohort expansion will now enroll the correct number of patients even when accrual is very fast.\nIn CRM, when using a cohort expansion or a two group design, the trial state on which both of these concepts depends is now the final state of the previous trial, not the state of the previous trial when its stopping criteria were met.\nIn CRM, when using two groups and expansion cohorts in both groups, the group 2 expansion cohort now correctly uses its own cap regarding maximum number of subjects. In the same setting, a rare circumstance led to the allocation in the group 1 cohort expansion to continue beyond the max cap – this is now resolved.\nIn CRM, stopping based on the max cap of subjects for escalation now does not lead to an overrun in patients in rare circumstances.\nIn CRM with both an MTD and an MED phase (toxicity and efficacy endpoints), the transition between the phases is now handled correctly.\nIn CRM, improved labels and default values in the GUI, such as improved values for the overdose control and open enrollment queue lengths and clearer labels in the Allocation tab.\nSeveral GUI stability improvements in the CRM engine.\nIn CRM, there are several improvements to the “Per Sim Allocation History”, “Alloc and Tox History”, “Cohort Band Probabilities” and “Cohort Response” graphs, including showing the cohort expansions subjects separately, showing the max cleared dose at any point in time and showing the selected MTD/MED at the end of the trial.\n\n\n\n6 General Improvements\n\nFACTS has now been integrated with AIRSHIP, which allows simulation results to be explored graphically in a much more generic, versatile way. Once simulation have completed, results can be explored with AIRSHIP by clicking on “Explore Results…” &gt; “Compare Scenarios in AIRSHIP”. Note that use of AIRSHIP requires at least two scenarios to have been simulated and their results aggregated.\nThe process of making the FACTS inputs more intuitive has started as of FACTS 7.1.0. In FACTS Core Continuous/Dichotomous designs, tooltips will appear against many of the inputs (when hovering over the relevant input) with explanatory text about their use and impact on the design. Tooltips can be disabled by going to Settings &gt; Options &gt; Tooltip Configuration.\nThe data associated with all graphs displayed in FACTS can now be exported into a CSV format. In addition, hovering over these graphs will display the associated data point value as a tooltip.\nWhen simulating multiple scenarios, each simulated scenario can now be simulated with a different random number seed.\nWhen running simulations for a directory of FACTS files in FACTS Command Line or FLFLL, a different base seed can be set for each design within the directory.\nThe order of scenarios to simulate as displayed on the Simulation tab has been set to be consistently displayed in alphabetical order.\nWhen aggregating simulation results for a design using variants, the relevant variant number will now correctly be displayed in the aggregated files.\nSeveral bug fixes and improvements have been made to all design reports.\nThe associated engine executable now provides a new -o output flag argument specifying the location where to output all files generated by the engine [Enterprise licensees only].\n\nPlease contact us regarding any questions.",
    "crumbs": [
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.1.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts610.html",
    "href": "releaseNotes/v6/facts610.html",
    "title": "FACTS 6.1.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.1.0\nBerry Consultants is delighted to announce that FACTS 6.1.0 is ready for release! Building on FACTS 6.0.0, FACTS 6.1.0 adds two new Dose Escalation simulation types: “FACTS 2D-CRM” and “FACTS mTPI”:\n\nFACTS “2D-CRM” is a simulator that runs simulations of dose escalation trials testing combinations of doses from 2 drugs. The implementation follows that of the 2D-CRM prototype that was available earlier this year.\n\n\n\n\n\n\n\nFACTS mTPI is an implementation of Yuan Ji’s “Modified toxicity probability interval method for dose-finding trials”.\n\n\n\n\n\n\nFACTS 6.1.0 also adds a major piece of simulation functionality across (almost) all FACTS engines: ‘Design Variants’, these allow you to have within one “.facts” file, multiple designs with different maximum sample sizes. This makes it much easier to estimate the required sample size for a design. The feature includes the ability to mark specific treatment arms or groups as ‘correct choices’, and FACTS now summarizes not only the proportions of successful and unsuccessful trials, but also proportions of successful trials that also made correct choices.\n\n\n\n\n\nFACTS 6.1.0 is fully backwards compatible with FACTS 6.0.0 and 5 – it can load and run all your FACTS 6.0.0 and FACTS 5 designs – and then use new FACTS 6.1.0 features with those designs. You can have FACTS 6.1.0 and FACTS 6.0.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Dose Escalation:\n\nDesign Variants in N-CRM.\n2D-CRM\nmTPI\n\nFACTS Enrichment Designs:\n\nDesign Variants\nThe ability to extend hierarchical modeling with clustered model.\n\nFACTS Core:\n\nDesign Variants\nBetter control over which frequentist calculations are performed.\nThe ability to use p-value QOIs for early success/futility decision making.\n\nFACTS Staged Design:\n\nDesign Variants\nThere is now an ‘Analysis’ tab in Staged Design.\n\n\n\n\n3 Downloading FACTS 6.1.0\nThe FACTS 6.1.0 release is available for download from the Berry Consultants FACTS 6 ftp repository at http://berry1.berryconsultants.com/facts_6/.\nLogin using:\n\nUsername: facts_6\nPassword: DynIrgyur4\n\nThere are, as usual, 4 files to download: Documents.zip, Examples.zip, Setup.msi, setup.exe. Obfuscated versions of setup.exe and Setup.msi have been included for those of you accessing through firewalls that prevent the download of .exe and .msi files. Having downloaded setup.e_e and Setup.m_i simply rename the file suffix before using.\n\n\n4 Installing FACTS 6.1.0\nAs with previous version of FACTS, FACTS 6.1.0 installs on a PC. Simply place the downloaded files on the PC / laptop it is to be installed on and run the file “setup.exe”. You will need a new FACTS license key for this new version. It will be sent to you separately.\nFor any support questions or issues, please contact us.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.1.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts600.html",
    "href": "releaseNotes/v6/facts600.html",
    "title": "FACTS 6.0.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.0.0\nBerry Consultants is delighted to announce that FACTS 6.0.0 is ready for release!\nBuilding on FACTS 5, FACTS 6.0.0 adds a new simulation type: “FACTS Staged Design”.\n\nFACTS “Staged Design” is a simulator that runs a “FACTS Core” simulation followed by a second “FACTS Core” simulation that can take decisions based on the result of the first simulation and include data from the first simulation. This allows, for example, the simulation of a Phase II trial followed by a Phase III trial, whether as separate trials or as a seamless Phase II/III.\nFACTS Enrichment Designs includes the flexibility over the timing of interims and the ability to set different decision thresholds at different interims.\n\nFACTS 6.0.0 is fully backwards compatible with FACTS 5 – it can load and run all your FACTS 5 designs – and then add new FACTS 6.0.0 features to them. In particular you can load a FACTS Core design into FACTS Staged Design as the starting point for the design of the first stage. You can have FACTS 5 and FACTS 6.0.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Staged Design:\n\nThe simulation of one treatment selection stage followed by another.\nThe stages can be connected on a scale from completely seamless to completely independent.\nFACTS Staged Design can be used to simulate:\n\na Phase II and the consequential Phase III trials, or a seamless Phase II/III trial\na Phase IIA and the consequential Phase IIB trials, or a seamless Phase IIA/B trial\na Phase II trial with a treatment arm selection and expansion stage\n\nThe simulations include:\n\nDifferent options for specifying the interval between the stages\nDifferent options for which data from the first stage can be included in the second stage: all of it, none of it, all the data on the arms retained in the second stage, all the data on the study drug arms in the first stage pooled on the one study drug arm retained in the second stage and just subjects from the first stage who did not complete in that stage.\nRules for selecting which treatment arms are kept in the second stage or are dropped after the first stage, including rules on specific arms (such as “retain the top dose if …”), rules on specific target arms (such as “retain the Minimum Efficacious Dose which has a Hazard Ratio of X or less compared to the Control Arm”) rules across all arms (such as “retain the 2 treatment arms with the highest probability of having a response greater than control, as long as their probability of toxicity is less than …”) and rules applied to groups of treatment arms (such as “retain the two arms that are once a day treatments rather than the two that are twice a day treatments if …”).\nDifferent analysis models, allocation rules, interims and decision criteria for each stage.\n\nThe ability to take decision in Stage 1 based on the predictive probability of the outcome of stage 2.\nThe full simulation output of both stages.\nGraphs of the Stage 1, Stage 2, Dose Selection and Overall results.\n\nFACTS Enrichment Designs:\n\nAs in FACTS Core, the scheduling of interims can now be specified by the number of subjects who have completed or have completed up to a particular visit.\nThe decision criteria thresholds can be specified separately for different interims.\n\nFACTS Core:\n\nThe option to specify a deterministic accrual and/or deterministic allocation sequence, for example allowing custom dose escalation trials with cohort accrual, while allowing the full functionality of the Core engine\n\nFACTS Dose Escalation:\n\nIs unchanged.\n\n\n\n\n3 Downloading FACTS 6.0.0\nThe FACTS 6.0.0 release is available for download from the Berry Consultants FACTS 6 ftp repository at http://berry1.berryconsultants.com/facts_6/.\nLogin using:\n\nUsername: facts_6\nPassword: DynIrgyur4\n\nThere are, as usual, 4 files to download: Documents.zip, Examples.zip, Setup.msi, setup.exe. Obfuscated versions of setup.exe and Setup.msi have been included for those of you accessing through firewalls that prevent the download of .exe and .msi files. Having downloaded setup.e_e and Setup.m_i simply rename the file suffix before using.\n\n\n4 Installing FACTS 6.0.0\nAs with previous version of FACTS, FACTS 6.0.0 installs on a PC. Simply place the downloaded files on the PC / laptop it is to be installed on and run the file “setup.exe”. You will need a new FACTS license key for this new version. It will be sent to you separately.\nFor any support questions or issues, please contact us.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.0.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts641.html",
    "href": "releaseNotes/v6/facts641.html",
    "title": "FACTS 6.4.1 Release Notes",
    "section": "",
    "text": "1 Introduction\nBerry Consultants would like to announce a new maintenance release, FACTS 6.4.1. FACTS 6.4.1 contains the following improvements to the FACTS 6.4.0 version. Please contact us regarding any questions.\n\n\n2 FACTS (Staged) Core Improvements\n\nIn Time-to-Event designs, the sigmoidal, 3-parameter logistic and hierarchical logistic dose response models have been improved to better handle their respective likelihood evaluation. Namely, when the dose response is non-monotone, or the doses are widely separated.\nIn Time-to-Event designs, the prior for the sigmoidal model’s a2 parameter is now properly applied. As a result, estimates for the sigmoidal model’s a1 and a2 parameter have now been corrected.\nIn Time-to-Event designs, the option to model control separately in TTE predictor models is now applied correctly.\nIn Dichotomous designs, selecting the “Log-odds” parametrization of Posterior Probability QOIs will no longer be rejected as invalid if the Delta values for comparison are outside of [-1, 1].\nIn Multiple Endpoint designs with a dichotomous endpoint, Posterior Probability QOIs with the “Log-odds” parametrization will now be computed correctly.\nA very rare bug has been fixed that occurred when an adaptive design was converted back to a fixed design. The simulator would check the now irrelevant details of the interims and crash.\n\n\n\n3 FACTS Dose Escalation Improvements\n\nIn CRM(Efficacy) designs, FACTS files created with FACTS 6.1.0 or older versions will have their “Model control separately” setting correctly migrated over in FACTS 6.4.1 and later versions.\nIn N-CRM designs, the number of beta distribution samples in the specific quantiles prior derivation algorithm has been increased from 1,000 to 10,000.\nIn Dose Escalation designs, Windows and Linux simulation result differences have been resolved.\nIn 2D-CRM dose values of 0 are now allowed with some restrictions:\n\nany combination where the transformed dose strengths of both drugs are very low (or 0) must be excluded from the study and not have any prior toxicities specified as to have occurred on that combination. The model cannot fit toxicity on such combinations.\nif there is a combination where the transformed dose strength of both drugs are 0, the response model must be re-scaled (using the “Asymptotes” option) so the lower bound is not asymptotically 0, but some value slightly above that (such as 0.0001).\n\nIn 2D-CRM the prior graph on the Response Model tab can now show the sampled priors for the individual drugs without the lowest dose being plotted (when a dose 0 or very low dose is included this can compress the plot for all the other doses). The x-axis has also been re-labelled to make it clear the doses are being plotted at the log of their transformed dose values.\nIn N-CRM if using Open Enrolment and Backfill, the “Max Study Allocation for Escalation” was not being respected, this is fixed in FACTS 6.4.1.\n\n\n\n4 FACTS Enrichment Design Improvements\n\nFACTS will no longer error when running multiple scenarios when using external data files.\n\n\n\n5 Framework Improvements\n\nSimulation of FACTS files stored on a shared drive will be handled more robustly in the case of intermittent connectivity to the shared drive.\nRenaming of FACTS analyses on the Analysis tab will now correctly handle the situation when the analysis name has been unchanged.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.4.1 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts630.html",
    "href": "releaseNotes/v6/facts630.html",
    "title": "FACTS 6.3.0 Release Notes",
    "section": "",
    "text": "FACTS 6.3.0 is now available for official release. This version contains significant changes to FACTS N-CRM Open Enrollment to make it more efficient, and adds to FACTS Core and FACTS Staged Designs (Continuous, Dichotomous and Multiple Endpoint) options to model arms that differ in strength along 2 dimensions (for example, but not limited to: dose strength and dosing frequency). Please contact us regarding any questions.\nIn detail the new features in FACTS 6.3.0 are:\n\nIn FACTS Dose Escalation N-CRM the open enrollment option has been improved:\n\nThe limit of the “maximum number of subjects without final results” is now applied per dose not overall. This means that after escalating to a higher dose, allocation is not held up waiting for later subjects on the lower dose to complete. Accrual is faster, fewer subjects are lost. If you are thinking of doing an open enrollment N-CRM design, we strongly recommend you update to FACTS 6.3.0.\nThe user supplies two limits, one used while allocating to an “uncleared” dose, the other used when allocating to a “cleared” dose (and hence allocating to the MTD).\nIf recruiting 2 groups, different maximums can be specified for the second group.\nThere is now an option so that the simulation of Open enrollment only “pauses” when the early stopping criteria are met, allowing enrollment to be re-started if the final follow up data move MTD to a dose where the stopping criteria are not met.\n\nIn FACTS Dose Escalation N-CRM the open enrollment option has a new feature, the option to use “backfill”. Enabling “backfill” allows a subject who would otherwise be lost (because the “maximum number of subjects without final results” is currently met) to be allocated to a lower dose. There are parameters to control the backfill:\n\nseparate trial maximums can be specified for the subjects allocated in escalation or to the MTD, or in backfill.\nlimits on how many subjects can be on a dose for it to be open for backfill.\nlimits on how high the dose must be before it is open for backfill.\nlimits on how close a dose must be to the current dose for it to be open for backfill.\n\nIn FACTS Dose Escalation N-CRM there are now more “run-in” options:\n\nsimple run-in (as in FACTS 6.2.0)\ncustom run-in – where the user precisely specifies the sequence of doses to be tested and the number of subjects to test at each dose.\nsmall cohort pre-escalation – this follows the full escalation rules, including overdose control but with a smaller cohort size – and the same number of cohorts required to clear doses. Like all run-ins, it ends when a toxicity is observed.\n\nIn FACTS Dose Escalation N-CRM the calculation of the likelihood when analyzing an Ordinal Toxicity endpoint has been improved. This means however that a design using Ordinal Toxicity created under FACTS 6.2.0 is likely to behave noticeably differently under FACTS 6.3.0. If the design is well advanced, or in use, you are advised to stay with using FACTS 6.2.0 for that design. If you are just starting out designing an Ordinal Toxicity endpoint N-CRM we recommend upgrading to FACTS 6.3.0.\nFACTS Core and FACTS Staged Designs features a new 2D treatment arm option and associated 2D response models. The 2D options are available for the Continuous, Dichotomous and Multiple Endpoints. The 2D treatment arm option allows:\n\nArms to be defined as a combination of 2 “factors” e.g. dose strength and dosing frequency, or dose strengths of two different agents.\nThe combinations can be analyzed independently, mapped onto a 1D ordering and analyzed with any of the standard 1D dose models, or with one of the three new 2D response models: a 2D NDLM, a 2D continuous factorial model, or a 2D discrete factorial model.\nTarget Quantities of Interest can be defined to be confined to those combinations in a particular row or column (e.g. the calculate the Pr(max) of the once a day doses).\n\nIn FACTS Enrichment Designs the implementation of fitting of the Hierarchical model (options for treatment arms across groups and control arms across group) have been improved. They should converge somewhat faster and at the FACTS default MCMC sample length (2500), will typically be more accurate than before.\n\nThis release addresses some situations in FACTS 6.2.0 that could cause errors. If any of your designs replicate these circumstances, you are recommended to upgrade to FACTS 6.3.0:\n\nIn FACTS 6.2.0 Dose Escalation 3+3, the simulations don’t properly implement the re-escalation rules after de-escalation. This was introduced when we made the significant extensions to N-CRM in FACTS 6.2.0.\nIn FACTS 6.2.0 Dose Escalation N-CRM many “pseudo-patients” parameters are not interpreted correctly.\nIn FACTS 6.2.0 Enrichment Designs with a Continuous endpoint, when using the Linear Regression Longitudinal Model, it fitted incorrectly when informative priors were used.\nIn FACTS 6 Core with a Continuous endpoint and simulating baseline, calculating a p-value QOI, with BOCF for missing data, the BOCF value for missing subjects was being set incorrectly (only a problem if baseline values are very difference from 0).\n\nThe following minor issues in the FACTS GUI were also fixed:\n\nIn FACTS Dose Escalation with N-CRM when specifying an open enrollment design, maximum subjects on MTD for “clearing” a dose and for stopping are meant to be entered in “subjects” but the GUI interpreted the input as “cohorts’ using whatever was the last cohort size in that “.facts” file.\nWhen using the “Ppn Correct Arm” in FACTS Core by marking arms as “should succeed” in the VSR profiles, if variants were not enabled, the variant target QOI arm selection criteria would incorrectly re-set to “Pr(Max)” when re-opening the file.\nWhen using a large external data file, running simulations with lots of packets could cause “out-of-memory” issues. Finally, some enhancements and fixes in the Design Report in FACTS Core have been implemented.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.3.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts625.html",
    "href": "releaseNotes/v6/facts625.html",
    "title": "FACTS 6.2.5 Release Notes",
    "section": "",
    "text": "Berry Consultants would like to announce a new maintenance release, FACTS 6.2.5. FACTS 6.2.5 contains the following improvements to the FACTS 6.2.4 version:\nThis release addresses three rare situations in FACTS 6.2.4. If any of your designs replicate these exact circumstances you are recommended to upgrade to FACTS 6.2.5:\n\nIn FACTS Staged Design with a Time-to-Event end point and a predictor, if using, in stage 1, a predictive probability of success in stage 2, the imputation from the predictor was not being performed correctly.\nIn FACTS Staged Design, if all recruitment is completed in the first stage, so that only follow up remains in the second stage, if the second stage contains interims by time, these interims were not simulated.\nIn FACTS Core, if a Dunnett’s adjusted p-value QOI was defined and there was an additional p-value QOI defined after it, the results reported for the Dunnett’s adjusted QOI were corrupted.\n\nThe remaining, minor enhancement is in the FACTS 6.2.5 GUI:\n\nIn FACTS Core TTE, if QOIs using a Predictor endpoint were defined over and above the default ones, the GUI could delete these on re-opening the “.facts” file. Should this have happened to you, you would have seen FACTS display a warning message that it was deleting these QOIs. The GUI has been fixed so that this deletion no longer occurs. There have been no changes to Dose Escalation or Enrichment Designs. There have been no updates to the documentation or examples.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.2.5 Release Notes"
    ]
  },
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html",
    "href": "concepts/facts/DropoutsDeepdive.html",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "For the continuous and dichotomous engines, and the multiple endpoint engine, the default dropout scenario is that no subjects drop out of the study before observing their final endpoint data. If dropouts are expected, the user can specify either the “Dropouts per Dose,” or “Dropouts per Dose per Visit.”\n\n\nIf “Dropouts per Dose” is selected, then each subject has a probability of not having an observable final endpoint value equal to the dropout rate of the dose that subject is randomized to. If each subject has multiple visits and “Dropouts per Dose” is selected, then the conditional probability of dropping out before each visit given that the subject had not dropped out up to the visit before rates are all equal. In other words, if the total dropout rate is π_D, the probability of dropping out between visits i and i+1 given that the subject had not dropped out at visit \\(i\\) is \\[\n1-\\left(1-\\pi_D\\right)^{(\\frac{1}{V})}\\text{ where } V \\text{ is the total number of visits.}\n\\] The following interactive widget takes in the desired dropout rate for a single dose and the number of visits a subject will have during their follow-up, and returns information about the dropout rate that FACTS will simulate by visit for that dose.\nThe columns in the dynamic output table are:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"Dropout Per Dose\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"dropoutRate\", \"Desired Dropout Rate:\", value = 0.1, min = 0.0, max = 1.0, step = 0.05),\n      sliderInput(\"numVisits\", \"Number of visits:\", \n                  min = 1, max = 20, value = 5)\n    ),\n    mainPanel(\n      tableOutput(\"visitsGuide\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  output$visitsGuide &lt;- renderTable({\n    \n    if(is.na(input$dropoutRate)) {stop(safeError(\"Please enter a dropout rate.\"))}\n    else if(input$dropoutRate &lt; 0 | input$dropoutRate &gt; 1) {stop(safeError(\"Dropout rate must be between 0 and 1.\"))}\n    else {\n      df = data.frame(Visits = 1:input$numVisits,\n                                                     ConditionalDropoutRate = 1-(1-input$dropoutRate)^(1/input$numVisits))\n      df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n      df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  \n      df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n      colnames(df) = c(\"Visit number\",\n                \"Conditional Visit Dropout Rate\",\n                \"Marginal Visit Dropout Rate\",\n                \"Cumulative Dropout Prob.\")\n    df}\n  }, digits = 4)\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\nIf “Dropouts per Dose per Visit” is selected, then each subject has a user specified probability of dropping out before a visit v that is specified as the conditional probability of dropping out before visit v given that that they had not dropped out by visit v-1. This leads to a total dropout rate π_D for a participant that is equal to:\n\\[\n\\pi_D = 1-\\Pi_{v=0}^V(1-\\pi_v)\n\\] When specifying Dropouts Per Dose Per Visit in FACTS, the user inputs the conditional visit dropout rate for each visit. This can make it not completely intuitive to get the exact desired cumulative dropout rate. The below application allows the user to enter either the conditional visit dropout rates or the marginal visit dropout rates for a single dose, and provides the implied dropout profile from those inputs. Again, the columns of the table represent:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(htmltools)\n\nalignCenter &lt;- function(el) {\n  htmltools::tagAppendAttributes(el,\n                                 style=\"margin-left:auto;margin-right:auto;\"\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(\n    # Note the wrapping of the string in HTML()\n    tags$style(HTML(\"\n      .my_col_class {\n         align-content: center;\n      }\")\n    )\n  ),\n  \n  fluidRow(\n    titlePanel(h1(\"Dropout Per Dose Per Visit\", align = \"center\")),\n    alignCenter(sliderInput(\"numVisits\", \"Number of visits:\", \n            min = 1, max = 20, value = 5)),\n    DTOutput(\"dataInputTable\")\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  df = data.frame(Visits = 1:5,\n                       ConditionalDropoutRate = 0.05)\n  df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n  df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n  colnames(df) = c(\"Visit number\",\n                   \"Conditional Visit Dropout Rate\",\n                   \"Marginal Visit Dropout Rate\",\n                   \"Cumulative Dropout Prob.\")\n  \n  ## Render DF to actually change\n  output$dataInputTable = renderDT(datatable(df, options = list(\n    pageLength = 20,\n    dom = \"t\",\n    autoWidth = TRUE,\n    columnDefs = list(list(width = '150px', targets = 1:4, orderable = FALSE))), selection = 'none', editable = \"cell\") |&gt; formatRound(2:4, digits = 4))\n  \n  ## Update from Conditional\n  \n  proxy = dataTableProxy('dataInputTable')\n  \n  observeEvent(input$dataInputTable_cell_edit, {\n    info = input$dataInputTable_cell_edit\n    str(info)\n    i = info$row\n    j = info$col\n    v = info$value\n    df &lt;&lt;- editData(df, info)\n    if(j == 2) {\n      ## Update other columns from Conditional\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    } else if(j == 3) {\n      ## Update other columns from Marginal\n      df[,4] &lt;&lt;- cumsum(df[,3])\n      df[,2] &lt;&lt;- c(df[1,3],\n                   df[-1,3]/(1-df[-nrow(df),4]))\n    } else if(j == 4) {\n      ## Update other columns from Cumulative\n      \n      df[info$row, 3] &lt;&lt;- df[info$row, 4] - ifelse(info$row == 1, 0, df[info$row-1, 4])\n      df[info$row, 2] &lt;&lt;- ifelse(info$row == 1, df[info$row,3], df[info$row,3]/(1-df[info$row-1,4]))\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    }\n    replaceData(proxy, df) \n  })\n  \n  observe({\n    input$numVisits\n    nv = input$numVisits\n    if(nv &gt; nrow(df)) {\n      if(nrow(df) == 0) {\n        df &lt;&lt;- data.frame(Visits = 1,\n                   ConditionalDropoutRate = 1-(1-.1)^(1))\n        df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n        df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n        df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n        colnames(df) = c(\"Visit number\",\n                         \"Conditional Visit Dropout Rate\",\n                         \"Marginal Visit Dropout Rate\",\n                         \"Cumulative Dropout Prob.\")\n      } else {\n        for(i in 1:(nv-nrow(df))) {\n          df &lt;&lt;- rbind(df, setNames(data.frame(matrix(c(1+nrow(df), 0.05, 0, 0), nrow = 1)), names(df)))\n        }\n        df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n        df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n      }\n    } else if(nv &lt; nrow(df)) {\n      df &lt;&lt;- df[1:nv,]\n    }\n    replaceData(proxy, df) \n  })\n}\n\nshinyApp(ui = ui, server = server)\nIn order to simulate dropouts from the below profile in FACTS, enter the Conditional Visit Dropout Rate into the FACTS Dropout Rate tab.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose",
    "href": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "If “Dropouts per Dose” is selected, then each subject has a probability of not having an observable final endpoint value equal to the dropout rate of the dose that subject is randomized to. If each subject has multiple visits and “Dropouts per Dose” is selected, then the conditional probability of dropping out before each visit given that the subject had not dropped out up to the visit before rates are all equal. In other words, if the total dropout rate is π_D, the probability of dropping out between visits i and i+1 given that the subject had not dropped out at visit \\(i\\) is \\[\n1-\\left(1-\\pi_D\\right)^{(\\frac{1}{V})}\\text{ where } V \\text{ is the total number of visits.}\n\\] The following interactive widget takes in the desired dropout rate for a single dose and the number of visits a subject will have during their follow-up, and returns information about the dropout rate that FACTS will simulate by visit for that dose.\nThe columns in the dynamic output table are:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"Dropout Per Dose\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"dropoutRate\", \"Desired Dropout Rate:\", value = 0.1, min = 0.0, max = 1.0, step = 0.05),\n      sliderInput(\"numVisits\", \"Number of visits:\", \n                  min = 1, max = 20, value = 5)\n    ),\n    mainPanel(\n      tableOutput(\"visitsGuide\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  output$visitsGuide &lt;- renderTable({\n    \n    if(is.na(input$dropoutRate)) {stop(safeError(\"Please enter a dropout rate.\"))}\n    else if(input$dropoutRate &lt; 0 | input$dropoutRate &gt; 1) {stop(safeError(\"Dropout rate must be between 0 and 1.\"))}\n    else {\n      df = data.frame(Visits = 1:input$numVisits,\n                                                     ConditionalDropoutRate = 1-(1-input$dropoutRate)^(1/input$numVisits))\n      df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n      df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  \n      df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n      colnames(df) = c(\"Visit number\",\n                \"Conditional Visit Dropout Rate\",\n                \"Marginal Visit Dropout Rate\",\n                \"Cumulative Dropout Prob.\")\n    df}\n  }, digits = 4)\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose-per-visit",
    "href": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose-per-visit",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "If “Dropouts per Dose per Visit” is selected, then each subject has a user specified probability of dropping out before a visit v that is specified as the conditional probability of dropping out before visit v given that that they had not dropped out by visit v-1. This leads to a total dropout rate π_D for a participant that is equal to:\n\\[\n\\pi_D = 1-\\Pi_{v=0}^V(1-\\pi_v)\n\\] When specifying Dropouts Per Dose Per Visit in FACTS, the user inputs the conditional visit dropout rate for each visit. This can make it not completely intuitive to get the exact desired cumulative dropout rate. The below application allows the user to enter either the conditional visit dropout rates or the marginal visit dropout rates for a single dose, and provides the implied dropout profile from those inputs. Again, the columns of the table represent:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(htmltools)\n\nalignCenter &lt;- function(el) {\n  htmltools::tagAppendAttributes(el,\n                                 style=\"margin-left:auto;margin-right:auto;\"\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(\n    # Note the wrapping of the string in HTML()\n    tags$style(HTML(\"\n      .my_col_class {\n         align-content: center;\n      }\")\n    )\n  ),\n  \n  fluidRow(\n    titlePanel(h1(\"Dropout Per Dose Per Visit\", align = \"center\")),\n    alignCenter(sliderInput(\"numVisits\", \"Number of visits:\", \n            min = 1, max = 20, value = 5)),\n    DTOutput(\"dataInputTable\")\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  df = data.frame(Visits = 1:5,\n                       ConditionalDropoutRate = 0.05)\n  df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n  df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n  colnames(df) = c(\"Visit number\",\n                   \"Conditional Visit Dropout Rate\",\n                   \"Marginal Visit Dropout Rate\",\n                   \"Cumulative Dropout Prob.\")\n  \n  ## Render DF to actually change\n  output$dataInputTable = renderDT(datatable(df, options = list(\n    pageLength = 20,\n    dom = \"t\",\n    autoWidth = TRUE,\n    columnDefs = list(list(width = '150px', targets = 1:4, orderable = FALSE))), selection = 'none', editable = \"cell\") |&gt; formatRound(2:4, digits = 4))\n  \n  ## Update from Conditional\n  \n  proxy = dataTableProxy('dataInputTable')\n  \n  observeEvent(input$dataInputTable_cell_edit, {\n    info = input$dataInputTable_cell_edit\n    str(info)\n    i = info$row\n    j = info$col\n    v = info$value\n    df &lt;&lt;- editData(df, info)\n    if(j == 2) {\n      ## Update other columns from Conditional\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    } else if(j == 3) {\n      ## Update other columns from Marginal\n      df[,4] &lt;&lt;- cumsum(df[,3])\n      df[,2] &lt;&lt;- c(df[1,3],\n                   df[-1,3]/(1-df[-nrow(df),4]))\n    } else if(j == 4) {\n      ## Update other columns from Cumulative\n      \n      df[info$row, 3] &lt;&lt;- df[info$row, 4] - ifelse(info$row == 1, 0, df[info$row-1, 4])\n      df[info$row, 2] &lt;&lt;- ifelse(info$row == 1, df[info$row,3], df[info$row,3]/(1-df[info$row-1,4]))\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    }\n    replaceData(proxy, df) \n  })\n  \n  observe({\n    input$numVisits\n    nv = input$numVisits\n    if(nv &gt; nrow(df)) {\n      if(nrow(df) == 0) {\n        df &lt;&lt;- data.frame(Visits = 1,\n                   ConditionalDropoutRate = 1-(1-.1)^(1))\n        df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n        df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n        df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n        colnames(df) = c(\"Visit number\",\n                         \"Conditional Visit Dropout Rate\",\n                         \"Marginal Visit Dropout Rate\",\n                         \"Cumulative Dropout Prob.\")\n      } else {\n        for(i in 1:(nv-nrow(df))) {\n          df &lt;&lt;- rbind(df, setNames(data.frame(matrix(c(1+nrow(df), 0.05, 0, 0), nrow = 1)), names(df)))\n        }\n        df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n        df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n      }\n    } else if(nv &lt; nrow(df)) {\n      df &lt;&lt;- df[1:nv,]\n    }\n    replaceData(proxy, df) \n  })\n}\n\nshinyApp(ui = ui, server = server)\nIn order to simulate dropouts from the below profile in FACTS, enter the Conditional Visit Dropout Rate into the FACTS Dropout Rate tab.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/InverseGammaDistribution.html",
    "href": "concepts/facts/InverseGammaDistribution.html",
    "title": "Inverse Gamma Distribution in FACTS",
    "section": "",
    "text": "The Inverse Gamma distribution is used as a prior for most variances in FACTS. The standard parameterization of the Inverse Gamma distribution using \\(\\alpha\\) and \\(\\beta\\) as the shape and scale parameter is not always intuitive for specifying a prior. In order to assist with prior specification, FACTS reparameterizes the Inverse Gamma distribution to be a function of the expected center of the standard deviation and a prior weight.\nThe application below is intended to help users of FACTS understand what the distribution they are specifying for the prior of a variance actually looks like.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\n\nqinvgamma = function (p, shape, rate = 1, scale = 1/rate, lower.tail = TRUE, \n                      log.p = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  qgamma(1 - p, shape, rate, lower.tail = lower.tail, log.p = log.p)^(-1)\n}\npinvgamma = function (q, shape, rate = 1, scale = 1/rate, lower.tail = TRUE, \n                      log.p = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  pgamma(1/q, shape, rate, lower.tail = !lower.tail, log.p = log.p)\n}\ndinvgamma = function (x, shape, rate = 1, scale = 1/rate, log = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  log_f &lt;- dgamma(1/x, shape, rate, log = TRUE) - 2 * log(x)\n  if (log) \n    return(log_f)\n  exp(log_f)\n}\nrinvgamma = function (n, shape, rate = 1, scale = 1/rate) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  1/rgamma(n, shape, rate)\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  tags$head(\n    tags$style(HTML(\"\n      #radioButtonDiv {\n      display: flex;\n      justify-content: center;\n      }\"\n    ))\n  ),\n  withMathJax(),\n  titlePanel(h1(\"Inverse Gamma Distribution in FACTS\", align = \"center\")),\n  h5('$$\\\\sigma^2 \\\\sim \\\\text{IG}\\\\left(\\\\alpha=\\\\frac{\\\\text{weight}}{2}, \\\\beta=\\\\frac{\\\\text{center}^2\\\\;*\\\\;\\\\text{weight}}{2}\\\\right)$$'),\n  sidebarLayout(\n    sidebarPanel(width = 3,\n                 style = \"border: 1px solid #000000\",\n                 titlePanel(h4(\"Center/Weight Parameterization\")),\n                 fluidRow(\n                   column(width = 10, offset = 1, numericInput(inputId = \"center\", label = \"Center of SD:\", value = 5, min = 0, max = Inf, step = \"any\")),\n                   column(width = 10, offset = 1, numericInput(inputId = \"weight\", label = \"Weight:\", value = 2, min = 0.001, max = Inf, step = \"any\")),\n                 ),\n                 titlePanel(h4(\"Alpha/Beta Parameterization\")),\n                 fluidRow(\n                   column(width = 10, offset = 1, numericInput(inputId = \"alpha\", label = \"Alpha:\", value = 1, min = 0.0005, max = Inf, step = \"any\")),\n                   column(width = 10, offset = 1, numericInput(inputId = \"beta\", label = \"Beta:\", value = 25, min = 0, max = Inf, step = \"any\"))\n                 )\n    ),\n    mainPanel(width = 9,\n              wellPanel(style = \"background: white; border: 1px solid #000000\",\n                        fluidRow(\n                          column(12,\n                                 div(\n                                   radioButtons(\"whichParam\", \n                                                \"Which parameter should be summarized?\", \n                                                choiceNames = c(\"Variance \\\\((\\\\sigma^2)\\\\)\", \"Std. Dev. \\\\((\\\\sigma)\\\\)\"), \n                                                choiceValues = c(\"sigma2\", \"sigma\"), \n                                                selected = \"sigma2\", \n                                                inline = TRUE),\n                                   id = \"radioButtonDiv\")\n                          )\n                        ),\n                        uiOutput(\"sectionTitle\"),\n                        fluidRow(\n                          column(width = 4, value_box(\"Mode\", value= uiOutput(\"mode\"), theme = value_box_theme(bg = \"#0b2545\"))),\n                          column(width = 4, value_box(\"Median\", value= uiOutput(\"median\"), theme = value_box_theme(bg = \"#ba5a31\"))),\n                          column(width = 4, value_box(\"Mean\", value= uiOutput(\"mean\"), theme = value_box_theme(bg = \"#06402b\")))\n                        ),\n                        br(),\n                        plotOutput(\"igDistributionPlot\")\n              )\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output, session) {\n  \n  update &lt;- reactiveVal(TRUE)\n  \n  observeEvent(input$center | input$weight, {\n    cat(\"CenterWeightChanged\\n\")\n    ctr = input$center\n    wgt = input$weight\n    \n    if(update() & !is.null(ctr) & !is.null(wgt) & !is.na(ctr) & !is.na(wgt) & ctr &gt; 0 & wgt &gt; 0) {\n      a = wgt/2\n      b = ctr^2*wgt/2\n      \n      updateNumericInput(session, \"alpha\", value = a)\n      updateNumericInput(session, \"beta\", value = b)\n      \n      update(FALSE)\n    } else {\n      update(TRUE)\n    }\n  })\n  observeEvent(input$alpha | input$beta, {\n    cat(\"AlphaBetaChanged\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(update() & !is.null(a) & !is.null(b) & !is.na(a) & !is.na(b) & a &gt; 0 & b &gt; 0) {\n      wgt = 2*a\n      ctr = sqrt(b/a)\n      \n      updateNumericInput(session, \"center\", value = ctr)\n      updateNumericInput(session, \"weight\", value = wgt)\n      \n      update(FALSE)\n    } else {\n      update(TRUE)\n    }\n  })\n  \n  meanHolder = reactiveVal(NA)\n  \n  output$mean = renderText({\n    cat(\"CalcMean\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(a &gt; 1) {\n      if(input$whichParam == \"sigma2\") {\n        meanHolder(b/(a-1))\n        return(round(b/(a-1), 2))\n      } else {\n        tmp = mean(sqrt(rinvgamma(10000, a, b)))\n        meanHolder(tmp)\n        return(round(tmp, 2))\n      }\n    } else {\n      meanHolder(NA)\n      return(\"-\")\n    }\n  })\n  output$median = renderText({\n    cat(\"CalcMedian\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(input$whichParam == \"sigma2\") {\n      return(round(qinvgamma(0.5, shape = a, rate = b), 2))\n    } else {\n      return(round(sqrt(qinvgamma(0.5, shape = a, rate = b)),2))\n    }\n  })\n  \n  output$mode = renderText({\n    cat(\"CalcMode\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(input$whichParam == \"sigma2\") {\n      return(round(b/(a+1),2))\n    } else {\n      lmode = 0\n      hmode = max(b/(a+1), 2)\n      smode = seq(lmode, hmode, length.out = 100001)\n      dmode = dinvgamma(smode, a, b)*(1/(2*sqrt(smode)))\n      calcMode = sqrt(smode[which.max(dmode)])\n      \n      return(round(calcMode, 2))\n    }\n  })\n  \n  output$sectionTitle = renderUI({\n    cat(\"ChangeHeader\\n\")\n    if(input$whichParam == \"sigma2\") {\n      return(h4(\"Characteristics of the Variance\"))\n    } else {\n      return(h4(\"Characteristics of the Standard Deviation\"))\n    }\n  })\n  \n  output$igDistributionPlot = renderPlot({\n    cat(\"MakePlot\\n\")\n    a = input$alpha\n    b = input$beta\n    wchParam = input$whichParam\n    isolate({\n      if(!is.null(a) & !is.null(b) & !is.null(input$center) & !is.null(input$weight) &\n         a &gt; 0 & b &gt; 0 & input$center &gt; 0 & input$weight &gt; 0) {\n        if(wchParam == \"sigma2\") {\n          lowerbound = 0\n          upperbound = qinvgamma(.995, a, b)\n          upperbound = min(1e15, upperbound)\n          \n          sq0to1 = seq(1e-10, 1-(1e-10), length.out = 1000)\n          sq = qinvgamma(sq0to1, a, b)\n          #  sq = seq(1e-10,upperbound, length.out = 1001)\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b))\n          \n          modeDensity = dinvgamma(b/(a+1), a, b)\n          \n          if(df$density[sum(is.finite(df$density))] &gt; modeDensity*.01) {\n            maxPlot = sq[length(sq)]\n          } else {\n            maxPlot = sq[min(which(df$density &lt; modeDensity*.01 & sq &gt; sq[which.max(df$density)]))]\n          }\n          maxPlot = max(maxPlot, 2*(b/(a+1)))\n          \n          sq0to1 = c(10^seq(-17, -2, length.out = 51), seq(.0101, pinvgamma(maxPlot*1.1, a, b), length.out = 1001))\n          sq = qinvgamma(sq0to1, a, b)\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b))\n          \n          p1 = ggplot(data = df) + geom_line(aes(x = sq, y = density)) + geom_area(aes(x = sq, y = density), fill = \"gray80\") +\n            theme_bw() +\n            xlab(\"Variance\") + ylab(\"Density\") +\n            scale_y_continuous(expand = c(0, 0)) +\n            ggtitle(\"Probability Distribution of the Variance\") +\n            coord_cartesian(xlim = c(0, maxPlot), ylim = c(0, modeDensity*1.3)) +\n            theme(text = element_text(size = 18))\n          \n          if(a &gt; 1) {\n            meantmp = meanHolder()\n            p1 = p1 + geom_vline(xintercept = meantmp, color = \"#06402b\") + annotate(geom=\"label\", color = \"white\", fill = \"#06402b\", x = meantmp, y = modeDensity*1.15, label = \"mean\", angle=90, size = 4.5)\n          }\n          p1 = p1 + geom_vline(xintercept = b/(a+1), color = \"#0b2545\") + annotate(geom=\"label\", color = \"white\", fill = \"#0b2545\", x = b/(a+1), y = modeDensity*1.15, label = \"mode\", angle=90, size = 4.5)\n          \n          if(is.finite(qinvgamma(.5, a, b))) {\n            p1 = p1 + geom_vline(xintercept = qinvgamma(.5, a, b), color = \"#ba5a31\") + annotate(geom=\"label\", color = \"white\", fill = \"#ba5a31\", x = qinvgamma(.5, a, b), y = modeDensity*1.15, label = \"median\", angle=90, size = 4.5)\n          }\n          \n          p1 = p1 + geom_vline(xintercept = b/a) + annotate(geom=\"label\", x = b/a, y = modeDensity*1.15, label = \"center\", angle=90, size = 4.5)\n          \n          p1\n        }\n        else {\n          #df = data.frame(sq = sq,\n          #density = dinvgamma(sq, a, b)*(1/(2*sqrt(sq))))\n          \n          lowerbound = 0\n          upperbound = qinvgamma(.995, a, b)\n          upperbound = min(1e15, upperbound)\n          \n          sq0to1 = seq(1e-10, 1-(1e-10), length.out = 1000)\n          sq = qinvgamma(sq0to1, a, b)\n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b)*(2*sqrt(sq)))#(1/(2*sqrt(sq))))\n          \n          lmode = 0\n          hmode = max(b/(a), 2)\n          smode = seq(lmode, hmode, length.out = 1001)\n          dmode = dinvgamma(smode, a, b)*(2*sqrt(smode))#(1/(2*sqrt(smode)))\n          wchMax = which.max(dmode)\n          if(wchMax &gt; 1) {\n            smode2 = seq(smode[wchMax-1], smode[wchMax + 1], length.out = 1001)\n            dmode2 = dinvgamma(smode2, a, b)*(2*sqrt(smode2))#(1/(2*sqrt(smode2)))\n            wchMax = which.max(dmode2)\n          } else {\n            smode2 = seq(0, smode[2], length.out = 1000)\n            dmode2 = dinvgamma(smode2, a, b)*(2*sqrt(smode2))#(1/(2*sqrt(smode2)))\n            wchMax = which.max(dmode2)\n          }\n          calcMode = sqrt(smode2[wchMax])\n          \n          modeDensity = dinvgamma(calcMode^2, a, b)*(2*calcMode)#(1/(2*calcMode))\n          \n          if(df$density[sum(is.finite(df$density))] &gt; modeDensity*.01) {\n            maxPlot = sq[length(sq)]\n          } else {\n            maxPlot = sq[min(which(df$density &lt; modeDensity*.01 & sq &gt; sq[which.max(df$density)]))]\n          }\n          maxPlot = max(maxPlot, 2*smode2[wchMax])\n          \n          sq = (seq(1e-10,sqrt(maxPlot*1.1), length.out = 1001))^2\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b)*(2*sqrt(sq)))#(1/(2*sqrt(sq))))\n          \n          p1 = ggplot(data = df) + geom_line(aes(x = sqrt(sq), y = density)) + geom_area(aes(x = sqrt(sq), y = density), fill = \"gray80\") +\n            theme_bw() +\n            xlab(\"Standard Deviation\") + ylab(\"Density\") +\n            scale_y_continuous(expand = c(0, 0)) +\n            ggtitle(\"Probability Distribution of the Standard Deviation\") +\n            coord_cartesian(xlim = c(0, sqrt(maxPlot)), ylim = c(0, modeDensity*1.3)) +\n            theme(text = element_text(size = 18))\n          \n          if(a &gt; 1) {\n            meantmp = meanHolder()\n            p1 = p1 + geom_vline(xintercept = meantmp, color = \"#06402b\") + annotate(geom=\"label\", color = \"white\", fill = \"#06402b\", x = meantmp, y = modeDensity*1.15, label = \"mean\", angle=90, size = 4.5)\n          }\n          \n          p1 = p1 + geom_vline(xintercept = calcMode, color = \"#0b2545\") + annotate(geom=\"label\", color = \"white\", fill = \"#0b2545\", x = calcMode, y = modeDensity*1.15, label = \"mode\", angle=90, size = 4.5)\n          if(is.finite(sqrt(qinvgamma(.5, a, b)))) {\n            p1 = p1 + geom_vline(xintercept = sqrt(qinvgamma(.5, a, b)), color = \"#ba5a31\") + annotate(geom=\"label\", color = \"white\", fill = \"#ba5a31\", x = sqrt(qinvgamma(.5, a, b)), y = modeDensity*1.15, label = \"median\", angle=90, size = 4.5)\n          }\n          p1 = p1 + geom_vline(xintercept = sqrt(b/a)) + annotate(geom=\"label\", x = sqrt(b/a), y = modeDensity*1.15, label = \"center\", angle=90, size = 4.5)\n          \n          p1\n        }\n      }\n    })\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Inverse Gamma Distribution in FACTS"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/adaptiveDesigns3.html",
    "href": "concepts/adaptiveDesigns/adaptiveDesigns3.html",
    "title": "Adaptive Designs 3",
    "section": "",
    "text": "Third Article on Adaptive Designs",
    "crumbs": [
      "Concepts",
      "Adaptive Designs",
      "Adaptive Designs 3"
    ]
  },
  {
    "objectID": "concepts/index.html",
    "href": "concepts/index.html",
    "title": "Concepts",
    "section": "",
    "text": "Welcome to the Concepts section — your resource for building a strong intellectual foundation on clinical biostatistics and clinical trial simulation. Here, we break down the core principles you’ll need to understand clinical trials and Bayesian statistics and provide pro tips to get the most out of FACTS.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#what-youll-find-here",
    "href": "concepts/index.html#what-youll-find-here",
    "title": "Concepts",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nBasics of Clinical Trials: Learn the essentials, terminology, and best practices that define the clinical trial landscape. From study design and data collection to regulatory considerations, we’ll walk you through the elements that matter most.\nStatistical & Mathematical Fundamentals: Uncover the foundational concepts in statistics and mathematics that empower you to make data-driven decisions. Understand key analyses, Bayesian approaches, and appreciate why certain metrics and models are essential to reliable results.\nTips & Tricks for Using FACTS: Gain insider know-how that helps you work smarter and faster.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#how-to-make-the-most-of-this-section",
    "href": "concepts/index.html#how-to-make-the-most-of-this-section",
    "title": "Concepts",
    "section": "How to Make the Most of This Section",
    "text": "How to Make the Most of This Section\nUse the Concepts section as a springboard to build your expertise, whether you’re new to clinical trials or looking to refine your analytical skills. By solidifying your grasp of fundamentals and strengthening your analytical toolkit, you’ll be better equipped to understand results and ask the right questions. Check back often as we continue expanding these resources to support your growth.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/bayes/bayes1.html",
    "href": "concepts/bayes/bayes1.html",
    "title": "Bayes 1",
    "section": "",
    "text": "First Article on Bayesian Concepts",
    "crumbs": [
      "Concepts",
      "Bayesian Statistics",
      "Bayes 1"
    ]
  },
  {
    "objectID": "notes/posts/2024-10-13.html",
    "href": "notes/posts/2024-10-13.html",
    "title": "Post2",
    "section": "",
    "text": "TEXT TEXT\nTEXT"
  },
  {
    "objectID": "notes/posts/2024-10-14.html",
    "href": "notes/posts/2024-10-14.html",
    "title": "Post1",
    "section": "",
    "text": "TEXT TEXT\nTEXT"
  },
  {
    "objectID": "get.html",
    "href": "get.html",
    "title": "Get FACTS",
    "section": "",
    "text": "FACTS is a powerful software tool that is trusted by half of the top 20 pharma companies to help them design efficient and successful clinical trials. We are proud that also numerous academic, government and regulatory institutions trust FACTS.\n\n\nIndustry\nWe offer a 3-months free FACTS Evaluation License to showcase the power and features of our FACTS simulation tool. Please contact us to get a free demo, or learn more about this offer and our regular licenses.\n\n\nAcademia / Charities / Regulatory Bodies / Government\nTo academic and other non-profit research institutions and regulatory bodies, we will generally offer a free FACTS license under certain conditions. Please contact us to see if your organization qualifies."
  },
  {
    "objectID": "introduction/tutorials/tutorial2.html",
    "href": "introduction/tutorials/tutorial2.html",
    "title": "Tutorial 2",
    "section": "",
    "text": "Second tutorial",
    "crumbs": [
      "Introduction",
      "Tutorials",
      "Tutorial 2"
    ]
  },
  {
    "objectID": "introduction/index.html",
    "href": "introduction/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the Introduction section — your starting point for learning how you can use FACTS to set yourself up for success. Here, you’ll find everything you need to understand the fundamentals of our software, from step-by-step tutorials and guided walkthroughs to links to relevant webinars and other community resources.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#what-youll-find-here",
    "href": "introduction/index.html#what-youll-find-here",
    "title": "Introduction",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nGetting Started: Begin with the fundamentals. Understand the initial setup steps, basic navigation, and how to configure FACTS to meet your needs.\nWebinars: Explore in-depth demonstrations led by our experts. Gain clarity on complex features, see best practices in action, and deepen your FACTS knowledge with real-time guidance.\nTutorials: Learn how key functionalities work and discover creative ways to integrate them into your workflows. See how other teams and users are leveraging FACTS to achieve their goals.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#how-to-use-this-section",
    "href": "introduction/index.html#how-to-use-this-section",
    "title": "Introduction",
    "section": "How to Use This Section",
    "text": "How to Use This Section\nWhether you’re a prospective client, brand new to FACTS or brushing up on fundamentals, this Introduction area is designed to help you gain confidence with FACTS. Start by reviewing the fundamentals, watch a webinar to see the product in action, and use the resources provided to support your ongoing growth. We’re here to ensure you have a smooth and rewarding learning experience. If you have any questions, don’t hesitate to contact us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "We pride ourselves in delivering fast support and will go above and beyond for you.\n\n\nGeneral Inquiries and Help using FACTS\nPlease contact us directly via e-mail at facts@berryconsultants.com.\n\n\nGet FACTS\nTo directly apply for a free 3-months FACTS Evaluation license, please use the following online form.\nTo directly enquire about a free demo or a regular license, please use the following online form.\nIf you are unsure, feel free to contact us directly via email at facts@berryconsultants.com.\n\n\nGeneral Inquiries about Berry Consultants\nPlease use the following online form."
  },
  {
    "objectID": "documentation/index.html",
    "href": "documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Welcome to the Documentation section — your go-to repository of in-depth user guides and practical examples that illuminate every aspect of FACTS. Whether you’re just getting started or delving into advanced functionalities, these resources are designed to help you navigate our software with confidence and ease.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/index.html#what-youll-find-here",
    "href": "documentation/index.html#what-youll-find-here",
    "title": "Documentation",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nUser Guides: Access detailed manuals covering the full range of our engines. From foundations to advanced features, learn how to set up your desired trial design, learn about best practices and obtain focused understanding of the subject matter, and troubleshoot common issues.\nInstallation Instructions: Get up and running quickly with clear guidance. Our installation resources walk you through every phase — downloading the software, deploying it in various environments, and FACTSing!\nExamples: Explore a range of constructed and real-world studies that demonstrate FACTS’s capabilities in action and discover strategies to optimize performance.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/index.html#how-to-get-the-most-out-of-this-section",
    "href": "documentation/index.html#how-to-get-the-most-out-of-this-section",
    "title": "Documentation",
    "section": "How to Get the Most Out of This Section",
    "text": "How to Get the Most Out of This Section\nThis section contains resources for the current and past versions of FACTS, which you can choose in the sidebar. If necessary, begin by reviewing the installation guides. Later, you may consult engine-specific user manuals for deeper insights, and explore our curated examples to bring theory into practice. With these resources at your fingertips, you’ll gain a richer, more informed understanding of FACTS and maximize your potential for success.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/v71/examples/Staged/example2.html",
    "href": "documentation/v71/examples/Staged/example2.html",
    "title": "Example 2",
    "section": "",
    "text": "Second example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "Staged",
      "Example 2"
    ]
  },
  {
    "objectID": "documentation/v71/examples/CRM/example2.html",
    "href": "documentation/v71/examples/CRM/example2.html",
    "title": "Example 2",
    "section": "",
    "text": "Second example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "CRM",
      "Example 2"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html",
    "href": "documentation/v71/userguides/FACTSfromR.html",
    "title": "Calling FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "href": "documentation/v71/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "title": "Calling FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#software-prerequisites",
    "href": "documentation/v71/userguides/FACTSfromR.html#software-prerequisites",
    "title": "Calling FACTS from R",
    "section": "2.1 Software prerequisites",
    "text": "2.1 Software prerequisites\nTo call FACTS from R, you will need the following installed on your Windows machine on which you are using FACTS:\n\nA reasonably recent version of R (v3.5.3+).\nFACTS v6.4 or later, the command line executable versions of the FACTS simulation engines – these are currently available to Enterprise licensees.\nThe supplied R file: factR.R",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#factr.r",
    "href": "documentation/v71/userguides/FACTSfromR.html#factr.r",
    "title": "Calling FACTS from R",
    "section": "2.2 factR.R",
    "text": "2.2 factR.R\nProvides an ‘R’ wrapper for accessing FACTS analysis models for:\n\nCore and Enrichment Design, allowing you to use the following FACTS analysis features:\n\nDose Response models\nLongitudinal models\nHierarchical Prior on Control (borrowing from historical data)\nTTE predictor endpoint\nBAC\n\nInputs\n\nFACTS param file with trial info and model specifications\nData file\n\nOutput\n\nMCMC file",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "href": "documentation/v71/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "title": "Calling FACTS from R",
    "section": "2.3 Steps for calling FACTS from R",
    "text": "2.3 Steps for calling FACTS from R\nTo call FACTS from R, you will need to do the following sequence of steps:\n\nCreate a (non-adaptive) FACTS project for your Engine type with the general study info: Number of Arms, number and timing of Visits (if using), Dose response (& longitudinal if using) model specification and MCMC setup.\nConfigure VSR and Execution profiles to allow a simple simulation run.\nRun 1 simulation to produce:\n\nA ‘param’ file which will be passed as an input to the R function.\nA ‘patients’ file. This may be useful to illustrate data file format for the input data. See FACTS Execution Guides for details.\nAn ‘mcmc’ file. This will show you what to expect in the output MCMC file.\n\nIf using FACTS to analyze a data set, then\n\nput the data set into the required format\nwrite an R script to call FACTS with the data set\nprocess the MCMC output\n\nIf using FACTS within a simulation framework, then:\n\nWrite an R script that generates the data you wish to simulate and pass to FACTS to analyze\nWrite a loop that\n\ngenerates the data for a simulation\ncalls FACTS with generated data\nprocess the MCMC output\naccumulate the statistics for the overall operating characteristics to be computed\n\nOutput the resulting OCs",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#runfacts-usage-notes",
    "href": "documentation/v71/userguides/FACTSfromR.html#runfacts-usage-notes",
    "title": "Calling FACTS from R",
    "section": "2.4 runFACTS() Usage Notes",
    "text": "2.4 runFACTS() Usage Notes\n\n2.4.1 Run FACTS MCMC Model from R\nrunFACTS(\n engine, \n data.file = “patients.dat”, \n param.file = “nuk1_e.param, \n mcmc.file.num = 0, \n rng.seed = 1, \n exec.path = getwd()\n)\nReturn Value: runFACTS returns a TRUE/FALSE to indicate a successful/failed execution. In case of errors, R error messages may be printed and in case of a FACTS execution error, a file called ‘error.txt’ will be output, containing the error description.\nArguments:\n\nengine: Name of the FACTS engine to use. Can be one of the following:\n\nFor Core Engines: “contin”, “dichot”, “ME”, “TTE”\nFor Enrichment Design Engines: “ed_contin”, “ed_dichot”, “ed_tte”\n\ndata.file: Name of the input data file. Default is “patients.dat”. This file format should exactly match the file format of the ‘patients’ file corresponding to the ones produced by FACTS for the design you setup in FACTS to specify the analysis model. (See the FACTS Execution Guide under the FACTS Help menu for details.)\nparam.file: Name of the FACTS ‘.param’ file that specifies the model setup. Default is ‘nuk1_e.param’.\nmcmc.file.num: The MCMC output is written to a file named ’mcmcNNNNN.csv. This argument set the NNNNN. Therefore, mcmc.file.num = 1 will create an MCMC output file called mcmc00001.csv. Default value is 0.\nrng.seed: Integer-valued random number generator seed. Will use the value from the ‘.param’ file if unspecified.\nexec.path The path to the directory where the FACTS executable program is available. Default is the current working directory.\n\n\n\n2.4.2 Set Up Files and Folders\nIt is important to pass files and parameters correctly, as there is not much in the way of helpful error messaging. Setting up the required folder and files is not hard but should be done carefully. The following example shows how.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "href": "documentation/v71/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "title": "Calling FACTS from R",
    "section": "3.1 The “core.dichot.example.facts”",
    "text": "3.1 The “core.dichot.example.facts”\nIn this example we wish to use FACTS to fit a simple NDLM dose response model across 6 arms (control & 5 doses) with a dichotomous endpoint.\nWe have entered the following parameters:\n\nStudy:\n\nStudy Info:\n\nNon-adaptive\nRecruit subjects continuously\nMax subjects: 300\nResponse is a positive outcome\nTime to final endpoint: 4 weeks\n\nTreatment Arms:\n\nControl and 5 doses with strengths 1, 2, … 5\n\n\nVirtual Subject Response\n\nExplicitly defined\n\nDose Response\n\nresponses: 0.1, 0.1, 0.125, 0.15, 0.2, 0.25\n\n\n\nExecution\n\nAccrual\n\n1 region with mean accrual of 5 subjects per week\n\nDropout\n\nNo dropouts\n\n\nQuantities of Interest\n\nPosterior probability: Pr(P_d &gt; P_Control)\nProbability of being target: Pr(Max)\nDecision Quantity: Pr(P_d &gt; P_Contorl); d=Greatest Pr(Max)\n\nDesign\n\nDose Response\n\nSimple NDLM\n\nInitial Dose ~N(0,22)\nTau IG(1,1) “central value”, “weight”\n\n\nRequentist analysis: none\nAllocation: 1:1:1:1:1:1\nSuccess/Futility Criteria\n\nSuccess: Pr(P_d &gt; P_Control); d= Greatest Pr(Max) &gt; 0.9\n\n\n\nNot all these parameters will effect our analysis, but we have to enter sufficient parameters to be able to run a simulation and get a bin1_e.param file. This can be found in the scenario simulation results folder. We only need to run 1 simulation on order to have one written out. This file is copied to our “Example” directory. If we want to change something in the analysis – the model or the prior for example, we can modify this facts file, re-run one simulation, and copy the new bin1_e.param file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#dichot-demo.r",
    "href": "documentation/v71/userguides/FACTSfromR.html#dichot-demo.r",
    "title": "Calling FACTS from R",
    "section": "3.2 dichot-demo.R",
    "text": "3.2 dichot-demo.R\nWe start by setting the current working directory to the “Example” folder, and setting up some file locations and sourcing the factR.R file.\n## Set up Folders and Paths\n\n# This is the directory where the parameter file and patient data must be located\n# It will be where the MCMCM files are written\nsetwd(\"Z:/FACTS test/FACTS 6 Training/FACTS R interface/Example\")\n\n# This must be the location of the factR.R file\nFactR.src = \"../factR.R\"\n\n# This must be the location of the executable files\nExec.dir = \"../WindowsExecutables\"\n\n# Load runFACTS\nsource(FactR.src)\nWe can then copy an example patients file from the simulation results and check that we can run facts.\n# Test to check its working\n# Copy an example patients file from the simulations results to this folder before running.\nsystem.time(\n  runFACTS(\n    engine='dichot', \n    data.file = 'patients00001.csv', \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 1, \n    rng.seed = 1, \n    exec.path = Exec.dir\n  )\n)\n\n3.2.1 genBinaryData()\nWe now define a simple function to generate the data for a single run of FACTS.\n# generates a data frame that can be used to drive a FACTS analysis\n# dichotomous endpoint\n# no visits\n# n.per.arm: int, the number of subjects to be simulated for each arm\n# rates: int[], the response rate to be simulated for each arm\n# the length if rates defines the number of arms\n# returns a dataframe with n.per.arm * length(rates) simulated subjects\n\ngenBinaryData &lt;- function(nPerArm, rates) {\n  \n  patientID &lt;- 1:(nPerArm * length(rates)) # Generate a list of patients\n  region &lt;- rep(1, nPerArm * length(rates)) # all patients come from region 1\n  date &lt;- 1:(nPerArm * length(rates)) # Generate a list of enrolment dates - here simply one per day\n  doseAlloc &lt;- rep(1:length(rates), each = nPerArm) # Allocate patients equally to each dose\n  lastVisit &lt;- rep(1, nPerArm * length(rates)) # all patients have last visit data\n  dropout &lt;- rep(0, nPerArm * length(rates)) # no patients have dropped out\n  baseline &lt;- rep(-9999, nPerArm * length(rates)) # not simulating baseline\n  visit1 &lt;- rep(0, nPerArm * length(rates)) # create the outcome vector\n\n  for (d in 1:length(rates)){ # get responses for each dose\n  ix &lt;- which(doseAlloc == d) # get indices of patients on dose d\n  # assign them a final response based on the rate to simulate for dose d\n  if (length(ix) &gt; 0) {\n  visit1[ix] &lt;- \n    sample(\n     c(0,1), \n     size = length(ix), \n      replace = TRUE, \n      prob = c(1-rates[d], rates[d])\n    )\n  }\n  \n}\n\ndat &lt;- data.frame(\n  SubjectID = patientID, \n  Region = region, \n  Date = date, \n  Dose = doseAlloc, \n  LastVisit = lastVisit, \n  Dropout = dropout, \n  Baseline = baseline, \n  Visit1 = visit1, \n  row.names = NULL\n)\n\nreturn(dat)\n}\n\n\n3.2.2 runSims\n########### Toy Example Trial Sim ##########\n### Constants\nDATAFILE = \"patients.csv\"\nMCMCFILE = \"mcmc00000.csv\"\n# function to simulate an example data set with dichotomous endpoint\n# nSims - the number of sims to run\n# nBurnin - the number of MCMC smaples to discard\n# (the number of MCMC samples is specified in the parameter file)\n# details - a boolean. If TRUE the function returns a data frame with\n# the results of each individual simulation,\n# otherwise just the win proportion and probabilities of being control\n\nrunSims &lt;- function(\n    nSims = 10, \n    nBurnin = 1000, \n    rates = c(0.1, 0.1, 0.125, 0.15, 0.2, 0.25),\n    details = FALSE\n) {\n\nwinPpn = 0\npr.gt.ctl.sum &lt;- rep(0, length(rates) - 1)\nif (details) {\nperSim &lt;- data.frame(Sim = 1)\n}\n\nfor(sim in 1:nSims) {\ndat = genBinaryData(nPerArm = 50, rates = rates)\nwrite.csv(dat,DATAFILE, row.names = FALSE)\nif (details) {\nperSim[sim, \"Sim\"] &lt;- sim\n# record true rates and observed rates\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"sim.rate.\", d, sep=\"\")] &lt;- rates[d]\n}\n\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"obs.rate.\", d, sep=\"\")] &lt;- mean(dat[dat[,\"Dose\"]==d, \"Visit1\"])\n}\n}\n\ncat(\"run FACTS: \", sim, \"\\n\")\n\nret &lt;- \n  runFACTS(\n    engine = 'dichot', \n    data.file = DATAFILE, \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 0, \n    rng.seed = sim, \n    exec.path = Exec.dir\n  )\n\ndat = read.csv(MCMCFILE, skip = 1)\n\n# discard burnin rows and just estimates of rate - the \"Pi\" columns\ndat = dat[(nBurnin + 1):nrow(dat), grep(\"Pi\", names(dat))]\n\nif (details) {\n# record est rate\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"est.rate.\", d, sep=\"\")] &lt;- mean(dat[,paste(\"Pi.\", d, sep=\"\")])\n}\n}\n\n# success if the first dose is not in the top 2 .. i.e. the resposnse on any 2 doses is &gt; control\n\nsuccess &lt;- apply(dat, 1, FUN = function(x) {ifelse(length(x) - which(order(x)==1) &gt;= 2, 1, 0)})\n\nif (details) {\nperSim[sim, \"Pr.Success\"] &lt;- mean(success)\nperSim[sim, \"Success.flag\"] &lt;- ifelse(mean(success) &gt; 0.9, 1,0)\n}\n\nwinPpn = winPpn + ifelse(mean(success) &gt; 0.9, 1,0)\n\n# example: calc pr(theta_d &gt; theta_ctl)\ngt.ctl.flag &lt;- apply(dat, 1, FUN = function(x){x[2:length(x)] &gt; x[1]})\npr.gt.ctl &lt;- apply(gt.ctl.flag,1,sum)\npr.gt.ctl &lt;- pr.gt.ctl / length(gt.ctl.flag[1,])\npr.gt.ctl.sum &lt;- pr.gt.ctl.sum + pr.gt.ctl\n\nif (details) {\nfor (d in 1:length(pr.gt.ctl)) {\nperSim[sim, paste(\"Pr.pi.\", d+1, \"&gt;pi_ctl\", sep=\"\")] &lt;- pr.gt.ctl[d]\n    }\n  }\n}\n\ncat(\"win proportion: \", winPpn/nSims, \"\\n\")\n\nif (details) {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims, perSim))\n} else {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims))\n}\n}\n\n\n3.2.3 Running the Example\nHaving sourced the above functions and variables holding paths and fllenames we can run the simulations with the default scenario:\nrunSims(details=FALSE)\n\n&gt; run FACTS: 1\n&gt; run FACTS: 2\n&gt; run FACTS: 3\n&gt; run FACTS: 4\n&gt; run FACTS: 5\n&gt; run FACTS: 6\n&gt; run FACTS: 7\n&gt; run FACTS: 8\n&gt; run FACTS: 9\n&gt; run FACTS: 10\n&gt; win proportion: 0.3\n\n&gt; [[1]]\n&gt; [1] 0.3\n\n&gt; [[2]]\n&gt; Pi.2 Pi.3 Pi.4 Pi.5 Pi.6\n&gt; 0.30572 0.45824 0.51936 0.77944 0.92916\nIf “details” is set to TRUE then the list of results has a dataframe at the end that contains a row per simulation and details of that simulations results.\nHopefully this is sufficient to get you started.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html",
    "href": "documentation/v71/userguides/flfll.html",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems.\n\n\n\nFLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#purpose-and-scope-of-this-document",
    "href": "documentation/v71/userguides/flfll.html#purpose-and-scope-of-this-document",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#overview",
    "href": "documentation/v71/userguides/flfll.html#overview",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "",
    "text": "FLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#linux",
    "href": "documentation/v71/userguides/flfll.html#linux",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "2.1 Linux",
    "text": "2.1 Linux\n\nInstall Mono version 6.12 or later from https://www.mono-project.com/docs/about-mono/releases onto the target machine/server running FLFLL and ensure that all users of FLFLL can run Mono. A simple test would be to ask FLFLL users to run “mono –version”.\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” (via Mono) present in the application folder.\nRetrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server.\nWithin the FLFLL application folder, go to the “bin” folder (which contains the Linux engines used by FLFLL) and elevate the Linux engine permissions to being executable by running “chmod +x [name of linux engine executable here]” for each of the Linux engines. If you do not have permission to do so, please ask your IT administrator to run this command.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#windows",
    "href": "documentation/v71/userguides/flfll.html#windows",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "2.2 Windows",
    "text": "2.2 Windows\n\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL. We recommend using 7Zip (https://www.7-zip.org/) to perform the unzipping rather than the Windows in-built unzipping tool: the latter can result in the corruption of the FLFLL application as a security precaution.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” present in the application folder.\nIf the machine/server running FLFLL already has a licensed version of FACTS installed on it, the following step can be skipped. Otherwise, retrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#linux-1",
    "href": "documentation/v71/userguides/flfll.html#linux-1",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "3.1 Linux",
    "text": "3.1 Linux\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\n\nmono “FLFLL.exe” -file “home/mono/FLFLL/Input/myfile.facts” -nSim 1 -seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath “home/mono/FLFLL/Output” -logPath “/home/mono/Log”\n\nRun FLFLL to generate parameter files only for multiple FACTS project files in a single directory:\n\nmono “FLFLL.exe” -file “home/mono/FLFLL/Input” -nSim 1 -seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath “home/mono/FLFLL/Output” -logPath “/home/mono/Log”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\n\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS files older than FACTS 6.2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#windows-1",
    "href": "documentation/v71/userguides/flfll.html#windows-1",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "3.2 Windows",
    "text": "3.2 Windows\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\n\nFLFLL.exe -file “C:.facts” -nSim 1 -seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath “C:” -logPath “C:”\n\nRun FLFLL to generate parameter files only for multiple FACTS project files:\n\nFLFLL.exe -file “C:” -nSim 1 -seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath “C:” -logPath “C:”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\n\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS file older than FACTS 6.2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#overview-1",
    "href": "documentation/v71/userguides/flfll.html#overview-1",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nUsage: FLFLL.exe. [options]. where [options] are:\n-[h|help] - Display the help menu. Default (False).\n-[f|file] - Specifies the file or top-level directory to open.\n-[n|nSim] - Number of simulations to run. Default (5).\n-[p|packet] - Packet size for parallelization. Default (1000).\n-[g|grid] - Flag indicating sims to run on grid. Default (False).\n-[a|agg] - Aggregation mode. Default (None).\n-[aggPrefix] - Prefix for aggregated files. Default (agg).\n-[nBurn] - Number of MCMC burn-in iterations. Default (1000).\n-[nMCMC] - Number of MCMC sample iterations. Default (2500).\n-[nWeeksFiles] - Number of weeks files to generate. Default (100).\n-[nSubjectFiles] - Number of subjects files to generate. Default (1).\n-[nMCMCFiles] - Number of MCMC output files to generate. Default (0).\n-[nMCMCThin] - MCMC thinning parameter. Default (0).\n-[nMCMCImpute] - MCMC length per imputation parameter. Default (1).\n-[seed] – Set the random number seed. Default (3500).\n-[logPath] – If provided, specifies a directory where a log file is generated\n-[outputPath] – Specifies the directory where output will be generated. Default (“out”).\n-[endToEndRun] – Flag indicating if simulations should be run.\n-[skipMissingParamsCheck] – Flag indicating to skip checking for missing parameters.\n-[scenarios] – Flag indicating which scenarios should be processed.\n-[useDifferentSeedPerScenario] – Flag indicating whether to use a different seed for each simulated scenario. Default (False).\n-[useDifferentSeedPerDesign] – Flag indicating whether to use a different seed for each simulated design. Default (False).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#arguments",
    "href": "documentation/v71/userguides/flfll.html#arguments",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "4.2 Arguments",
    "text": "4.2 Arguments\n\n4.2.1 –[h | help] (Help)\nThe –h command line option displays the command line help options in the terminal. No simulations will be performed when the –h option is specified.\n\n\n4.2.2 –[v | version] (Version)\nThe –v command line option displays the FLFLL version in the terminal. No simulations will be performed when the –v option is specified.\n\n\n4.2.3 –[f | file] FILE (Run a specific .facts file)\nThe –f command line option is used to specify the “.facts” file to process. The –f option must be followed by a valid path to an existing “.facts” file or directory containing one or more “.facts” files. Hint: Remember to use quotes around the path if it includes spaces.\nIf the supplied file name is a directory, then FACTS will process each “.facts” file in the directory in turn. As this only starts FACTS once, this can be quite a bit quicker than using a batch file or script that loops and starts FACTS separately for each “.facts” file.\nFurthermore, if the supplied file name is a directory then FACTS also automatically recurses through every sub-directory processing every “.facts” file it finds.\n\n\n4.2.4 –[n | nSim] N (Run N simulations for each scenario)\nThe –n command line option is used to specify the number of simulations to run. The –n option must be followed by an integer value greater than 0. For each scenario in the FACTS project file, the application will run N simulations. Defaults to 5 if unspecified.\n\n\n4.2.5 –[p | packet] N (Set the packet size for simulations)\nThe –p command line option is used to specify the packet size for parallelization of simulations. The –p option must be followed by an integer value greater than 0. When using the –g option to run on a grid, or when running on a multicore machine it is often beneficial to parallelize simulations using the packetization process (see grid documentation for more information on packetization). The packet size must be greater than zero, but as in the GUI, there is no restriction that it be less than the number of simulations. If it is greater than the number of simulations, the simulations will not be packetized. Defaults to 1000 if unspecified.\n\n\n4.2.6 –[g | grid] (Run on grid)\nThe –g command line option instructs the application to send simulations to the grid (assumes that the grid is correctly configured). When running on the grid, the action is still performed synchronously (i.e. FACTS will wait while the simulations run and collect the results before exiting). This option is useful to parallelize long running simulations more than they can be parallelized locally. Defaults to run locally if unspecified.\n\n\n4.2.7 –[a | agg] Mode (Aggregation Mode)\nThe –a command line option specifies the aggregation action to take for completed simulation results. The available modes for this option are:\n\nNone – no aggregation will be performed\nNoPivot – Only standard aggregation will be performed\nPivot – Both standard and pivoted aggregation will be performed.\nDefault, if unspecified, is None.\n\n\n\n4.2.8 –aggPrefix prefix (Prefix for aggregation files)\nThe –aggPrefix command line option specifies the prefix to use when naming aggregated files and must be followed by a valid file prefix. This option is only used when –a is set to NoPivot or Pivot. The aggregation files produced by aggregating across all scenarios will be named using the prefix&lt;_pivot&gt;_(filename).csv pattern, where &lt;_pivot&gt; is included for pivoted files only, and (filename) is replaced by the name of the file being aggregated. Defaults to “agg” if unspecified.\n\n\n4.2.9 –nBurn N (Number of MCMC burn-in iteractions)\nThe –nBurn command line option specifies the number of burn-in MCMC iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 1000 if unspecified.\n\n\n4.2.10 –nMCMC N (Number of MCMC sample iterations)\nThe –nMCMC command line option specifies the number of MCMC sampling iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 2500 if unspecified.\n\n\n4.2.11 –nWeeksFiles N (Number of weeks files to output)\nThe –nWeeksFiles command line option specifies the number of weeks files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 100 if unspecified.\n\n\n4.2.12 –nSubjectFiles N (Number of subjects files to output)\nThe –nSubjectFiles command line option specifies the number of subject files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 1 if unspecified.\n\n\n4.2.13 –nMCMCFiles N (Number of MCMC files to output)\nThe –nMCMCFiles command line option specifies the number of MCMC sample files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 0 if unspecified.\nNote: This option potentially produces a very large amount of output data and may fail if sufficient disk space is not available.\n\n\n4.2.14 –nMCMCThin N (MCMC output thinning value)\nThe –nMCMCThin command line option specifies the MCMC thinning value to apply to the MCMC output and must be followed by a valid integer value at least &lt;x&gt;. The thinning parameter applies only to the MCMC output, all MCMC samples are used for analysis. Defaults to &lt;x&gt; if unspecified.\n\n\n4.2.15 –nMCMCImpute N (MCMC length per imputation value)\nThe –nMCMCImpute command line option specifies the number of MCMC sampling iterations to use in the simulation for each imputation. Defaults to 1 if unspecified.\n\n\n4.2.16 –seed (Random number seed)\nThe –seed command line option sets the random number generator seed value. The default value (3500) is the same as that used in the GUI. It can be set to any positive integer.\n\n\n4.2.17 -logPath Path (Path where the optional log file is placed)\nThe –logPath command line option (if given) is used to specify the directory where a log file is generated. If not specified, a log file will not be generated. The –logPath option must be followed by a valid path. If the path does not exist, it will be created.\nNote: Remember to use quotes around the path if it includes spaces.\n\n\n4.2.18 -outputPath Path (Path where output is generated)\nThe –outputPath command line option (if given) is used to specify the directory where parameter files and optional simulation files are placed. If not specified, the output path will be the directory in which the .facts file is present. The –outputPath option must be followed by a valid path. If the path does not exist, it will be created.\n\n\n4.2.19 -endToEndRun (Flag to indicate if simulations should be run also)\nThe –endToEndRun command line instructs FLFLL to run full simulations. If unspecified, only parameter files will be generated.\n\n\n4.2.20 -skipMissingParamsCheck (Flag to indicate to skip checking of missing parameters)\nThe –skipMissingParamsCheck command line instructs FLFLL to skip the process of checking for missing parameters in legacy FACTS project files (.facts). If not specified, when running FACTS projects files prior to version 6.2, errors will prevent FLFLL from completing.\n\n\n4.2.21 -scenarios (Flag indicating which scenarios should be processed)\nThe –scenarios command line instructs FLFLL to run the specified scenarios by name. The names of the scenarios to run should be provided as a comma separated list. If not specified, all scenarios will be processed.\n\n\n4.2.22 -useDifferentSeedPerScenario (Flag indicating whether to use a different seed for each simulated scenario)\nThe –useDifferentSeedPerScenario command line option provides FLFLL with an option to set a different random number seed for each of the scenarios that are being simulated. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated scenario and then adds 1111 to the random number seed of any additional scenarios being simulated. For example, if three scenarios are being simulated and the -seed flag is set to 3500, the first scenario will have a random number seed of 3500, the second scenario a random number of 4611 and the third scenario a random number seed of 5722. This deterministic way of setting different random number seeds allows for reproducible simulation results.\n\n\n4.2.23 -useDifferentSeedPerDesign (Flag indicating whether to use a different seed for each simulated design)\nThe –useDifferentSeedPerDesign command line option provides FLFLL with an option to set a different base random number seed for each of the designs that are being simulated. This option is only used when a directory containing multiple FACTS designs is passed as an argument to the FLFLL command line, rather than a single design. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated design and then adds 1234 to the random number seed of any additional designs being simulated. For example, if three designs are being simulated and the -seed flag is set to 3500, the first design will have a base random number seed of 3500, the second scenario a random number of 4734 and the third design a base random number seed of 5968. This deterministic way of setting different random number seeds allows for reproducible simulation results.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/dd_execution.html",
    "href": "documentation/v71/userguides/core/dd_execution.html",
    "title": "Execution",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/hh_longmodels/longmodels_tte.html",
    "href": "documentation/v71/userguides/core/hh_longmodels/longmodels_tte.html",
    "title": "Study Tab",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Longitudinal Models",
      "Study Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#purpose-of-this-document",
    "href": "documentation/v71/userguides/core/core.html#purpose-of-this-document",
    "title": "Core Designs - Shared Features",
    "section": "1.1 Purpose of this document",
    "text": "1.1 Purpose of this document\nThis document describes how to use the ‘Quantities of Interest’ and ‘Design’ options that are common across the FACTS Core design engines. It is intended for all end users of the system."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#scope-of-this-document",
    "href": "documentation/v71/userguides/core/core.html#scope-of-this-document",
    "title": "Core Designs - Shared Features",
    "section": "1.2 Scope of this document",
    "text": "1.2 Scope of this document\nThis document covers the design options that are common across the four FACTS Core Design Engines: Continuous, Dichotomous, Time-to-Event, and Multiple Endpoint. Design elements that are unique to a particular engine (primarily data simulation and simulation output) are covered in the endpoint specific Core Engine User Guide.\nThis document does not address the use of FACTS Enrichment Designs, Dose Escalation, or Platform Trials, which have separate User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS V7 & V6 installed on Windows 10 or 11. If FACTS is installed on different versions of Windows, or with different Windows ‘themes’ there will be some differences in appearance introduced by Windows. The contents of each tab, however, will still be consistent with the screenshots in this document."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#context-of-this-issue",
    "href": "documentation/v71/userguides/core/core.html#context-of-this-issue",
    "title": "Core Designs - Shared Features",
    "section": "1.3 Context of this Issue",
    "text": "1.3 Context of this Issue\nThis is the version of the user guide for inclusion with the FACTS 7.1 release."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#citing-facts",
    "href": "documentation/v71/userguides/core/core.html#citing-facts",
    "title": "Core Designs - Shared Features",
    "section": "1.4 Citing FACTS",
    "text": "1.4 Citing FACTS\nIf writing in and using Bibtex, if you wish to cite FACTS (thank you!), we suggest the following:\n@techreport{FACTS71,\n  author = {{FACTS Development Team}},\n  title = {{FACTS}: Fixed and Adaptive Clinical Trial Simulator},\n  year  = {2024},\n  month = {03},\n  number = {Version 7.1},\n  type         = {Computer Software},\n  institution = {Berry Consultants LLC},\n  address = {Austin, TX},\n  note   = {&lt;https://www.berryconsultants.com/software/facts/&gt;}\n}\nThis will result in a reference that, for example in the APA style, will look like the following:\nFACTS Development Team (2024). FACTS: Fixed and adaptive clinical trial\nsimulator. Computer Software Version 7.1, Berry Consultants LLC, Austin, TX. &lt;https://www.berryconsultants.com/software/facts/&gt;."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#subject-responses",
    "href": "documentation/v71/userguides/core/core.html#subject-responses",
    "title": "Core Designs - Shared Features",
    "section": "3.1 Subject Responses",
    "text": "3.1 Subject Responses\nThe methods used to simulate subject responses vary by endpoint type. For each endpoint, the endpoint specific user guides provide information about simulating subject responses.\nFor simulating dichotomous responses see: FACTS Core Dichotomous User Guide\nFor simulating continuous responses see: FACTS Core Continuous User Guide\nFor simulating time to event responses see: FACTS Core Time-to-Event User Guide\nFor simulating multiple endpoint responses see the continuous or dichotomous user guide, depending on the type of endpoints used in the multiple endpoint study."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#accrual",
    "href": "documentation/v71/userguides/core/core.html#accrual",
    "title": "Core Designs - Shared Features",
    "section": "3.2 Accrual",
    "text": "3.2 Accrual\nThe Accrual sub-tab provides an interface for specifying accrual profiles. Accrual profiles define the mean recruitment rate week by week during the course of the trial. Virtual subjects are simulated from a Poisson process in which the expected number of subjects per week is allowed to change week by week.\nAccrual profiles are shown as a list on the left of the screen, as depicted below. These accrual profiles may be renamed by double-clicking on them and typing a new profile name. After creating a profile, the user must create at least one recruitment region. Early in the trial design process, detailed simulation of the expected accrual pattern is typically not necessary and a single region with a simple mean accrual rate is sufficient.\nTo model the expected accrual rates more precisely over the course of the trial, the user may specify multiple regions for each accrual profile and separately parameterize them. Regions are added via the table in the center of the screen (Figure 7‑1). Within this table, the user may modify:\n\nthe peak mean weekly recruitment rate,\nthe start date (in weeks from the start of the trial) for this recruitment region,\nwhether the region will have a ramp up phase and if so when the ramp up will be complete (in weeks from the start of the trial).\nWhether the region will have a ramp down, and if so when the ramp down start and when the ramp down will complete (in weeks from the start of the trial).\n\nRamp up/ramp down define simple linear increase/decreases in mean recruitment rate from the start to the end of the ramp. Note that simulation of accrual is probabilistic, but ramp downs are defined in terms of time, so even if ramp downs are planned so that at the average accrual rate they will occur as the trial reaches cap, there is a risk in simulations when accrual has been slower than average, that ramp downs occur before the full sample size is reached. It is advisable to have at least one region that doesn’t ramp down to prevent simulations being unable to complete.\nA graph of the recruitment rate of the highlighted region is shown as well. As the recruitment parameters are changed, the graph will update to show the time at which full accrual is reached. An accrual profile that does not reach full accrual is invalid and cannot be used to run simulations.\n\nIn the screenshot above you can see the two step ramp up in accrual from two regions – each starting at different offsets into the trial.\nNote that the accrual profile graph is only the mean expectation; actual accrual is simulated using exponential distributions for the intervals between subjects, derived from the mean accrual profile specified here. Thus some simulated trials will recruit more quickly than this and some more slowly.\nThere are commands to import and export region details from/to simple external XML files. When importing, the regions defined in the external file are added to the regions already defined, they don’t replace them.\nThis is an example of a very simple region file defining just one region:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;regions&gt;\n&lt;region&gt;\n&lt;name&gt;Region 1&lt;/name&gt;\n&lt;rate&gt;5&lt;/rate&gt;\n&lt;start&gt;0&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;/regions&gt;\n\n3.2.1 Deterministic Accrual\nIf “Deterministic” accrual has been specified on the Study &gt; Study Info tab, then on the Accrual tab, rather that specifying an accrual profile from which subject recruitment times are simulated, the user loads a file of specific accrual dates for every subject.\n\nThe user specifies a “.dat” file to load that contains the subject accrual dates in weeks from the start of the trial.\nThe required file format is a text file with comma separate values. One row per subject, with 3 fields on each row:\n\nthe subject ID, (an integer)\nthe ID of the region where the subject was recruited (an integer)\nand the subjects randomization date (in weeks from the start of the trial – this is a ‘real’ number allowing fractions of a week to be specified)\n\nThe file must contain sufficient entries to allow the maximum number of subjects specified on the Study &gt; Study Info tab to be recruited.\nAfter successfully loading a file, the FACTS GUI shows a plot of the resulting weekly accrual rate"
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#drop-out-rates",
    "href": "documentation/v71/userguides/core/core.html#drop-out-rates",
    "title": "Core Designs - Shared Features",
    "section": "3.3 Drop-out Rates",
    "text": "3.3 Drop-out Rates\nFor the continuous and dichotomous engines, and the multiple endpoint engine, the default dropout scenario is that no subjects drop out of the study before observing their final endpoint data. If dropouts are expected, the user can specify either the “Dropouts per Dose,” or “Dropouts per Dose per Visit.”\nIf “Dropouts per Dose” is selected, then each subject has a probability of not having an observable final endpoint value equal to the dropout rate of the dose that subject is randomized to. If each subject has multiple visits and “Dropouts per Dose” is selected, then the conditional probability of dropping out before each visit given that the subject had not dropped out up to the visit before rates are all equal. In other words, if the total dropout rate is \\(\\pi_D\\), the probability of dropping out between visits \\(i\\) and \\(i+1\\) given that the subject had not dropped out at visit \\(i\\) is \\(1 - \\left( 1 - \\pi_{D} \\right)^{\\frac{1}{V}}\\) where \\(V\\) is the total number of visits.\nIf “Dropouts per Dose per Visit” is selected, then each subject has a user specified probability of dropping out before a visit \\(v\\) that is specified as the conditional probability of dropping out before visit \\(v\\) given that that they had not dropped out by visit \\(v-1\\). This leads to a total dropout rate \\(\\pi_D\\) for a participant that is equal to:\n\\[\\pi_{D} = 1 - \\prod_{v = 0}^{V}{(1 - \\pi_{v})}\\]"
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#posterior-probabilities",
    "href": "documentation/v71/userguides/core/core.html#posterior-probabilities",
    "title": "Core Designs - Shared Features",
    "section": "4.1 Posterior Probabilities",
    "text": "4.1 Posterior Probabilities\nThese are Bayesian quantities to be calculated at each interim and at the final analysis.\n\nA Posterior Probability is specified as:\n\nCompare:\n\nContinuous: Means\nDichotomous: Rates or Log-odds\nTime-to-Event: Hazard Ratio or Hazard Rates.\n\n\n\n\nCondition: “&gt;” or “&lt;” a comparison value.\nRelative to an absolute value or relative to the response on a specific dose.\nThe comparison can include a delta, which is the absolute value to be compared against if the comparison is absolute, or a value that the difference relative to the comparison arm is compared to.\nThe QOI will be given a “name” derived from these details and a short or Alternative name that will be used in the output files to allow easier access from other software, e.g. from within R.\nIf the endpoint is TTE and the design includes a predictor endpoint, then the definition of the QOI includes the specification of which endpoint it applies to.\n\n\n4.1.1 Notes on setting Deltas\nIn the three endpoints delta’s are defined as:\n\nContinuous\n\nA CSD (Clinically Significant Difference) in the estimates of the mean response.\n\nDichotomous\n\nA CSD in the estimate of the response rates if Rates is selected in the QOI, and of Odds Ratios\n\nTime-to-Event\n\nA CSHRD (Clinically Significant Hazard Ratio Difference) in the estimate of the Hazard Ratio\n\n\nA standard hypothesis test for demonstrating superiority to control uses a delta of 0. Testing for superiority with a non-zero delta is different, and the implications need to be carefully understood. Where the term CSD is used in this document, it should be taken to refer to CSHRD as well unless it is specifically stated otherwise.\nWhen setting a target treatment difference from control for the study to beat, it is also common to require a degree of confidence that the target has been beaten. To achieve posterior probabilities of &gt;50% that the target has been beaten, the estimated mean difference will have to be greater than the target difference.\nThus, when setting a delta, we are setting up two hurdles the study drug must beat, first the delta and then on top of that an additional margin to give a &gt;50% confidence that the margin has been beaten. Thus, it is necessary to avoid setting the target delta too large. A common mistake is to set the delta to the ‘expected difference’ (the value that might have been used in a conventional sample size calculation). In scenarios where the simulated response is equal to the ‘expected difference’ and hence the CSD this will give probabilities of being better than control by the delta of 50% on average, regardless of the sample size.\nIt is inadvisable to require a posterior probability of 50% that the response is better than the Control by the delta margin as this turns the test into one that simply depends on whether the point estimate of the response is better.\nIt is inadvisable to require posterior probability of less than 50% for success, as such criteria have the undesirable characteristic that they can be met in circumstances where it can be seen that if further data was gathered consistent with what has already been seen, it would lead the threshold no longer being met! The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a delta that is less than the expected difference that it is hoped to achieve, somewhat like having a non-inferiority margin around the target. So, when the design is simulated with a response at the expected difference the target will be clearly exceeded. A useful default value to use for a delta is half the ‘expected difference’. This usually yields “comprehensible” probability thresholds for both success and futility.\nUsing a delta has benefits however – simply being better than control but for very small difference can be established simply by having large sample sizes. Being better than control by a delta establishes some confidence that there is some patient benefit. Use of a delta is also useful if otherwise success thresholds “well out in the tail” (e.g. &gt; 0.99) are required such as the equivalent of an early look in a conservative Group Sequential design. Though the use of a delta does mean that there is a less direct equivalence to a p-value.\n\n\n4.1.2 P-value Delta’s\nSeparately from the CSD/CSHRD delta, with a continuous or dichotomous endpoint (but not time-to-event), a frequentist super-superiority/non-inferiority delta can be specified.\nThese use the same selection of super-superiority/non-inferiority as the CSD\n\nCurrently, unlike the CSD delta that is only applied to the default posterior probability QOI and MED target QOI, the p-value delta is applied to all the p-value QOIs and it cannot be overridden.\nThe value of the p-value delta is constrained to be 0 or positive. The ‘direction’ of the delta depends then on the direction of the endpoint (whether “higher/lower is better” or “a response is a positive/negative outcome”.\n\n\n\nFigure 8‑1: Accrual\n\n\n\n\n\n\n\n\n\n\n\n\nHigher is better / Response is positive\n\n\nLower is better / Response is negative\n\n\n\n\n\n\nSuper-Superiority\n\n\nTrt – Control &gt; delta\n\n\nTrt – Control &lt; -delta\n\n\n\n\nNon-inferiority\n\n\nTrt – Control &gt; -delta\n\n\nTrt – Control &lt; delta\n\n\n\n\nFigure 8‑1: Accrual\n\n\n4.1.3 P-value Comparisons with No Control Arm\nIf no control arm is included p-values, with a continuous or dichotomous endpoint, the treatment response is compared to a fixed value. The fixed value is specified as “Frequentist response/rate to compare to for p-value QOIs:” in the Standard Evaluation Variables section at the bottom of the QOIs tab. It is not currently possible to compare different p-value QOIs to different fixed responses or rates."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#predictive-probabilities",
    "href": "documentation/v71/userguides/core/core.html#predictive-probabilities",
    "title": "Core Designs - Shared Features",
    "section": "4.2 Predictive Probabilities",
    "text": "4.2 Predictive Probabilities\nThere are two types of predictive probabilities –\n\nBayesian predictive probabilities, which are Bayesian predictions of frequentist outcomes and take into account the uncertainty around a set of parameters, and\nConditional Power, which aims to calculate the frequentist probability of success assuming a set of parameters to be true.\n\nThe primary difference between the Bayesian predictive probabilities and the conditional power calculations is that the Bayesian predictive probabilities are calculated taking the variability around the estimated treatment effect into account, while the conditional power calculations assume that the observed test statistic of past data is the exact effect that future data will be generated from.\nFor both Bayesian predictive probabilities and conditional power, we differentiate two types, predicting the outcome in the current trial and predicting the outcome in a future trial.\n\n4.2.1 Bayesian predictive probabilities\n\n4.2.1.1 Current Trial Bayesian Predictive Probabilities\nIn the current trial, the outcome can be predicted under one of two assumptions:\n\nThat no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nThat the trial continues recruiting using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until complete.\n\nPredictive probabilities currently can only predict outcomes based on p-values. Note that since p-values are only calculated as comparisons against a control arm, predictive probabilities of success in the current trial are only available if the current trial includes a control arm.\n\nThe user specifies how missingness is handled in the final analysis, the p-value test type – unadjusted, Bonferroni or Dunnett’s and the (one sided) alpha level for the significance test. The QOI assumes that the specified default p-value delta is used in the final test.\nThe predictive probability of the current trial at the maximum sample size is only available:\n\nIf the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation.\n\nThe predictive probability is calculated by simulating the remaining subjects, assuming their allocated at the current probabilities of allocation and simulating their final response based on the rate of response or (Normal) distribution of responses observed so far for each arm.\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\nThere is one simulation of the remaining data per MCMC loop, potentially significantly increasing the time to perform the overall simulation of the trial.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still perform the simulation of future subject outcomes based on posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\n4.2.1.2 Current Trial Bayesian Predictive Probabilities – Time-to-Event\nUnlike the Continuous and Dichotomous endpoints, to predict the probability of success at full enrollment with a Time-to-Event endpoint, it is also necessary to simulate the accrual rate (and hence how many events might be observed).\n\nFor TTE, for a Predictive Probability of Success at Full Enrollment, there are new parameters to determine how accrual is modeled. There are 3 models for accrual\n\nFixed Rate, the parameters for this are:\n\nThe fixed (mean) accrual rate per week to simulate.\n\nEstimated from the Last ‘W’ Weeks accrual data, using a Poisson distribution, with a Gamma prior. The parameters for this are\n\nThe number of past weeks W to use the accrual data from.\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\nEstimated from all the accrual data from the start of the trial, using a Poisson distribution, with a Gamma prior, the parameters for this are:\n\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\n\n\n\n4.2.1.3 Future Trial Bayesian Predictive Probabilities\nFor predictive probabilities for a future trial, a predictive probability of success in a subsequent phase 3 trial is estimated during the current trial. The probability of success in phase 3 is calculated for each study arm, based on the specification of the phase 3 trial given here of:\n\nwhether the aim is to show superiority or non-inferiority,\nthe sample size per arm,\nthe required one-sided alpha,\nand the super-superiority margin or non-inferiority margin (if any).\n\nGiven these criteria, FACTS calculates the predicted probability of success of the subsequent trial for that endpoint assuming the estimate of the response on that endpoint, integrated over the uncertainty in those estimates. The conventional expected power of the specified future trial is calculated for the treatment effect in each MCMC sample and then averaged. This predicted probability of success can then be used in the stopping criteria and final evaluation criteria for the current trial.\nThis QOI is an extension of the “probability of success in phase 3” in earlier versions of FACTS – the difference now is that multiple different possible future trials can be specified in different QOIs and used for different decisions.\n\nThis predictive probability has the following parameters that must be specified:\n\nWhether the future trial will be for Superiority or Non-inferiority.\nThe size of the future trial in terms of the number of subjects on each arm.\nThe (one sided) alpha level that will be used to determine the significance of the trial.\nThe Super-superiority margin (if any) or the non-inferiority margin. The default p-value delta is not used for this QOI it is specified as part of the QOI and can be different from the default.\n\nAs with all QOIs, the future trial predictive probability QOI will be given an alternative shorter name that can be used when accessing the output files from other software such as R.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still calculate the predictive probability of the future trial being a success based on the estimated posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\n\n4.2.2 Conditional Power\nConditional power is currently available for Continuous and Dichotomous endpoints in Core and Staged Designs.\nWhen creating a Conditional Power QOI, the first specification that must be made is whether the conditional power should be calculated for the current trial or for a hypothetical future trial. The two selections are fundamentally different.\nThe Current Trial conditional power takes the information collected in the trial so far, and assumes that for the remainder of the current trial the data collected has an effect equal to the point estimate of the already collected data. The previously collected trial data is then combined with hypothetical future data and weighted based on the amount of information that has been and will be observed by the end of the trial. The result is the probability that the current trial will reject the null hypothesis at a provided alpha level under the assumption that the future patients are generated from a population having a treatment effect exactly equal to the past data’s test statistic.\nThe Future Trial conditional power also uses the information accrued in the trial so far to calculate the assumed treatment effect moving forward, but assumes that a new study would be started that does not use the data from the current trial in its analysis. The new study is assumed to be a 1:1 randomized study if there is a control arm, and a single arm study if no control, with a sample size per arm, objective, and alpha level specified at the time of QOI creation.\nIf a conditional power is ever calculated for an arm with no final response data or against a control arm with no final response data, the conditional power will be called missing (-9999 in FACTS).\n\n4.2.2.1 Current Trial Conditional Power\nWhen creating a current trial conditional power, the missingness strategy, multiplicity adjustment, final sample size, and alpha level for the significance test must be specified.\n\n\nHandle missingness using:\nMissingness handling for a continuous endpoint can be specified as:\n\nIgnore: subjects that are known dropouts are not included in the current or final sample size, and are not used to estimate the treatment effect.\nLast Observation Carried Forward (LOCF): subjects that are known dropouts have their last observed endpoint value assumed to be their final endpoint. Subjects with no early observed data are ignored. Subjects that have data to carry forward are considered complete, and are used to estimate the current treatment effect.\nBaseline Observation Carried Forward (BOCF): subjects that are known dropouts have their baseline value assumed to be their final endpoint. They are included in the current information as complete subjects and are used to estimate the current treatment effect. BOCF is only available for the continuous endpoint and when simulating baseline for subjects.\nFailure: subjects that are known dropouts are assumed to have a negative outcome at their final visit. If responses are good, the subject is a non-response, and if responses are bad, the subject is a responder. Failure is only available for dichotomous endpoints.\n\n\n\nTest Type\nThe test type can either be Unadjusted or Bonferroni. An unadjusted test will always use the specified alpha level in the significance test. The Bonferroni adjustment will divide the specified alpha value by the number of non-control arms enrolling in the study in the enrolment period leading up to the current analysis time.\n\n\nSample Size:\nThe current trial conditional power can be calculated at two different future time points.\n\nCurrent Enrollment: That no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nTrial Maximum: That the trial would continue recruiting subjects using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until they have the opportunity to complete.\n\n\n\nOne-sided Alpha\nThe threshold that the current trial would have to be less than in order to be considered a success. The success/futility values specified in the design tab are not used in the conditional power calculation. This value may be adjusted by the “Test type:” input.\n\n\nSuper-Superiority (Non-inferiority) margin for p-value:\nThis value cannot be changed on the QOI pop-up. It’s instead modified at the bottom of the QOI page. The objective of the current trial cannot be different for each individual current trial conditional power QOIs.\n\n\nAdditional Notes\nCurrently, conditional power will always assume that there is no correction applied to combine the different p-values from the analysis timepoints, i.e. no combination test is used.\nThe conditional power of the current trial at the maximum sample size is only available if the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation or deterministic allocation.\nConditional power for the current trial is calculated\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\n\n\n\n4.2.2.2 Future Trial Conditional Power\nConditional power of a future trial is reduced to a power calculation for a future trial, given the current frequentist estimates of treatment effect and standard deviation and a set of assumptions for the future trial, such as sample sizes, hypothesis to be tested, and significance levels to be used.\nThe test type of the future trial can be set to Superiority or Non-inferiority. If the superiority or non-inferiority margin is set to 0, then both types of tests simplify to traditional tests of superiority with no margin. If a non-zero margin is set, then the future trial must meet the success criteria indicated by the test type and the margin. The margin cannot be negative, but a super superiority test with a margin of -0.5 is equivalent to a non-inferiority test with a margin of 0.5.\nThe subjects per arm dictates how many subjects would be enrolled on each arm in the future study. If there is a control arm in the current trial, then the future trial is assumed to be randomized 1:1 between the active arm and the control. If there is no control arm in the current study, then the future trial is a single arm trial testing against the performance goal specified in the Freq. Comparison Response box in the QOI creation pop-up.\nThe One-sided Alpha is the significance level of the final analysis test of the future trial. The future trial is assumed to have 1 active arm, so there are no adjustments to the alpha level available.\nThe superiority margin or non-inferiority margin indicate the amount that the active arm must be better/not worse than the control arm by. If there is no control arm then this is the Freq Comparison response, which is the value that the active arms must be significantly better than in order to be declared a success. The frequentist margin for p-value QOIs on the QOI tab is not used for Future conditional power calculations – the future trial can have a different objective than the current trial.\n\n\n\n4.2.2.3 Technical Aspects of Conditional Power Calculations\nThe conditional power calculations in FACTS are all calculated similarly to Jennison and Turnbull.\nFor continuous endpoint conditional power calculations, the conditional power of the future trial is calculated using a pooled standard deviation from all arms. For dichotomous conditional power calculations, each arm has its own standard deviation based on the MLE estimate of its proportion. These decision result in the conditional power tests matching how standard p-value QOIs are calculated for continuous and dichotomous endpoints.\nThe following sections will provide formulae to calculate the conditional power in the case where there is a control arm. Simplifications for when the arm is being compared to a fixed value rather than a control arm are trivial: information fractions are only a function of the active arm standard deviation and the test statistic does not have a second sample mean subtracted from the active arm.\nThe value of \\(\\delta\\), which can be a non-inferiority margin or a super superiority margin, is always positive coming out of FACTS. To make the math more concise we can define a couple of coefficients for the \\(\\delta\\) term that allow it to be used without re-writing formulas for different hypothesis test setups. If high values of the endpoint are good, then \\(s_1 = 1\\), and if low values of the endpoint are good, then \\(s_1=−1\\). If the specified \\(\\delta\\) is a non-inferiority margin, then \\(s_2 = 1\\), and if it’s a super superiority margin then \\(s_2=-1\\).\n\nContinuous Conditional Power for the Current Trial\nLet t be the interim index that the conditional power is being computed at, and T be the time of the analysis that the conditional power is being computed for. Then \\(Z_k\\) is the test statistic of the data collected up to the current interim analysis in the study, \\(I_k\\) is the information level at the time of the interim analysis, and \\(I_K\\) is the information level at the end of the study that the conditional power is being calculated for.\nLet arm 1 be the control and arm 2 be the active arm, \\(\\bar{x_{it}}\\) be the sample mean of arm \\(i\\) at time \\(t\\), \\(\\widehat{\\sigma_{i}^{2}}\\) be the sample variance of arm \\(i\\) at time \\(t\\), \\(n_{it}\\) be the number of subjects with complete known final data on arm \\(i\\) at interim analysis \\(t\\), and \\(n_{iT}\\) be the number of subjects with complete known final data on arm \\(i\\) at the time that conditional power is being calculated for. The pooled variance estimate is \\(\\widehat{\\sigma^{2}} = \\sum_{d = 1}^{D}\\widehat{\\frac{\\sigma_{d}^{2}}{n_{dt}}}\\) where D is the total number of arms in the study.\nThen,\n\\[I_{t} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1t}} + \\widehat{\\frac{\\sigma^{2}}{n_{2t}}} \\right)^{-1}\\]\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1T}} + \\widehat{\\frac{\\sigma^{2}}{n_{2T}}} \\right)^{-1}\\]\n\\[Z_{t} = \\left( {\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{1}s_{2}\\delta \\right)\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the non-inferiority or super superiority margin.\nThen for a one-sided alpha level of \\(\\alpha\\), let \\(Z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Continuous Conditional Power for a Future Trial\nMost of the future trial conditional power calculation is the same as the above current trial conditional power. There are some simplifications.\n\\({\\overline{x}}_{it}\\) and \\(\\widehat{\\sigma_{i}^{2}}\\) are the same as in the current conditional power calculation. \\(I_t\\), the weight of the current trial Z-score, is set to 0. \\(I_T\\) is now the information at the end of the future trial, and is calculated as:\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{T}} + \\widehat{\\frac{\\sigma^{2}}{n_{T}}} \\right)^{- 1}\\]\nwhere \\(n_T\\) is the sample size per arm in the future trial and again \\(\\widehat{\\sigma^{2}}\\) is the pooled variance.\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for the Current Trial\nThe dichotomous conditional power calculations are similar in spirit to the continuous conditional power calculations. One substantial difference is in how the dichotomous conditional power tests handle a non-inferiority or super superiority margin, \\(\\delta\\). The method of calculating an appropriate test statistic for a frequentist hypothesis test is not obvious when the null scenario has a non-zero \\(\\delta\\).\nWhen there is no margin, the estimate for each treatment is simply based on the observed response proportion \\(\\widehat{p_{i}}\\) for arm \\(i\\), and the test statistic for a comparison of the control arm, \\(c\\), with dose \\(d\\) is the usual Wald test\n\\[Z_{d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}}}{\\sqrt{\\frac{\\widehat{p_{d}}(1 - \\widehat{p_{d}})}{n_{d}} + \\frac{\\widehat{p_{c}}(1 - \\widehat{p_{c}})}{n_{c}}}}\\]\nWhen there is a margin, FACTS uses the Farrington-Manning Likelihood Score test statistic to estimate quantities \\(\\widetilde{p_{d}}\\) and \\(\\widetilde{p_{c}}\\) based on the MLEs of the arm proportions governed by the constraint that \\(\\widetilde{p_{d}} - \\widetilde{p_{c}} = - s_{1}s_{2}\\delta\\). These constrained MLE estimates are used in the standard error of the test statistic. The FM test statistic is then,\n\\[Z_{FM,d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}} + s_{1}s_{2}\\delta}{\\sqrt{\\frac{\\widetilde{p_{d}}(1 - \\widetilde{p_{d}})}{n_{d}} + \\frac{\\widetilde{p_{c}}(1 - \\widetilde{p_{c}})}{n_{c}}}}\\]\nSee the PASS documentation or SAS documentation for a complete description of the calculations that go into the FM test. In the SAS documentation, note that the FM test is the same as the Miettinen-Nurminen test without including the \\(\\frac{n}{n - 1}\\) variance correction. The FM test was used rather than the MN test because as \\(\\delta \\rightarrow 0\\), the FM test converges to the simple Wald test.\nOnce the test statistic has been resolved, the dichotomous conditional power for the current trial is calculated as follows. For calculating the conditional power for arm 2 compared to the control arm, called arm 1 without loss of generality, let \\(I_t\\) be the current information amount and \\(I_T\\) be the amount of information that the conditional power is being calculated for. Then,\n\\[I_{t} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1t}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2t}} \\right)^{- 1}\\]\n\\[I_{T} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1T}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2T}} \\right)^{- 1}\\]\n\\[Z_{t} = \\left( \\widehat{p_{2}} - \\widehat{p_{1}} + s_{1}s_{2}\\delta \\right)*\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the super superiority or non-inferiority margin, and \\(n_{1t}\\) and \\(n_{2t}\\) are current number of completers on the control and active arm, and \\(n_{1T}\\) and \\(n_{2T}\\) are the number of completers that will be on the control and active arm at time that the conditional power is being calculated for. Additionally, if there is no non-inferiority or super superiority margin \\(\\delta\\), then all \\(\\widetilde{p_{*}}\\) values are equal to their corresponding \\(\\widehat{p_{*}}\\) values.\nFor a one-sided alpha level of \\(\\alpha\\), let \\(z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for a Future Trial\nMost of the calculation quantities for the future trial conditional power are the same as the current trial conditional power. The only difference is that for the future trial conditional power we discard the influence of the current trial test statistic on the final analysis, so \\(I_t=0\\). Then the conditional power calculations become:\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]"
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#p-values",
    "href": "documentation/v71/userguides/core/core.html#p-values",
    "title": "Core Designs - Shared Features",
    "section": "4.3 P-values",
    "text": "4.3 P-values\nA p-value QOI simply allows the user to specify which test type to be used (unadjusted, Bonferroni, Dunnett’s, or Trend Test), and how missing data is to be handled (ignored, LOCF, BOCF, and missing is failure). If a control arm is present, p-values are comparisons against the control arm. If there is no control arm present, p-value QOIs are comparisons against a fixed response or fixed rate that is specified at the bottom of the QOI tab (it cannot be set differently for separate QOIs).\nNote that for dichotomous endpoints p-value are calculated using the Test of Proportions, this test statistic asymptotically approaches a normal distribution, and at least 5 success and 5 failures should be observed for this to be reasonable.\nThe p-values are calculated including the default frequentist delta that has been specified at the bottom of the screen. The delta margin cannot be modified as part of the QOI, the value of the delta and nature of the test is the same for all the p-value QOIs.\nIn a TTE design with a predictor, the p-values are only calculated for the final event endpoint, not the predictors.\n\n\n4.3.1 P-values when there is no control arm\nIf there is no control arm (not currently an option in Time-to-Event), the p-value delta is instead interpreted as the objective response or response rate to compare to. The option to set the trial type is also disabled (“Superiority” is selected and the “non-inferiority option is greyed out). For a non-inferiority trial comparing to an objective response or response rate simply use the objective rate minus the non-inferiority delta (if a higher response is better) or the objective rate plus the non-inferiority delta (if a lower response is better).\nIt is currently only possible to have one objective rate to compare against.\nThe same objective rate will be used for the target p-value test in the predictive probabilities.\n\n\n\n4.3.2 Fisher-Exact Test\nWhen specifying the QOIs for a dichotomous endpoint in a trial with a control arm, the bottom of the QOI tab allows the user to specify what statistical test to use when calculating p-values. Options are “Normal approximation” (default) and “Fisher exact test”. “Normal approximation” uses a t-test and should yield similar results to a Chi-Square test without continuity correction, while “Fisher exact test” uses an exact Fisher’s exact test for 2x2 tables.\nIf “Fisher exact test” is chosen as the test type, all p-value QOIs will use a Fisher’s exact test, except for future trial predictive probabilities, which will still use a t-test.\nIf “Fisher exact test” is chosen as the test type, only “Bonferroni” and “Unadjusted” are available as multiplicity corrections and any QOIs previously created using different multiplicity corrections will be deleted.\n“Fisher exact test” is not available for non-inferiority comparisons."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#target-doses",
    "href": "documentation/v71/userguides/core/core.html#target-doses",
    "title": "Core Designs - Shared Features",
    "section": "4.4 Target Doses",
    "text": "4.4 Target Doses\nThe target dose QOIs are Bayesian posterior probabilities based on MCMC sampling of how often each dose meets the specified target criteria. There are 3 types of target specifiable\n\nMax – the dose with the maximum response, this has no parameters to specify, so is available as a pre-defined default QOI.\nMED – a Minimum Effective Dose, the lowest dose that has a response better than an absolute or relative target.\nEDq – an effective dose, the dose that achieves a specified proportion (quantile \\(q\\)) of the maximum improvement in response relative to an absolute value or the performance of a specific treatment arm."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#decision-quantities",
    "href": "documentation/v71/userguides/core/core.html#decision-quantities",
    "title": "Core Designs - Shared Features",
    "section": "4.5 Decision Quantities",
    "text": "4.5 Decision Quantities\nThe QOIs described so far have defined values to be calculated across all doses. For a Success/Futility decision to be made it is necessary to specify the treatment arm whose QOI value is to be used in comparison to the success and/or futility criteria. This selection can be done by specifying a specific arm, but normally it is specified by using a “Probability of being Target” selection criteria. A simple and common example is “The probability of the response being better than that of the Control of the Treatment arm with the maximum response”:\n\n\nA decision QOI consists of (1) a QOI that has been calculated for each dose, and (2) a method of choosing a dose to use the QOI value of. Choosing the dose can be done either by using a target Dose QOI like Pr(Max), Pr(EDq…), etc, by choosing the dose with the highest or lowest value of a QOI, or by explicitly choosing a dose level in advance.\nAs an example using a target QOI, you can imagine evaluating a decision QOI that is specified to choose the probability of being better than Control by 2 units Pr(\\(\\theta_d - \\theta_0 &gt; 2\\)) based on the arm with the highest ED90 EDq relative to control; Quantile 0.9.\nInstead of a Target Dose QOI, it is also possible to specify a specific dose, or to use the terms “Max probability over all doses” and “Min probability over all doses” which mean that the dose with the minimum or maximum value of the per dose QOI is used. This enables:\n\nDecisions QOIs testing specific doses, for example by defining a Posterior probability QOI that compares all doses against the lowest dose, and then a Decision QOI using that posterior probability with the highest dose selected as the target dose it is possible to based decisions on the posterior probability that the highest dose is better than the lowest dose.\nA Decision QOI using “Max probability over all doses” allows a decision to be based on whether a probability or p-value is greater than a threshold at any dose, or less than a threshold at all doses.\nA Decision QOI using “Min probability over all doses” allows a decision to be based on whether a probability or p-value is less than a threshold for any dose or greater than a threshold at all doses.\n\nThere is one special case: if the QOI chosen is the Trend Test p-value, then because this is not actually a per-dose value, only one “Target dose” can be selected: “Overall Significance”."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#standard-evaluation-variables",
    "href": "documentation/v71/userguides/core/core.html#standard-evaluation-variables",
    "title": "Core Designs - Shared Features",
    "section": "4.6 Standard Evaluation Variables",
    "text": "4.6 Standard Evaluation Variables\nThese 2 parameters are used across some of the default QOIs and hence specified outside the normal QOI dialog boxes.\n\nThe CSD value\nand whether absolute or relative to the Control arm\n\nthese are used in both the default “Pr(CSD)” and the “MED relative to Control” QOIs.\n\nNote that the CSD value here is designed to be usually entered as a positive value, as in earlier versions of FACTS. Its sign is automatically adjusted if “lower score means subject improvement” or if the trial is a non-inferiority trial.\nThese adjustments are not made for other user entered QOIs. The directions of comparison both for the definition of the probability to be calculated and the comparison of the resulting probability with a threshold are under the user’s control, as are whether delta’s are negative or positive. This allows the user to define QOIs in whatever fashion is natural to them and their team.\n\n4.6.1 The direction of comparison for default QOIs\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the NIM must always be a positive value. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD will need to be subtracted from the control score before comparing with the estimate of response on a treatment arm)."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#evaluation-of-bayesian-posterior-estimates",
    "href": "documentation/v71/userguides/core/core.html#evaluation-of-bayesian-posterior-estimates",
    "title": "Core Designs - Shared Features",
    "section": "5.1 Evaluation of Bayesian Posterior Estimates",
    "text": "5.1 Evaluation of Bayesian Posterior Estimates\nAt every interim and final analysis there is a Bayesian model fit to the data observed up to that point in the trial. The Bayesian model contains a dose response model and, often, a longitudinal model. In the absence of a longitudinal model, the posterior is calculated as:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)}\\]\nwhere \\(\\phi\\) is the set of parameters of the selected response model, \\(p(\\phi)\\) is the prior for those parameters, \\(y_i\\) is the final response for each subject and \\(n\\) is the number of subjects with complete data.\nWith a longitudinal model, this becomes:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)\\prod_{i = 1}^{n}{\\prod_{j = 1}^{L}{p(y_{ij}|\\psi)p(\\psi)}}}\\]\nwhere \\(\\psi\\) is the set of parameters of the selected longitudinal model, \\(p(\\psi)\\) is the prior for those parameters, \\(y_{ij}\\) is the response for each subject \\(i\\) at each visit \\(j\\) and \\(L\\) is the number of visits.\nThe posterior is evaluated using MCMC with individual parameters updated by Metropolis Hastings (or Gibbs sampling where possible), using only the \\(y_i\\) and \\(y_{ij}\\) data available at the time of the update."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#continuous-dichotomous-and-time-to-event",
    "href": "documentation/v71/userguides/core/core.html#continuous-dichotomous-and-time-to-event",
    "title": "Core Designs - Shared Features",
    "section": "6.1 Continuous, Dichotomous, and Time-To-Event",
    "text": "6.1 Continuous, Dichotomous, and Time-To-Event\nWith the exception of two dose response models specific to a dichotomous endpoint, the same dose response modeling facilities are available for all endpoints.\nLet there be D total doses including the control arm if it exists. For any endpoint, the estimate of dose response model is called \\(\\theta_d\\) for a dose \\(d \\in \\{1, \\ldots , D\\}\\).\n\nContinuousDichotomousTime-To-Event\n\n\nIn the continuous dose response models the individual response or change from baseline (if it is being used) \\(Y_i\\) of the \\(i^{th}\\) subject allocated to dose \\(d_i\\) is modeled: \\[\nY_i \\sim \\text{N}(\\theta_{d_i}, \\sigma^2)\n\\] The variance \\(\\sigma^2\\) has an inverse-gamma prior. For a description in how FACTS elicits parameterizations for the Inverse Gamma distribution, see here.\n\n\nIn the dichotomous case the final endpoint of the \\(i^{th}\\) subject who has been allocated to dose \\(d_i\\) is modeled:\n\\[ Y_i \\sim \\text{Bernoulli}(P_{d_i}) \\] where \\(P_{d_i}\\) is the probability of response for a subject on dose \\(d_i\\). The probability \\(P_d\\) is modeled on the logit scale, so \\[P_{d} = \\frac{e^{\\theta_{d}}}{1 + e^{\\theta_{d}}},\\] and \\(\\theta_d\\) is the log-odds ratio: \\[\\theta_{d} = ln\\left( \\frac{P_{d}}{1 - P_{d}} \\right)\\],\n\n\nIn the time-to-event case, the time to a subject’s response, \\(Y_i\\) is modeled as piece-wise exponentially distributed with hazard rates, \\(\\lambda_s\\), for pieces \\(s \\in \\{1,\\ldots,S\\}\\). So,\n\\[ Y_i \\sim \\text{PWExp}(\\{\\lambda_1,\\ldots,\\lambda_S\\})\\]\nfor a subject on the control arm, and\n\\[ Y_i \\sim \\text{PWExp}(\\{\\lambda_1 e^{\\theta_d},\\ldots,\\lambda_S e^{\\theta_d}\\})\\]\nfor non-control doses. Then, \\(\\theta_d\\) is the log-hazard ratio \\(\\left( \\ln\\left( \\frac{\\lambda_{s}e^{\\theta_{d}}}{\\lambda_{s}} \\right) = \\ln\\left( e^{\\theta_{d}} \\right) = \\theta_{d} \\right)\\) averaged over the observation time segments. This formulation implies a proportional treatment effect across the pieces of the piece-wise exponential.\n\n\n\nEach dose response model is parameterized in terms of \\(\\theta_d\\), but each endpoint models this parameter on a different scale. The dichotomous dose response models are on the log-odds scale, and the time-to-event endpoint models are on the log hazard ratio. When specifying a prior distribution for a continuous endpoint dose response model the expected data mean and variance determine which priors should be considered non-informative. When estimating a probability in the dichotomous case, using a prior with standard deviation above, say, 10 leads to a diffuse distribution on the log-odds scale, but results in a prior distribution on the probability scale that is heavily peaked at 0 and 1. This can lead to undesirable model results and decisions being made in small sample size situations, and numerical instability in extreme cases. Similarly on the time-to-event scale, the prior put on the log-hazard ratio \\(\\theta_d\\) is exponentiated before being multiplied by the hazard rate, so diffuse priors on the log-hazard can have unexpected modeled results. Again, time-to-event \\(\\theta_d\\) priors that have standard deviations less than about 10 are generally acceptably diffuse for most situations while avoiding edge case curiosities."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#descriptions-of-dose-response-models",
    "href": "documentation/v71/userguides/core/core.html#descriptions-of-dose-response-models",
    "title": "Core Designs - Shared Features",
    "section": "6.2 Descriptions of Dose Response Models",
    "text": "6.2 Descriptions of Dose Response Models\nThe Dose Response section of the Design tab allows the user to specify how to analyze the relationship between dose/treatment arm and the final response and hence estimate the values \\(\\theta_d\\). The interpretation of \\(\\theta_d\\) depends on the nature of the endpoint:\n\nContinuousDichotomousTime-To-Event\n\n\nWhere the response is continuous, \\(\\theta_d\\) is the estimate of the mean response/change from baseline on treatment arm \\(d\\), and the common inter-subject variance of the response \\(\\sigma^2\\), is also estimated.\nThe response on the control arm, \\(\\theta_0\\), is estimated either in the dose response model or modeled separately.\n\n\nWhere the response is dichotomous, \\(\\theta_d\\) is the estimate of the log-odds of the probability of observing a response on the treatment arm \\(d\\).\nThe response on the control arm, \\(\\theta_0\\), is estimated either in the dose response model or modeled separately.\n\n\nWhere the response is time-to-event, \\(\\theta_d\\) is the estimate of the log hazard ratio compared to the control arm on the treatment arm \\(d\\).\nWhen the primary endpoint is time-to-event, the response rate on the control arm, \\(\\lambda\\), is estimated and \\(\\theta_d\\) for \\(d\\in \\{1,2,\\ldots,D\\}\\) is the log hazard of the response rate of each treatment arm compared to the control arm, so \\(\\theta_0\\equiv 0\\).\n\n\n\nSome, but not all, of the dose response models use the effective dose strength, \\(\\nu_d\\), to model the dose response \\(\\theta_d\\). The effective dose strength is specified on the Study &gt; Treatment Arms tab. It is always fixed at 0 for the control arm (\\(\\nu_0\\equiv 0\\)).\n\n6.2.1 Independent Dose Model\nThe “Independent Dose Model” providing a simple pair-wise comparison between the study drug arms and the control arm and/or the active comparator arm. The doses are modelled as independent and normally distributed with a prior of:\n\\[\\theta_d \\sim \\text{N}(\\mu_d, \\nu_d^2)\\]\nWhere \\(\\mu_d\\) and \\(\\nu_d^2\\) are specified in FACTS and can either be the same or vary across arms.\nThis model is useful:\n\nWhen there is only one or two experimental arms\nWhen the study drug arms don’t differ by dose but in other ways, so there is no ordered sequence of treatments – e.g. each arm is the study drug in combination with a different additional drug.\nFor simulating simple trial designs\nFor simulating a ‘conventional’ or ‘strawman’ design to compare more complex designs against\n\nOtherwise for trials with multiple related arms that differ by dose, it is usually more efficient to use a dose-response model.\n\n\n6.2.2 Independent Beta-Binomial Model (Dichotomous Only)\nThis is a “no model” option similar to the above, but only available for the Dichotomous endpoint. Unlike any other dose response model, this model uses the beta distribution to model the probability of response directly rather than fitting a model to the log-odds of the probability.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(P_d)\\] where \\(P_d\\) is the probability that a patient is a response at the final endpoint for subjects randomized to dose \\(d\\). With posterior\n\\[P_d \\sim \\text{Beta}(\\alpha_d + \\text{responders}_d, \\beta_d + \\text{non_responders}_d)\\]\nWhere \\(\\alpha_d\\), \\(\\beta_d\\) are the priors for the arm \\(d\\), \\(\\text{responders}_d\\) is the number of responders on arm \\(d\\) and \\(\\text{non_responders}_d\\) is the number of non-responders on arm \\(d\\).\nThis model has the advantages of an easier to understand prior, and better estimation of \\(P_d\\) when the number of responders and non-responders is small (either is &lt; 5) compared to the log-odds model. As it’s a independent dose model, it is well suited to analyzing single arm, two arm or three arm trials, or trials where the experimental arms are unrelated, and so there is no basis on which to borrow information between them. With trials with multiple arms of the same treatment at different doses it is usually more advantageous to fit a dose response model.\n\n\n6.2.3 Simple NDLM\nThe Normal Dynamic Linear Model (NDLM) estimates the final endpoint as a smoothed curve across doses included in the model. Doses are tied directly to their nearest neighbor, and the prior expectation for a dose is that it is equal to its neighboring doses, tending to a constant dose response across doses in the absence of data. The model is defined as follows:\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately.\nThe dose response of the first dose, \\(d'\\), has a prior of:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nwhere \\(\\mu_{d'}\\) and \\(\\tau_{d'}^2\\) are specified directly in FACTS. Subsequent dose response estimates \\(\\theta_{d'+1}, \\ldots, \\theta_D\\) have priors centered at the previous dose response with variances based on the distance between the dose \\(d\\) strength and the dose \\(d-1\\) strength. Specifically,\n\\[\\theta_d \\sim N\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\text{ for } d=d'+1, \\ldots, D\\]\nwhere for dose strengths \\(\\nu_d\\) and \\(\\nu_{d-1}\\), \\(\\tau_{d-1}^2\\) is defined as \\[\\tau^2_{d-1}=\\tau^2\\left(\\nu_d-\\nu_{d-1}\\right)\\]\nThe prior distribution for the “drift” parameter, which controls the amount of smoothing is:\n\\[\\tau^{2}\\sim IG\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) and \\(\\tau_n\\) are specified in the Dose Response tab in FACTS under Model Parameters. See here for help with specifying an inverse gamma distribution with center and weight.\nIn the continuous case the residual error around the estimated dose response is\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nwhere \\(\\sigma_\\mu\\) and \\(\\sigma_n\\) are specified on the Dose Response tab in FACTS under Error Parameters.\nThe Simple NDLM is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Simple NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a ‘null’ scenario where the response on all the doses is the same as control, the simple NDLM reduces type-1 error. The estimate of \\(\\tau^2\\) tends towards zero and the estimate of the dose response tends to a flat line. The smoothing of the estimates of the response reduces the random fluctuations in the estimates from dose to dose and is a very effective counter to the problem of type-1 error inflation due to the multiplicity of doses being tested. A small prior for the mean value of \\(\\tau\\) will accentuate this effect. The price to be paid for this benefit is a tendency to underestimate large changes in response or where there is a peak in the response. This over-smoothing can reduce power. A prior for the mean of \\(\\tau\\) centered at large values will minimize over-smoothing, and a very large prior would simply remove any smoothing effect at all. A standard prior for the mean of \\(\\tau\\) would be the expected average change in response from dose to dose, scaled to take into account the differences in the specified effective dose strengths.\nUsually, the choice of prior for \\(\\tau^2\\) is tuned based on simulation results, trading off reducing type-1 error in the null case against loss of power and reduced estimate of maximum response in the effective scenario with the largest jump in response or steepest peak.\nAside: When using the NDLM model or any of its alternatives (2\\(^{nd}\\) order or Monotonic NDLM), and including doses in a design that receive no subject allocation (ie. Allocation ratio set to 0), some important characteristics of the model should be considered in the interpretation of results. First, the NDLM model “borrows” information from neighboring doses, and therefore including unallocated intermediate doses will increase uncertainty in the fitted response. Information must “pass through” these intermediate doses, and uncertainty should be expected to increase as the number of intermediate unallocated doses between two allocated doses increases. Secondly, the NDLM model estimates the response at unallocated doses, but without subject data at these doses the uncertainty in the estimate can be considerably larger than doses with subject data. This high degree of uncertainty can lead to a vestigial probability that an unallocated dose is the target dose (Pr(EDq), Pr(MED), Pr(Max)) even though the mean response estimate and probabilities at neighboring doses with subject data would not suggest this to be the case.\n\n\n6.2.4 Monotonic NDLM\nThe Monotonic NDLM endpoint model is similar to the NDLM in the sense that it smooths dose response estimates using neighboring doses, but is constrained to be either monotonically increasing or monotonically decreasing across doses. The direction of monotonicity depends on the value specified in the “Monotonic:” selection in the Model Parameters section of the Monotonic NDLM Dose Response page.\nThe use of this model is generally limited to endpoints where a strong presumption of increasing/decreasing response with dose is reasonable – such as a biomarker or a toxicity endpoint.\nLet doses \\(d = d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately. The following model is the monotonically positive NDLM:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nand\n\\[\\theta_d \\sim N^+\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\] where \\(\\tau_{d-1}^2\\) is defined as in the NDLM, and \\(X \\sim \\text{N}^+(\\mu, \\sigma^2)\\) refers to a positive truncated normal distribution with density function:\n\\[f_{X}(x) = \\frac{1 - \\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&gt;0\\]\nThe result of this dose-response model is that the curve is monotonically increasing, in that \\(\\theta_d&gt;\\theta_{d-1}\\).\nThe monotonically decreasing NDLM is similar except: \\[\\theta_d \\sim N^-\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\]\nwhere \\(X \\sim \\text{N}^-(\\mu, \\sigma^2)\\) refers to a negative truncated normal distribution:\n\\[f_{X}(x) = \\frac{\\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&lt;0\\]\nThe result of this dose-response model is that the curve is monotonically decreasing, in that \\(\\theta_d&lt;\\theta_{d-1}\\).\n\n\n6.2.5 Second Order NDLM\n\n\n\n\n\n\nNote\n\n\n\nThe second order NDLM described in this section is the version utilized in FACTS version 4.0 and later. The model labelled “Second Order NDLM” in versions before 4.0 is was maintained as the model labelled “Legacy 2nd Order NDLM” until the release of FACTS 7.1, at which time it was removed.\n\n\nThe second order NDLM serves as a “smoother” similar to the NDLM, however it tends to smooth toward a linear fit with a slope (the NDLM itself tends to shrink each dose’s fit toward its neighbors, while the second order NDLM prefers any trend in the neighbors).\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control arm or control is included in the dose response model, and \\(d'=2\\) if the control arm is modelled separately. The initial dose \\(d'\\) is modeled:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{0},\\tau^2_{0}\\right)\\]\nwhere \\(\\mu_0\\) and \\(\\tau_0^2\\) are specified directly in FACTS.\nIn the case of a time-to-event endpoint, the initial dose \\(d'\\) is the control arm, and has a \\(\\theta_{d'}= 0\\) by definition, so no prior distribution is needed.\nThe prior distribution for the dose response for the dose directly after the initial dose is specified on the difference between the \\(d'\\) and \\(d'+1\\) level doses:\n{d’+1} - {d’} N({1},^2{1})\nSuccessive doses are then modeled based on differences in slope between the dose and the two doses below them. Let:\n\\[\\theta_{d} = \\theta_{d - 1} + \\Delta_{d}\\zeta_{d} + \\frac{\\Delta_{d}}{\\Delta_{d - 1}}\\left( \\theta_{d - 1} - \\theta_{d - 2} \\right)\\]\nfor doses \\(d=d'+2,\\ldots,D\\), where \\(\\Delta_d=\\nu_d-\\nu_{d-1}\\) and \\(\\Delta_{d-1}=\\nu_{d-1}-\\nu_{d-2}\\). The priors for the dose response smoothing terms \\(\\zeta_d\\) are:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau_2^2)\\] The smoothing is determined by the parameter \\(\\tau_2\\). Small values of \\(\\tau_2\\) lead to more smoothing, while large values of \\(\\tau_2\\) lead to little information sharing between doses (above dose 1). Note that “small” and “large” is dependent on the scale of the doses. The prior on the smoothing parameter is:\n\\[\\tau_{2}^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau_2\\), and \\(\\tau_n\\) is the prior weight. See here for help with specifying an inverse gamma distribution with center and weight.\nNote that that in this formulation, \\(\\tau_2^2\\) can be thought of as the deviation from linearity. The linear scaling of deviations thus leads to a conditional variance that scales as the square of the dose difference:\n\\[\\text{Var}[\\theta_d \\mid \\theta_{d-1}, \\theta_{d-2}]=\\tau_2^2\\cdot (\\nu_d-\\nu_{d-1})^2\\]\nThe second order NDLM, like the simple NDLM, is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Second Order NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a ‘null’ scenario where the response on all the doses is the same as control the Second Order NDLM, like the Simple NDLM, tends to reduce type-1 error. As the estimate of \\(\\tau^2\\) tends to zero the estimate of the dose response tends to a line (with non-zero slope if appropriate).\nThe second order NDLM has more power than the Simple NDLM when the dose response is smooth, but will tend to shrink estimates to the control by more than the Simple NDLM if there are marked peaks or steps in the response. Because it borrows from two neighboring doses, it is better at estimating a response for a dose where there is no data than the simple NDLM can, and can be used with burn-in’s where not all doses have been allocated to, but doesn’t cope well if there are consecutive doses where there is no data.\nHowever, unlike the simple NDLMs where the variance structure is preserved if doses are added to or dropped from the design, this is not true for the 2nd order NLDM. Thus, if using the 2nd order NDLM and the doses that are available to the model are changed, then the parameters for the prior for \\(\\tau_2^2\\) may need to be re-visited.\nThe simple NDLM is likely to be preferred where there are fewer doses (4-5), and the second order NDLM is often preferred when there are 7 or more (it will depend on the expected smoothness of the response – the smoother it is the more useful the second order NDLM will be).\nAs with the simple NDLM, the choice of prior for \\(\\tau_2^2\\) can be tuned based on simulation results: balancing having a smaller prior that limits type-1 error in the null case, against having a larger prior that yields more power and better estimate of maximum response in the effective scenarios with the large jumps in response or pronounced peaks.\n\n\n6.2.6 3-Parameter Logistic\nThe 3-parameter logistic dose response model estimates efficacy as a smooth curve using an intuitive parameterization. The dose response for a dose \\(d\\) with effective dose strength \\(\\nu_d\\) is:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}}\\]\nWhere the \\(a\\) parameters have the following description:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated change in dose response for a dose with strength ∞ when compared to a dose of strength 0.\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum (\\(a_2\\))\n\n\nThe shape of the 3-parameter logistic curve can vary wildly based on the parameter values, but always starts at \\(a_1\\) at dose strength 0 and monotonically increases to \\(a_1+a_2\\) as the effective dose strength goes to infinity.\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nIn the continuous case, the prior distribution for the error term in the 3-Parameter Logistic is Inverse Gamma:\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nAn advantage of the 3-Parameter Logistic model is that it is has a small number of parameters that have defined, intuitive interpretations. Since there are few parameters it’s generally easy to estimate the parameters even on small amounts of data. Because of the limited flexibility of the logistic curve, the 3-parameter logistic model should only be used when there is evidence that it is appropriate. See the Hierarchical Logistic and Sigmoid (Emax) models for dose response models with a similar pattern, but slightly more flexibility in shape.\n\n\n\n6.2.7 Hierarchical Logistic\nThe hierarchical logistic model is an extension of the 3-parameter logistic with the form:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}} + \\zeta_{d}\\]\nwhere \\(\\zeta_d\\) is a random intercept term that modifies \\(a_1\\) differently for each dose under the constraint that all \\(\\zeta_d\\) must sum to 0.\nThe additional term \\(\\zeta_d\\) is a per dose ‘off-curve’ effect. It allows for a secondary non-parametric dose effect over and above the logistic model. It has been used in circumstances in which there is actually some falling away of effect at the highest dose. This is an example mean fit from 100 simulations. The model fit and error bars are in green, and the true dose responses are shown by the black line.\n\n\\(\\zeta_d\\) is modelled as:\n\\[\\zeta_d \\sim \\text{N}(0, a_4^2)\\]\nconditioned that\n\\[\\sum_{d}^{}\\zeta_{d} = 0\\]\nAnd \\(a_4^2\\) has an inverse gamma prior:\n\\[a_{4}^{2}\\sim IG\\left( \\frac{\\Lambda_{n}}{2},\\frac{\\Lambda_{\\mu}^{2}\\Lambda_{n}}{2} \\right)\\]\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nA typical recommended value for the center of the prior distribution of \\(\\alpha_4\\) is the largest expected likely deviation in true response from the underlying logistic with a weight of 1. The effect of this prior is then checked through simulation of a range of plausible dose response profiles, with a range of designs to check the sensitivity of the final estimate of response to this prior. Additionally, see here for help with specifying an inverse gamma distribution with center and weight.\nIn this model, compared to the ‘plain’ 3-parameter logistic above, it is important that the true ED50 lies within the expected dose range, and that the prior for the ED50, \\(a_3\\), has the majority of its probability mass in the available dose range. For example, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be:\n\\[a_{3}\\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\nUsing a weaker prior, such as \\(\\text{N}^+(\\nu_D, \\nu_D^2)\\) leads to a more linear fit. With just this change to the prior for \\(a_3\\) the average of the estimated of the mean response changes from the graph above to:\n\n\n\n\n6.2.8 Sigmoid Model\nA sigmoid model (Emax model) provides a coherent parameterization of the dose response trend similar to the 3-parameter logistic, but with one extra shape parameter, \\(a_4\\).\nThe model formula is:\n\\[\\theta_{d} = a_{1} + \\frac{(a_{2} - a_{1})v_{d}^{a_{4}}}{{a_{3}}^{a_{4}} + v_{d}^{a_{4}}}\\]\nThe interpretation of the four parameters is:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated dose response for a dose of strength \\(\\infty\\) (slight difference from Logistic models)\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum attainable effect (\\(a_2-a_1\\))\n\n\\(a_4\\)\n\ncontrols the slope of the dose response model at the ED50. A larger value of \\(a_4\\) corresponds to a steeper slope. A value of \\(a_4=1\\) makes the Sigmoid model equivalent to a Three Parameter Logistic model with \\(a_2\\) equal to \\(a_1 + a_2\\) from the Sigmoid model. A value of \\(a_4\\) approaching 0 corresponds to a dose response model that is nearly flat at \\(\\frac{a_{1} + a_{2}}{2}\\). By differentiation, it can be seen that the slope where the effective dose \\(\\nu_d=a_3\\) is \\((a_{2} - a_{1})\\frac{a_{4}}{4a_{3}}\\).\n\n\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_4, \\lambda_4^2)\\]\nThe advantage of this model is that it is considerably more flexible than the logistic, but still more efficient to estimate than an NDLM. This model (if applicable) is ideal if the target is an Effective Dose (ED) such as an ED90. Estimating an ED requires estimating the response on control, the maximum response, and the gradient which is requires a lot of data to do well if using no model or a smoothing model such as one of the NDLMs. With the 4-parameter Sigmoid model (or 3-parameter logistic – but that has a more limited applicability due to being significantly less flexible in shape), it’s usually not an issue to estimate the EDx well.\nThe caveats to using this model are:\n\nWhilst being much more flexible than the logistic, it still has model assumptions, in particular monotonicity, that must be met for this to be a reasonable model to be used.\nThe curve is only well estimated if the true ED50 lies within the doses tested.\nLike the hierarchical logistic model above, the prior for \\(a_3\\) should be weakly informative, with the majority of its probability mass in the available dose range. So similarly, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be: \\[a_3 \\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\n\n\n\n\n6.2.9 U-Shaped Model\nThe U-shaped model when the allows for an initial increase (decrease) in the dose response with dose, followed by a leveling out of the mean, then a decrease (increase). Whether the U-Shaped model increases then decreases or decreases then increases is specified by the user on the Dose Response tab. An example curve for this model is given in the figure below.\nThe dose-response curve can be characterized in four different regions of doses. For the first region, which contains the lowest doses, \\(0&lt;\\nu_d&lt;p_{min}\\), the dose-response curve is increasing (decreasing):\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( \\frac{\\nu_{d}}{p_{\\min}} \\right)^{\\alpha}\\]\nThe next region is the plateau, where the dose-response curve is constant. For \\(p_{min} &lt; \\nu_d &lt; p_{min}+p_{width}\\): \\[\\theta_d=\\theta_0 + S\\cdot\\delta\\] For the third region, the dose-response curve is decreasing (increasing). For \\(p_{min}+p_{width} &lt; \\nu_d &lt; p_{min}+p_{width} + w_{width}\\),\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( 1 - \\frac{\\nu_{d} - \\left( p_{\\min} + p_{width} \\right)}{w_{width}} \\right)^{\\beta}\\]\nFor the final region, the dose-response curve is again constant, at the same level as the zero-dose. For \\(\\nu_d &gt; p_{min}+p_{width} + w_{width}\\), \\[\\theta_d = \\theta_0\\]\nThe parameters of the model are described below:\n\n\\(S\\) is \\(1\\) or \\(-1\\), as determined by the Model is increasing/decreasing radio buttons. \\(S=1\\) if Model is Increasing is selected, indicating that the model starts increasing at low doses.\n\\(\\theta_0\\) represents the zero-strength dose response. Its prior is: \\[\\theta_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\]\n\\(\\delta\\) represents the maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[\\delta \\sim \\text{N}^+(\\mu_\\delta, \\sigma_\\delta^2)\\]\n\\(p_{min}\\) represents the lowest dose at which the maximal change in response from the zero-strength is achieved. It is restricted to be positive, and has a prior of: \\[p_{min} \\sim \\text{N}^+(\\mu_{min}, \\sigma_{min}^2)\\]\n\\(p_{width}\\) represents the width of the plateau – the region where all doses achieve maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[p_{width} ~ \\sim \\text{N}^+(\\mu_{width}, \\sigma_{width}^2)\\]\n\\(w_{width}\\) represents the width of the region for doses beyond the plateau, where response is returning to the zero-strength dose level. It is restricted to be positive, and has a prior of: \\[w_{width} ~ \\sim \\text{N}^+(\\mu_{w}, \\sigma_{w}^2)\\]\n\\(\\alpha\\) determines the rate of change of the dose response curve for doses below the plateau. Values less than \\(1\\) indicate a rapid initial change from baseline, with slower change as the plateau is reached. Values greater than \\(1\\) indicate a slow initial change from baseline, with more rapid change as the plateau is reached. To help avoid identifiability issues, \\(\\alpha\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). \\(\\alpha\\)’s prior is: \\[\\alpha \\sim \\text{LN}^*(\\mu_\\alpha, \\sigma_\\alpha^2)\\] where \\(\\text{LN}^*()\\) represents the lognormal distribution with truncation constraints at \\(10^{-1}\\) and \\(10^{1}\\).\n\\(\\beta\\) determines the rate of change of the dose response curve for doses beyond the plateau. Values less than \\(1\\) indicate a slow initial change from the plateau level, with more rapid change subsequently. Values greater than \\(1\\) indicate a rapid initial change from the plateau level, with slower change subsequently. To help avoid identifiability issues, \\(\\beta\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). The prior on \\(\\beta\\) is: \\[\\beta \\sim \\text{LN}^*(\\mu_\\beta, \\sigma_\\beta^2)\\]\n\nThe U-shaped model has seven free parameters, and thus it is inadvisable to use such a model for dose-finding trial designs that contain only a few dose levels. When a small number of doses are available, it may help to constrain the values of \\(\\alpha\\) and \\(\\beta\\) by utilizing small standard deviations in the priors.\n\n\n\n6.2.10 Plateau Model\nThe plateau model is a special case of the U-shaped model, in which \\(p_{width}=\\infty\\). That is, there is no return to baseline for high doses. This model eliminates three parameters from the U-Shaped model, since \\(p_{width}\\), \\(w_{width}\\), and \\(\\beta\\) are not used. See the figure below to see an example dose response model for the plateau model that mimics the beginning of the U-Shaped model above.\n\n\n\n6.2.11 3 Parameter Exponential Logistic (Dichotomous Only)\nThe 3-parameter exponential logistic model has the following structure:\n\\[\\theta_d = a_1 + a_2 \\nu_d^{a_3}\\]\nWhere \\(\\nu_d\\) is the effective dose strength of dose \\(d\\). This is a logistic model for the dichotomous endpoint because \\(\\theta_d\\) is the log odds ratio of the probability of the response, \\(P_d\\) at dose \\(d\\).\nThe exponent parameter \\(\\alpha_3\\) allows the change in slope at the lower end of the curve to be asymmetric in comparison to the change in curve at the upper end of the curve.\nThe priors for the parameters are:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] The interpretations of the parameters defining this model are:\n\n\\(a_1\\)\n\nthe dose response for a dose with strength 0\n\n\\(a_2\\)\n\nthe slope associated with the exponentiated dose strength\n\n\\(a_3\\)\n\na shape parameter modifying the effective dose strength through exponentiation.\n\n\nThe figure below shows an example of two different 3-parameter exponential logistic model fits. Notably, the fit shown in green has an \\(a_3\\) parameter greater than \\(1\\), which leads to faster increases of the response rate model as the effective dose strength increases.\n\n\n\n6.2.12 Hierarchical Model\nLike the Independent Dose Model, the hierarchical model treats the arms as if they are an unordered set of treatments, but unlike the Independent Dose Model the Hierarchical Model shares information between doses included in the model. The hierarchical model is a way of encouraging the estimates of arm responses to be similar, and the degree of this encouragement can be controlled using parameters of the prior distribution. The hierarchical model assumes that the arm parameters are drawn from the same normal distribution:\n\\[\\theta_d \\sim \\text{N}(\\mu, \\tau^2)\\]\nWhere \\(d\\) is the set of doses included in the model. The prior distributions for \\(\\mu\\) and \\(\\tau^2\\) are\n\\(\\mu \\sim \\text{N}(\\Lambda_\\mu, \\lambda_\\mu^2)\\)\nand\n\\[\\tau^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau\\), and \\(\\tau_n\\) is the prior weight. \\(\\tau^2\\) governs the amount of information shared between doses in the model. The estimates of the dose parameters can be encouraged to be close together by choosing a small value for \\(\\tau_\\mu\\) and a large value for \\(\\tau_n\\). See here for a tool to help understand the inverse gamma distribution specified by center and weight parameters.\nThe control arm can be included in the hierarchical model if it is desired to encourage the experimental arms to be similar to the control arm, or excluded from it if the experimental arms are encouraged to be similar to each other but not necessarily to the control arm. This is not true with time-to-event data, when the control arm can only be excluded from the hierarchical model.\n\n\n6.2.13 Linear Model\nThe linear model uses a strong assumption of exact linearity between the responses and the dose strengths. (For dichotomous models, the linearity is on the log-odds scale, and the linearity is on the log hazard ratio scale for TTE models.) The model is\n\\[\\theta_d=\\alpha+\\beta\\nu_d\\] for all doses \\(d\\) in the model. Both \\(\\alpha\\) and \\(\\beta\\) are given normal prior distributions:\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\]\nThe linear model is appropriate when there is strong scientific knowledge that the shape of the response curve should be linear as a function of dose strength. Note that if an arm has a dose strength between two other arms in the model, its response will always be estimated to be between the responses of those two arms. As a result, the linear model may not be appropriate in combination with certain decision quantities such as Pr(Max), because some doses are guaranteed to have probability zero of being the best arm. If the control arm is included in the linear model and it has the smallest dose value, then all experimental arms will always have exactly the same probability of being better than control.\nWe recommend creating scenarios where the true parameters deviate from linearity and simulating them to test the robustness of a design based on the linear model.\n\n\n\n\n\n\nNote\n\n\n\nFor dichotomous models, the linear model replaces the 2-parameter logistic model available in FACTS versions before 6.4. The two models are identical.\n\n\n\n\n6.2.14 Hierarchical Linear Model\nA more flexible version of the linear model is the hierarchical linear model. This model encourages the arm responses to have a linear-like shape, but also includes a term allowing parameters to deviate from linearity when appropriate. The arm parameters satisfy the relationship\n\\[\\theta_d = \\alpha + \\beta \\nu_d + \\zeta_d\\] where the \\(\\alpha\\) and \\(\\beta\\) parameters are as in the linear model, with prior distributions\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\] and the \\(\\zeta_d\\) parameters are the deviations from linearity. These deviations are assumed to have a normal distribution constrained so that their average value is zero:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau^2) \\text{ with } \\sum_d\\zeta_d=0.\\]\nThe prior distribution for \\(\\tau^2\\) is\n\\[\\tau^{2}\\sim\\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nIf \\(\\tau^2\\) is small, which can be encouraged by choosing \\(\\tau_\\mu\\) to be small and \\(\\tau_n\\) to be large, then the dose parameter estimates will lie close to a line. See here for help understanding FACTS’s parameterization of the inverse gamma distribution.\nThe hierarchical linear model is a good choice when, for example, one expects a linear relationship between the dose strengths and the response but one is not prepared to assume exact linearity. It can also be a good way of encouraging, but not requiring, monotonicity in the response."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#d-treatment-dose-response-models",
    "href": "documentation/v71/userguides/core/core.html#d-treatment-dose-response-models",
    "title": "Core Designs - Shared Features",
    "section": "6.3 2D Treatment Dose Response Models",
    "text": "6.3 2D Treatment Dose Response Models\nIf on the Study &gt; Treatment Arms tab, the “Use 2D treatment arm model” option has been checked, the user may either use any of the 1D Dose Response options described above, or may use a dose response model specifically modelling the two dosing dimensions.\nIf using one of the 1D dose response models, the effective dose strength \\(\\nu_d\\) is as specified by the user on the “Select doses to be used in the trial” tab. These calculated dose levels are forced to be distinct values, and this results in a 1D ordering of the combinations.\nThere are also three 2D Dose Response models that can be used:\n\n2D Continuous Factorial Model\n2D Discrete Factorial model\n2D NDLM\n\nThese are described in the next sections.\nThe 2D dose response models work with a slightly different notation to accommodate that treatments are defined as the combination of two factors. Rather than \\(\\theta_i\\) being the estimated mean of the dose response estimate for dose \\(i\\), the estimated dose response for the treatment created from row factor level \\(r\\) and column factor level \\(c\\) is denoted \\(\\theta_{rc}\\). \\(Y_{rc}\\) denotes the mean of the observed data in the cell.\nIn the continuous case, the likelihood for the data is,\n\\[Y_{rc} \\sim \\text{N}(\\theta_{rc}, \\sigma^2)\\] \\[\\sigma^{2}\\sim\\text{IG}\\left(\\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nWhere the form of \\(\\theta_{rc}\\) varies based on dose response model selection.\nSimilarly, in the dichotomous case,\n\\[Y_{rc} \\sim \\text{Bernoulli}(P_{rc})\\] \\[P_{rc} = \\frac{e^{\\theta_{rc}}}{1 + e^{\\theta_{rc}}}\\]\n\n6.3.1 2D Continuous Factorial Model\nThe 2D Continuous Factorial Model fits a regression on the dose response that uses the numeric dose strength as a continuous valued covariate. The dose response (with \\(\\eta_r\\) and \\(\\zeta_c\\) denoting dose strength of the row level and column level, respectively) is modeled as:\n\\[\\theta_{rc} = \\alpha_0 + \\alpha_1 \\zeta_c + \\alpha_2 \\eta_r + \\alpha_3\\zeta_c \\eta_r\\] With priors\n\\[\\alpha_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\] \\[\\alpha_1 \\sim \\text{N}(\\mu_1, \\sigma_1^2)\\] \\[\\alpha_2 \\sim \\text{N}(\\mu_2, \\sigma_2^2)\\] \\[\\alpha_3 \\sim \\text{N}(\\mu_3, \\sigma_3^2)\\]\nThen, \\(\\alpha_0\\) is the response at the control combination, \\(\\alpha_1\\) is the linear coefficient of the response to the column factor strengths \\(\\zeta_c\\), and \\(\\alpha_2\\) is the linear coefficient of the response to the row factor strengths \\(\\eta_r\\).\nThe user has the option to simplify the model and exclude the interaction term \\(\\alpha_3\\), which is the coefficient of the product of the two factor strengths.\nNote that despite being called “continuous” this dose response model applies to all endpoints. It’s called continuous because the coefficients act on the dose strengths as continuous measures of strength.\n\n\n\n6.3.2 2D Discrete Factorial Model\nThe 2D Discrete Factorial Model fits a regression on the dose response that uses the dose strength as a factor covariate rather than a numeric dose strength. Each dose is an independent level within each row or column factor, so any dose is equidistant from all other doses\n\\[\\theta_{rc} = \\alpha + \\gamma_r + \\beta_c\\]\nWith priors\n\\[\\alpha \\sim \\text{N}(\\mu_\\alpha, \\sigma_\\alpha^2)\\]\n\\[\\beta_c \\sim \\text{N}(\\mu_{\\beta_c}, \\sigma_{\\beta_c}^2)\\] \\[\\gamma_r \\sim \\text{N}(\\mu_{\\gamma_r}, \\sigma_{\\gamma_r}^2)\\]\nThe parameters associated with lowest level of each factor, \\(\\gamma_0\\) and \\(\\beta_0\\), are constrained to be \\(0\\).\n\n\n\n6.3.3 2D NDLM\nThe 2-D NDLM Model takes the idea of smoothing across doses that was used in the standard NDLM model, but smooths across row factors and column factors separately.\n\n6.3.3.1 The Base Model, with Control Included\nThe treatment effect for the combination of level \\(r\\) in the row factor and level \\(c\\) in the column factor is denoted as \\(\\theta_{rc}\\), and \\(Y_{rc}\\) is the observed data in that cell. The borrowing parameters are denoted as \\(\\phi\\) for the row factor smoothing, and \\(\\tau\\) for the column factor smoothing. The dose strengths are denoted as \\(\\nu_r\\) for the row factors, and \\(\\omega_c\\) for the column factors. Let \\(\\Delta \\nu_r = \\nu_r - \\nu_{r-1}\\) and \\(\\Delta \\omega_c = \\omega_c - \\omega_{c-1}\\) (for \\(r&gt;0\\) and \\(c&gt;0\\)). For notational convenience at the grid edge, let \\(\\theta_{-1, c} = 0\\), \\(\\theta_{r,-1}\\), \\(\\Delta\\nu_0\\equiv\\infty\\), and \\(\\Delta\\omega_0\\equiv\\infty\\).\nThe 2-D NDLM Model with control included in the model can then be specified as:\n\\[\\theta_{0,0} \\sim \\text{N}(\\mu_0, \\tau_0^2)\\] \\[\\theta_{rc} \\sim \\text{N}(\\mu_{rc}, \\tau_{rc}^2)\\] where\n\\[\\tau_{rc}^{2} = \\left( \\frac{1}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{1}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)^{- 1}\\]\n\\[\\mu_{rc} = \\tau_{rc}^{2}\\left( \\frac{\\theta_{r - 1,c}}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{\\theta_{r,c - 1}}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)\\]\nwith priors\n\\[\\tau^{2}\\sim\\text{IG}\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\n\\[\\phi^{2}\\sim\\text{IG}\\left( \\frac{\\phi_{n}}{2},\\frac{\\phi_{\\mu}^{2}\\phi_{n}}{2} \\right)\\]\nNote: that not all combinations of \\(r\\) and \\(c\\) will have data associated with them. However, FACTS will still evaluate the prior at all combinations. Thus, in the situation depicted below where the grayed-out point indicates a treatment combination with no allocation, \\(\\theta_{1,2}\\) is not modeled conditioned only on \\(\\theta_{1,1}\\). \\(\\theta_{0,1}\\) also informs on \\(\\theta_{1,2}\\) via \\(\\theta_{0,2}\\).\n\n\n\n6.3.3.2 Fix smoothing ratio for row factor and column factor\nOptionally, the user might want to specify the row factor borrowing as a function of the column factor borrowing:\n\\[\\phi\\equiv k \\cdot \\tau\\] where \\(k\\) is a user-specified constant. This can be performed by selecting “Fixed smoothing ratio” in the \\(\\phi^2\\) prior specification area.\n\n\n6.3.3.3 Control not in model, no zero-level doses\nIf neither treatment arm allows zero-level doses (e.g. like one would expect if the column factor represented doses and the row factor represented frequency), then a prior for the lowest dose would instead be needed as input from the GUI:\n\\[\\theta_{1,1} \\sim \\text{N}(\\mu_1, \\tau_1^2)\\]"
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#baseline-adjusted-dose-response-models-continuous-only",
    "href": "documentation/v71/userguides/core/core.html#baseline-adjusted-dose-response-models-continuous-only",
    "title": "Core Designs - Shared Features",
    "section": "6.4 Baseline Adjusted Dose Response Models (Continuous Only)",
    "text": "6.4 Baseline Adjusted Dose Response Models (Continuous Only)\nIf a baseline endpoint is simulated, the user has the option of adding a linear covariate effect to the dose response model. If the chosen dose response model states that for dose \\(d\\), \\[Y\\sim \\text{N}(\\theta_d, \\sigma^2)\\], then the distribution including the baseline adjustment term is \\[Y\\sim \\text{N}(\\theta_d+\\beta Z, \\sigma^2)\\]. where \\(Z\\) is the standardized baseline value \\(\\left(Z=\\frac{X-\\bar X}{s_x}\\right)\\), and \\(\\beta\\) is an estimated parameter that is distinct from any parameter called \\(\\beta\\) within the dose response model.\nThe baseline adjustment model uses a normal prior for \\(\\beta\\) for which the user enters a mean and standard deviation.\n\n\n\n\n\n\nTechnical Note\n\n\n\nThe VSR based simulation of baseline is more general than the model adjustment for baseline (this is to allow baseline to be incorporated in a number of different ways). This means that the parameters entered in the VSR (for example the \\(\\beta\\) parameter) will not always match the corresponding parameter estimated in the dose response model.\nAs a simple example, suppose we enter into the VSR response \\(Y\\sim \\text{N}(\\mu_Y, \\sigma_Y^2)\\), baseline \\(X\\sim \\text{N}(\\mu_X, \\sigma_X^2)\\), and use the baseline adjustment (\\(c\\), \\(s\\), and \\(\\beta_{VSR}\\) are user inputs) so the actual simulated response \\(Y\\) is \\(Y^{'} = Y + \\beta_{VSR}\\frac{X - c}{s}\\).\nIf the values of \\(c\\) and \\(s\\) used in the baseline adjustment are the same as the mean \\(\\mu_X\\) and standard deviation \\(\\sigma_X\\) of the simulated baseline, then the \\(\\beta_{VSR}\\) will converge to the estimated \\(\\beta\\) parameter as the sample size of the study increases. If \\(c\\ne\\mu_X\\) or \\(s\\ne\\sigma_X\\), then the estimated \\(\\beta\\) will not converge to the \\(\\beta_{VSR}\\) used in data simulation. Additionally, if the baseline values are simulated from a truncated normal distribution, then the estimated \\(\\beta\\) will not converge to \\(\\beta_{VSR}\\)."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#control-and-comparator-priors",
    "href": "documentation/v71/userguides/core/core.html#control-and-comparator-priors",
    "title": "Core Designs - Shared Features",
    "section": "6.5 Control and Comparator Priors",
    "text": "6.5 Control and Comparator Priors\nFor most dose response models the control arm can be modeled either as part of the dose response model or separately. If it is modeled separately, it may have a simple user specified Normal prior or a “historical” prior. A historical prior in FACTS is a hierarchical prior that models the response on control as coming from a distribution that also contains trial outcomes from external historical studies.\n\nThe active comparator is always modeled separately, and as with a control arm modeled separately, it can be modeled with a user specified Normal prior or a “historical” prior.\nWhen a user specified Normal prior is used, the user specifies the mean and standard deviation of the prior normal distribution for \\(\\theta_0\\). This is useful if the control response is not thought to be consistent with the model being used to model the study doses – for instance if using an NDLM and there might be a sharp step in response from control to the lowest dose.\nIf “historical” prior is selected for either the control arm or an Active Comparator arm, an “Augmented Priors” tab is created."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#inverse-gamma-priors",
    "href": "documentation/v71/userguides/core/core.html#inverse-gamma-priors",
    "title": "Core Designs - Shared Features",
    "section": "6.6 Inverse-gamma priors",
    "text": "6.6 Inverse-gamma priors\nFACTS uses inverse-gamma priors for parameters of variance – these are conjugate and allow efficient computation and avoid problems of convergence. Andrew Gelman’s 2006 paper however notes a potential problem with this model, the problem is specifically\n\nWhen updating the estimate of the variance of a hierarchical model parameter there will be typically relative few actual data observations (e.g. relatively few historic studies for estimating the variance of the hyper parameter in a Bayesian Augmented Control model for the response on a control arm, and relatively few observations of the change in response from one dose to the next when using an NDLM dose-response model).\nThe conventional ‘non-informative’ gamma-prior of IG(0.001, 0.001) has an effect when the observed variance is small, of over-shrinking the posterior estimate of variance.\n\nIn FACTS this possibility arises in the dose response models in the context of priors for the \\(\\tau\\) parameter for the NDLM models and the \\(a_4\\) parameter for the hierarchical logistic dose response model, where the number of observations is the number of doses or dose intervals.\nTo avoid the problem reported by Gelman we recommend using a weakly informative prior. Using the settings that control how the inverse-gamma distribution is parameterized (Settings &gt; Options &gt; Gamma Distribution Parameters), use the ‘center and weight’ options and use a weight of 1, with a ‘reasonable’ expectation for the upper limit difference in the values being modeled entered as the center for the SD. The inverse gamma distribution is the prior of the variance, so the center parameter being specified is more correctly an expectation of the square root of the mean of the variance.\n\n\n\n\n\n\nHelp with Inverse Gamma specification\n\n\n\nWe have created a tool that can help visualize the inverse gamma distribution and relate the center/weight parameterization, which FACTS uses by default, to the alpha/beta parameterization that is common.\nSee here.\n\n\nThis ‘reasonable’ upper limit for the difference is a value that a clinical team will usually have an intuition about: for the largest change in mean response from one dose to another for instance, it is (often)[## “If the trial has doses that are more closely spaced than usual, a smaller figure can be used.”] reasonable to assume that the upper limit for the expected change in response from one dose to the next is the ‘expected difference in effect size’ that might have been used to power the trial in a conventional setting.\nIn the dose response setting the smoothing parameters like \\(\\tau\\) in the NDLM are nuisance parameters in the sense that they are necessary to estimate in order to get appropriate estimates of the dose response values, but are rarely of inferential interest on their own. This can ease some of the undue burden of setting a prior on \\(\\tau\\). If it gives estimates of the dose responses that look like they should, then it is generally good enough."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#handling-missing-data",
    "href": "documentation/v71/userguides/core/core.html#handling-missing-data",
    "title": "Core Designs - Shared Features",
    "section": "6.7 Handling Missing Data",
    "text": "6.7 Handling Missing Data\nFACTS allows a “pre-processing” step before fitting the dose response model that helps with handling of data that is missing due to dropouts. If a subject has incomplete data due to a dropout, the user may specify all dropouts have their unknown final endpoint treated as known with the following options:\n\nBOCF (continuous only, requires baseline be simulated) – All dropouts are assumed to have a final endpoint equal to their observed baseline value.\nLOCF (continuous or dichotomous, requires longitudinal data be present) – All dropouts are assumed to have a final endpoint equal to their last observed visit value. If no post-baseline visits are available, but the subject has a baseline visit value, then the baseline values is carried forward to their final endpoint.\nMissing is failure (dichotomous only) – All dropouts are assumed to be failures (which may be coded as 0 or 1 depending on whether a response is considered a success).\n\nSubjects who are imputed in this pre-processing step have final endpoint values known and used for the purposes of estimating the dose response curve. However, these pre-processed final endpoint values are not used in the updating on the longitudinal model, which is based only on observed visit data.\nIf the user does not specify one of the dropout imputation methods specified above, the dropout subjects and incomplete subjects (subjects who have not reached their final endpoint but are still continuing in the study) will have their final endpoints multiply imputed using Bayesian Multiple Imputation, described in the Longitudinal Modeling section.\nGenerally, patients with “no data” do not affect the posterior distribution, and thus are omitted from the analysis. However, one must take into account a subject can have no visit data but still have “data” based on these dropout imputation methods. For example, if one selects “missing as failure” and a patient drops out before any visit data is recorded, then the subject still supplies information through the dropout imputation (similarly for BOCF). However, if LOCF is selected and no visit data is available, there remains no information on the subject to be used for the LOCF dropout imputation, and thus these subjects are omitted from the analysis. All subjects are included if they either 1) have some visit data available, or 2) are dropouts before visit 1 with sufficient information to impute their final endpoint with this pre-processing step.\n\n6.7.1 Time-to-Event Missingness\nFor a time-to-event endpoint, as is the convention in this endpoint, subjects are observed up to the time they have an event (or the end of follow up), so no subject drops out after having had an event. Subjects that do drop-out are included in the analysis as not having had an event up to the last observation of the subject."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#setting-priors-for-hierarchical-model-hyper-parameters",
    "href": "documentation/v71/userguides/core/core.html#setting-priors-for-hierarchical-model-hyper-parameters",
    "title": "Core Designs - Shared Features",
    "section": "7.1 Setting Priors for Hierarchical Model Hyper Parameters",
    "text": "7.1 Setting Priors for Hierarchical Model Hyper Parameters\nUnless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.\nIn this case the following would be reasonable:\n\nSet the prior mean value for Mu as the mean of the mean responses on the control arm in the historic studies\nSet the prior SD for Mu equal to the average SD of response divided by the square root of the number of studies.\nSet the center for tau to the same value as the prior SD for Mu.\nSet the weight for tau to be &lt; 1.\n\nOne can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small (relative to the number of studies), then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.\nTo give some prior preference towards pooling or separate analysis the weight for tau has to be large (relative to the number of historic studies) – to have a design that is like pooling all the historic studies the mean for tau needs to be small (say 10% or less of the value suggested above). For there to be no borrowing from the historic studies the value for tau needs to be large (say 10x or more the value suggested above).\nThe best way to understand the impact of the priors is try different values and run simulations."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#bayesian-augmented-control-bac-example",
    "href": "documentation/v71/userguides/core/core.html#bayesian-augmented-control-bac-example",
    "title": "Core Designs - Shared Features",
    "section": "7.2 Bayesian Augmented Control (BAC) Example:",
    "text": "7.2 Bayesian Augmented Control (BAC) Example:\nIt is easiest to study the impact of BAC in a simplified version of the trial being designed, for example use a 2 arm fixed design with ‘no model’, no adaptation, no longitudinal modeling, etc.\nFor instance, in a continuous setting with an expected sample size of 50 on control, a mean response of 5 and an SD in the response of 2, looking at the effect of having 4 prior, similarly sized studies:\n\n\n\nExternal Study Sufficient Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of subjects\n\n\nMean Response\n\n\nSD of Response\n\n\n\n\n\n\nStudy 1\n\n\n50\n\n\n4.76\n\n\n2\n\n\n\n\nStudy 2\n\n\n50\n\n\n4.93\n\n\n2\n\n\n\n\nStudy 3\n\n\n50\n\n\n5.07\n\n\n2\n\n\n\n\nStudy 4\n\n\n50\n\n\n5.24\n\n\n2\n\n\n\n\nFor simplicity we have just varied the means of the studies – they are all consistent with a normally distributed estimate, with mean of 5 and a standard error of \\(\\frac{2}{\\sqrt{50}}\\).\nBy simulating a fixed trial with a sample size of 100 and fixed 1:1 allocation between control and a study arm we can easily observe the effects of a BAC on the control arm. Creating 6 VSR profiles with sigma = 2, and the same mean response for control and the study arm in each profile of: 4.53, 4.76, 5.07, 5.24 and 5.47 we can look at the bias of the final estimate and the shrinkage in the SD of the estimate that could be brought about by using BAC:\n\n\n\nQuick simulation study of how the hierarchical model for BAC effects estimates of the control rate under different true control rate scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Mean\n\n\nRaw mean\n\n\nRaw SD\n\n\nEstimate inc BAC\n\n\nSD inc BAC\n\n\nBias\n\n\nEffective additional Subjects\n\n\n\n\n\n\n4.53\n\n\n4.55\n\n\n0.28\n\n\n4.64\n\n\n0.255\n\n\n2.1%\n\n\n11.1\n\n\n\n\n4.76\n\n\n4.78\n\n\n0.28\n\n\n4.83\n\n\n0.250\n\n\n1.0%\n\n\n13.5\n\n\n\n\n4.93\n\n\n4.95\n\n\n0.28\n\n\n4.96\n\n\n0.248\n\n\n0.2%\n\n\n14.3\n\n\n\n\n5.07\n\n\n5.09\n\n\n0.28\n\n\n5.07\n\n\n0.248\n\n\n-0.4%\n\n\n14.2\n\n\n\n\n5.24\n\n\n5.26\n\n\n0.28\n\n\n5.20\n\n\n0.250\n\n\n-1.1%\n\n\n13.2\n\n\n\n\n5.46\n\n\n5.49\n\n\n0.28\n\n\n5.38\n\n\n0.255\n\n\n-1.9%\n\n\n10.7\n\n\n\n\nNote it is important to use the column “SD Mean resp 1” from the simulations file to see the SD of the estimate and not use the SD of the estimates in the summary table.\nThe small shrinkage in the SD is more impressive when viewed in terms of how many additional subjects would have to be recruited to achieve the same shrinkage in the raw estimate. These are useful savings for small additional bias.\nThe “effective additional subjects” was calculated: \\[\\left( \\frac{\\text{True sigma}}{\\text{AVG(SD Mean resp)}} \\right)^{2} - \\left( \\frac{\\text{True sigma}}{\\text{AVG(SE Mean Raw Response)}} \\right)^{2}\\] where in this example \\(\\text{True sigma}\\) was 2."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#continuous-endpoints-1",
    "href": "documentation/v71/userguides/core/core.html#continuous-endpoints-1",
    "title": "Core Designs - Shared Features",
    "section": "8.1 Continuous Endpoints",
    "text": "8.1 Continuous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing unadjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\n(p-value)[## “1-sided p-value is reported in all the frequentist results. This is done in order to be consistent with comparisons with 1-sided α-values elsewhere.”],\nconfidence interval for the mean difference,\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nIf selected, using Dunnett-adjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\np-value,\nconfidence interval for the mean difference\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nIf neither placebo nor an active comparator are simulated, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out.\nIf high values of the endpoint are good, P-values are calculated using a one-sided t-test testing \\[H_0: \\mu_T &lt; \\mu_C\\] against \\[H_1: \\mu_T \\ge \\mu_C\\] with \\(\\mu_T\\) being the true treatment response mean and \\(\\mu_C\\) being the true control response mean. If low values of the endpoint are good, then the signs of the hypotheses are flipped."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#dichotomous-endpoints-1",
    "href": "documentation/v71/userguides/core/core.html#dichotomous-endpoints-1",
    "title": "Core Designs - Shared Features",
    "section": "8.2 Dichotomous Endpoints",
    "text": "8.2 Dichotomous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing the methodology described by Agresti, Mee and Nurminem for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nIf checked, using Dunnett-adjusted dose-placebo comparisons for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nP-values are calculated by user choice either using a normal approximation (see previous section on p-value calculations for continuous endpoints) or a Fisher’s exact test, which is a conservative test to test the null hypothesis that the outcome is independent of the treatment assignment. For the normality approximation to have good asymptotic coverage, the expected number of successes and failures under the null hypothesis should be greater or equal to 5 for both groups. In cases where this is not guaranteed, Fisher’s exact test is recommended when strict type 1 error control is required.\nIf neither placebo nor active comparator are specified, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#time-to-event-frequentist-analysis",
    "href": "documentation/v71/userguides/core/core.html#time-to-event-frequentist-analysis",
    "title": "Core Designs - Shared Features",
    "section": "8.3 Time-to-Event Frequentist Analysis",
    "text": "8.3 Time-to-Event Frequentist Analysis\nFor each simulated trial, the following frequentist analyses will be performed:\n\nDose-placebo comparisons based on log-rank test and the Cox proportional hazards model. Summaries include:\n\nThe log-rank and Wilcoxon test statistics and the corresponding p-values,\nEstimated hazard ratio and its confidence interval from Cox model,\nFor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were, based on the hazard ratio inference from the Cox model).\n\nMedian survival times based on the Kaplan-Meier method.\nFor the predictor, descriptive statistics are computes – a 7 number summary for a continuous predictor, percentages for the dichotomous predictor, and median time, average hazard, and a cox model with predictor as covariate are computed for a time to event predictor.\n\nThe following variations for adjusting the alpha level for multiple comparisons should be available using the user-specified 1-sided alpha: unadjusted, and Bonferonni adjusted."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#multiple-imputation",
    "href": "documentation/v71/userguides/core/core.html#multiple-imputation",
    "title": "Core Designs - Shared Features",
    "section": "9.1 Multiple Imputation",
    "text": "9.1 Multiple Imputation\nUnless a deterministic method is used such as LOCF or BOCF, longitudinal models inform the dose response estimates by using the longitudinal model to stochastically impute subjects’ final endpoint data when it’s not been directly observed.\nFirst, data from subjects with both intermediate and final observations is used to estimate the parameters of whatever longitudinal imputation model has been selected.\n\nSubjects with missing final data have final data sampled from the posterior distribution of the longitudinal model given the subjects most recent intermediate visit (or in some models, all their available intermediate visit data). Subjects with no final or intermediate data have final data sampled from the posterior distribution of the dose response model given the dose arm the subject was allocated to.\n\nOnce every randomized subject has either a real known final endpoint or an imputed final endpoint, the dose response model is re-estimated using that complete dataset.\n\nThis impute-then-fit-dose-response process is built into the MCMC estimation sampling loop. Each time we draw a new set of parameters from the longitudinal model they are used to impute the final endpoint of incomplete subjects. These subjects then inform the dose response model. Then we get a new sample from the longitudinal model and so on. The longitudinal models, with one exception, are not conditioned on the dose response models.\nThe missing final endpoint values are imputed with the uncertainty in the longitudinal model, and, as a result, the dose response model is estimated including both the uncertainty in the longitudinal model and the usual uncertainty in its parameters.\nAbove, it says “fit the dose response model,” as a step in the iterative process. The idea is that once you create a dataset through imputation you should update the dose response model MCMC chain until it converges. By default only 1 step is taken on the dose response model MCMC chain. It is safer, especially if doing a real analysis, to allow the dose response model parameter chains to converge slightly before imputing the missing data again. This can be done on the MCMC settings control on the Simulation tab and setting the “Samples per Imputation” parameter. As a rough guide, if it at some early interims &gt; 5% of the data being analyzed will be imputed, a value in the range 2 to 10 is recommended to avoid underestimating the uncertainty. A higher number should be used the greater the proportion of imputed data.\nA similar procedure is used when imputing event times based on a predictor endpoint in FACTS Core Time-to-Event.\n\n\n\n\n\n\nComputational Note\n\n\n\nFACTS is not fitting a joint Bayesian model of the longitudinal and dose response models. This would require a full MCMC fit of one model for every MCMC step of the other. Thus, if taking 2,500 samples, we would require a total of 2,5002 samples. This would make running simulations with longitudinal model prohibitively expensive."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#how-many-longitudinal-models",
    "href": "documentation/v71/userguides/core/core.html#how-many-longitudinal-models",
    "title": "Core Designs - Shared Features",
    "section": "9.2 How many longitudinal models?",
    "text": "9.2 How many longitudinal models?\nWhen specifying a longitudinal model in FACTS, the user must clarify whether the longitudinal model is shared across all doses, or if different arms are to estimate different functions connecting early endpoints to the final endpoint.\nThe options that may be selected for the number of model instances are:\n\n“Single model for all arms” – just one set of longitudinal parameters is estimated using all the subject data regardless of their treatment.\n“Model control separately” – two sets of longitudinal parameters are estimated, one using data from just those subjects on the control arm, and the other using all the subject data from all the other arms.\n“Model comparator separately” (only available if there is an active comparator arm) - two sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, and the other using all the subject data from all the other arms (control and active treatments).\n“Model control and comparator separately” (only available if there is an active comparator arm) - three sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, one using data from just those subjects on the control arm, and the third using all the subject data from all the other arms.\n“Model all arms separately” – a set of longitudinal parameters is independently estimated separately for each arm, each set estimated just using the data available from the subjects on that arm.\n\nThe fewer models used, the more data is pooled, the more precisely the model parameters can be estimated, and the more informative the intermediate data can be. Pooling data assumes that subjects in different groups have the same longitudinal profile, although they may still have different magnitude or probability of response (estimated by Dose Response Model).\nIf the response profile could be different on different treatment arms – for example, if subjects on control recover constantly over time, but those on an effective dose of the study drug recover very quickly early on, and more gradually later in their follow-up - a longitudinal model based on pooled data would tend to over-estimate the final endpoint of subjects on an effective dose of the study drug when only their early data was available. In a case like that, it would be better to use separate models. If the rapid early response was likely on all study drug treatment arms, then possibly use “model control separately.” If, however, the rapid response will only be seen on some study drug arms then use “model all arms separately”.\nIn addition to declaring how many longitudinal models should be estimated, when there is more than one model instance the user must declare if the prior distributions should be specified differently across each model instance or not. The options that may be selected for prior specification (for all LMs but linear regression) are:\n\nSame priors across all model instances\n\n\nEach instance of the model has identical prior distributions on its parameters. Estimation is still performed independently on each model instance.\n\n\nSpecify priors per model instance\n\n\nEach instance of the model has its own priors that may vary across instances.\n\nThe linear regression longitudinal model has a different parameter set estimated for each visit, so its prior specification rules are slightly different. See the linear regression section below for specifics about prior specification specific to this model."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#longitudinal-models-for-a-continuous-endpoint",
    "href": "documentation/v71/userguides/core/core.html#longitudinal-models-for-a-continuous-endpoint",
    "title": "Core Designs - Shared Features",
    "section": "9.3 Longitudinal Models for a Continuous Endpoint",
    "text": "9.3 Longitudinal Models for a Continuous Endpoint\n\n9.3.1 LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {yit} is the set of observed responses from early visits, and yitm is the last observed value of y*i**t, then the LOCF model for the final endpoint Yi* is\nYi|{yit} = yitm\nIn the continuous engine tm can be any earlier observed visit including the baseline value.\n\n\n9.3.2 Linear Regression\nThe linear regression model fits a simple linear model from the data at each visit with the final visit\nYi|yit ∼ αt + βtyit + N(0, λt2)\nThe parameter \\(\\alpha\\)t is the intercept of the model for visit t, and the parameter βt is a multiplicative modifier (slope) of the response observed longitudinal at visit t to adjust the prediction of the final endpoint.\nImputation of the final endpoint value for a subject using the linear regression longitudinal model is based on only the latest observed visit’s endpoint value.\nThe default setting of “Same priors across all model instances and visits,” implies that each parameter \\(\\alpha\\), β, and λ have the same prior for all t. Estimation of those parameters is still done independently for each model instance. The one prior across all model instance are formulated as:\nαt ∼ N(αμ, ασ2)\nβt ∼ N(βμ, βσ2)\n\\[\\lambda_{t}^{2}\\\\\\sim\\\\IG\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nThe above prior formulation may not be desirable if specifying priors that are not extremely diffuse – especially on the β parameters. Instead, selecting “Specify priors per visit across all model instances,” will share the prior specification across all instances of the model, but allows for different priors to be put on the parameters associated with each visit. The user inputted prior parameters are now subscripted with t to denote the visit they correspond to. These priors apply to all model instances:\nαt ∼ N(αμt, ασt2)\nβt ∼ N(βμt, βσt2)\n\\[\\lambda_{t}^{2}\\\\\\sim\\\\IG\\left( \\frac{\\lambda_{n_{t}}}{2},\\frac{\\lambda_{\\mu_{t}}^{2}\\lambda_{n_{t}}}{2} \\right)\\]\nIt is also possible to specify priors “Per model instance and visit,” in which every visit has separate priors, and those differing priors vary across model instances. This is the most flexible prior specification method. The user inputted prior parameters are now subscripted by both t for visit and i for model instance.\nαti ∼ N(αμti, ασ*t**i*2)\nβti ∼ N(βi, βσti2)\n\\[\\lambda_{ti}^{2}\\\\\\sim\\\\IG\\left( \\frac{\\lambda_{n_{ti}}}{2},\\frac{\\lambda_{\\mu_{ti}}^{2}\\lambda_{n_{ti}}}{2} \\right)\\]\nA potential starting place for non-informative prior values would be\n\n\\(\\alpha\\) mean of 0, SD &gt;= largest expected response\nβ mean of either 0 or (final visit time / early visit time), SD &gt;= largest expected ratio of final visit to first visit\nλ mean of expected SD of the endpoint (‘sigma’), weight of 1. The variability of the prediction from the longitudinal model (based on an observed intermediate response) should be less than that based solely on the treatment allocation, thus this is a weakly pessimistic prior on the effectiveness of the longitudinal model that should be quickly overwhelmed by the data.\n\nThis model is easy to understand and can be used even if there is only one visit, but doesn’t get more powerful if there are more visits. Its power will depend on the degree of correlation between the intermediate visit response and the final response.\n\n\n9.3.3 Time Course Hierarchical\nThe Time Course Hierarchical models the relationship between subjects’ early responses and their final response. It incorporates a per-subject offset from the mean response and a scaling factor from each visit to the final endpoint, but with no model of the change from visit-to-visit.\nThe response at the tth visit for the ith subject, having been randomized to the d*t**h* dose is modeled as:\ny*i**t ∼ eαt(θd + \\(\\delta\\)i) + N(0, λt*2)\nThe imputed final response (visit T) for the ith subject, having been randomized to the dth dose is modeled as:\nY*i**T ∼ θd + \\(\\delta\\)i + N(0, λT*2)\n(i.e. \\(\\alpha\\)T is 0).\nThe model parameters can be interpreted as follows:\nθd is the estimated mean response at the final visit in dose d from the dose response model.\n\n\\(\\delta\\)i is the estimated patient level random effect around the mean final response (θd) for the dose d that patient i is randomized to.\n\\(\\alpha\\)t is a scaling parameter that determines the proportion of the final response that is observable at visit t. A value of \\(\\alpha\\)t = 0 indicates that the expected value of early visit t is equal to the estimated final visit mean θd. A value of \\(\\alpha\\)t = −0.69315 indicates that the expected value of early visit t is 50% of the estimated final visit mean θd.\nλt2 is the variance of the endpoint around the estimated mean response at visit t.\n\nThe prior for \\(\\alpha\\)t is a normal distribution with a user specified the mean and standard deviation:\nαt ∼ N(αμ, ασ2),\nThe prior for the \\(\\delta\\)i terms is a normal distribution with a mean of 0 and variance τ2.\n\\(\\delta\\)i ∼ N(0, τ2)\nτ2 is estimated as part of the longitudinal model, and has an inverse gamma prior distribution with prior central value τμ and weight (in terms of “equivalent number of observations”) τn:\n\\[\\tau^{2}\\\\\\sim\\\\IG\\left( \\frac{\\tau_{n}}{2},\\\\\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nThe prior for the λt2 terms is an inverse gamma distribution with prior central value λμ and weight (in terms of “equivalent number of observations”) λn:\n\\[\\lambda_{t}^{2}\\\\\\sim\\\\IG\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be\n\n\\(\\alpha\\)t mean of -2, SD of 2, … so the prior ~70% interval for \\(\\alpha\\)t is between -4 and 0 (+/1 1 SD) and thus the prior 70% interval for eαt to be between 0.02 and 1.\nτ mean set to the expected SD of the endpoint (‘sigma’), with a weight of 1.\nλt mean set to the expected SD of the endpoint (‘sigma’), with weight of 1.\nWe would expect τ2 + λ2 ≈ σ2, thus to specify a prior mean of σ for each with a weight of 1 is a weakly pessimistic prior that should be quickly overwhelmed by the data.\n\nThis model is useful if there is thought to be a significant per-subject component to the response that should be manifest at the intermediate visits, and sufficient visits for the per-subject component to be estimated.\n\n\n9.3.4 Kernel Density\nThe Kernel Density Model longitudinal model is a non-parametric re-sampling approach that is ideal for circumstances where the relationship between the interim time and the final endpoint is not known or not canonical.\nThe procedure is as follows. Assume an interim value for patient i at time t, Y*i**t. Patient i* does not have an observed final endpoint at time T, so one is to be imputed. Let (X1t, X1T), …, (Xnt, XnT) be the set of values for the previous subjects for whom there exists an interim value X*t and final value X*T.\nTo impute a value of YiT given Yit, a pair (Xkt, XkT) is selected with probability based on the pair’s time t visit response’s proximity to the observed Y*i**t*:\n\\[\\Pr\\left( Selecting\\\\\\left( X_{kt},\\\\X_{kT} \\right) \\right) = \\frac{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}{\\sum_{k = 1}^{n}{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}}\\]\nThen, a value of YiT is imputed from the following distribution, which uses the selected pair’s final endpoint response XkT:\nYiT ∼ N(XkT, hXT2)\nThe bandwidths hXt and hXT are selected based on the criterion given by Scott (1992). That is,\n\\[h_{X_{j}} = \\sigma_{X_{j}}\\\\\\left( 1 - \\rho^{2} \\right)^{\\frac{5}{12}}\\\\\\left( 1 + \\\\\\frac{\\rho^{2}}{2} \\right)^{- \\frac{1}{6}}{\\\\n}^{- \\frac{1}{6}}\\\\\\\\\\\\\\\\\\\\\\\\for\\\\j = t\\\\and\\\\T\\]\nwhere σXj is the standard deviation of the observed responses at time j, n is the number of pairs (X*t, X*T) that were chosen between, and ρ is the correlation coefficient between Xt and XT in the pairs (X1t, X1T), …, (Xnt, XnT).\nThe Kernel Density model does not take prior distributions as input. So long as each early visit has more subjects with an observed early response and final response than the value entered in “Kernel minimum number of subjects:” then this algorithm runs without regard for user input.\nIf any visit has fewer subjects with early data and final data than the value of “Kernel minimum number of subjects:”, then instead of calculating the values of hXt or hXT the input values of Kernel bandwidth and standard deviation of prior mean are used.\nPossible starting values for these parameters are:\n\nhx and hy the expected SD of the endpoint (‘sigma’)\nThe minimum number of subjects completed: 6\n\nThe Kernel Density model is effective and flexible with no model assumptions, but its computational overhead is large. Simulations may take ~10 times longer to run, and having no prior model there has to be ‘in trial’ data before it can come into play.\n\n\n9.3.5 ITP\nThe ITP (Integrated Two-component Prediction) model fits an observation for patient i on dose d at visit t as:\n\\[y_{idt} = \\left( \\theta_{d} + s_{id} + \\epsilon_{idt} \\right)\\left( \\frac{1 - \\text{exp}\\left( kx_{idt} \\right)}{1 - \\text{exp}(kX)} \\right)\\]\nwhere  ϵidt ∼ N(0, λ2)\ns*i**d ∼ N(0, τ*2)\nand θd is the mean estimate of the final endpoint for dose d using all complete and partial data and assuming an independent dose response model on the doses. s*i**d is a subject specific random effect, k* is a shape parameter, xidt is the time yidt is observed, X is the time to final endpoint, and each ϵidt is a residual error.\nThe ITP model is similar to the Time Course Hierarchical above in that both models estimate subject specific component of the response. The biggest difference between the two is that the ITP models the response change over time as a parametric function based on the parameter k, rather than having a separately estimated eαt for each visit.\nThe shape parameter k determines the rate at which the final endpoint’s eventual effect is observed during a subject’s follow-up. A value of k = 0 indicates that the proportion of effect observed moves linearly with time. A value of k &lt; 0 means that the eventual final effect is observed earlier in follow-up and plateaus off as time moves towards the final endpoint. A value of k &gt; 0 indicates that less of the total final endpoint effect is observed early in follow up, but as time approaches the final endpoint time the proportion of the effect observed increases rapidly. Values of k less than 0 tend to be more common than values of k greater than 0. See the figure below for a visualization of the different response-over-time curves that can be estimated with the ITP model.\n\nThe priors for the parameters in the ITP model are:\nk ∼ N(μk, σk)\nθd ∼ N(μθd, σθd2)\n\\[\\tau^{2}\\\\\\sim\\\\IG\\left( \\frac{\\tau_{n}}{2},\\\\\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\n\\[\\lambda^{2}\\\\\\sim\\\\IG\\left( \\frac{\\lambda_{n}}{2},\\\\\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be:\n\nθd mean of the expected mean improvement from baseline on the Control arm, with SD greater than or equal to the expected treatment effect size.\nk a mean of 0 and SD of 2, if subject improvement is expected to be rapid and then diminishing, the prior mean might be -0.5 or -1, if subject improvement is expected to be initially slow or non-existent then increasing, the prior mean might be 0.5 or 1.\nτ mean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\nλ mean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\n\nThe ITP model implies that the variance of the observations shrinks towards 0 with the mean (so early visits have reduced expected responses and variances). The ITP model may result in biased estimates of θd and/or the variance terms τ2 and λ2 if the mean-variance relationship assumed by the ITP model is not present in the observed data. If the model assumptions are correct this can be a very effective longitudinal model."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#longitudinal-models-for-a-dichotomous-endpoint",
    "href": "documentation/v71/userguides/core/core.html#longitudinal-models-for-a-dichotomous-endpoint",
    "title": "Core Designs - Shared Features",
    "section": "9.4 Longitudinal Models for a Dichotomous Endpoint",
    "text": "9.4 Longitudinal Models for a Dichotomous Endpoint\n\n9.4.1 LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {yit} is the set of observed responses from early visits, and yitm is the last observed value of y*i**t, then the LOCF model for the final endpoint Yi* is\nYi|{yit} = yitm\n\n\n9.4.2 Beta Binomial\nThe Beta Binomial longitudinal model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response Yi is modeled as:\nYi ∼ Bernoulli(πtyit)\nwhere πtyit is the probability that a patient is a response at the final endpoint given its early observed endpoint at time t is y*i**t*,\nπtyit = Pr (Yi = 1 | yit) ∼ Beta(αtyit, βtyit)\nWe use the set cardinality operator |…| to obtain the posterior distributions of \\(\\alpha\\)t* and βt* as:\nαt0 = αμ0 + |Yi = 1, y*i**t* = 0|\nβt0 = βμ0 + |Yi = 0, y*i**t* = 0|\nαt1 = αμ1 + |Yi = 1, y*i**t* = 1|\nβt1 = βμ1 + |Yi = 0, y*i**t* = 1|\ni.e. a prior value (αμ0, αμ1, βμ0, βμ1) plus the number of subjects for which the final response is known to be 1 for \\(\\alpha\\)tx (or 0 for βtx) and the response at time t is x.\nThe \\(\\alpha\\)tx and βtx parameters are independently estimated using only patients in their model instance, and may or not have identical priors \\(\\alpha\\)μ* and βμ* depending on the Model Priors selection in FACTS. A common non-informative prior for the πt0 and πt1 parameters is Beta(1,1).\n\n\n9.4.3 Logistic regression\nThe Logistic regression longitudinal model works similarly to the Beta Binomial imputation model. The difference is in the method of modeling the probability of a transition from an interim visit to the final visit Pr (Yi = 1|y*i**t*). Like the Beta Binomial model, the logistic regression model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response Yi is modeled as:\nYi ∼ Bernoulli(πtyit)\nwhere πtyit is the probability of a response at the final endpoint time given that its early observed endpoint at time t is y*i**t. Then, we define the parameter \\(\\theta_{ty_{it}} = logit\\left( \\pi_{ty_{it}} \\right) = \\log\\left( \\frac{\\pi_{ty_{it}}}{1 - \\pi_{ty_{it}}} \\right)\\). The priors on θt0 and θt*1 are:\nθt0 ∼ N(μ0, σ02)\nθt1 ∼ N(μ1, σ12)\nThe model computes the posterior distribution of θt0 and θt1 using all patients in arms belonging to the model instance that have observed endpoint values at time t and the final endpoint time T.\nThe priors on θt0 and θt1 may be shared across model instances and/or visits depending on the selection made in the Model Priors section of the FACTS Logistic regression Longitudinal model page.\nA possible starting place for non-informative priors in this model would be: μ = 0,  σ = 2. A weakly informative set of priors that an early response makes a final response more likely could be θt0 ∼ N(−.75, 1.252) and θt1 ∼ N(0.75, 1.252).\n\n\n9.4.4 Restricted Markov Model (Absorbing Markov Chain)\nThe Absorbing Markov Chain model assumes that at each visit a subject is in one of three states – responder (1), stable (S), or failure (0). The responder and failure states are absorbing, meaning that once a patient has entered one of those states they must remain in that state for the remainder of the visits. Patients in the stable state may move to a responder or a failure in subsequent visits.\nUnlike most other longitudinal models in FACTS, the Restricted Markov Model estimates the probability of a result at a visit based on the visit right before it, rather than predicting directly to the final endpoint from the early visit.\nPr (yit = n |yi, t − 1  = S) ∼ Dirichlet({α0t, α1t, αSt})   for t ≥ 2\nWhere n can be 0, 1, or S, denoting the probability of going to the Fail state, the Responder state, or the Stable state at visit t from the Stable state at visit t − 1. t must be greater than or equal to 2, because we do not impute the first visit – a subject missing visit 1 and all visits after does not contribute to the longitudinal model or dose response model.\nThe priors for the \\(\\alpha\\) parameters are specified in terms of the prior number of transitions from Stable at t − 1 to each different state at time t. For example, if the prior value for the parameter γ13 is 2, we are putting a priori information into the Dirichlet distribution suggesting that 2 patients transitioned from Stable at visit 2 to Responders at visit 3. Specified priors can either be common or different across model instances based on user specification.\nThe parameters defining the posterior distribution of the state probabilities are available in closed form as:\nα0t = γ0t + |y*i**t = 0, yi, t − 1 = S*|\nαSt = γSt + |y*i**t = S, yi, t − 1 = S*|\nα1t = γ1t + |y*i**t = 1, yi, t − 1 = S*|\nTo create a dichotomous endpoint, the user specifies in the Study &gt; Study Info &gt; Design Options section whether patients remaining in a stable state at the final visit should be dichotomized as responders or failures.\n\n\n9.4.5 Dichotomous Endpoint: Dichotomized Continuous Longitudinal Model\nThe user may select (on the study tab) to assume that the dichotomous final endpoint is generated by observed continuous longitudinal data and then dichotomizing the final endpoint. The user specifies the dichotomization thresholds and whether a response occurs for values above or below the threshold. If the user selects this option, then the user may select any of the continuous longitudinal models specified in the Continuous Longitudinal Models section above. The engine will impute incomplete subjects according to the continuous model, resulting in a continuous imputed final endpoint. The imputed dichotomous final endpoint is then simply the dichotomized version of the continuous imputation.\nAll priors and methods are identical to the continuous longitudinal models mentioned above."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#time-to-event-predictor-models",
    "href": "documentation/v71/userguides/core/core.html#time-to-event-predictor-models",
    "title": "Core Designs - Shared Features",
    "section": "9.5 Time-to-Event Predictor Models",
    "text": "9.5 Time-to-Event Predictor Models\nFor all predictors (Z) for time to event endpoints, the engine estimates both a marginal distribution (normal mean and variance for continuous, probability of response for dichotomous, and a piecewise exponential hazard model for time to event predictors) and a working model relating the predictor to the final endpoint. The marginal distribution is used to impute predictors for subjects lacking an observed predictor value and may also be used for stopping (see section on stopping). The working model is used to impute final endpoints for subjects lacking a final endpoint. For subject missing both a predictor and final endpoint, the predictor is imputed first from its marginal distribution, and then the final endpoint is imputed conditionally on the predictor Z.\n\n9.5.0.1 Continuous Predictor\nWithin each dose (including control and active comparator), the marginal distribution of Z is a normal distribution with mean θZd and standard deviation σZ. The standard deviation is common across the doses, but the means θZd are allowed to vary across the same range of dose response models as the final endpoint (NDLM, Logistic, etc.). The prior specification for these predictor dose response models is identical in structure to the final endpoint, although the user selects a separate set of parameter values. The dose response for the predictor does not need to match the dose response for the final endpoint.\nThe working model assumes the final event time T is related to the predictor Z by assuming T|Z ∼ Exp(λde*βZ), where λd varies by dose and has separate priors λd ∼ Gamm**a(αd, βd) for each dose. The coefficient in the exponent β (no subscript) is constant across doses with prior β ∼ N(m, s*). \n\n\n9.5.0.2 Dichotomous Predictor\nA dichotomous predictor is handled similarly to a continuous predictor, with a marginal distribution having a predictor dose response model. However, in this case the predictor dose response relates the log-odds rather than the probability of response itself. The working model for dichotomous is identical to the working model for a continuous predictor, with T|Z ∼ Exp(λde*βZ). In this situation the working model is simpler to understand, as T|(Z = 0) ∼ Exp(λd) and T|(Z = 1) ∼ Ex**p(λdβ*)).\n\n\n9.5.0.3 Time to Event Predictor\nThe time to event predictor is qualitatively different than the continuous or dichotomous predictors. Instead of the predictor adjusting the hazard, the time to event predictor is viewed as an offset. The final endpoint is viewed as a sum of a predictor time Z1 and a post-predictor time Z2, where Z1 and Z2 are independent random variables and the final endpoint is thus Z1 + Z2.\nFor the working model, Z1 ∼ *PExp(λ1s * θ1d) and Z2 ∼ Exp(λ2d), with priors λ1s ∼ Gamma(α1s, β1s), λ2d ∼ Gamm**a(α2d, β2d) (with Z1’s control hazard model potentially being piecewise exponential). For imputation, a subject missing both the biomarker and final endpoint times has both Z1 and Z2 imputed, with the final endpoint imputed as the sum. For a subject with a predictor time but no final endpoint, Z*2 is imputed and added to the observed predictor time to impute the final endpoint."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#non-adaptive-designs",
    "href": "documentation/v71/userguides/core/core.html#non-adaptive-designs",
    "title": "Core Designs - Shared Features",
    "section": "10.1 Non-adaptive designs",
    "text": "10.1 Non-adaptive designs\nIf the design is non-adaptive, then on this tab the user simply specifies the fixed allocation ratio to use between all the treatment arms for the duration of the study. The allocation is implemented using a blocking scheme – the block size is the sum of the allocation ratios entered and each arm is given the number of slots in the block corresponding to its allocation ratio. Consequently, the values entered must be integers."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#fixed-allocation",
    "href": "documentation/v71/userguides/core/core.html#fixed-allocation",
    "title": "Core Designs - Shared Features",
    "section": "10.2 Fixed Allocation",
    "text": "10.2 Fixed Allocation\nIf allocation is to be fixed, then on this tab the fixed allocation ratios and block size are specified. For each arm in the study allocation ratios are entered as for fixed designs, and allocation uses a block size that is the sum of the ratios. Fixed allocation works identically to the non-adaptive design randomization. The difference is that this Fixed allocation can be performed concurrently with interim analyses being performed."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#arm-dropping",
    "href": "documentation/v71/userguides/core/core.html#arm-dropping",
    "title": "Core Designs - Shared Features",
    "section": "10.3 Arm Dropping",
    "text": "10.3 Arm Dropping\nAdaptive arm dropping trials allow accruing data to inform the adaptive design that an arm, or a set of arms, can be dropped, meaning they no longer have subjects randomized to them. FACTS Core supports designs in which some number of arms that are clearly ineffective can be dropped. Designs where at an interim one or more arms are selected to be continued and all other arms are dropped can be simulated using FACTS Staged Designs.\nFor each arm in the study, allocation ratios are entered as for fixed designs. There are options for specifying the arm dropping rules, evaluated at each interim:\n\nThe arm dropping criteria, the user may select any combination of Bayesian “Per Dose” QOIs: Posterior Probabilities or Predictive Probabilities, or a Target Dose QOI. For each criteria selected (at least one must be selected), the user supplies a direction of comparison and probability threshold. If at any interim, any arm meets the specified arm dropping criteria, then that arm becomes a candidate for dropping. After candidates are identified for dropping, the rest of the setup rules determine which, if any, of the candidates will be dropped.\n\n\n\nIn the “Setup” area a number of rules for arm dropping are specified:\n\nThe maximum number of arms that can be dropped – this can be set to from 0 to the number of active arms, the Control and Active Comparator are never dropped. This figure is the ‘maximum number of arms that can be dropped over the duration of the trial’. At any interim any number between 0 and the ’maximum number of arms can be dropped as long as this limit is not exceeded on the overall number of arms that would have been dropped.\n\n\n\nIf the maximum number of arms that can be dropped is set to the number of study drug arms, then if all arms are dropped this is recorded as an “early termination for futility”.\n\n\nArm dropping can be restricted to ‘pruning from the lowest dose’ and/or ‘pruning from the highest dose’. If ‘prune from the lowest dose’ is selected, then until the lowest dose meets the arm dropping criteria the second lowest dose cannot be dropped and so on. At an interim when the lowest dose does meet the arm dropping criteria, and the second dose does too, then the second dose can be dropped as well (and third and fourth …), it does not have to wait until the next interim.\nIf no pruning is selected or pruning from both ends is selected, then the user must select to prioritize the higher or lower dose to cover the instance when more doses are candidates for dropping than allowed by the maximum threshold. If “Prioritize dropping the lowest dose” is selected then when forced to choose between two or more doses to drop, the one with the lowest effective dose strength will be dropped.\n\n\nLastly the user specifies what is to be done with the unused subjects that would have been allocated to an arm that has been dropped. There are three options:\n\nMaintain study size, maintain combined block size of treatments: subjects that would have been allocated to arms that have been dropped are now randomized between all the remaining active treatment arms (excluding control and active comparator) using their original allocation ratios. For example, if the allocation ratio was 2:1:1:1 between Control and D1, D2 and D3, then after dropping D1 the block size remains size 5: 2 to Control and 1 to each of D2 and D3, with the 5th slot being allocated 1:1 between the remaining two study arms D2 & D3. Thus, the overall proportion of subjects on Control (and Active Comparator) is maintained.\nMaintain study size, reduce combined block size of treatments: subjects that would have been allocated to any arms that have been dropped are randomized between all remaining arms (including control) using a reduced total block size. Thus, if the allocation ratio was 2:1:1 between Control and D1 and D2, then after dropping D1 the block size drops to total size three: 2 to Control and 1 to D2.\nDecrease the study size, reduce combined block size of treatments: subjects that would have been allocated to any arms that have been dropped are no longer recruited and the total sample size of the study is reduced; the remaining subjects are recruited using a reduced block. If interims are specified by number of subjects recruited, these interim sizes are reduced proportionally. As an example of this, if an arm that gets 20% of the randomization is dropped when there are 100 subjects enrolled, then if the next interim is scheduled at 200 subjects enrolled, it will actually be performed at 180 subjects enrolled."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#adaptive-allocation",
    "href": "documentation/v71/userguides/core/core.html#adaptive-allocation",
    "title": "Core Designs - Shared Features",
    "section": "10.4 Adaptive Allocation",
    "text": "10.4 Adaptive Allocation\nIn adaptive allocation, the relative probabilities of assigning each of the doses to a subject may change throughout the trial. The adaptive allocation design that FACTS supports is one where the allocation ratio is modified at each interim to increase the allocation to doses that have the preferred characteristics.\nAn adaptive allocation design has two phases: the “burn-in” before any adaptation takes place and the “adaptive phase”. The burn-in lasts until the first interim occurs, and is defined on the Interims tab. The adaptive phase lasts until the early stopping criteria are met or the maximum sample size is reached.\nThe allocation of subjects in burn-in is specified by entering the allocation ratio to each arm for the burn-in period. Once the first interim is reached the allocation becomes adaptive.\n\nIt is possible to specify that specific treatment arms are not to be allocated to adaptively by specifying a fixed allocation for that arm.\nAdaptive and fixed allocation is combined within an overall blocking scheme, within which some slots are fixed and some are adaptive.\n\n\n\nThe arms to allocate to with a fixed probability are selected by clicking the “Fix Alloc” check box for that arm.\n\n\n\nThe overall block size is set by entering it in the ‘Block size’ field.\nThe number of ‘slots’ in the block to be given to the fixed arms are set by entering the number on the “Post Burn-in Alloc per Block” cell for the fixed arms.\nThe non-fixed arms will be allocated to adaptively and probabilistically in the slots not taken by the fixed allocation (so the Block size must exceed the number of fixed slots being used!)\n\nCommonly, the control arm and active comparator arms are allocated to in this way and the treatment arms are left to be allocated to adaptively. If the Control arm is not given a fixed allocation, it is allocated to with an adaptive probability that is an equal weight to the treatment arm with the highest weight.\nThere are options for specifying the adaptive allocation, evaluated at each interim:\n\n“Probability set to zero for values less than:” - specifies an allocation probability threshold, which if an arms probability of allocation falls below this value then it is set to 0 and the remaining allocation probabilities re-normalized.\nIn the “Adaptive Allocation Weights” the user selects which QOI to use to determine the adaptive allocation, and what relative weight to give them. The user also selects for each target whether the weight to use should be “probability” that the dose is the target or the “information” allocating to the dose would give about the target.\n\n\nAny Bayesian “Per Dose” QOI, or “Target Dose” QOI can be used to determine the adaptive allocation weights. In addition there is the option to use a static weighting – this is of course not adaptive! What it does do is allow an adaptive allocation to be combined with a “guaranteed minimum allocation” (see example discussion below). If a Static target is included, a small table is displayed the specification of the ratio of the division of the static weight between the study arms that are being adaptively allocated to.\n\n\nWhen weighting for probability, the weight is derived simply from the value of the QOI.\nWhen weighting for information, the weight uses the value of the QOI but adjusted by the current variance of the estimate and the number of subjects already allocated to the dose – an estimate of the additional information that would result from adding one more subject to that arm.\nWeighting for probability is ‘more aggressive’ in adapting in pursuit of the target. The risk with a probability-based weighting is never allocating again to an arm where the initial data is so poor that its initial probability of being the target is so low it is never allocated to again. So, it is appropriate when the available sample size is small (and risks must be taken), the number of study arms is small (so it is unlikely an arm will not be allocated to again), or the allocation during the burn-in is large and evenly distributed (so that having unrepresentative data is very unlikely).\nWeighing for information will tend to spread the allocation around the most likely target, reducing the risk of never learning that dose is better than its initial data. If a dose response model is being used, allocation to doses around the target dose will contribute to the accuracy of the estimate of response on the target dose, but compared to the probability weighting will tend to result in fewer subjects allocated to the actual target dose.\nWhichever weighing rule is selected the adaptation can be further ‘sharpened’ or ‘softened’ by adjusting the power to which the allocation probability is raised. Setting the power to 0.5 will significantly soften the allocation, setting it to 2 will significantly sharpen it.\n\n\n\n10.4.1 Weighting and Calculation of Adaptive Allocation Probabilities\nFor a QOI Pi, d that is specified as an Adaptive Allocation Target, if probability weighting is selected, the allocation target for dose d is:\nVd, i* = [Pd, i]γ\nwhere γ is the allocation probability power.\nIf information weighting is selected, then the allocation target for Pi, d is:\n\\[V_{d,i}^{\\*} = \\left\\lbrack \\sqrt{\\frac{P_{d,i}Var\\left( \\theta_{d} \\right)}{n_{d} + 1}} \\right\\rbrack^{\\gamma}\\]\nwhere γ is the allocation probability power, nd is the number of subjects on dose d, and θd is the dose-response model mean estimate for arm d.\nIf there are multiple allocation targets used for an adaptive randomization, then these allocation targets are combined to create a single allocation weight Wd. The allocation weight for a dose is the weighted average of the allocation targets for the dose.\n\\[W_{d} = \\sum_{i = 1}^{I}{w_{i}V_{d,i}}\\]\nwhere I is the number of allocation targets, wi is the weight of allocation target i, and Vd, i is the value of the allocation target for dose d on target i.\nIf there is only 1 allocation target, then the allocation weight Wd = Vd, 1.\nFinally, for the doses which are adaptively allocated (not fixed or within their burn-in period), these allocation weights are renormalized to sum to the probability remaining after all fixed doses have been allocated.\n\n10.4.1.1 Non-fixed Control Adaptive Allocation\nIf the control allocation ratio is not fixed when performing adaptive allocation, then the control arm gets a randomization ratio that targets matching the number of control subjects to the active arm with the most subjects.\nTo derive the control allocation rate, let Vd for d = 1, 2, …D be the allocation probabilities for each of the D non-control dose arms (these may be obtained by calculating based on the probability or information criteria as described above or the fixed allocation proportion with respect to the block size), and let nd for d = 1, 2, …D be the number of subjects allocated to those arms.\nThen, the allocation to the control arm V0 is:\n\\[V_{0} = \\min\\left\\\\ \\sum_{d = 1}^{D}{V_{d}\\frac{(n_{d} + 1)}{(n_{0} + 1)},\\\\\\\\max\\\\ V_{1},V_{2},\\ldots,\\\\V_{D}\\\\} \\right\\\\\\]\nFollowing fixing this control rate, the allocation probabilities for the non-fixed doses are renormalized to add up to the total non-fixed probability.\n\n\n10.4.1.2 Zero Out Allocation Probabilities\nIf, at the end of the adaptive allocation probability calculation, any adaptively allocated arms have a randomization probability smaller than the value provided for “Allocation probability set to zero for values less than:”, then the allocation probabilities are adjusted.\nTo adjust the probabilities, first the arm with the smallest randomization probability is given a fixed allocation rate of 0. Then, the remaining adaptively allocated arms have their allocation probabilities re-normalized to sum to the probability remaining after all fixed doses have been allocated. Then, if any doses remain below the zero-out threshold, then this process is repeated. Continue the repetition as necessary. If none of the allocation probabilities are below the threshold, then the allocation probabilities are set.\n\n\n\n10.4.2 Adaptive Allocation Calculation Examples\n\n10.4.2.1 Simple Response Adaptive Randomization Example\nSuppose you have a control and 3 active doses. The control arm is guaranteed 2/6 slots in every block and a fixed 20% probability must be placed on the first active dose (dose A). So, the control arm gets 33.3% of the total allocation probability and dose A gets 20% of the total allocation. Suppose the target QOIs Vd for the three active doses are 0.20, 0.50, and 0.30 and that we’re using probability weighting with weight 1. The second two doses are the only doses with unknown randomization probabilities, so they split the amount of non-fixed allocation probability proportionally based on their Vd.\nThe allocation probability of the second active dose is \\(\\left( 1 - (0.333 + 0.2) \\right)\\*\\left( \\frac{0.5}{0.5 + 0.3} \\right)\\) and the allocation probability of the third active dose is \\(\\left( 1 - (0.333 + 0.2) \\right)\\*\\left( \\frac{0.3}{0.5 + 0.3} \\right)\\).\nThus, the final allocation probabilities for the control and the three active doses are: (0.333, 0.2, 0.292, 0.175).\nIf any of the adaptively allocated probabilities are less than a user specified minimum (“Allocation probability set to zero for values less than….” in the GUI) then these probabilities would be set to zero and the resulting probability is reallocated among the non-fixed probability doses. If all non-fixed doses drop at this point, then the probability is reallocated to the fixed doses.\n\n\n10.4.2.2 Using Static Weighting Example: Ensuring a minimum allocation to all arms\nAs an example of using the static weight to allow adaptive randomization along with a guaranteed minimum allocation of 10% to each of 5 study arms. Let us say that we want the adaptive allocation to be evenly divided between targeting an EDq and MED target.\nAs there are 5 study arms, allocating 10% each amounts to 50% of the total. Thus we could specify weights of 1 to the MED target, 1 to the EDq target, and 2 to the Static target, so the Static target gets 50% of the total weighting.\nHowever, this ignores the possible allocation to a Control arm. What we have achieved above either allocates 10% to each study arm if there is no Control arm, or if there is a Control arm, allocates 10% of the subjects allocated to the study arms, not 10% of the overall.\nLet us say that in addition to the 5 study arms you have a Control that we want to have 20% allocation and we want each study arm to have at least 10% of the overall allocation. The simplest way to specify this is to set a “Post first interim block size” of 10 and that 2 slots are allocated to Control.\nThis leave 8 slots to be allocated across the treatment arms, to ensure a 10% allocation for each of the 5 treatment arms - that takes up 5 more slots in the block – but we don’t want to allocate 1 in the block to each using fixed allocation – because then that’s all they would get. We need to allow them to be allocated to adaptively and use the Static target to ensure they get a minimum of 10%. We can achieve this by giving the static allocation a weight of 5 and divide a weight of 3 between the MED and EDq targets, so they get a weight of 1.5 each.\n\n\n10.4.2.3 Using Static Weighting Example: Ensuring a minimum allocation to top dose\nImagine an early Phase II Proof-of-Concept study that wants to compare 3 doses of the study drug (low, medium and high) to Control, and adaptively allocate to the dose with the maximum response. The study team also wish to ensure a minimum allocation to the top dose as they have a strong prior that it will have the maximum effect. They want to minimize the risk that the adaptive allocation avoids the top dose because of randomly poor results on that dose early on in the trial.\nSolution 1 uses a fixed allocation to Control of 30%. The optimal allocation to control in a multi armed study is approximately √N:1:1:… where N is the number of study arms and in √3:1:1:1 the proportion on control would be 37%. Here we’ve rounded down to 30% rather than up to 40% to reflect a typical clinical teams desire to get more data on their study drug rather than Control.\nAfter the first Interim, we specify allocation to be in blocks of 10 with the Control allocate 3 slots in each block, the weight on the Static target is 2 and the Weight on the Pr(Max) target is 5. So 50% of the allocation is adaptive targeting the dose with the Maximum response and 20% is fixed and allocated to the top dose by specifying that the static weighting is split 0:0:1.\n\nSolution 2 uses adaptive allocation to Control. Adaptive allocation to Control allocates to control with the same weight as the adaptive allocated arm with the highest weight. Under this scheme the top dose will receive its minimum allocation when there is an arm with close to 100% Pr(Max). If that arm is the top dose, then allocation will be close to 50% to Control and 50% to the top dose, so the 20% minimum allocation requirement is met. If it is a different arm with close to 100% Pr(Max) then, if the allocation to top dose is t and the allocation to Pr(Max) 100 - t, the allocation ratio is (100 – t) : (100 – t) : t, (Control : Best Dose : Top Dose).\nThe total allocation is (200 – t) and we want to select t so that t/(200 – t) ≅ 0.2. So t = 33.3. With the static weight allocated 0:0:1."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#deterministic-allocation",
    "href": "documentation/v71/userguides/core/core.html#deterministic-allocation",
    "title": "Core Designs - Shared Features",
    "section": "10.5 Deterministic Allocation",
    "text": "10.5 Deterministic Allocation\nThe deterministic allocation option allows for the treatment assignments of subjects accrued into the simulated trials to be assigned without randomness based on an uploaded file. The file providing the assignments should be a comma separated .dat file with 2 columns, but no column labels.\nThe first column should be an increasing, unique column of numbers from 1 to at least the maximum number of subjects that can be enrolled in the study. The second column should be the set of treatment assignments in order from 1 to the number of patient IDs. The treatment assignments are used in row order – they do not use the subject ID order. So, the first subject accrued in the study is given the assignment indicated by the first row, second column value, the second subject accrued in the study is given the assignment indicated by the second row, second column value, and so on.\nThe treatment assignments column in the .dat file should always have a minimum value of 1 and a maximum of the number of total arms in the study. If there is a control arm in the study, then treatment assignment 1 corresponds to a control arm randomization. If there is no control arm in the study, then treatment assignment 1 in the .dat file corresponds to a subject randomized to the lowest active dose."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#cohort-recruitment-fixed-allocation",
    "href": "documentation/v71/userguides/core/core.html#cohort-recruitment-fixed-allocation",
    "title": "Core Designs - Shared Features",
    "section": "10.6 Cohort recruitment – fixed allocation",
    "text": "10.6 Cohort recruitment – fixed allocation\nIf allocation is to be fixed the “Early Stopping Only” option should be selected. Then on this tab the fixed allocation ratios and block size are specified – for the first cohort and then for all subsequent cohorts. In each case the user specifies the number of each arm to allocate in each cohort and the sum must equal the size of the cohort."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#cohort-recruitment-adaptive-allocation",
    "href": "documentation/v71/userguides/core/core.html#cohort-recruitment-adaptive-allocation",
    "title": "Core Designs - Shared Features",
    "section": "10.7 Cohort recruitment – adaptive allocation",
    "text": "10.7 Cohort recruitment – adaptive allocation\nIf allocation is to be adaptive the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort by sampling from allocation vector” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation ratios for the remaining subjects in the cohort as for adaptive allocation (see Adaptive Allocation above)."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#cohort-recruitment-allocate-to-best-dose",
    "href": "documentation/v71/userguides/core/core.html#cohort-recruitment-allocate-to-best-dose",
    "title": "Core Designs - Shared Features",
    "section": "10.8 Cohort recruitment – allocate to best dose",
    "text": "10.8 Cohort recruitment – allocate to best dose\nIf allocation is to be adaptive, but at each cohort just Control and the “best dose” are to be allocated to, the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort to single treatment (plus control)” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation weighting to calculate for the non-Control arms, (see Adaptive Allocation above) and all the subjects not allocated to control will be allocated to this arm."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#continuous-and-dichotomous-endpoints",
    "href": "documentation/v71/userguides/core/core.html#continuous-and-dichotomous-endpoints",
    "title": "Core Designs - Shared Features",
    "section": "11.1 Continuous and Dichotomous Endpoints",
    "text": "11.1 Continuous and Dichotomous Endpoints\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have actually completed a specified visit\nthe number of subjects who have had the opportunity to complete a specified visit (includes drop-outs)\ninterims can be by time with the first interim defined by information and time thereafter, or each information can be defined by the amount of information required to trigger the interim..\n\n \nIf defining interims by time, these are defined by frequency (number of weeks between interim) – fractions of weeks can be used for very frequent interims! The first interim is defined in terms of an information threshold, with the type of information selected above.\nIf the accrual completes before the first interim threshold is reached, and the first interim was defined in terms of the number of subjects enrolled, then the interims by time start at full accrual. If the first interim is defined in any other terms (by events, subjects complete or subjects with opportunity to complete) then interims only start when this is reached (which might be never).\nIf defiing interims by information, then each interim is defined individually, by number of patients/observations and iif information is interms of completers, then the week of the visit that is being used to define “complete”. Successive interims must be in terms of the same or more observations at the same or later visit and at least one needs to be “more” or “later”.\nIf interims are governed by time, completers or events there is the option as to whether interims should continue after full accrual, or discontinue."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#time-to-event-endpoint",
    "href": "documentation/v71/userguides/core/core.html#time-to-event-endpoint",
    "title": "Core Designs - Shared Features",
    "section": "11.2 Time-to-Event Endpoint",
    "text": "11.2 Time-to-Event Endpoint\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have observed their predictor endpoint\nthe number of subjects who have had the opportunity to observe their predictor endpoint (includes drop-outs)\nspecified numbers of events have observed (time to event trials only). With early stopping only, or arm dropping designs, the occurrence of the first interim is also specified, with an adaptive allocation design, the first interim is at the end of the burn-in."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#follow-up",
    "href": "documentation/v71/userguides/core/core.html#follow-up",
    "title": "Core Designs - Shared Features",
    "section": "11.3 Follow-up",
    "text": "11.3 Follow-up\nRegardless of endpoint, the Interims tab contains options that control the behaviour should a trial stop at an interim. The option allows the user to specify whether or not to complete the follow-up of subjects who have been accrued, but have not had time to observe their final endpoint.\nThe default options available for Subject Follow-Up are:\n\nContinue follow-up if study stopped for success\n\n\n\nContinue follow-up if study stopped for futility\n\nIf the check box corresponding to an interim decision is checked, then at the time of an interim analysis decision accrual will be stopped, all subjects currently enrolled will be followed-up until they have had the opportunity to observe their final endpoint, and then the final analysis will be performed.\nIf the check box corresponding to an interim decision is not checked, then at the time of an interim analysis decision accrual is stopped, the data is locked, and no follow-up on randomized patients is collected. The interim dataset is the final dataset. The final analysis is then performed using the same data and model as was used for the interim analysis.\n\nIf the allocation method is selected as, “Arm Dropping” then an additional check box is provided in the Subject Follow-up Options box asking whether the user would like to “Continue follow-up if arm dropped.” If the box is checked, then subjects randomized to an arm that is dropped before they have the opportunity to complete their follow-up will have to opportunity to observe their final endpoint for subsequent analyses. If the box is not checked then incomplete subjects on an arm that is dropped will never have future endpoint values observed."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#interim-analysis-criteria",
    "href": "documentation/v71/userguides/core/core.html#interim-analysis-criteria",
    "title": "Core Designs - Shared Features",
    "section": "12.1 Interim Analysis Criteria",
    "text": "12.1 Interim Analysis Criteria\nOn the success/futility criteria page the user can specify rules for judging the study for futility or success at an interim and at the final evaluation. If the trial has no interims there will be just a tab for the Final Evaluation criteria. If the trial has interim analyses, then there can be tabs that define different early success/futility criteria at the different interims.\nAt the top of the main tab is a control to allow tabs to be created for different interims. In a newly created adaptive design, FACTS will create a tab for interim 1.\nIf early success/futility criteria are specified for an interim, then they will be taken to apply to all subsequent interims until the next one for which criteria are supplied, then those criteria will apply until the next interim for which criteria are applied and so on.\nNote:\n\nEarly stopping for success/futility only occurs at interims.\nThere will be no stopping for success or futility until the first interim for which early stopping criteria have been defined.\nIt is left to the user to ensure that the early stopping criteria at any interim are mutually exclusive and it is not possible to stop for both success and futility at the same time. The FACTS design engines will stop the trial but the user should not rely on one or other outcome taking precedence. It is not considered good practice to have success and futility rules that could both be true, so FACTS does not guarantee a “tie break” rule.\nIn the output files there are columns labeled “Success &lt;QOI&gt;” and “Futile &lt;QOI&gt;” indicating whether any decision criteria became true, and “Success Combined” and “Futile Combined” indicating if all the criteria for a success or futility determination have been met. If both sets of criteria are met simultaneously, FACTS will only flag one of “Success Combined” and “Futile Combined” as being met, corresponding to how the outcome of the trial has been flagged.\n\n\nHaving created a tab to define the early success/futility criteria at an interim, the user has options to delete the tab or to copy criteria that have already been entered on another tab before editing them.\nThe user specifies:\n\nWhether early stopping for futility or success is to be allowed by selecting the “Futility criteria” or “Success criteria”\nThe stopping rules are specified, by selecting the QOI to be tested, the direction of the comparison and the threshold value for the criteria to be met.\nThe user can specify whether, if multiple criteria have been specified, they all need to be meet (Criteria combined using “AND”) or only any one of them needs to be met (Criteria combined using “OR”). These are specified independently for stopping for success and for futility.\nIf stopping is allowed, the user specifies what the minimum amount of information (using the information type specified on the Interims tab). This can be specified both in overall terms and information on specific doses. If multiple minimum-information criteria are specified they must all be met for the QOI criteria to be evaluated.\n\n\nNote that stopping is only assessed at interims, not immediately when these criteria are met."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#final-evaluation-criteria",
    "href": "documentation/v71/userguides/core/core.html#final-evaluation-criteria",
    "title": "Core Designs - Shared Features",
    "section": "12.2 Final Evaluation Criteria",
    "text": "12.2 Final Evaluation Criteria\nOn the Final Evaluation Criteria tab, the user can specify rules for judging the study for final futility or final success at its end. The tab layout is the same as an interim Success/Futility tab except there are no minimum information rules to be specified.\nThe Final Evaluation Criteria are always applied at the time of the final analysis, even if the study stops early for success or futility, and whether or not “Continue follow-up if study stopped for success” or “Continue follow-up if study stopped for futility” are selected. As a result, setting final analysis criteria that are easier to satisfy than interim analysis criteria may result in the labelling of trials that meet the interim analysis criteria but do not satisfy the final analysis criteria as flip-flops."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#state-descriptions",
    "href": "documentation/v71/userguides/core/core.html#state-descriptions",
    "title": "Core Designs - Shared Features",
    "section": "13.1 State descriptions",
    "text": "13.1 State descriptions\nFor the purposes of interims, the different states or stages of a trial are:\n\nIn Burn-in / before first analysis. In adaptive allocation designs, the burn-in denotes the initial group of subjects explicitly assigned to treatment arms before probabilistic allocation begins. FACTS prevents the burn-in requiring more subjects than the maximum number of subjects – but the numbers can be equal. Regardless of the interim schedule specified (e.g., frequency, number of subjects or number of events), a) no analyses are performed during this period of time, and b) the first analysis is performed at its conclusion. In arm dropping and early stopping designs the first analysis occurs at the explicitly specified initial interim. Interims are never performed in non-adaptive designs; rather, the first and only analysis occurs when the max subject’s final observation is taken. In all cases, it is possible to reach the maximum number of events prior to the first analysis, making the study complete.\nMid Trial. Interims are performed and all rules assessed. If arms have been dropped, the per-arm posterior probabilities are still calculated for the dropped arms but the drop decision for the arms is absorbing and not re-assessed.\nFully Accrued but not complete. All subjects have been recruited and being followed up. If interims are by subject or ‘interim analysis beyond full accrual’ is set to FALSE, then the only possible next event is ‘last subject final observation’.\nStopped Early but still following up. The study stopping rules are not evaluated, but arm dropping rules are. Interims may still occur.\nComplete. All possible data collected for every enrolled subject."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#analysis-trigger-events",
    "href": "documentation/v71/userguides/core/core.html#analysis-trigger-events",
    "title": "Core Designs - Shared Features",
    "section": "13.2 Analysis trigger events",
    "text": "13.2 Analysis trigger events\nThe following are the events that trigger interims, stopping or changes of trial state:\n\nLast event is observed (TTE trial with a max number of events – this is the only circumstance that can arise before the end of burn-in / first analysis that can stop the trial).\nA “number of subjects interim” occurs (other than last subject of burn-in / first analysis)\nA “cohort complete” interim occurs (when using cohort enrolment). A cohort complete event is slightly different from a “number of subjects interim” in that after a cohort is fully enrolled the interim does not occur until all the subjects are complete.\nStudy max subject size is reached.\nRecruit last subject of burn-in / first analysis occurs.\nA “time” or “number of events” interim occurs.\nLast subject’s final observation is taken (in a TTE trial this might be that the last subject to have an event has their event, or last subject recruited reaches the end of the maximum follow-up and is censored)."
  },
  {
    "objectID": "documentation/v71/userguides/core/core.html#trial-state-transitions",
    "href": "documentation/v71/userguides/core/core.html#trial-state-transitions",
    "title": "Core Designs - Shared Features",
    "section": "13.3 Trial State Transitions",
    "text": "13.3 Trial State Transitions\nNote: the term “study arms” is used to refer to the treatment arms that are not the control or active comparator arm. In an arm dropping design only the study arms can be dropped, and the trial stops if all study arms are dropped.\n\n\n\nFigure 3‑2 Add Posterior Probability QOI dialog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nState transition table\n\n\n\n\n\n\n\nTriggers\n\n\nProcessed in order …\n\n\n\n\n1: In Burn-in / before first analysis\n\n\n[This is the starting state for adaptive allocation designs]\n\n\n\n\n2: Mid Trial\n\n\n[This is the starting state for arm dropping, early stopping, and non-adaptive designs]\n\n\n\n3: Fully Accrued but not complete\n\n\n\n4: Stopped Early but following up\n\n\n[This state can only be entered if “Continue follow-up if study stopped for success/futility is set]\n\n\n\n\n5: Complete\n\n\n[This is the final state]\n\n\n\n\n\nA: Last event is observed [can only occur in TTE trials]\n\n\n\nA “final evaluation” is output.\n\n\nGo to “5: Complete”\n\n\n\n\nA “final evaluation” is output.\n\n\nGo to “5: Complete”\n\n\n\n\nA “final evaluation” is output.\n\n\nGo to “5: Complete”\n\n\n\n\nA “final evaluation” is output.\n\n\nGo to “5: Complete”\n\n\n\nN/A\n\n\n\n\nB: A “number of subjects” interim occurs\n\n\nN/A\n\n\n\nAn interim is output.\n\n\nIf arm dropping, check not yet dropped arms to see if any should be dropped and flag those that are.\n\n\nIf stopping conditions are met or all study arms dropped then if “Continue follow-up” for the appropriate decision is selected go to “4: Stopped Early” otherwise a “final evaluation” is output and\n\n\nGo to “5: Complete”\n\n\nIf the interim size = Max subjects then go to “3: Fully Accrued”\n\n\nOtherwise stay in “2: Mid Trial”\n\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n\n\nC: A “cohort complete” interim occurs [for trials using cohort enrolment]\n\n\n\n[Can only occur when the final observation of a burn-in cohort is observed]\n\n\nAn interim is output.\n\n\nIf stopping conditions are met a “final evaluation” is output and go to “5: Complete”.\n\n\nOtherwise go to “2: Mid Trial”.\n\n\n\n\nAn interim is output.\n\n\nIf stopping conditions are met a “final evaluation” is output and go to “5: Complete”.\n\n\nIf the interim size = Max cohort or the max cohort size is reached then a “final evaluation” is output and go to “5: Complete”.\n\n\nOtherwise stay in “2: Mid Trial”\n\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n\n\nD: Study max subject size is reached\n\n\n\n[Can only occur if Burn-in / first interim size = Max subjects]\n\n\nGo to “3: Fully Accrued”.\n\n\n\nGo to “3: Fully Accrued”\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n\n\nE: Recruit last subject of burn-in / first analysis\n\n\n\nAn interim is output.\n\n\nIf arm dropping, check not yet dropped arms to see if any should be dropped and flag those that are.\n\n\nIf stopping conditions are met or all study arms dropped then ff “Continue Follow-up if study stopped“ for the appropriate decision is set go to “4: Stopped Early but following up” otherwise a “final evaluation” is output and go to “5: Complete”\n\n\nOtherwise go to “2: Mid Trial”\n\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n\n\nF: A “time” or “number of events” interim occurs\n\n\nN/A\n\n\n\nAn interim is output.\n\n\nIf arm dropping, check not yet dropped arms to see if any should be dropped and flag those that are.\n\n\nIf stopping conditions are met or all study arms dropped then ff “Continue Follow-up if study stopped“ for the appropriate decision is set go to “4: Stopped Early but following up” otherwise a “final evaluation” is output and go to “5: Complete”\n\n\nOtherwise stay in “2: Mid Trial”\n\n\n\n\nIf “Discontinue interim analysis after full enrollment” is set, then there is no output. Stay in “3: Fully Accrued”\n\n\nOtherwise:\n\n\nAn interim is output.\n\n\nIf arm dropping, check not yet dropped arms to see if any should be dropped and flag those that are.\n\n\nIf stopping conditions are met or all study arms dropped then ff “Continue Follow-up if study stopped“ for the appropriate decision is set go to “4: Stopped Early but following up” otherwise a “final evaluation” is output and go to “5: Complete”\n\n\nOtherwise stay in “3: Fully Accrued”\n\n\n\n\nIf “Discontinue interim analysis after full enrollment” is set then there is no output, stay in “4: Stopped Early”\n\n\nOtherwise:\n\n\nAn interim only checking arm-dropping is output. Stopping conditions not checked.\n\n\nIf arm dropping, check not yet dropped arms to see if any should be dropped and flag those that are.\n\n\nIf all study arms are dropped then if “Continue follow-up if arm dropped” is set stay in “4: Stopped Early” otherwise a “final evaluation” is output and go to “5: Complete”\n\n\n\nN/A\n\n\n\n\nG: Last subject’s final observation is taken\n\n\nN/A\n\n\nN/A\n\n\n\nA “final evaluation” is output.\n\n\nGo to “5: Complete”\n\n\n\n\nA “final evaluation” is output.\n\n\nGo to “5: Complete”\n\n\n\nN/A\n\n\n\n\nFigure 3‑2 Add Posterior Probability QOI dialog"
  },
  {
    "objectID": "documentation/v71/userguides/core/mm_analysis.html",
    "href": "documentation/v71/userguides/core/mm_analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/jj_interims.html",
    "href": "documentation/v71/userguides/core/jj_interims.html",
    "title": "Interims",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Interims"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/ee_qois.html",
    "href": "documentation/v71/userguides/core/ee_qois.html",
    "title": "Quantities of Interest",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/bb_vsr/vsr_dichotomous.html",
    "href": "documentation/v71/userguides/core/bb_vsr/vsr_dichotomous.html",
    "title": "Dichotomous Endpoints",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Dichotomous Endpoints"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/bb_vsr/vsr_tte.html",
    "href": "documentation/v71/userguides/core/bb_vsr/vsr_tte.html",
    "title": "Time-to-event Endpoints",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Time-to-event Endpoints"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/gg_freqtab.html",
    "href": "documentation/v71/userguides/core/gg_freqtab.html",
    "title": "Frequentist Analysis",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Frequentist Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/bb_vsr/vsr_continuous.html",
    "href": "documentation/v71/userguides/core/bb_vsr/vsr_continuous.html",
    "title": "Continuous Endpoints",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Continuous Endpoints"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/ff_drmodels.html",
    "href": "documentation/v71/userguides/core/ff_drmodels.html",
    "title": "Dose Response Models",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/aa_study.html",
    "href": "documentation/v71/userguides/core/aa_study.html",
    "title": "Study",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/ii_allocation.html",
    "href": "documentation/v71/userguides/core/ii_allocation.html",
    "title": "Allocation",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/ll_simulation.html",
    "href": "documentation/v71/userguides/core/ll_simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/hh_longmodels/longmodels_dichot.html",
    "href": "documentation/v71/userguides/core/hh_longmodels/longmodels_dichot.html",
    "title": "Study Tab",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Longitudinal Models",
      "Study Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/hh_longmodels/longmodels_cont.html",
    "href": "documentation/v71/userguides/core/hh_longmodels/longmodels_cont.html",
    "title": "Study Tab",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Longitudinal Models",
      "Study Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/kk_successfutility.html",
    "href": "documentation/v71/userguides/core/kk_successfutility.html",
    "title": "Success/Futility Criteria",
    "section": "",
    "text": "Placeholder.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Success/Futility Criteria"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html",
    "href": "documentation/v71/userguides/crm.html",
    "title": "FACTS Dose Escalation CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system.\n\n\n\nThis document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nThis document has been updated for the version 7.1 release of Dose Escalation FACTS.\n\n\n\nIf writing in LaTex and using Bibtex, if you wish to cite FACTS (thank you!), can we suggest the following:\n@techreport{FACTS71,\n  author = {{FACTS Development Team}},\n  title = {{FACTS}: Fixed and Adaptive Clinical Trial Simulator},\n  year  = {2024},\n  month = {12},\n  number = {Version 7.1},\n  type         = {Computer Software},\n  institution = {Berry Consultants LLC},\n  address = {Austin, TX},\n  note   = {https://www.berryconsultants.com/software/facts/}\n}\nThis will result in a reference that, for example in the APA style, will look like the following:\nFACTS Development Team (2024). FACTS: Fixed and adaptive clinical trial simulator. Computer Software Version 7.1, Berry Consultants LLC, Austin, TX. https://www.berryconsultants.com/software/facts/.\n\n\n\nTable 1 gives an overview of the acronyms and abbreviations used in this document.\n\n\n\nTable 1: List of terms used in the CRM user guide\n\n\n\n\n\n\n\n\n\nName\nDefinition\n\n\n\n\nCap\nA limit on the number of subjects recruited. In FACTS N-CRM users can specify a cap on the overall number of subjects to be recruited in the trial (the ‘Overall Cap’) and specify stopping rules to define when the trial should stop before it reaches cap.\n\n\nControl\nIs the treatment arm with which the novel treatment(s) are principally being compared. Control may be placebo, or some existing standard of care, or therapy, against which the novel treatment has to be benchmarked in order to determine its likely usefulness.\n\n\nCore\nFACTS Core: A mode of FACTS for designing trials where multiple treatments, (possibly different doses of a novel treatment) are tested against a control and optionally an active comparator.\n\n\nCRM\nContinual Reassessment Method – a dose escalation design where the dose-toxicity is estimated using a simple Bayesian model, and the resulting estimates used to control the dose escalation and estimate the Maximum Tolerated Dose (MTD).\n\n\nDE\nDose Escalation: a mode of FACTS where subjects are treated in cohorts and dose escalation is determined by the number of toxicities observed.\n\n\nED\nEnrichment Designs: a mode of FACTS for designing trials where the same treatment is testing in different settings for example different sub-populations or different but related indications.\n\n\nFACTS\nFixed and Adaptive Clinical Trial Simulator.\n\n\nFinal Endpoint\nThe final value, or state, of a subject’s endpoint.\n\n\nGUI\nGraphical User Interface, the part of the FACTS application that the user interacts with.\n\n\nMethod\nIn the FACTS documentation we try to reserve the term ‘method’ for the algorithms used in the simulation (as opposed to the analysis) part of the program. In the analysis part we use the term ‘Model’, see below.\n\n\nModel\nIn the FACTS documentation we try to reserve the term ‘model’ for the statistical models used in the analysis of the trial data (in the ‘design’ section of the FACTS user interface). Where mathematical algorithms are used for other purposes in FACTS (for instance in the generation of the simulated data) we try to use the term ‘method’. We have found that initially it is very easy for users to be confused between these two parts of FACTS and we feel that using distinct terminology may help to reduce this.\n\n\nMTD\nThe dose most likely to be the Maximum Tolerated Dose (MTD) – the dose with the highest Pr(MTD).\n\n\nMTD+\nThe dose most likely to be the MTD+ – the dose with the highest Pr(MTD+).\n\n\nPr(MTD)\nA dose’s probability of being MTD is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable toxicity band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. This estimate of MTD is constrained to select one of the available doses.\n\n\nPr(MTD+)\nA dose’s probability of being the MTD+ is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. Unlike Pr(MTD), Pr(MTD+) includes estimating whether a dose below or a dose above the range of those being tested is more likely to have a toxicity in the acceptable band than any of the doses in the range.\n\n\nProfile\nA profile is a specification of one aspect of a scenario. A scenario is made up of one profile of each of the required types for the type of trial being simulated. FACTS allows the user to specify multiple profiles of each type and then presents all the possible combinations of profiles as scenarios that can be used to drive simulations.\n\n\nResponse\nThe change in a subject’s endpoint compared to their baseline state.\n\n\nScenario\nA scenario is the complete specification of the unknown external factors that determine the data observed on the trial and its timing. The exact factors depend on the type of trial being simulated but typically include:  -) the distribution of the final change from base line, or probability of response or rate of events in the different treatment groups  -) the properties of subjects’ early responses and the correlation with their final outcome  -) the rate at which subjects are recruited into the trial  -) the rate at which subjects drop out of the trial.\n\n\nSPEC\nThe Design Engine Specification, describes the system algorithms, and meaning of parameters.\n\n\nResponse\nThe change in a subject’s endpoint compared to their baseline state.\n\n\nSubject\nSomeone recruited onto a clinical trial for the purposes of learning about the properties of a treatment. Depending on the type of trial they might be patients or they might be healthy volunteers.\n\n\nTreatment Arm\nSubjects on entering the study are randomized to different ‘treatment arms’. Subjects randomized to the same arm receive the same treatment and the responses of the subjects in the arm analyzed to determine the expected response to that treatment, allowing the expected responses to the different treatments to be compared.\n\n\nUG\nThe User Guide; describes how to use the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-purpose",
    "href": "documentation/v71/userguides/crm.html#sec-purpose",
    "title": "FACTS Dose Escalation CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-scope",
    "href": "documentation/v71/userguides/crm.html#sec-scope",
    "title": "FACTS Dose Escalation CRM",
    "section": "",
    "text": "This document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-context",
    "href": "documentation/v71/userguides/crm.html#sec-context",
    "title": "FACTS Dose Escalation CRM",
    "section": "",
    "text": "This document has been updated for the version 7.1 release of Dose Escalation FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-citing",
    "href": "documentation/v71/userguides/crm.html#sec-citing",
    "title": "FACTS Dose Escalation CRM",
    "section": "",
    "text": "If writing in LaTex and using Bibtex, if you wish to cite FACTS (thank you!), can we suggest the following:\n@techreport{FACTS71,\n  author = {{FACTS Development Team}},\n  title = {{FACTS}: Fixed and Adaptive Clinical Trial Simulator},\n  year  = {2024},\n  month = {12},\n  number = {Version 7.1},\n  type         = {Computer Software},\n  institution = {Berry Consultants LLC},\n  address = {Austin, TX},\n  note   = {https://www.berryconsultants.com/software/facts/}\n}\nThis will result in a reference that, for example in the APA style, will look like the following:\nFACTS Development Team (2024). FACTS: Fixed and adaptive clinical trial simulator. Computer Software Version 7.1, Berry Consultants LLC, Austin, TX. https://www.berryconsultants.com/software/facts/.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-definitions",
    "href": "documentation/v71/userguides/crm.html#sec-definitions",
    "title": "FACTS Dose Escalation CRM",
    "section": "",
    "text": "Table 1 gives an overview of the acronyms and abbreviations used in this document.\n\n\n\nTable 1: List of terms used in the CRM user guide\n\n\n\n\n\n\n\n\n\nName\nDefinition\n\n\n\n\nCap\nA limit on the number of subjects recruited. In FACTS N-CRM users can specify a cap on the overall number of subjects to be recruited in the trial (the ‘Overall Cap’) and specify stopping rules to define when the trial should stop before it reaches cap.\n\n\nControl\nIs the treatment arm with which the novel treatment(s) are principally being compared. Control may be placebo, or some existing standard of care, or therapy, against which the novel treatment has to be benchmarked in order to determine its likely usefulness.\n\n\nCore\nFACTS Core: A mode of FACTS for designing trials where multiple treatments, (possibly different doses of a novel treatment) are tested against a control and optionally an active comparator.\n\n\nCRM\nContinual Reassessment Method – a dose escalation design where the dose-toxicity is estimated using a simple Bayesian model, and the resulting estimates used to control the dose escalation and estimate the Maximum Tolerated Dose (MTD).\n\n\nDE\nDose Escalation: a mode of FACTS where subjects are treated in cohorts and dose escalation is determined by the number of toxicities observed.\n\n\nED\nEnrichment Designs: a mode of FACTS for designing trials where the same treatment is testing in different settings for example different sub-populations or different but related indications.\n\n\nFACTS\nFixed and Adaptive Clinical Trial Simulator.\n\n\nFinal Endpoint\nThe final value, or state, of a subject’s endpoint.\n\n\nGUI\nGraphical User Interface, the part of the FACTS application that the user interacts with.\n\n\nMethod\nIn the FACTS documentation we try to reserve the term ‘method’ for the algorithms used in the simulation (as opposed to the analysis) part of the program. In the analysis part we use the term ‘Model’, see below.\n\n\nModel\nIn the FACTS documentation we try to reserve the term ‘model’ for the statistical models used in the analysis of the trial data (in the ‘design’ section of the FACTS user interface). Where mathematical algorithms are used for other purposes in FACTS (for instance in the generation of the simulated data) we try to use the term ‘method’. We have found that initially it is very easy for users to be confused between these two parts of FACTS and we feel that using distinct terminology may help to reduce this.\n\n\nMTD\nThe dose most likely to be the Maximum Tolerated Dose (MTD) – the dose with the highest Pr(MTD).\n\n\nMTD+\nThe dose most likely to be the MTD+ – the dose with the highest Pr(MTD+).\n\n\nPr(MTD)\nA dose’s probability of being MTD is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable toxicity band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. This estimate of MTD is constrained to select one of the available doses.\n\n\nPr(MTD+)\nA dose’s probability of being the MTD+ is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. Unlike Pr(MTD), Pr(MTD+) includes estimating whether a dose below or a dose above the range of those being tested is more likely to have a toxicity in the acceptable band than any of the doses in the range.\n\n\nProfile\nA profile is a specification of one aspect of a scenario. A scenario is made up of one profile of each of the required types for the type of trial being simulated. FACTS allows the user to specify multiple profiles of each type and then presents all the possible combinations of profiles as scenarios that can be used to drive simulations.\n\n\nResponse\nThe change in a subject’s endpoint compared to their baseline state.\n\n\nScenario\nA scenario is the complete specification of the unknown external factors that determine the data observed on the trial and its timing. The exact factors depend on the type of trial being simulated but typically include:  -) the distribution of the final change from base line, or probability of response or rate of events in the different treatment groups  -) the properties of subjects’ early responses and the correlation with their final outcome  -) the rate at which subjects are recruited into the trial  -) the rate at which subjects drop out of the trial.\n\n\nSPEC\nThe Design Engine Specification, describes the system algorithms, and meaning of parameters.\n\n\nResponse\nThe change in a subject’s endpoint compared to their baseline state.\n\n\nSubject\nSomeone recruited onto a clinical trial for the purposes of learning about the properties of a treatment. Depending on the type of trial they might be patients or they might be healthy volunteers.\n\n\nTreatment Arm\nSubjects on entering the study are randomized to different ‘treatment arms’. Subjects randomized to the same arm receive the same treatment and the responses of the subjects in the arm analyzed to determine the expected response to that treatment, allowing the expected responses to the different treatments to be compared.\n\n\nUG\nThe User Guide; describes how to use the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "title": "FACTS Dose Escalation CRM",
    "section": "2.1 FACTS 7.1 Changes to N-CRM",
    "text": "2.1 FACTS 7.1 Changes to N-CRM\nIn FACTS 7.1 there were new features added to N-CRM:\n\nIt is now possible to backfill to the current escalation dose (also known as “frontfilling”).\nIt is now possible to specify a third queue concept – maximum number of patients in their DLT period on the current MTD estimate.\nIt is now possible to define the concept of “near” target/MTD as part of stopping rules, for both fine-grained and regular dosing.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "title": "FACTS Dose Escalation CRM",
    "section": "2.2 FACTS 7.0 Changes to N-CRM",
    "text": "2.2 FACTS 7.0 Changes to N-CRM\nIn FACTS 7.0 the only change was on the Analysis tab, when entering subject data manually, the FACTS GUI now ensures that a cohort number is entered for every subject.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "title": "FACTS Dose Escalation CRM",
    "section": "2.3 FACTS 6.5 Changes to N-CRM",
    "text": "2.3 FACTS 6.5 Changes to N-CRM\nIn FACTS 6.5 there was a new feature added to N-CRM:\n\nIt is now possible to generate a design report – a Word document describing design - once the design has been simulated. In FACTS 6.5 there was two small changes to the functionality:\nWhen deriving toxicity/efficacy priors from quantiles at the lowest and highest doses, the “middle” probability option now refers to the median dose rather than the reference dose and is now optional. When pre-6.5 FACTS files are loaded in FACTS 6.5, the “middle” probability option will be ignored and replaced by 0.5.\nWhen deriving toxicity/efficacy priors from specific quantiles the specification of at least two dose levels is now required whereas previously the specification of at least three dose levels was required.\n\nIn FACTS 6.5 there were some improvements in the simulated behavior:\n\nDesigns which include efficacy, the “Maximum cohorts used to determine MTD” parameter on the Allocation Rule tab is now observed, in FACTS 6.4 and earlier it was ignored.\nIn N-CRM designs using open enrollment without accrual pausing, FACTS will now output entries in the cohort file for subjects that have been lost following stopping rules being met.\nIn N-CRM designs using open enrollment, FACTS will no longer prematurely apply stopping conditions when there are still subjects whose outcomes have been observed, but not yet processed.\nIn N-CRM designs using open enrollment, FACTS will now report the selected MTD/MED/OSD and associated state to be the dose where the stopping conditions were first met, unless the MTD has subsequently decreased post stopping rules being met. This is to prevent a dose above the one selected when the stopping conditions were met being reported as the MTD when it is very likely that there is insufficient data on this higher dose to justify its selection. If rather than reporting the MTD at the point when the stopping rules where met, you would like the trial to resume if the dose selected as MTD has changed (and this the stopping rules possibly no longer met), ensure that the ”Pause accrual and wait for completers” option is selected on the “Stopping Criteria” tab. This allows the trial to resume if the recruitment cap has not been met.\nFACTS Command Line mode and FLFLL (FACTS Linux File Loader Lite) now correctly handle the processing of N-CRM designs whose prior was derived using the “Legacy Prior” option.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "title": "FACTS Dose Escalation CRM",
    "section": "2.4 FACTS 6.4 Changes to N-CRM",
    "text": "2.4 FACTS 6.4 Changes to N-CRM\nIn FACTS 6.4 there were no changes to the Dose Escalation simulators.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "title": "FACTS Dose Escalation CRM",
    "section": "2.5 FACTS 6.3 Changes to N-CRM",
    "text": "2.5 FACTS 6.3 Changes to N-CRM\nIn FACTS 6.3 a number of changes were made to improve facilities in N-CRM, or improve the way existing facilities were implemented. These were:\n\nNew run-in options: the existing run-in scheme is available as “simple run-in”, “custom run-in” allows a specific sequence of doses and number of subjects to test at each dose to be specified, “small cohort pre-escalation” allows a run that uses a smaller cohort size but follows the dose escalation rules and over dose control.\nNew “backfill” options in open enrolment. Backfill allows subjects that become available at a time when they can’t be allocated to the current dose (because the maximum number of subjects without final results have already been allocated to the current dose).\nImproved handling of “maximum subjects without final results” in open enrolment. In earlier versions of FACTS this was a “global” maximum, which led to a suboptimal allocation pattern and overly cautious rejection of subjects that became available. The new model applies a maximum “per dose” so that once the trial has escalated to a new dose strength, any subjects without final results on lower doses do not block allocation to the new dose, in addition it is possible to specify two different maximums – one for when a dose has just been escalated to but has not been “cleared” (typically smaller and more cautious), and one when a dose has been cleared but we continue to allocate to it because it is the target dose (typically larger and more confident). This method is such an improvement that we recommend moving any design using open enrolment to this new version of FACTS.\nImproved Ordinal Toxicity model – the way the likelihood is calculated has been improved – reducing the uncertainty in the model fit. Any design using an ordinal model will need to re-calibrate the prior if you move the design to FACTS 6.3. If you have a design already complete, or in execution we recommend you remain using the earlier version of FACTS for that trial.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "title": "FACTS Dose Escalation CRM",
    "section": "2.6 FACTS 6.2 Changes to N-CRM",
    "text": "2.6 FACTS 6.2 Changes to N-CRM\nIn FACTS 6.2 features available separately in the other FACTS CRM engines (CRM (Toxicity), bCRM & CRM Ordinal) were all incorporated into N-CRM. This allowed these features to be used in conjunction with N-CRM’s target toxicity band methodology, overdose control and open enrollment features, as well as in conjunction with each other for the first time.\nThe new features are:\n\nFrom CRM (Toxicity) the option to specify that the data is coming from ‘two groups’ and for the toxicity experienced in the two groups to be modelled with a joint model [CRM 2 Sample]. This allows a trial where there are two patient populations (such as adults and children) or where there are two versions of the treatment to be simulated.\nFrom bCRM the option to model a second binary efficacy endpoint [bCRM] and the for dose allocation to proceed in two stages – the first to establish an MTD and the second to establish an MED.\nFrom CRM Ordinal the option for the toxicity endpoint to be modelled not as binary endpoint, but one with different categories of toxicity, and with a joint model applied to the different categories [CRM Ordinal]. The endpoint can be to model either 3 or 4 categories of toxicity:\n\ncategory 1 is “no toxicity”,\ncategory 2 is “mild toxicity”,\ncategory 3 is “toxicity”\ncategory 4 (if included) is “severe toxicity”\n\n\nAll decision making is made in terms of the probability of observing a category 3 (or worse) toxicity.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "title": "FACTS Dose Escalation CRM",
    "section": "2.7 FACTS 6.1 Changes to N-CRM",
    "text": "2.7 FACTS 6.1 Changes to N-CRM\nIn FACTS 6.1 N-CRM has the new feature of ‘design variants’ that allow the user to easily simulate and evaluate an N-CRM design at different sample sizes. This change includes 4 elements:\n\nUnder the ‘Study’ tab the user can now specify the number of design variants, and for each variant the maximum study size in Cohorts.\nOn the simulation tab FACTS will display a copy of each simulation scenario for each variant.\nThe simulation results now include the Ppn of trials that stopped for each stopping reason: stopping because all doses are too toxic (the toxicity estimates exceed the overdose criteria), because a stopping rule was met or because the study cap was reached.\nThere are now a set of cross variant graphs that show trellis plots of the key summary graphs by design variant and scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#overdose-control",
    "href": "documentation/v71/userguides/crm.html#overdose-control",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.1 Overdose Control",
    "text": "3.1 Overdose Control\nOverdose control can be specified on the Study &gt; Toxicity tab. Overdose control specifies a limit on the probability that a dose has a toxicity rate above a certain level. After fitting the Bayesian logistic regression model, all doses for which the posterior probability that their toxicity rate lies above the specified level exceeds the specified limit are ineligible for allocation. Because the Bayesian Logistic regression is monotonic, this means that after every analysis either all doses are permitted for allocation or there will be a dose level above which no dose is permitted for allocation.\n\n\n\n\n\n\nFigure 1: Setting the overdose control limit\n\n\n\nThe overdose control is specified in terms of the “toxicity bands” (concept of allowing ranges for the target toxicity, excess toxicity, unacceptable toxicity and under-dosing explained in more detail in this section) and can either be in terms of the “excess and unacceptable toxicity bands” or just the “unacceptable toxicity band”. The “excess and unacceptable toxicity band” is every toxicity rate above the upper bound of the target band. Care should be taken when setting the permitted threshold for this joint band. If set below 0.5, it will likely exclude doses whose mean expected toxicity rate is within the target band with the risk that this makes the escalation decision in the design too cautious. Initially it might be recommended to just use the “unacceptable band” for specifying the overdose control. This allows an overdose control that is more strict – for example: “exclude any dose where the probability that the toxicity rate is above 0.6, is greater than 20%“. The lower bound for the unacceptable band can be set wherever desired, its only role is in defining this band for overdose control. It is also possible to specify that the limit changes over the course of the trial, allowing the overdose control to become stricter as more information becomes available. For example, one could reduce the permitted probability of a dose having a toxicity rate in the unacceptable band from 50% to 25% in steps of 2.5% after every cohort.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#dose-escalation-rules",
    "href": "documentation/v71/userguides/crm.html#dose-escalation-rules",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.2 Dose Escalation Rules",
    "text": "3.2 Dose Escalation Rules\nThe dose escalation could be solely controlled by the overdose control (as originally proposed in (Neuenschwander, Branson, and Gsponer 2008)), however this means that the escalation behavior is very dependent on the interplay between the prior and the observed data. Usually, teams prefer to have a fixed set of rules in place ensuring the escalation behavior is sufficiently cautious. FACTS has an option to just use overdose control or to use a combination of overdose control and a set of fixed escalation rules. In the latter case, the following rules can be set in the Design &gt; Allocation Rule &gt; Allocation tab:\n\n\n\n\n\n\nFigure 2: Dose escalation rules\n\n\n\nWe introduce the notion of whether a dose has been “cleared”. A dose is cleared once we have sufficient data on it (usually, but not necessarily, the results of one cohort, but if the cohort size is small, for example 2 subjects, perhaps more than one cohort will be required). This can be supplemented by a rule that if the observed raw toxicity rate at the dose exceeds a certain limit, then the dose is not counted as cleared (this rule is usually unnecessary if overdose control limits have been set). Once a dose has been cleared, it stays cleared, meaning there is “maximum cleared dose”. The number of dose increments or the factor of dose strength above the current cleared dose that can be allocated to is then specified. For example, with doses of 12.5, 25, 50, 100, 150, 200, 250, we might allow escalation at two dose increments a time. In the figure below, you see the combination of settings used to achieve this behaviour alongside the “Fastest Possible Dose Escalation” plot on the right:\n\n\n\n\n\n\nFigure 3: Escalation by number of dose increments\n\n\n\nAlternatively, we can specify the permitted escalation as a ratio, for example we might allow the dose strength to be at most tripled at each escalation, which, with the example dose strengths, makes the initial escalation more cautious:\n\n\n\n\n\n\nFigure 4: Escalation by dose strength factor\n\n\n\nThe escalation rules can be adjusted so that instead of a single increment rule, there are different increments depending on the dose, or depending on the number of observed toxicities. To modify our earlier example, we can allow escalation by 2 dose levels while no toxicities have been observed, but limit it to only one dose level once one or more toxicities have been observed:\n\n\n\n\n\n\nFigure 5: Escalation increment varying by number of toxicities\n\n\n\nLastly escalation can be relative to the highest cleared dose, or relative to the last dose allocated.\nTo summarize the allocation procedure:\n\nThe current maximum cleared dose is identified.\nThe current data is analyzed using the Bayesian Logistic Regression model.\nThe overdose rules are evaluated and all doses exceeding the overdose control limit are excluded from this escalation selection.\nFrom the remaining doses, the dose best meeting the target MTD or target toxicity interval objective based on the model is selected as the “target dose” (TD).\nIf the TD is at or below the current maximum cleared dose, the next cohort is allocated to the TD.\nIf the TD is within the escalation rules of the current maximum cleared dose, the next cohort is allocated to the TD.\nOtherwise, the next cohort is allocated to the highest dose above the current maximum cleared dose as allowed by the escalation rules.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#initial-run-in",
    "href": "documentation/v71/userguides/crm.html#initial-run-in",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.3 Initial Run-in",
    "text": "3.3 Initial Run-in\nThe purpose of defining a run-in is to define a fixed allocation behavior to be followed up to the first toxicity being observed. The specified number of subjects to allocate to each dose in the run-in and which doses to test are specified. This scheme is followed until a toxicity is observed or we reach the end of this fixed scheme.\nThree forms of run-in specification are available:\n\nSimple: allocates a small cohort to every defined dose in ascending order (unless fine grain doses - see this section – have been specified, in which case the escalation rules are followed).\nCustom: allocates a defined number of subjects (possibly varying by dose) to selected doses in ascending order.\nSmall cohort pre-escalation: allocates a small cohort, but follows the escalation rules assuming just a single small cohort is required to clear a dose.\n\nAll run-in schemes can be modified in a number of ways:\n\nSpecifying a maximum dose at which the run-in stops if no toxicities are observed until that dose.\nIf ordinal toxicities are being simulated, the run-in may should at the first observed category 2 toxicity (rather than a category 3 toxicity)\nWhether the subjects used in the run-in should be counted towards the trial sample size or not.\nWhen a toxicity is observed the standard behavior is to allocate to the minimum of: the last dose tested in the run-in, the current TD or the highest dose that can be allocated to by the overdose rules. This can be replaced by expanding the allocation on the current dose to make it a full cohort as specified in Study &gt; Study Info tab (this option is particularly useful in conjunction with stopping for a category 2 toxicity).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#two-groups",
    "href": "documentation/v71/userguides/crm.html#two-groups",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.4 Two Groups",
    "text": "3.4 Two Groups\nFACTS has the option to model the subjects in the trial as belonging to two different groups, these can be either:\n\nTwo groups distinguished by a baseline property of the subjects, for example adults and paediatrics.\nTwo groups separated by a difference in treatment (and selected randomly), for example the study drug alone or in combination with an additional drug.\n\nThere are options for when group 2 starts enrolling:\n\nThey can be recruited sequentially – group 1 then group 2.\nThey can be recruited in parallel\nThe second group can be started when the allocation to the first group reaches a particular dose\nThe second group can be started when the number of subjects allocated to group 1 reaches a particular threshold.\n\nA joint model is fitted to the two groups.\nThe first group is modeled:\n\\[\nlogit(p_{1j}) = \\alpha + \\beta \\hat{x}_j\n\\]\nThe second group is modeled:\n\\[\nlogit(p_{2j}) = (\\alpha + a) + (\\beta + b) \\hat{x}_j\n\\] With separate priors and some optional constraints on \\(a\\) and \\(b\\). Dose escalation and stopping are judged independently for the two groups.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-efficacy",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-efficacy",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.5 Efficacy",
    "text": "3.5 Efficacy\nFACTS has the option to additionally model an efficacy endpoint. There are currently two limitations in simulation:\n\nOnly a binary efficacy endpoint can be simulated\nThe efficacy endpoint is assumed to be available at the same time as the toxicity endpoint.\n\nThe efficacy and toxicity endpoints are modelled separately. There are options to specify early stopping rules for finding the MTD, and to specify a cap on the sample size that can be spent finding the MTD. Once these rules are met, then allocation is towards the Minimum Efficacious Dose (MED) – if this is below the MTD. If the estimated MED lies at or above the estimated MTD, the allocation is at the estimated MTD.\nIf while allocating to the estimated MED further toxicity results change the estimate of the MTD, and if there is now insufficient information on the MTD as specified by the early stopping rules for finding the MTD, allocation switches back to allocating to the estimated MTD, if the sample size cap for finding the MTD allows.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-fgd",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-fgd",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.6 Fine Grain Dosing",
    "text": "3.6 Fine Grain Dosing\nIn some settings, e.g. when the drug is delivered in solution by IV or when manufacturing allows any dose in a range from say 100mg to 400mg in steps of 10mg, dose strengths need not be restricted to just a small number of pre-defined levels. FACTS has a feature that allows this to be simulated, not with a continuous range of doses, but with “fine grain” dosing.\nFACTS supports the specification of a range of doses from a minimum to a maximum with doses either equally spaced or spaced with equal ratio. Using dose ratio makes most sense it you want to use the dose strength whilst believing the effect will be roughly log-dose. Using dose ratios, it’s necessary to accept FACTS reporting dose strengths only close to those desired. As an example, if the main doses followed a dose doubling scheme: 12.5, 25, 50, etc., one might use fine grain dosing with dose space ratios of approximately the 4th root of 2 (1.1892). The resulting doses are 12.5, 14.865, 17.677, 21.022, 24.999, 29.729, 35.354, 42.043, 49.998, etc., which means there are three dose levels between each of the original doses.\nThere are two alternatives:\n\nUse nominal dose strengths 1, 2, 3, 4, … (i.e. assuming the dose spacing is linear in expected effect) and label the doses according to their actual strength.\nUse a fixed dose interval (e.g. 12.5 resulting in doses of 12.5, 25, 37.5, 50, 62.5, etc.) so the lower doses (of the original scheme) have fewer (or no) intermediate doses and the higher doses have many more. The dose escalation rules can be specified in terms of dose strength ratio to achieve the required escalation, for example allowing dose escalation with a dose strength ratio of 2 will result in the initial escalation using doses 12.5, 25, 50, 100, etc.\n\nAs well as possibly adjusting the dose escalation step size to accommodate the new dose levels on the Design &gt; Allocation Rule tab, there are two other rules that may need modification:\n\nTo count a dose as “cleared”, we might now count cohorts on nearby doses to count towards the required clearing total. This is specified as the “Max ratio of dose strengths considered as near” (if dose allocation rules apply to ratio of dose strength) or “Delta in dose strength considered as near” (if dose allocation rules apply to dose strength) on the Design &gt; Allocation Rule tab.\n\nFor example, if we have doses at roughly 4th root of 2 intervals, we might count any dose within a ratio of 1.2 as “near” so that any cohorts allocated to immediate neighbor doses count towards clearing a dose.\nAlternatively, if we have doses every 12.5mg from 12.5 to 400, counting any dose within a ratio of 1.1 will mean that from dose 125 and above, immediate neighbor doses (within 12.5) count towards clearing a dose, and from dose 250 and above, doses within 25mg (two immediate neighbor doses) count towards clearing a dose.\n\nThe concept of “near doses” in fine grained dosing allows us to skip certain doses in the escalation phase, which might make sense if there is reason to believe that doses of similar dose strengths behave similarly and don’t provide enough additional information to justify assigning more cohorts to.\n\n\n\n\n\n\nFigure 6: Doses from 12.5 to 400mg, with fixed spacing of 12.5. Showing dose escalation by dose doubling.\n\n\n\nWhen requiring a certain number of cohorts to have been allocated to the estimated MTD before the trial can stop / to allow the trial to stop, we might now count cohorts on doses near the estimated MTD as counting towards that total. This is set on the Design &gt; Stopping Criteria tab. In considering which doses are near, the same logic as on the Design &gt; Allocation Rule tab regarding Dose Strength or Ratio of Dose Strength will be used.\n\nIf Dose Strength is used, then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength. For example, by +/- 12.5 mg:\n\n\n\n\n\n\nFigure 7\n\n\n\nIf ratio of Dose Strength is used then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength ratio. For example, by +/- 10%:\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nNote that with Fine Grain dosing, if a band is specified for a dose to count as cleared, then the maximum cleared dose will be the maximum dose within that band, and if incrementing relative to the Maximum cleared dose, then the maximum permitted increment will be relative to the maximum dose within the cleared band.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-oe",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-oe",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.7 Open Enrollment",
    "text": "3.7 Open Enrollment\nOpen enrollment (Broglio et al. 2015) can be used instead of cohort enrollment. Cohort enrolment enrolls a fixed number of subjects to a given dose, then waits until their treatment and follow up is complete (so their final status – whether they suffered a DLT (Dose Limiting Toxicity) or not – is known) before deciding on the next dose to allocate to and then recruiting the next cohort. This likely means that subjects become available for inclusion in the trial, but have to be turned away as the trial waits for the current cohort to complete. Open enrollment attempts to address this by allowing subjects to be enrolled whilst the current “cohort” is completing. However, this may come with some risk – more than a cohort’s worth of subjects may now be exposed to a new dose before we have any estimate of its DLT rate. To allow this risk to be managed, open enrollment introduces two new concepts:\n\nWhen allocating to an uncleared dose, a cap can be set on the number of subjects that can be allocated to that dose who have not yet got their final results (“OE cap 1”). For example, if this number is set to 3 (to be the same as a common cohort size), after 3 subjects have been recruited and allocated to the current dose, no more subjects will be allocated to this dose until at least one of these subjects has completed. Until then, potential subjects that become available will be turned away unless backfilling is enabled (see next point below). But unlike cohort enrolment, as soon as the first of the subjects on the dose completes, a subject that comes available could now be allocated to the dose, depending on further rules explained below (frontfilling) – unless of course that subject’s result has changed the estimated MTD. Note that the trial won’t escalate beyond the current dose until the required number of subjects to clear the dose have completed. By default, the trial won’t allocate more than the number of subjects required to clear the dose until the dose is cleared, meaning if 3 subjects are required to clear a dose and 3 subjects have been allocated to this dose, even when 1 or 2 of these subjects have their final results and a new subjects is enrolled, they won’t be allocated to this dose. If this is regarded as over cautious, it can be modified by enabling frontfilling, allowing 3 subjects without final results simultaneously. In the above example, this would mean we could place a fourth subject on the dose when the result of the first subject has come in and a fifth subject as soon as the result of the second subject has come in.\nWhen the cap on the number of subjects without final results on an uncleared dose has been reached (“OE cap 1”) new potential subjects will be turned away, unless backfilling is enabled. Enabling backfilling allows these subjects to instead be included, allocating them to a lower dose that has already been cleared. Whilst such an allocation may not contribute as much to identifying the MTD as allocating to the current dose would, it can still contribute by:\n\nIncreasing the information on the next lower dose can inform the estimate of toxicity on the current dose through the Bayesian logistic model.\nProviding additional information on a dose that it may be necessary to de-escalate too if the current dose turns out to be too toxic.\n\nIt can also contribute information on other endpoints (such as efficacy). Once backfilling has been enabled, it is also possible to enable frontfilling. For more information on backfilling and frontfilling, see this section.\n\nAssume at a given point in time we want to allocate a subject to a specific dose, denoted by “candidate dose”. FACTS allows 3 different caps to be specified on how many subjects who have not yet got their final results (i.e. are not yet complete) can be allocated to this candidate dose:\n\n\n\n\n\n\nFigure 9\n\n\n\n\nMaximum subjects without final results if dose is uncleared: As described early in this section, we encounter this cap during escalation when the candidate dose is not yet cleared. This cap takes into account subjects not yet complete on the candidate dose and any higher dose (“OE cap 1”).\nMax subjects without final results if dose is cleared and below MTD: We encounter this cap when the candidate dose is cleared and below the estimated MTD (which can happen if the estimated MTD is beyond the range of available doses, when backfilling, or when allocating during the efficacy phase of a toxicity plus efficacy trial). This cap takes into account subject not yet complete on the candidate dose and any lower doses (“OE cap 2”).\nMax subjects without final results if dose is cleared and at MTD: We encounter this cap when the candidate dose is cleared and the current model estimated MTD (which can happen when after clearing a dose we decide not to escalate, or after de-escalating) (“OE cap 3”).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-bf",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-bf",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.8 Backfilling and Frontfilling",
    "text": "3.8 Backfilling and Frontfilling\nAs described in the preceding section, Backfilling is the allocation of subjects to a lower dose when, due to restrictions, it is not possible to allocate a subject who comes available to the current dose (Dehbi, O’Quigley, and Iasonos 2021). FACTS provides a number of options to configure how backfilling behaves. Backfilling can be enabled in the Study &gt; Study Info tab. The total sample size can be divided between the subjects allocated as part of conventional dose escalation and those allocated using backfill. When backfill is enabled, it is important to increase the total sample size and then limit the number that can be allocated using backfill, as subjects allocated using backfilling will not contribute to the escalation and the confirmation of the MTD and it’s usually important to retain sufficient sample size to achieve this aim.\n\n\n\n\n\n\nFigure 10\n\n\n\nWhen enabling backfilling, several options can be specified in the Design &gt; Backfill Allocation tab.\n\n\n\n\n\n\nFigure 11\n\n\n\n\nTwo maximum caps can be specified on the number of subjects that are assigned in the process of backfilling to a given dose:\n\nan overall cap on subjects per dose that cannot be exceeded by backfill, counting also subjects that were assigned to that dose through regular allocation\na cap on the number of subjects per dose that were allocated by backfill, counting only subjects that were assigned to that dose using backfilling.\n\nHow many dose levels below the current dose can be allocated to when backfilling. Backfilling will always be to the highest dose possible (which might be the current dose if frontfilling is enabled, otherwise it will be below the current dose). Allocation to the next highest dose might be limited either by an open enrolment cap if there are already subjects allocated to that dose who have not yet completed, or it might be limited by the backfill caps described above. If allocation to the dose below the current dose is not possible, backfilling will by default look at the dose below that (two levels below the current dose) and so on. Using this option can ensure no backfilling happens to doses that are too far below the current dose.\nThe lowest dose that can be allocated to when backfilling. This option is particularly useful when there is reason to believe doses below a certain level will not be effective.\nWhether frontfilling is allowed – frontfilling allows allocating more subjects to uncleared doses than the number required to clear that dose (see this section).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "href": "documentation/v71/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "title": "FACTS Dose Escalation CRM",
    "section": "3.9 Open Enrollment, Backfilling and Fine Grain Dosing",
    "text": "3.9 Open Enrollment, Backfilling and Fine Grain Dosing\nWhen using open enrolment and fine grain dosing, the interval defined on the Design &gt; Allocation Rule tab “Delta in dose strength considered as near +/-“, or “Max ratio of dose strengths considered as near” is crucial: it is used to define the range of doses where subjects allocated to any of them count towards clearing a dose.\n\nIf dose allocation rules are selected to apply to “Dose strength”, the interval is defined “Delta in dose strength considered as near +/-“. Thus, for example, if this is set to 2, then subjects complete on doses with strength in the range 4-8 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 8.\nIf dose allocation rules are selected to apply to “Ratio of strength”, the interval is defined “Max ratio of dose strengths considered as near“. Thus, for example if this is set to 1.5, then subjects complete on doses with strength in the range 4-9 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 9. These ranges also apply when assessing “OE cap 1-3”, and how many subjects have been allocated overall, or via backfilling. In backfilling, FACTS checks each dose strength and the doses in its “near” interval range, at (if frontfilling) or below the current dose, until the first dose strength is found where backfilling can take place.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#the-file-menu",
    "href": "documentation/v71/userguides/crm.html#the-file-menu",
    "title": "FACTS Dose Escalation CRM",
    "section": "6.1 The “File” Menu",
    "text": "6.1 The “File” Menu\nFACTS has a File menu with commands similar to those found in most Windows applications, but also some unique to FACTS. The commands can be found in Table 2.\n\n\n\nTable 2: List of commands in the CRM file menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nNew\nAllows the user to select a trial design type and start a new design; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nOpen\nAllows the user to select an existing FACTS file and open it; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nSave\nSaves the current changes in the design to the currently open FACTS file.\n\n\nSave As\nAllows the current version of the design to be saved to a new FATCS file; this file is then the “currently open” FACTS file going forward.\n\n\nExport Project\nCreates a “Zip” archive containing the current FACTS file and all the simulation results, so these can be easily archived, emailed, saved, etc.\n\n\nRecent Projects\nContains entries for the last ten FACTS files opened: selecting one re-opens it.\n\n\nRecent Folders\nContains entries for the folders that contain the last ten FACTS files opened: selecting one opens a file browser window on that folder allowing a ‘.facts’ file in that folder to be selected (or allowing the user to navigate to some different folder and select a ‘.facts’ file there).\n\n\nExamples\nContains example projects for the different design engines: selecting one opens it.\n\n\nClose\nCloses the current FACTS file, returning the user to the FACTS introduction screen.\n\n\nExit\nCloses FACTS altogether.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-settings-menu",
    "href": "documentation/v71/userguides/crm.html#facts-settings-menu",
    "title": "FACTS Dose Escalation CRM",
    "section": "6.2 FACTS Settings Menu",
    "text": "6.2 FACTS Settings Menu\nThe “Settings” command menu allows the user to do 2 things:\n\nSet various FACTS options to local settings – see below for details.\nReset the options based on the stored configuration file. This file, “config.xml”, will initially be installed during the FACTS installation process and is stored in the Windows “Program Files” folder, in the sub-folder where FACTS get installed.\nChange the stored configuration file. This command allows you to select a new configuration file and have FACTS copy it to the sub-folder within the Windows “Program Files” folder, where FACTS get installed so it becomes the new stored configuration file. This allows IT support to easily disseminate configuration changes.\nEnter a new or changed license key.\n\n\n6.2.1 Set Options\nThe FACTS Options dialog allows the user to:\n\nSet and Test the connection parameters to access a compute grid for running simulations.\nConfigure the version and location of R or R Studio that can be launched from within FACTS\nSelect how gamma distributions are parameterized.\n\n\n6.2.1.1 Grid Configuration\nA grid compute facility for running simulations will only be available if your local IT services have set one up. If they have done so, they\n\nMay have already set the appropriate parameters In the FACTS configuration file included with the FACTS installation files.\nInform you of the parameters to be set manually via this dialog\nSend a new configuration file that can be installed using the “Load Options” menu command.\n\nIf modifying the grid options manually, select the “Options” menu command and enter the values on the “Grid Configuration” tab of the displayed dialog window.\n\n\n\n\n\n\nFigure 16: Webservice Configuration\n\n\n\nFirst select the type of interface to the grid to be used, this is either:\n\nVia a network shared drive (with a “sweeper script” running on a client machine to transfer jobs to the grid management system and return results from it).\nVia a web service system using a webserver and database to communicate to a grid management system. The IT group supporting the grid should be able to tell you which interface they have implemented, if any. If access to the grid is via a Network Share it is necessary to specify:\nThe location of the network share folder, usually in the form \\&lt;server name&gt;\\&lt;folder name\\&gt;.\nWhether the grid client is running Windows or Linux (so end-of-line characters can be corrected)\nThe listener delay – this is the interval between “looks” when FACTS is waiting for simulation results to be complete\n\nOnce specified it is possible to use the “Test” button to check that the Network Shared folder is accessible and writeable.\nIf access to the grid is via a web service:\n\nThe location of the web service endpoint.\n\nClicking on “Test Configuration” and will cause FACTS will attempt to connect to the FACTS grid controller. The control will show which components of the connection are working.\nSee the FACTS Installation Guide and FACTS Simple Grid Interface Guide for more details of setting up a grid.\n\n\n6.2.1.2 R Configuration\nIn FACTS on the Simulation tab there are two controls that launch R – “Open in R” and “ Design Report” (in FACTS 6.2.0 the latter only available for FACTS Core designs).\nTo enable these to work the user must specify where the R or RStudio executable is installed and (if there is more than one version of R installed) which version of R to use.\n\n\n\n\n\n\nFigure 17: The R Configuration Dialog\n\n\n\nThe dialog allows the user to Add, Edit, Test and Remove links to versions of R.\n\n\n\n\n\n\nFigure 18: Adding a link to R\n\n\n\nClicking on “Add” opens a normal Windows directory browser window, the user must navigate to the location of an R installation (for example “C:\\Program Files\\R\\R-2.15.2\\bin”, select the file R.exe, and click “Open”. This adds a new entry on the R configuration dialog.\nClicking on “Edit” operates similarly to “Add” above, except the selected location replaces that currently selected entry on the R configuration dialog rather than adding a new one.\nClicking on “Test” checks whether the currently selected entry on the R configuration dialog is available, if it is not an error dialog is displayed:\n\n\n\n\n\n\nFigure 19: Example of R Configuration error\n\n\n\nClicking on “Remove” removes the currently selected location on the R configuration dialog.\nThe version of R to use by default is selected by clicking on the ‘Active’ check box of the version to use.\n\n\n6.2.1.3 Gamma Distribution Parameters\nIn FACTS a number of parameters require inverse gamma distributions to be specified as priors for the parameter value. There are two different parameterization of the inverse gamma provided so that the user can select the form they find the most intuitive.\n\n\n\n\n\n\nFigure 20: The parameterisation of Inverse Gamma Distributions\n\n\n\nThe first form uses parameters that are the mean of the distribution and the equivalent weight in terms of the equivalent number of observations. The second form uses an ‘Alpha’ and ‘Beta’ parameterization that some statisticians are familiar with and will find natural to use.\n\n\n\n6.2.2 Enter a license key\nIf a new license key is required, this command can be used to enter one. There are two ways of entering a new license:\n\n\n\n\n\n\nFigure 21: Enter FACTS License Key\n\n\n\nThe key can be entered directly, along with the associated Organization name, or by selecting a supplied license file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#the-help-menu",
    "href": "documentation/v71/userguides/crm.html#the-help-menu",
    "title": "FACTS Dose Escalation CRM",
    "section": "6.3 The Help Menu",
    "text": "6.3 The Help Menu\nFACTS has a Help menu with commands to assist you with the use of FACTS, providing links to users guides, tutorial and training videos. The commands are:\n\n\n\nTable 3: List of commands in the CRM help menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nUser Guides\nProvides access to documents such as this one, with (mainly) one user guide to each design type within FACTS. Exceptions to this simple structure are:  1. Core Design User Guide: A guide to the options under the ’Design” tab for FACTS Core for all endpoints.  2. Staged Design User Guide: As the staged design allows the design of one FACTS Core stage followed by a second, most of the interface is common to the basic FACTS Core. This guide describes the differences and additional aspects for all endpoints.  3. Dose Escalation User Guide: This covers all the Dose Escalation engines except for N-CRM and 2D-CRM that have their own. It thus covers the 3+3, mTPI, CRM(Toxicity), CRM(Ordinal), CRM(Efficacy) and bCRM engines.\n\n\nTutorials\nProvides access to all the tutorial documents, which describes detailed examples of use of all the engines in FACTS and many of their options. The examples under the File &gt; Examples menu option largely correspond to the different tutorials described here.\n\n\nDesign Specifications\nThese are technical documents that describe the mathematical models implemented in FACTS in detail.\n\n\nExecution Guides\nThe FACTS GUI can be run in command line mode so simulations can be run/re-run from scripts. With the simulation command line flag, and passed a directory rather than a file, FACTS will run simulations for every “.facts” file in that directory – and recurse into any sub-directories and simulate any “.facts” files there too. A full guide to command line mode can be found here. The FACTS simulation engines are also available in “command line executable” form. There are guides here that document their command line parameters and how to use them to analyse a data set – e.g. to perform an interim analysis whilst executing a trial designed with FACTS.\n\n\nFACTS file XML Specs\nThese guides describe the parameters in the “.facts” files, which are text files in XML format. For expert users understanding this format allows them to use scripts to generate versions of an initial “.facts” file with slight variations in the parameters such as stopping thresholds or priors. Modification of “.facts” files outside of FACTS needs to be done with care, errors may render the file unusable by FACTS.\n\n\nVideos\nProvides access to links to the introductory, training and webinar videos that Berry Consultants has recorded and makes available over the internet to FACTS users.\n\n\nView log…\nIf an error has occurred in FACTS, often the FACTS log file can shed light on what is going wrong. The log file is hidden away in some unfashionable and hard to locate Windows folder; this command option provides easy access to it. Allowing you to email facts support with a description of what occurred, attaching a copy of this log file having saved it somewhere convenient such as your desktop.\n\n\nSupport\nLaunch a simple editor for sending an email to our support account: facts@berryconsultants.net\n\n\nAbout\nDisplays a simple “about box” that includes the detailed version number of FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-studyinfo",
    "href": "documentation/v71/userguides/crm.html#sec-studyinfo",
    "title": "FACTS Dose Escalation CRM",
    "section": "7.1 Study Info",
    "text": "7.1 Study Info\nThe Study Info sub-tab provides parameters for specifying:\n\nWhether the trial has an Efficacy endpoint as well as a Toxicity one.\nWhether recruitment is in Cohorts or uses Open Enrolment,\nWhether the trial data is being analyzed as a single population (single group) or two groups (which could be 2 different patient types, or 2 different treatment types).\nThe option to specify that the trial should include an expansion cohort once the MTD has been identified.\nThe option (if using open enrolment) to specify the use of backfill.\n\nIncluding an efficacy endpoint – this allows the trial to include a binary efficacy outcome that is observed at the same time as the toxicity endpoint. Once the MTD has been sufficiently determined further cohorts are allocated to determine the MED (Minimum Effective Dose) as long as that is below the MTD, until the maximum sample size or MED stopping rules have been reached.\nCohort versus Open enrolment: cohort enrolment is the standard way of running a phase 1 trial, a cohort of subjects of pre-determined size are treated at the current dose and the trial pauses until all the subjects in the cohort are complete, then the dose for the next cohort is determined. A phase 1 trial using Open Enrolment recruits ‘all comers’ with a cap on the number of subjects who can be treated but have not yet had a final result. Subjects that arrive whilst this cap has been reached are deemed to be ‘lost’ to the study.\nIf Open Enrolment and Efficacy Endpoint options are being used together, then subjects who arrive who cannot be allocated to the MTD (because the cap number of subjects awaiting a final result has been reached), can be allocated to the MED (as long as that is below the MTD).\nIf the trial is analyzing 2 Groups then a joint statistical model is used with options to constrain the group 2 difference in the intercept term to be +ve or -ve, and options as to whether a common or separate estimates of the slope term are used.\nIf an expansion cohort is included, the this is a single cohort (or one per group, if 2 groups is being used) typically much larger than used during the dose escalation, that is assigned at the end of the study to the target dose. FACTS simulates the results that arise from this cohort and a final analysis.\nIf open enrolment is being used, the further option to use backfill becomes available. The parameters on this tab for backfill, are to specify the maximum number of subjects that can be allocated for escalation, and the maximum that can be allocated in backfill. These two maximums should not total less than the overall “Max subjects” that can be enrolled. If adding backfill to a trial, usually the previous “Max subjects” becomes the “Max study allocation for escalation”, and an additional sample is allowed for backfill and added to the overall Max subjects.\nIf the trial has two groups the backfill maximums are the sum of the subjects in the two groups.\nFor Cohort enrolment, the parameters are:\n\nMaximum Study Size, in cohorts: the maximum number of cohorts the trial can use, though designs can include conditions that cause them to stop earlier.\nIf the trial has two groups, the maximum number of cohorts of the second group.\nCohort size: The number of subjects in each cohort. The maximum sample size for the trial is simply: Max Trial Size (cohorts) * Cohort Size.\nCohort information delay: Normally left at zero, this allows for the trial to be simulated with cohorts recruited more quickly than if waiting for the last cohort to complete before treating the next cohort. This allows the trial to complete in shorter time, but is likely to lead to a larger incidence of toxicities. If the delay is set to 1, the second cohort will be recruited before the results for the first cohort are available, the third cohort will be recruited after the results of the first cohort are available but before the results for the second cohort are available, and so on.\nExecution rate: the time taken to recruit, treat and complete the observation of each cohort (in weeks). The value of this parameter does not affect the behavior of the simulations, but it allows a nominal “duration” of each simulation to be calculated. Unlike other FACTS simulations, this duration is not simulated stochastically, it is simply the number of cohorts * this duration. Its purpose is to give a figure to compare with open enrolment designs of the same trial.\n\n\n\n\n\n\n\nFigure 22: Study Info - Cohort Enrolment\n\n\n\n\n\n\n\n\n\nFigure 23\n\n\n\nIf rather than Cohorts, subjects are recruited using open enrolment, the parameters are:\n\nMax subjects: the maximum number of subjects who can be recruited into the study.\nTime unit – this is a text string that will change the “units” label for time on graphs. This allows data to be more easily entered when the natural time unit is not “weeks”, but “days” or “months”.\nMean recruitment rate: for simulation purposes this is the average number of subjects who will be expected to be available for recruitment during each time period. Recruitment is simulated using a standard Poisson process.\nTime until final result: the time for each subject for their final result to become available. This is simulated as a fixed length of time that is the same for all subjects.\nMaximum subjects without a final result: the cap on the number of subjects on the current dose or a backfill dose (if backfill is enabled) who can be recruited but not yet completed. This is also known as the ‘maximum queue length’. Subjects arriving and available for recruitment while the number of subjects on this dose who have been treated but have not yet completed is at this maximum, are dropped and assumed no longer available for recruitment. Once the current subjects complete the study has to await further new subjects to become available. There are three caps:\n\nMaximum subjects without final results if the dose is uncleared. This allows the design to be cautious when a new dose is used for the first time.\nMaximum subjects without final results if dose is cleared and below MTD. This allows a larger number of subjects without final results to be recruited at backfill doses or at the current escalation dose if backfill to the current escalation dose (“frontfilling”) is enabled, or at the MED in the efficacy phase of a trial that includes an efficacy endpoint.\nMaximum subjects without final results if dose is cleared and at MTD. This allows us to be more cautious if the model thinks all doses are toxic or if we are allocating at the model MTD and don’t want to expose too many subjects.\n\nBackfill – this can be enabled. Backfilling is the allocation of subjects to a dose below the current target dose, if the number of subjects allocated to the current target dose without final results has reached the maximum. Further parameters for backfill are set on the “Backfill” tab under the “Design” tab. On this tab, if backfill is enabled, two sub-maximums can optionally be specified:\n\nthe maximum number of subjects who can be allocated as part of usual allocation for escalation and MTD determination (and MED if efficacy is included in the trial),\nand the maximum number of subjects who can be allocated as part of “backfill”.\n\n\n\n\n\n\n\n\nFigure 24: Study Info - Open Enrolment\n\n\n\nGroups: a trial can be analyzed as a “single group” or as “two groups”. If analyzed as a single group, then all subjects are assumed to be the same and treated the same (except for the difference in the dose strength). If analyzed as two groups this allows either:\n\nThe subjects can be simulated as coming from two similar but distinct groups such as: adults and children, first line or recurrent, having some concomitant treatment or not. The separation into the two groups is based on some property of the subject.\nOr the subjects can be simulated as having been allocated (possibly randomized) to one of two versions of the treatment, with the same rang of dose strengths and differing in some other way such as dosing schedule, treatment duration or combination with an additional treatment. The separation of the subjects into the two groups is under the control of the protocol.\n\nIn either case the same analysis options are available (hence we use the generic term “groups” to describe this feature).\nIf enrolment is by cohort, the there are two separate “maximum study sizes” in cohorts – one for each group.\nThe Group 2 recruitment, while it overlaps in time with the Group 1 recruitment, is simulated as being in lock-step and the recruitment of the cohort in each group is concurrent and analyzed when both are complete. In the ‘cohorts.csv’ files that are output, the cohort numbers indicates which cohorts were concurrent. The options are:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the first group1 cohort has been allocated the specified dose (if cohorts can be accrued before the cohort before has completed, the group 2 is accrued too – it does not wait until the group 1 cohort completes unless the next group 1 also waits).\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the specified number of subjects had been recruited into group 1.\n\nIf enrolment is open then there are options similar to the cohort enrolment to control when enrolment into group 2 starts:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 subject can be recruited after the first group1 subject has been allocated the specified dose.\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 subject can recruited after the specified number of subjects had been recruited into group 1.\n\nBut in addition the user specifies the maximum number of subjects in Group 2 and how the recruitment is to be simulated:\n\nWith the group membership a property of the subject – along with a mean recruitment rate for group 2.\nWith subjects randomized between the two groups (the randomization is fixed at 1:1).\n\nThe user specifies the three “Maximum subjects without final results …” for the second group.\nIf backfill is enabled, the backfill totals apply to the total of the subjects on both groups.\n\n\n\n\n\n\nFigure 25: Study info - 2 group options with open enrolment\n\n\n\nEnable Final Expansion Cohort: if this is enabled a final cohort of specified size will be allocated the dose selected as MTD at the end of the N-CRM phase of the study:\n\nIf the study includes a control arm, the number of subjects in this expansion cohort to be allocated to control is also specified.\nIf the study has two groups, two separate expansion cohorts will be allocated, their sizes are set separately.\nIf the study includes observing efficacy then the target dose can be changed from MTD to MED or OSD.\n\n\n\n\n\n\n\nFigure 26\n\n\n\nSimulating an additional efficacy outcome is simply specified by checking the “include efficacy” checkbox.\n\n\n\n\n\n\nFigure 27: Study tab with “Include efficacy” checked\n\n\n\nSimulating an efficacy endpoint can be combined with all the other features (two groups, open enrolment, backfilling) already discussed, as well as with ordinal toxicity and fine grain dosing that are described below.\nCurrently there are two significant limitations to the simulation of an efficacy endpoint:\n\nThe endpoint is assumed to be dichotomous.\nThe endpoint is assumed to be observable at the same time as toxicity.\n\nWe hope to lift these restrictions in a later version of FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity",
    "href": "documentation/v71/userguides/crm.html#toxicity",
    "title": "FACTS Dose Escalation CRM",
    "section": "7.2 Toxicity",
    "text": "7.2 Toxicity\nThe Toxicity sub-tab provides parameters for specifying:\n\nWhether toxicity should be simulated as dichotomous or ordinal. If simulated as Ordinal, toxicity can be simulated as a 1-3 scale or 1-4 scale. In both cases ‘1’ is “no-toxicity”, 2 is “mild toxicity” and 3 is the toxicity of interest (from the point of view of defining the MTD, target toxicity band and Overdose Control. If a four category scale is selected, 4 is “sever toxicity” or death. Choosing whether to model the ordinal response is a separate option and is present on the Design &gt; Toxicity Response tab.\nType of target: this allows the user to specify whether the dose selection targets “the dose who’s estimated toxicity rate is closest to a specified target rate” or “the dose with the highest posterior probability of having a toxicity rate in a target band”. The former is the target rule used in the original CRM papers ((O’Quigley, Shen, and Gamst 1999), (deMoor et al. 1996), and the latter rule was introduced in (Neuenschwander, Branson, and Gsponer 2008).\nToxicity target (only displayed if the type of target is “a single dose”): this allows the target toxicity rate to be specified and whether the target dose is the one nearest, the one nearest but with a lower rate or the one nearest but with a higher rate.\nTarget: this panel allows the target toxicity bands to be specified along with overdose control limits. The panel is displayed and enabled even if the target type is “a single dose” to allow overdose control limits to be specified.\nType of Target: controls the selection of the dose for the next cohort – this can be to target a single dose (to replicate the original CRM behavior, see this section) or to target the dose with the highest probability that its toxicity rate lies in a target band.\n\n\n\n\n\n\n\nFigure 28: Toxicity tab targeting a toxicity band\n\n\n\n\n7.2.1 Targeting a Toxicity Interval\nTargeting a toxicity band or interval is an innovation introduced with the N-CRM design, unlike other CRM designs that select the dose that is expected to have a toxicity response closest to the desired tolerated limit, the N-CRM selects the dose that has the highest posterior probability of having a toxicity rate in a target toxicity band. This has the advantage of a) having a clearer probability statement and b) having in addition probability statements about the probability of under and overdosing (the toxicity rate being below or above the target toxicity band).\n\nThe uncertainty in the estimate of toxicity at each dose is expressed by calculating the posterior distribution of the estimate of the rate of toxicity at each dose and calculating the proportion of that distribution that falls in to each of 4 bands of toxicity: ‘Under-dosing’ (toxicity so low that it is likely that a higher dose could be used), ‘Target’ toxicity (we want to select doses whose toxicity rate is most likely to be in this band), ‘Excess’ toxicity (toxicity higher than desired) and ‘Unacceptable’ toxicity.\n\nUnder-dosing: this band always starts at 0.0; the user specifies the upper bound.\nTarget band: this band always starts at the upper-bound of the under-dosing band; the user specifies the upper bound.\nExcess toxicity: this band always starts at the upper-bound of the target band; the user specifies the upper bound.\nUnacceptable toxicity: this band always starts at the upper-bound of the target band, with an upper bound of 1.0. The graph shows the width of the different bands using a simple, fixed, example posterior probability distribution of a toxicity rate.\n\nIntervals are relative to control: if a control arm is included in the study, then toxicity bands can be defined as the difference in toxicity rate relative to control. Negative differences (a lower toxicity rate than control) are always treated as under-dosing. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\nLimit max excess/unacceptable toxicity: the ‘overdose control’ in terms of a maximum allowed posterior probability that a dose’s toxicity rate lies in either the ‘Excess’ or the ‘Unacceptable’ toxicity bands. Any dose with a posterior probability of having a toxicity rate in either of these two bands that is higher than the specified limit, cannot be selected for allocation to the next cohort, nor selected as MTD at the end of the study (this gives rise to slightly different results compared to those in (Neuenschwander, Branson, and Gsponer 2008) where the overdose control was not applied to final selection).\n\nIt is possible to have the overdose control limit vary with the number of cohorts allocated. In particular this can be used to reduce the overdose limit as the number of cohorts (and the amount of information) grows. For example for a particular prior and final level of overdose control, it may be that initial escalation is excessively constrained, one way to allow early escalation in this setting is to use these parameters to allow an higher initial overdose control limit and gradually reduce it over time to the final desired limit. Tuning the parameters will require some iteration and simulation. A varying limit is specified by the specifying amount to change the limit by per cohort and the final limit. The amount to change by is always entered as a number in the range (0,1), whether this is an increment or decrement depends on whether the target limit is greater or less than the initial limit. Leaving the change in limit at its default of 0 means the limit does not vary.\nLimit max unacceptable toxicity: as for the previous parameter, but here the overdose control is only in terms of the posterior probability that a dose’s toxicity rate lies in the ‘Unacceptable’ toxicity band.\nAs or the limit on excess/unacceptable toxicity it is possible to have the overdose control limit vary with the number of cohorts, see the description above.\n\n\n\n7.2.2 Targeting a single dose\nIt is possible to use the N-CRM design engine with a conventional CRM allocation strategy - to “allocate to the nearest / highest dose below the maximum tolerated toxicity”; this allows conventional CRM design to be simulated with some of the additional features of N-CRM:\n\nOverdose control\nEstimate both parameters of the 2 parameter logistic\nThe “Recommender” to analyze a specific data set.\n\nThe target is calculated by:\n\nIn the MCMC sampling loop finding the dose that meets the target criteria, a doses probability of being the target is then the proportion of times that dose meets the target criteria across the MCMC sampling.\nRather than selecting the dose with the highest probability, the dose at the 50% quantile is used. The cumulative probability of being the target is calculated over the doses in ascending dose strength, and the dose when the cumulative probability passes 50% is selected. This addresses some problems that can arise when very little data is available: that the dose with the highest probability is at one end of the dose range, but that probability is not that high, or that doses are not evenly spaced and a dose close to both its immediate neighbors may never have greater probability than both of them.\n\nSetting the Type of Target option to Target a single dose, modifies the tab thus:\n\n\n\n\n\n\nFigure 29: N-CRM, targeting a single dose, not a toxicity band\n\n\n\nWhen targeting a single dose FACTS allows the user to specify:\n\nThe target toxicity rate\nWhether to allocate to the dose with the mean estimate of its toxicity rate nearest the target, highest dose with a mean estimate of its toxicity rate below the target or lowest dose with a mean estimate of its toxicity rate above the target.\nAn option to use the toxicity rate relative to control, rather than the default of the absolute toxicity rate. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\n\nThe definition of the boundaries of the toxicity bands is still included in order to allow the specification of overdose control limits. These are calculated and applied in exactly the same way as when targeting a toxicity interval.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#efficacy",
    "href": "documentation/v71/userguides/crm.html#efficacy",
    "title": "FACTS Dose Escalation CRM",
    "section": "7.3 Efficacy",
    "text": "7.3 Efficacy\nIf include efficacy has been checked on the Study Info tab, a simple additional input page is included:\n\n\n\n\n\n\nFigure 30: The Efficacy tab\n\n\n\nIf simulating and modelling an efficacy endpoint is included there are two items to be specified on this tab:\n\nWhether a subject experiencing a toxicity can also count towards efficacy or not. If unchecked patients outcomes are simply sampled separately and a patient can both have a toxicity and an efficacy response. If checked, a patient’s toxicity outcome is sampled first, and only if there is no toxicity is an efficacy outcome sampled.\nThe efficacy target – this consists of:\n\nThe target efficacy rate required for the Minimum Efficacy Dose (MED).\nWhether the target dose is the nearest dose to the MED rate, the lowest dose above the MED rate or the highest dose below the MED rate.\nIf a control arm has been included, whether the target rate is absolute or relative to the observed rate on the control arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#treatment-arms",
    "href": "documentation/v71/userguides/crm.html#treatment-arms",
    "title": "FACTS Dose Escalation CRM",
    "section": "7.4 Treatment Arms",
    "text": "7.4 Treatment Arms\nOn this tab the number of treatment arms (doses) available to the study is specified. The user can either define a set of specific doses that can be used or a continuous dose range with some granularity.\nSelecting Explicit Doses allows the user to specify the specific doses that can be used on the trial:\n\nA single new dose or multiple doses can be added either by clicking “Add” or “Generate”. Initially each dose is defined by a simple integer name and level. The dose levels and dose names can then be edited on by clicking on them and entering the desired value. The dose level can also be set later on in the Design &gt; Toxicity Response tab.\nThere is also the option to include a control arm. Including a control arm allows the toxicity rate to be relative to the control arm.\n\n\n\n\n\n\n\nFigure 31: The Treatment Arms tab specifying explicit doses\n\n\n\n\n7.4.1 Finely Spaced Doses\nSelecting Finely Spaced Doses allows the user to specify the dose range that can be used on the trial:\n\nThe minimum and maximum dose strength to be used\nThe ‘granularity’ of the actual dose used, either as a fixed delta (Fixed spacing) or a dose ratio (the ratio specified must be greater than 1) (Ratio spacing).\nThe number of ‘bins’ or ‘doses for which to report’ – this is because FACTS will still produce summary statistics in columns, many with a “column per dose” – it is possible to use more doses than it is practical to report on (and a limit in MS Windows of 32K pixels for the width of a table means that the GUI can only display simulation results for a maximum of ~40 doses). However this limitation is only for reporting summary statistics; the dose strengths modeled and allocated in the simulations are unaffected.\n\nSelecting ‘Finely Spaced Doses’ will also affect how some of the other parameters are specified in the FACTS GUI.\n\n\n\n\n\n\nFigure 32: The Treatment Arms tab, specifying a finely spaced dose range",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-variants",
    "href": "documentation/v71/userguides/crm.html#sec-variants",
    "title": "FACTS Dose Escalation CRM",
    "section": "7.5 Variants",
    "text": "7.5 Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of cohorts).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with different maximum numbers of cohorts.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Max Cohorts” for each variant.\nThese will then appear on the simulations tab.\nIf open enrolment is being used, then the enrolment cap is specified by the number of subjects.\nIf there are two groups then separate caps are specified for each group.\n\n\n\n\n\n\nFigure 33: The Variants tab, specifying 5 variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#explicitly-defined",
    "href": "documentation/v71/userguides/crm.html#explicitly-defined",
    "title": "FACTS Dose Escalation CRM",
    "section": "8.1 Explicitly Defined",
    "text": "8.1 Explicitly Defined\nToxicity profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 35. The user enters the toxicity rate to simulate at each dose into the Probability of Toxicity column of the table, and the graphical representation of these toxicity values updates accordingly.\nThis form of toxicity profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter toxicity rates for all of them. When using “finely spaced” doses the toxicity rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 34: Explicitly defined toxicity - binary endpoint\n\n\n\nIf the design is using ordinal toxicity, the toxicity response rates can be specified either:\n\nOn the “Toxicity” tab by specifying the category 3 or greater toxicity rate at each dose and then two offsets – one for the category 2 or greater rate and one for the category 4 rate.\nOn the “Ordinal Toxicity” tab by separately specifying the toxicity rates for each category of toxicity at each dose.\n\nSpecifying offsets: to ensure that the specified category 3 rate plus the category 2 offset doesn’t sum to more than 1, or the category 3 rate plus the -ve category 4 offset sum to less than 0, the offsets are applied to the logit of the category 3 toxicity rate.\nThus for the category 2+ rate:\n\\[\nln(\\frac{p_{2+}}{1-p_{2+}}) = ln(\\frac{p_{3}}{1-p_{3}} + \\Delta_2)\n\\]\nwhere:\n\n\\(p_{2+}\\) is the probability of observing a category 2 or greater toxicity at a dose\n\\(p_3\\) is the probability of observing a category 3 or greater toxicity at a dose\n\\(\\Delta_2\\) is the difference in the log odds between the two probabilities\n\nThe offset is defined at the lowest dose and highest dose and then varied linearly with dose strength at the intermediate doses. The plot of the curve can either use Pr(Tox) or Log-odds(Tox) as the y-axis and dose strength or log(dose strength) as the x-axis. A graph is displayed of the toxicity rates that have been entered, and the category 2+ and category 4 toxicity rates if applicable. This graph, as with all graphs in the application, may be copied to the clipboard or to a file using the “right-click” menu.\n\n\n\n\n\n\nFigure 35: Virtual Subject Response – Explicitly-Defined – ordinal endpoint\n\n\n\n\n8.1.1 Explicitly defined – Ordinal Toxicity\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 36: Virtual Subject Response – Explicitly-Defined: Ordinal Toxicity tab\n\n\n\n\n\n8.1.2 Explicitly defined toxicity – when simulating 2 groups\nIf simulating toxicity as a binary outcome, when simulating 2 groups, the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group.\n\n\n\n\n\n\nFigure 37: Explicitly defined toxicity - 2 groups\n\n\n\nIf simulating 2 groups and ordinal toxicity, then on the explicitly defined tab once again the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group. As for a single group the Category 2 toxicity and Category 4 (if using) toxicity rates are defined by defining log odds offsets at the lowest and highest dose. Specification is limited to a single set of offsets that are applied to both groups.\n\n\n\n\n\n\nFigure 38: Explicitly defined toxicity - 2 groups and ordinal offsets\n\n\n\nAs in the single group case in addition to the Category 3 toxicity rates that are editable, columns showing the Pr(Tox 2+) and Pr(Tox 4) are shown, but these are not editable and derived from the Pr(Tox) rates and the offsets that have been specified. The ordinal toxicity rates are only shown for group 1.\n\n\n8.1.3 Explicitly defined – Ordinal Toxicity with 2 groups\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 39: Explicitly Defined, Ordinal Toxicity with 2 Groups\n\n\n\n\n\n8.1.4 Efficacy response profiles\nEntering efficacy response profiles is very similar to entering toxicity profiles. FACTS will construct scenarios to simulate of every combination of toxicity and efficacy response profiles.\n\n\n8.1.5 Explicitly Defined – Efficacy\nEfficacy profiles may be added, deleted, and renamed just like toxicity profiles. The user enters the efficacy rate to simulate at each dose into the Probability of Efficacy column of the table, and the graphical representation of these efficacy values updates accordingly.\nAs with toxicity this form of efficacy profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter efficacy rates for all of them. When using “finely spaced” doses the efficacy rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 40: Efficacy virtual subject response, explicitly defined\n\n\n\n\n\n8.1.6 Explicitly Defined – Efficacy with two groups\nIf the design included 2 groups, when explicitly defining an efficacy response profile, there is simply a second column of efficacy response rates to enter:\n\n\n\n\n\n\nFigure 41: Explicity defined efficacy response profile with 2 groups",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-parametric",
    "href": "documentation/v71/userguides/crm.html#sec-parametric",
    "title": "FACTS Dose Escalation CRM",
    "section": "8.2 Parametric",
    "text": "8.2 Parametric\nToxicity scenarios may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen. The user selects the model to use to determine the toxicity rate to simulate at each dose, and specifies the values of the model’s parameters. The graphical representation of these toxicity values updates accordingly.\nThe graph may be copied using the context menu functionality described in the previous section.\nFour models are available:\n\nLogistic: the probability of toxicity at dose x is given by: \\(P_x=\\frac{1}{1+e^{-s(x-x_{50})}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with linear effective doses \\(\\hat{x}=x-x_{ref}\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*(x_{ref}-x_{50})\\)\nEmax: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{x}{x+x_{50}}\\) with user specified parameter \\(x_{50}\\).\nLog Logistic: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{1}{1+e^{-s(ln(x)-ln(x_{50}))}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with log effective doses \\(\\hat{x}=ln(\\frac{x}{x_{ref}})\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*ln(\\frac{x_{ref}}{x_{50}})\\)\nPiecewise linear: with the probability of toxicity specified at a series of knots, with the probability linearly interpolated between knots.\n\n\n\n\n\n\n\nFigure 42: Virtual Subject Response - Parametric Toxicity tab\n\n\n\nIf Ordinal toxicity is being simulated then the category 2 and greater toxicity rates and category 4 toxicity rates are specified using the logit offset methods as on the Explicitly-Defined &gt; Toxicity tab.\n\n\n\n\n\n\nFigure 43: Virtual Subject Response - Parametric Toxicity tab with Ordinal toxicity\n\n\n\nIf Ordinal Toxicity and 2 groups are being simulated then both the Cat 2+ and Cat 4 toxicities and the Group 2 toxicities are defined using the logit offset methods.\n\n\n\n\n\n\nFigure 44: Parametric definition of ordinal toxicity response with 2 groups\n\n\n\n\n8.2.1 Parametric efficacy response\nParametric efficacy response profiles function exactly like toxicity profiles, with the same parametric models to choose from and if 2 groups are present the response of the second group is again defined by 2 log-odds offsets, one at the lowest dose and one at the highest.\n\n\n\n\n\n\nFigure 45: Parametric efficacy response profile",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#external",
    "href": "documentation/v71/userguides/crm.html#external",
    "title": "FACTS Dose Escalation CRM",
    "section": "8.3 External",
    "text": "8.3 External\nSubject response data may be simulated from a PK-PD model in place of, or in addition to, choosing a response model from FACTS. The importing of subject response data (which must be in the form specified in the System Requirements Document) may be done from the External Files sub-tab depicted below (Figure 46).\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id\nDose index (1, 2, 3,… if a Control is to be included it should be index 0) this is not the user settable dose name or dose level\nToxicity (0,1)\nEfficacy (0. 1) even if efficacy not being simulated this value must be present\nGroup (*1, 2) only required if groups are being simulated\n\nThe GUI requires that the file name has a “.dat” suffix.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.\nTo import an external file, the user must first add a scenario to the table. After adding a scenario, the user must click “Browse” to locate the externally simulated data via a standard file browser dialog.\n\n\n\n\n\n\nFigure 46: External Data",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#x-hats",
    "href": "documentation/v71/userguides/crm.html#x-hats",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.1 X-Hats",
    "text": "9.1 X-Hats\nOn this tab the user specifies the reference dose \\(d^*\\) for use in calculating the adjusted dose value (the “x-hat” values). The default value to use is Median dose is reference, this uses the median of the dose range for the reference dose, minimizing the correlation in the sampled values of \\(\\alpha\\) and \\(\\beta\\). Note though that when allocation is restricted to explicit doses it is also recommended that the value of the reference dose is not the same as an actual dose that can be used (at this dose \\(\\hat{x}\\) will be 0 and the data on this dose can have undue weight on the estimate of \\(\\alpha\\)).\nDifferent reference doses can easily be used however – between the two doses thought most likely to be MTD, just below the lowest dose, just above the highest dose. The bi-variate Normal prior for \\((\\alpha, ln(\\beta))\\) will need to be recalibrated to take the change into account.\nX-Hats are log(dose strength) allows the user to select between:\n\nlinear effective dose \\(\\hat{x}_j = d_j - d^*\\)\n\\(log(\\hat{x}_j) = ln(\\frac{d_j}{d^*})\\)\n\nIf you have entered linear dose strengths for the doses (1, 2, 3, 4, … or 100, 150, 200, 250, …) then use the linear effective dose. If however the dose strengths that have been entered are non-linear (12.5, 25, 50, 100, …) but expected to be roughly linear in effect, then use the log of the dose ratio.\n\n\n\n\n\n\nFigure 47: Specifying the dose transformation - the “x-hats”.\n\n\n\n\n9.1.1 The Pro’s & Con’s of using the median dose as the reference dose\nThe reason the median dose is recommended as the reference is that this minimizes the correlation in the fit of \\(\\alpha\\) and \\(ln(\\beta)\\), the parameters of the BLRM, and it maximises the flexibility of the fit of the model over the dose range.\nHowever care needs to be taken that the prior on \\(\\alpha\\) is not more restrictive than that on \\(ln(\\beta)\\) in order to avoid a phenomena observed when preparing tutorials: observing “no toxicities” below the reference dose resulted in a model with increased probability of toxicity above the reference dose compared to observing a toxicity below the reference dose. For a given value of \\(\\alpha\\), higher values of \\(ln(\\beta)\\) correspond to lower toxicity below the reference dose – as the \\(\\hat{x}\\)̂ values are -ve below the reference dose. The fitted curve thus “pivots” about the value of \\(\\alpha\\) at the reference dose.\nThere are two solutions to this:\n\nmove the reference dose, which involves a choice between two options\n\nmoving it to the first dose or below (normally allowing a relatively constrained prior around a low value for \\(\\alpha\\)),\nor to the highest dose or above (with a relatively uninformative prior).\n\nWe have seen both solutions perform well against the chosen scenarios – but the choice needs checking and refining with a full range of scenarios that represent the full uncertainty in the true response.\nor modify the priors on \\(\\alpha\\) and \\(ln(\\beta)\\) making the prior on \\(\\alpha\\) less informative (in particular increase the probability of low values) and make the prior on \\(ln(\\beta)\\) more informative (in particular lower the probability of high values less). Because the prior distribution on \\(\\beta\\), is on \\(ln(\\beta)\\), it is easy to make large values of \\(\\beta\\) more probable than intended.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-response",
    "href": "documentation/v71/userguides/crm.html#toxicity-response",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.2 Toxicity Response",
    "text": "9.2 Toxicity Response\nThe parameters that can be specified on this page are:\n\nThe parameters of the bivariate Normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\). Specifying the mean and standard deviation of \\(\\alpha\\) \\((\\mu_{\\alpha}, \\sigma_{\\alpha})\\), and \\(ln(\\beta)\\) \\((\\mu_{ln(\\beta)},\\sigma_{ln(\\beta)})\\) and the correlation coefficient \\(\\rho\\).\n\nIf ordinal toxicity is being simulated, it is possible to model the ordinal toxicity, specifying the mean and standard deviation of \\(\\alpha_2\\) and \\(\\alpha_4\\). These priors are separate from the \\(\\alpha_3\\) and \\(ln(\\beta)\\) prior, there is no correlation term in the prior. There is the constraint in the model that \\(\\alpha_2 &gt; \\alpha_3 &gt; \\alpha_4\\).\nUse fixed Alpha: the value of Alpha can be fixed to allow the N-CRM model to behave like the traditional CRM models. [Where \\(\\alpha\\) was set to 3 and the reference dose is set above the top of the available dose range]\n\n\nRather than entering the priors directly, they can be derived based on indirect prior information or beliefs, see ‘Deriving the Prior’ below.\n\nThe Minimum and Maximum rates that the model is to be fitted too. The model fits the range \\((0,1)\\), asymptotically approaching each limit as the adjusted dose value tends to \\(-\\infty\\) or \\(+\\infty\\). By specifying an alternative minimum and maximum, inside the range \\((0,1)\\), the user can have the model scaled to fit data to fit event rates where the asymptotic rates are not \\(0\\) or \\(1\\). For instance if the event being observed has a non-zero background rate (probability of being observed in placebo treated subjects), then the model may fit better if the minimum is set to the lower limit of this expected rate. Similarly if, even at the most toxic dose the event being observed is only expected to effect a proportion of subjects, the model may fit better if the maximum is set to the upper limit of this expected rate.\nIf a control arm is present, the user can specify to have this modelled separately, and if so the user specifies the parameters for a prior Beta distribution – in terms of numbers of prior observations on control of subjects with and without a toxicity.\nGroup 2 priors: if a second ‘Group’ is being simulated – whether this is a subset of subjects, or a modified treatment that subjects can be randomized to, then the BLRM is jointly fitted to the responses for both groups, with group 2 having offsets \\(a\\) and \\(b\\) from the first group’s \\(\\alpha\\) and \\(\\beta\\). The priors for \\(a\\) and \\(b\\) can be full bivariate Normal or can use constraints such as \\(b = 1\\), or \\(a &gt; 0\\) or \\(a &lt; 0\\).\n\n\n\n\n\n\n\nFigure 48\n\n\n\n\n9.2.1 Deriving the prior\nThe priors of \\(\\alpha\\) and \\(ln(\\beta)\\), can be specified directly or derived in one of four ways. When entered explicitly, the user specifies the parameters of the prior bivariate-normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\): the means, standard deviations and the correlation term \\(\\rho\\).\nAlternatively, the user may click the ‘derive prior’ button and select from:\n\nQuantiles at the lowest and highest dose: (based on the “uninformative prior” given in the paper (Neuenschwander, Branson, and Gsponer 2008), for details see this section) - the user specifies the probability of an unacceptable toxicity at the lowest dose, and the probability of under-dosing at the highest dose (0.1 for both is the default, and 0.05 for both is the value used in the paper). Optionally the probability that toxicity is less than the mid-point of the target toxicity band at the median dose can be specified. (Prior to FACTS 6.5 this third data point was not optional and constrained to be at the reference dose, but this had problems if the reference dose was not the media dose – it might also be the lowest dose for example).\nNote this method does not work so well if the reference dose is outside the dose range.\n\n\n\n\n\n\nFigure 49\n\n\n\nScenarios: the model is fitted to each of the toxicity response scenarios (MLE), the parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\n\n\n\n\n\nFigure 50\n\n\n\nSpecific quantiles: The user selects which doses and toxicity rates to provide an expectation – a prior probability that the toxicity rate on the dose will be the specified rate or less. At least 3 such expectations using at least 2 different doses strengths must be supplied. If a large number of specific quantiles are specified (e.g. reproducing the all quantiles method) the large number of different beta distributions sampled from, with the monotonicity constraint applied, results in losing too much variability. So this should only be used quantiles specified at 2-4 doses.\n\n\n\n\n\n\n\nFigure 51\n\n\n\n\nAll quantiles: the user specifies the prior expected toxicity rate at the 2.5%, 50% and 97% quantiles for each dose. (Only available when using explicitly defined doses, not a continuous dose range). Note that using Create Prior with this option will require the facts file to be saved and for there to be at least one virtual subject response profile.\n\n\n\n\n\n\n\nFigure 52\n\n\n\nIn all cases once prior values have been derived they are displayed along with a graph of 100 sampled curves from the prior. The user can accept the values, change derivation method, or cancel the derivation.\nThe plot of the samples can either be viewed as Pr(Tox) or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\n9.2.2 Derivation of the Prior from Quantiles\nDerivation of the parameters of the bivariate Normal prior for \\(\\alpha\\) and \\(ln(\\beta)\\)) in the Quantiles at lowest and highest dose, Specific quantiles and All quantiles cases:\n\nMinimally informative unimodal Beta distributions are fitted for each of the doses where a prior expectation of a toxicity has been specified. For doses where no prior expectation has been specified, the median expected toxicity rate are derived by assuming that the median expected toxicity is linear in log dose on the logit scale, and again a minimally informative unimodal Beta distribution is fitted with the same median.\nPreviously and following (Neuenschwander, Branson, and Gsponer 2008), the parameters of the bivariate Normal distribution were found using a stochastic fit to the prior expectations of toxicity, minimizing the error in the prior toxicity rates at the 2.5%, 50% and 97.5% quantiles. This is still used in the All quantiles and Legacy prior cases. However experience with this method with the standard priors (previously called “uninformative”) showed that it yielded priors with too little uncertainty in the \\(ln(\\beta)\\)) and too high a value for the correlation parameter for many cases and certainly for the prior to be called “uninformative”.\nConsequently, in the Quantiles at lowest and highest dose and Specific quantiles cases, the prior is now derived by sampling from the minimally informative unimodal Beta distributions, and fitting the model to each set of sampled toxicity rates. The parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\nIf a control arm has been included, it may be included in the model, or modelled separately using a beta-binomial model, the user specifies the prior values for the Beta distribution.\n\n\n9.2.3 Derivation of the prior from the scenarios\nIn this screenshot, priors have been derived from the scenarios:\n\n\n\n\n\n\nFigure 53: Design - Toxicity Response sub-tab (after prior derivation)\n\n\n\nClearly in this example the scenarios have a very high correlation between the toxicity at the reference dose (\\(\\alpha\\)) and the log gradient (\\(ln(\\beta)\\)) giving a high value for the correlation in the prior (\\(\\rho\\)). As there are very few scenarios, and they didn’t include extreme cases, the SDs of the priors of parameters will be underestimated.\nSo In this instance we might round the prior means to 2 decimal places, double the SD of \\(\\alpha\\), slightly increase the SD of \\(ln(\\beta)\\) and reduce the correlation to 0.5.\nHowever, it is much better to use Drive from Scenarios after entering a large number of varied and credible scenarios. Indeed is such a collection of scenarios exists, deriving the prior from the scenarios is the simplest approach and often very effective. Only if the performance in the simulations in some scenarios does the prior need re-visiting (usually to slightly increase the \\(ln(\\beta)\\) SD and/or reduce the correlation).\nWe strongly recommend checking the performance of the prior across a wide range of scenarios, and of entering the reported derived values of the fitted prior as an explicit prior and then manually modifying them in the light of the model’s performance on the various scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-legacy-prior-methods",
    "href": "documentation/v71/userguides/crm.html#sec-legacy-prior-methods",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.3 Legacy prior methods",
    "text": "9.3 Legacy prior methods\nIn old versions N-CRM (pre-FACTS 4.0) , the design could be left at the stage where the method of deriving the prior had been specified but the derivation postponed to the simulation stage. We now require that the derivation be performed first, this\n\nEnables the actual prior that results from the derivation to be inspected\nMakes simulation and recommendation faster, as the derivation is not repeated every time the design engine starts.\n\nWhen opening a FACTS N-CRM file created using a pre-FACTS 4.0 version of FACTS, to continue to use the original prior, select “Derive prior”, and the derive prior window will display the old prior and allow an explicit prior to be derived from it using the now deprecated methods in the older versions of FACTS N-CRM.\n\n\n\n\n\n\nFigure 54: Derivation from a legacy prior",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-efficacy-response",
    "href": "documentation/v71/userguides/crm.html#sec-efficacy-response",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.4 Efficacy Response",
    "text": "9.4 Efficacy Response\nThe Efficacy Response tab is displayed if an efficacy endpoint is included in the trial.\nThe Efficacy Response is specified separately from the Toxicity Response. The Toxicity and Efficacy models are completely separate except for the x-hat values for the transformed dose strengths, where a common set of values is used for both models. Note that the use of a common set of x-hats for both endpoints is a difference from FACTS bCRM, that means it may not be possible to exactly replicate a bCRM design in FACTS N-CRM, however we think that the additional features and options in FACTS N-CRM will make it possible to create an overall superior design in FACTS N-CRM.\nThe features available for specifying the Efficacy Response model are the same as the Toxicity Response model (see above) – with the exception that the Toxicity Response includes an option for modeling ordinal toxicity, there is no corresponding ordinal efficacy option.\n\n\n\n\n\n\nFigure 55\n\n\n\nThe same options for deriving the prior are available as for the Toxicity Response Model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prior-pseudo-subjects",
    "href": "documentation/v71/userguides/crm.html#sec-prior-pseudo-subjects",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.5 Prior Pseudo-subjects",
    "text": "9.5 Prior Pseudo-subjects\nThis option allows “prior data” or “pseudo data” to be specified that will be included in every analysis. This is equivalent to the model being fitted with the parameter prior to this pseudo data and the resulting posterior being the new prior, but it is easier and quicker to include the pseudo subject with the real data and do one analysis.\nThe user selects which dose levels at which to include the data and specifies the number of pseudo/prior subjects/observations and the number of toxicities. These are allowed to be fractional, and observations can be at dose levels not being tested in the trial. In each analysis the observed data is augmented with this specified data and the parameters of the toxicity response estimated.\nIf there is an efficacy endpoint as well as toxicity endpoint, pseudo subject data is specified separately for the two endpoints (not surprisingly!). If the data is to be analyzed as “2 groups” pseudo subject data is also specified separately for the two groups.\nThe effect of this data on the prior can be visualized by the “Update Plot” function that estimates the parameters of the toxicity response and plots the curves of 100 samples drawn from the posterior estimates of the parameters of the model.\nThe plot of the samples can either be viewed as Pr(Tox) vs Dose Strength or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\n\n\n\n\nFigure 56: Specifying prior pseudo subjects\n\n\n\n\n\n\n\n\n\n\nExample of effect of pseudo subjects on prior\n\n\n\n\n\nPrior Only\n\n\n\nPrior plus 0.5/0 subjects toxicities on dose 1 and 1/0.5 on dose 8.\n\n\n\nPrior plus 3/0 subjects / toxicities on dose 1 and 3/1.5 on dose 8.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation-rules",
    "href": "documentation/v71/userguides/crm.html#allocation-rules",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.6 Allocation Rules",
    "text": "9.6 Allocation Rules\nThe Allocation Rule sub-tab is depicted below (Figure 57); it allows the user to select and set the parameters for the allocation rules.\nThere is an option to mimic the original Neuenschwander paper and rely on just the toxicity model and the overdose control to guide allocation – i.e. allocate to as close to the estimate of the MTD as we can limited only by the overdose control limits. This is “Use only overdose control”. Normally however we think the clinical will want to impose additional allocation rules.\nThe next dose allocated is a combination of the current target dose, and 2 possible maximum allocatable doses:\n\nThe target dose is the dose with the highest probability of being the target: the dose with the greatest estimated probability that its toxicity rate lies in the target toxicity band, or the dose nearest or highest below the MTD depending on the options selected on the Study &gt; Toxicity tab.\nThe highest dose that meets the overdose criteria (if any): the highest dose that does not have a posterior probability that its toxicity rate lies in the excess & unacceptable toxicity bands or unacceptable toxicity bands above the threshold specified on the Study &gt; Toxicity tab.\nThe current cleared dose and how far above that dose can be allocated to as defined by the specified allocation rules (if any).\n\nThe next dose to be allocated to is the lowest of these 3 doses.\nIt is possible to specify a “run-in” phase before this dose escalation phase applies. A run-in phase has a fixed sequence of doses and cohort sizes (typically smaller than the cohort size used in the escalation phase) and lasts up to the first observed toxicity or the end of the sequence of doses.\n\n\n\n\n\n\nFigure 57\n\n\n\nIf the user selects to use an “Initial run-in” then there are 3 run-in types that can be selected from:\n\n“Simple run” (the only type available before FACTS 6.3): cohorts are of a single size and the dose sequence up to the “Run-in cannot go beyond” dose (if specifed) is either:\n\nat every dose starting at the specified “initial dose” if explicit doses have been defined on the the Study &gt; Treatment Arms tab,\nat the dose increment intervals defined in the allocation rules if finely spaced doses have been defined on the Study &gt; Treatment Arm tab.\n\n“Custom run-in” where the user specifies which doses are selected (leaving the “number of subjects” at a dose at 0 mans it is not selected) and at each dose how many subjects are allocated in the cohort at that dose.\nSmall cohort pre-escalation, cohorts are of a single size and the dose sequence follows the dose increment intervals defined in the allocation rules up to the “Run-in cannot go beyond” dose (if specifed).\n\n“Simple run-in” and “Small cohort pre-escalation” the user specifies the “small cohort size” and there is an option to specify a top dose that the “Run-in cannot go beyond”.\nFor all run-in schemes there are options:\n\nEnd run-in on 1st category 2 toxicity: The default for the simulation is to stop the run-in when the first full toxicity is observed, there is an option to instead stop when a lower grade toxicity is observed – a category 2 toxicity. If this option is selected, the simluation of virtual subjects is extended to include the simulation of category 2 tocxicity as well as category 3.\nInclude run-in subjects in overall maximum\nWhen run-in ends expand last cohort to full size\n\nIf this is not set then N-CRM model is applied and the next cohort is allocated as close to the target as possibly, restricted by the overdose restrictions and not allocating any higher than the dose reached in the run-in.\nIf this option is set then last allocated small cohort is treated as the start of a full cohort and the remaining subjects are allocated at the same dose. The N-CRM model is then applied and the next cohort allocated according to the overdose restrictions and allocation rules. [Note it is possible that after observing no toxicities in the run-in or in the expanded cohort, that the overdose control will force a de-escalation in dose, depending on the priors for the model parameters and the overdose control limits]\n\n\nIf simulating 2 groups, and a run-in is specified, group 2 will only use a run-in if on the Study tab the option “Recruit Group 1 and Group 2 together” has been selected. Otherwise only group 1 will use the specified run-in, once group 2 starts it starts with the full cohort (or if open enrollment is being used, the full number of subjects to clear the dose). If using a run-in and recruiting groups 1 and 2 together then both use the same run-in rules, and both will stop on the first toxicity regardless of which group the toxicity occurs in.\nIf used, the Allocation rules have the effect of setting a Highest Allocatable Dose that specifies the highest dose level that can be allocated to by the allocation rules.\n\nAt the start the Highest Allocatable Dose is the user specified Starting dose level.\nA dose is not ‘cleared’ until ‘The minimum cohorts on/near a dose before cleared’ have been allocated to it. If this is set to greater than one, then the specified number of cohorts must have been allocated to the current dose before it is ‘cleared’. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nIf using Open enrolment, this parameter is ’The minimum subjects on a dose before cleared’ and refers to the number of complete subjects.\nThe user can specify that Dose not cleared if proportion toxic on dose &gt; and a maximum level of toxicity that can be tolerated for the current dose to be ‘cleatred’. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe user specifies whether the dose increment rules are to be specified in terms of the number of dose levels that can be incremented or in terms of the ratio by which the dose strength can be increased. If using “dose strength” it is up to the user to ensure that the specified ratio is sufficient to allow all doses to be reached. For example if in the dose range there is a dose of strength 100 and the next dose is of strength 200, then if any dose increment rule is specified that doesn’t allow the dose strength to at least double (using “ratio of dose strength” of less than 2) then the rule will prevent escalation from the 100 dose to the 200 dose.\nThe maximum amount the dose can be incremented can be specified in 3 ways:\n\nSingle value: For all doses, once a dose has been cleared the next ‘highest allocatable dose’ is the dose above the cleared dose by the specified increment – either a number of dose levels or by a proportion of the dose strength.\nBy total number of toxicities: With this rule the maximum dose increase allowed depends on the total number of toxicities that have been observed in the trial so far. The user specifies the maximum increment (in terms of the number of dose levels or the maximum proportional increase in dose strength) when no toxicities, one toxicity, or more than one toxicity has been observed.\n\n\nFigure 9-5 Maximum dose increment varying by number of toxicities observed\n\n\n\n\n\n\nFigure 58: Maximum dose increment varying by number of toxicities observed\n\n\n\n\nBy dose levels: the maximal permitted increment is defined in terms of the number of dose levels or the proportional increase in dose strength that the dose can be increased, in up to 3 bands of dose strength. The user defines the upper and lower doses of the middle increment range, the lower range is then from the lowest up to this band and the upper band is from the top of the middle band to the highest dose. The user then specifies the maximum number of dose levels or the maximum proportional dose strength that can be incremented if the current dose is within each of these bands.\n\n\n\n\n\n\n\nFigure 59: Maximum dose increment varying by dose level\n\n\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently allocated dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\nThe “Fastest Possible Dose Escalation” graph shows the dose allocation permitted by the rules if no toxicities are observed. Or the graph can be changed to show the allocation if a toxicity occurs at a specific dose during the run-in.\nIf there are 2 groups and recruitment to the second group is delayed (either until the first group is complete, has reached a specific dose, or tested a specified number of subjects). Then starting dose for the second dose can be specified as\n\n\n\n\n\n\nFigure 60: Design - Allocation Rule Initial Dose Second Group\n\n\n\n\n9.6.1 Allocation rules when using a fine grained dose range\nWhen a fine grained dose range is being used, the specification of the allocation rules change to accommodate the fact that the trial is no longer stepping up a few pre-defined dose levels. As with the explicit doses, the allocation rules work with the notion of a Currently Permitted Maximum Dose (CPMD).\n\nAt the start the highest allocatable dose is the user specified Starting dose strength.\nA Run-in phase can be specified, this will always begin at the start dose and allocate ‘small cohorts’ following the maximum increment rules (this is different from when there are explicit doses – with explicit doses, the simple run-in simply allocates successive small cohorts to successive doses, ignoring the maximum permitted increment) for a simple run-in or small cohort pre-escalation, or the specified allocation pattern for a custom run-in, until the first toxicity is observed.\nThe degree of increment and specification of what counts as a ‘close’ dose, can be done either in Dose Strength or Ratio of dose strength.\nA dose is not cleared until ’The minimum cohorts on a dose before incrementing’ have been allocated to it. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nThe user can specify that Dose not cleared if ppn toxic on/near dose &gt; and a maximum level of toxicity that can be tolerated before the dose is cleared. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe Max ratio of dose strengths considered as near or Delta in dose strength considered as near defines a margin such that when evaluating whether a dose that has been allocated to counts as “cleared”, then cohorts allocated within that margin all count as being “on” the dose to allow incrementing.\nThe Maximum increment selected by option allows the user to specify varying maximum increments either dependent on the current cleared dose (Dose strength), or on the Total number of toxicities that have been observed, as follows:\n\nSingle value: the amount by which the highest allocatable dose can be above the current cleared doses is constant throughout the trial. (This simple rule can work well when combined with overdose control. The two more complex rules are essentially trying to achieve the same thing as overdose control but are more simplistic and it may be confusing as to whether the allocation rule or overdose control is preventing escalation at any given moment).\nTotal number of toxicities: the amount by which the highest allocatable dose is above the current cleared dose is specified separately for whether zero, one or two or more toxicities have been observed in the entire study.\nDose strength: The user specifies:\n\nthe increment at low doses;\nthe increment at medium doses, along with the upper and lower dose strengths that define the bounds of what constitutes a ‘medium dose’; and\nthe increment at high doses.\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently cleared dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\n\n\n\n\n\n\nFigure 61: Allocation rules with a finely spaced doses",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-backfill-allocation",
    "href": "documentation/v71/userguides/crm.html#sec-backfill-allocation",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.7 Backfill Allocation",
    "text": "9.7 Backfill Allocation\nIf the trial is using open enrolment then “Backfill” allocation is an option. Backfilling is enabled on the Study &gt; Study Info tab, where the maximum number of subjects that can be allocated when backfilling is specified.\nBackfilling is the allocation of subjects to a dose below the current dose when the maximum number of subjects on the current dose without final results has been reached.\nThe Backfill Allocation tab allows detailed control of when backfilling can be used. The user specifies:\n\nThe maximum overall number of subjects that can be on a dose for backfilling to be allowed to that dose.\nThe maximum number of subjects that can be allocated to a dose by backfill.\nThe maximum number of dose levels below the current dose that can be used for backfilling (the highest that can be backfilled to will be used)\nThe lowest dose strength that can be backfilled to.\nWhether or not the current escalation dose is a candidate for backfilling as long as the maximum number of subjects in their DLT period (set in the Study &gt; Study Info tab) is not exceeded (also known as “frontfilling”).\nIf frontfilling is enabled, whether these subjects should count towards the backfill allocation cap or the regular study allocation cap (specified in the “Study/Study Info” tab).\n\n\n\n\n\n\n\nFigure 62: Backfill Allocation",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-stopping-criteria",
    "href": "documentation/v71/userguides/crm.html#sec-stopping-criteria",
    "title": "FACTS Dose Escalation CRM",
    "section": "9.8 Stopping Criteria",
    "text": "9.8 Stopping Criteria\nThe simulation will always stop when the maximum number of cohorts or subjects has been allocated.\nThe user may also specify that the study may stop early in terms of the amount of information that has been gathered about the target dose. In the N-CRM, the target dose can either be the Maximum Tolerated Dose or the “the allowed dose that has the highest posterior probability of having a toxicity rate in the Target Toxicity Band”; we use the label MTD (Maximum Target toxicity Dose) for this target dose too, for brevity and familiarity from previous CRM methods.\nNote: if overdose limits have been set (see this section), then doses with posterior probabilities of having a toxicity rate in the Excess Toxicity or Unacceptable Toxicity bands that are greater than the specified thresholds, are disallowed for both dose allocation and selection as MTD.\nTo enable early stopping, the user must select “Rules for stopping trial early” there are then 2 rules which if selected are always “AND”ed together, and a further block of rules, the result of which (if specified) are “AND”ed with the first two rules. If more than one rule is selected within the block these may be either logically “AND”ed together or “OR”ed together to give the result of the rules in the block.\nThe two first standalone rules are:\n\nIf a “Required number of cohorts/subjects near MTD” rule is set then the trial will only stop early if at least this number of cohorts or subject has been allocated to the MTD dose or nearby doses. In the box above the “Count as MYD doses differing from MTD by less than or equal” allows how far away a dose can be and for cohorts/subjects on those doses to count towards this total. This can be set to 0 to count only cohorts/subjects on the MTD. It is provided for when fine grain dosing or a large number of explicit doses have been specified. If specified this rule this must always be met along with any other rules set for the trial to stop early.\nIf a “Minimum number of cohorts/subjects accrued” has been set then this specifies a lower limit on the sample size before early stopping is allowed. It is provided to allow a higher number of subjects on the MTD if one of the first doses appears to be the MTD. If specified this rule this must always be met along with any other rules set for the trial to stop early.\n\n\n\n\n\n\n\nFigure 63: Design - Stopping Criteria sub tab\n\n\n\nThe available stopping rules are:\n\nRequired number of cohorts/subjects on/near MTD: Once the specified number of cohorts has been allocated to the dose currently determined to be the MTD, the trial may stop.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\nThe required number of cohorts/subjects can be “near” (rather than “on”) the MTD if a “Count as MTD” interval is defined. This interval is either defined by an interval of “dose strength” using the dose strengths defined on the Study &gt; Treatment Arms tab, or by an interval of “dose strength ratios”. Which is used will correspond to the Selection for “Dose allocation rules apply to” on the Allocation Rules tab. A difference of 0, or factor of 1 can be used to only count cohorts/subjects actually on the selected MTD dose.\nIf open enrolment is being used, there is an option to only pause accrual not stop it when the stopping rules have been met, in case the final results of any subjects that were not complete at the time the stopping rules were met cause the rules to be no longer met (for example by having results that change the dose estimated to be the MTD). If this option is selected then the stopping rules are re-assessed when all the current subjects are complete and the trial resumed if the rules are no longer met. Note this option is not required for cohort enrolment as stopping rules are only evaluated between cohorts.\n\nMinimum cohorts/subjects accrued: this rule ensures that a minimum overall number of cohorts have been tested before the trial is allowed to stop. It makes no sense to use this rule on its own (it would effectively just lower the overall study cap) it should only be used in conjunction with other stopping rules.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\n\nThe Additional Stopping Rules:\n\nRange/Ratio of dose strengths within the credible interval is less than or equal to: This is only enabled if on the Study &gt; Study Info tab, the Type of Target has been set to “Target a single dose”. In this case the credible interval of the dose range that might contain the MTD is calculated, and this option allows the user to specify the width of the credible interval in “dose strength” to consider. How this works is a little counter intuitive: in each MCMC sample the engine determines which dose is the MTD and each doses’ Pr(MTD) is the proportion of the samples it was the MTD. The engine then calculates the minimum range of dose strengths required so that the sum of Pr(MTD) of the doses in the range exceeds 1 – Alpha. (So if a single dose has Pr(MTD) &gt; 1-Alpha, the CI has zero width). If the xhats are log(dose strength) then the minimum range of log(dose strength) is found and exp() of this range returned.\n\nThe stopping rule is met if the width of the CI returned is less than that specified by the user.\nIf fine grain dosing is being used then the width of the target credible interval is defined as a dose strength ratio rather than a number of doses.\n\nStop if adding another DLT free cohort does not alter the MTD: this rule is evaluated by analyzing the existing data supplemented by an additional cohort of a specified size where all the responses are no-toxicity; if this results in no change in selection of MTD then this stopping rule is met. If the study is using Open Enrolment, the user additionally specifies the size of the ‘virtual’ cohort to use.\nProbability of dose being in the target band greater than: in order to stop, the MTD’s posterior probability that its toxicity rate lies in the Target Toxicity band must be at or above the required threshold.\nMaximum Cohorts/Subjects near MTD: this option is supplied for use if the additional stopping rules are being OR’d together. (When they are AND’d together it simply functions the same way as the “Required Cohorts on MTD and stopping will not occur until the higher of the two targets is met).\nJoin condition: if more than one additional stopping rule is selected, whether only any one of them needs to be met for the trial to be able to stop (Join condition = “OR”), of if all of the selected rules need to be met for the trial to be able to stop (Join condition = “AND”).\n\n\nThe study will also be stopped if there are no allowed doses by the overdose rules. However this can occur early in the study if a toxicity is observed in the first or second cohort. It is likely that in practice the clinical team would override the design stopping the study. Whilst it is difficult to fully represent the team’s decision making, a simple rule is included that is intended to approximate it:\n\nMinimum toxicities required before stopping: This allows a requirement to be specified to observe a ‘minimum number of toxicities’ before the trial stops. If no doses are allowed by the overdosing rules, cohorts are assigned to the lowest dose until the minimum number of toxicities are observed, the stopping rules are met, or doses become allowable again after the model is updated after seeing no toxicities.\n\n\n\n\n\n\n\nFigure 64: Stopping Criteria tab with open enrolment and finely spaced doses\n\n\n\nIf an additional efficacy endpoint is used, the MTD stopping criteria refer to when the trial jumps from assigning subjects with the aim of finding the MTD (“MTD phase”) to assigning subjects with the aim of finding the MED (“MED phase”). In the MED phase, it is possible that because of new data being observed, the MTD stopping rules are no longer met and the trial switches back to the MTD phase. A new stopping rule for the MTD phase is added, “Maximum subjects used to determine the MTD”. After this number of subjects has been enrolled, the trial switches from MTD to MED phase and there is no going back.\nIn the MED phase, there are several rules for stopping early (i.e. before the maximum sample size of the trial):\n\nMaximum cohorts/subjects on MED. This behaves analogous to Required number of cohorts/subjects on/near MTD in the MTD stopping rules, with the exception of not using a concept of “near” doses.\nNumber of doses within the credible interval is less than or equal to with the sub-option Alpha for width of credible interval. This behaves analogous to Range/Ratio of dose strengths within the credible interval is less than or equal to in the MTD stopping rules.\nProbability of dose being MED greater than. This behaves analogous to Probability of dose being in the target band greater than in the MTD stopping rules.\n\nA special case arises when the MED estimate is larger than the MTD estimate. If that is the case, subjects are allocated to MTD or the highest cleared dose (whatever is smaller) even in the MED phase. The option “If MED &gt; MTD: Continue until subjects near MTD reach” specifies how many subjects should be assigned to MTD in the MED phase before stopping the trial (and therefore giving up hope that the MED is a safe dose).\n\n\n\n\n\n\nFigure 65: Stopping Criteria tab with open enrolment and both a toxicity and efficacy endpoint",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-simulation-with-variants",
    "href": "documentation/v71/userguides/crm.html#sec-simulation-with-variants",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.1 Simulation with Variants",
    "text": "10.1 Simulation with Variants\nThe only difference that specifying design variants (see this section) introduces is to create additional scenarios – one for each Virtual Subject Response (VSR) profile for each variant. For example if there were 5 VSR profiles and then 4 variants (different numbers of maximum cohorts) specified then would now be 20 scenarios in total. The scenario names have “_Var1”, “_Var2”, … appended to them. Once simulations have been run and the .facts file has been saved and re-opened the scenarios are listed in alphabetical order.\n\n\n\n\n\n\nFigure 67: Simulation tab showing variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-to-run-simulations",
    "href": "documentation/v71/userguides/crm.html#sec-to-run-simulations",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.2 To run simulations",
    "text": "10.2 To run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user may not modify any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-how-many-simulations-to-run",
    "href": "documentation/v71/userguides/crm.html#sec-how-many-simulations-to-run",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.3 How many simulations to run?",
    "text": "10.3 How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%). This degree of accuracy usually unnecessary for dose escalation designs.\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-simulation-results",
    "href": "documentation/v71/userguides/crm.html#sec-simulation-results",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.4 Simulation results",
    "text": "10.4 Simulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nOpen Results Folder: Opens the windows “File Explorer” tool in the results folder of the currently selected scenario. This makes it very easy to locate and open results files.\nCopy to Clipboard: will copy the values displayed in the summary to the clipboard as “CSV” data, enabling it to be pasted straight into a spreadsheet.\nAll: A window containing all the summary results columns\nHighlights: a separate window with the results shown on the main tab\nAllocation, Observed: summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity: summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc: summary results of the posterior probabilities of the properties of interest\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\n\nExplore Results: offers three options:\n\n“Show Per Scenario Highlighted Scenario Graphs” that shows the graph of the simulation results for a specific scenario (see Section 12 below for a description of these graphs)\n“Show Across Scenario Graphs” that displays a trellis plot of summary graphs for each variant and each scenario (see Section 13 below for a description of these graphs).\n“Compare Scenarios in AIRSHIP” to open the simulation results in R with the AIRSHIP R-shiny graphing app. See the AIRSHIP User Guide for details.\n\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nDesign Report: it uses an R script and R libraries to generate a MS Word document describing the design. See the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.\n\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options corresponding to the above options that can be reached from the buttons, in what is sometimes a more ergonomic manner.\n\n10.4.1 MCMC Settings\n\n\n\n\n\n\nFigure 68\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nThe Number of samples per imputation value only applies to analyses using imputed data from a longitudinal model and is irrelevant for N-CRM, hence it is greyed out.\nIf the Number of MCMC files to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output, allowing the user to check convergence.\nThe MCMC output thinning parameter can be used to reduce the amount of data output to the MCMC file. It does not reduce the amount of MCMC samples used within the model fitting.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/crm.html#sec-facts-grid-simulation-settings",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.5 FACTS Grid Simulation Settings",
    "text": "10.5 FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-detailed-simulation-results",
    "href": "documentation/v71/userguides/crm.html#sec-detailed-simulation-results",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.6 Detailed Simulation Results",
    "text": "10.6 Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 69) will reveal more detailed results about the simulation.\n\n\n\n\n\n\nFigure 69: Detailed Simulation Results for a particular scenario\n\n\n\nRight-clicking on a row displays a context menu from which the user can view other columns (the default are the “highlihgts” columns) and also view the corresponding cohort results:\n\n\n\n\n\n\nFigure 70: Cohort Results for a particular simulation\n\n\n\nRight clicking on the cohort results, displays a context menu from which the user can view other columns (the default are the “highlights” columns).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-sim-aggregation",
    "href": "documentation/v71/userguides/crm.html#sec-sim-aggregation",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.7 Aggregation",
    "text": "10.7 Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 71\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose, plus two extra rows for ‘below lowest’ and ‘above highest’.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nIf there is no value for the below lowest or above highest rows, they will be left blank.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. cohortsNNN.csv files are aggregated into a single agg_[pivot_]cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-design-report",
    "href": "documentation/v71/userguides/crm.html#sec-design-report",
    "title": "FACTS Dose Escalation CRM",
    "section": "10.8 Design Report",
    "text": "10.8 Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "href": "documentation/v71/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "title": "FACTS Dose Escalation CRM",
    "section": "11.1 Results when 2 groups have been simulated",
    "text": "11.1 Results when 2 groups have been simulated\nWhen 2 groups have been simulated, the results displayed on the Simulation tab and all the directly viewable summary tables under the “Show other columns” button are from Group 1.\nTo see the results from Group 2 you need to first display the Group 2 highlights – either by selecting “Group 2” on the “Show other columns” menu or after right clocking on a row of results.\nOnce the Group 2 “highlights” results are being displayed, the other sets of summary results can be viewed by right clicking on a row of results in the Group 2 “hightlights” window.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-highlights",
    "href": "documentation/v71/userguides/crm.html#sec-highlights",
    "title": "FACTS Dose Escalation CRM",
    "section": "11.2 Highlights",
    "text": "11.2 Highlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nSettings\n1\nDisplays an icon showing the status of the simulation results.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the “MTD” at the end of the study. In CRM this is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on “excess” or “unacceptable” toxicity have been specified, then doses with posterior probabilities above these limits are excluded from being chosen as MTD. (Note this is different from the calculation of MTD used in Neuenschwander, Branson, and Gsponer (2008) where doses with posterior probabilities above these limits were not excluded from being selected as MTD.)\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nIncluded if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the “MED” at the end of the study. This is the dose with the highest posterior probability of being the dose that is the ‘highest dose below’ / ‘nearest’ / ‘lowest dose above’ (as specified on the Study &gt; Effiacy tab) to the target efficacy rate.\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration in weeks of the simulated trials.\n\n\nSD Duration\n1\nThe standard deviation over the simulations of the duration in weeks of the simulated trials.\n\n\nMean Lost\n1\nIf the trial uses open enrollment, this is the number of subjects that could not be allocated a dose because the number of subjects treated but not yet complete had reached the specified “Maximum subjects without final result” limit. Otherwise the value is 0.\n\n\nSD Lost\n1\nThe standard deviation over the simulations of the number of subjects lost.\n\n\nPpn(All Tox)\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nPpn(early Success)\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nPpn(Cap)\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nPpn MTD Under\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Underdosing” toxicity range.\n\n\nPpn MTD Target\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Target” toxicity range.\n\n\nPpn MTD Excess\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Excess” toxicity range.\n\n\nPpn MTD Unacc\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Unacceptable” toxicity range.\n\n\nPpn Correct Under\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Underdosing” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Target\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Target” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Excess+Unacc\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Excess” or “Unacceptable” ranges. This will be 0 if none of the doses in that scenario had a true toxicity that fell in those ranges.\n\n\nPpn MED Under\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or less. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over))\n\n\nPpn MED Over\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or more. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over)",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-allocations-observed",
    "href": "documentation/v71/userguides/crm.html#sec-allocations-observed",
    "title": "FACTS Dose Escalation CRM",
    "section": "11.3 Allocations, Observed",
    "text": "11.3 Allocations, Observed\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nNum Phase 1\n1\nIncluded if efficacy is being simulated. This is the mean (over the simulations) of the number of subjects recruited in the first phase (up to the MTD stopping criteria being met) in this scenario.\n\n\nSubj. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subjects: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nCat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nCat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\n80% Num. Subj.\n1\nThe 80th percentile of the distribution of the number of subjects recruited in the simulations",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-fitted-toxicity",
    "href": "documentation/v71/userguides/crm.html#sec-fitted-toxicity",
    "title": "FACTS Dose Escalation CRM",
    "section": "11.4 Fitted Toxicity",
    "text": "11.4 Fitted Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nSD Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nMean Alpha Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nSD Mean Alpha Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nAlpha[234] Tox\n2-3\nIf Ordinal toxicity is being simulated, instead of Mean Alpha and SD Mean Alpha columns, there are pairs of columns Mean Alpha2, SD Mean Alpha2, … for Alpha2 & Alpha3 if a 3 point ordinal scale is being used and Alpha2, Alpha3 & Alpha4 if a 4 point ordinal scale is being used. These are the means (over the simulations) of the mean and standard deviation of the various Alpha coefficients in the BLRM model when fitting to the ordinal outcome.\n\n\nSD Alpha[234] Tox\n2-3\nsee row above\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Toxicity: &lt;Dose&gt;\nOne per dose\nThis is the true simulated toxicity rate for each dose in the scenario\n\n\nMean Fit Tox Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose.\n\n\nMean Fit Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prmtd-etc",
    "href": "documentation/v71/userguides/crm.html#sec-prmtd-etc",
    "title": "FACTS Dose Escalation CRM",
    "section": "11.5 Pr(MTD) Etc.",
    "text": "11.5 Pr(MTD) Etc.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study. The dose chosen as MTD is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on excess or unacceptable toxicity have been specified, then doses with posterior probabilities above the specified limit, of having a toxicity rate in those bands are excluded from being chosen as MTD.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum [Tox] Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MTD was rule was met at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 1” (see below).\n\n\nNum [Tox] Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MTD is less than the specified number. [This stopping rule only evaluated if targeting an MTD rather than a toxicity band] If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 2” (see below).\n\n\nNum [Tox] Stop Rule 3\n1\nNumber of times the Pr(MTD) – that the probability that the dose was MTD or dose’s toxicity rate lies in the target toxicity rate band - met the stopping rule threshold at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 3” (see below).\n\n\nNum Stop Rule 4\n1\nNumber of times that observing another cohort with no toxicities would not change the selected MTD stopping rule was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nNum Stop Rule 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose&gt;\nOne per dose\nAs MTD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nOSD+ Selection: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. As OSD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nOSD+ Selection: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose above the tested range of doses was selected as the optimum selected dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(Under): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPost CE MTD+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the maximum tolerated dose.\n\n\nPost CE MTD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was the maximum tolerated dose allowing for a probability that the MTD is at a dose below or above the range of tested doses.\n\n\nPost CE MTD+: plus\n1\nThe posterior probability, after the results of the Cohort Expansion, that a dose above the tested range of doses was the maximum tolerated dose.\n\n\nPost CE OSD+: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose was selected as the optimum selected dose allowing for a probability that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose above the tested range of doses was selected as the optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-fitted-efficacy",
    "href": "documentation/v71/userguides/crm.html#sec-fitted-efficacy",
    "title": "FACTS Dose Escalation CRM",
    "section": "11.6 Fitted Efficacy",
    "text": "11.6 Fitted Efficacy\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nSD Beta Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nAlpha Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nSD Alpha Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy response model for each dose.\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates of the efficacy rate across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Efficacy: &lt;Dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nMean Fit Eff Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.\n\n\nMean Fit Eff Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prmed-etc",
    "href": "documentation/v71/userguides/crm.html#sec-prmed-etc",
    "title": "FACTS Dose Escalation CRM",
    "section": "11.7 Pr(MED) Etc.",
    "text": "11.7 Pr(MED) Etc.\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study. The dose chosen as MED is the dose with the highest posterior probability of having a efficacy rate nearest the target rate / is the highest dose with a rate below the target rate / is the lowest dose with a rate above the target rate – as specified on the Study &gt; Efficacy tab.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum Eff Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Eff CI\n1\nThe mean (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nSd Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nPr(MED) &lt;dose&gt;\n\nThe mean (over the simulations) of the posterior probability that each dose is the MED.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose&gt;\nOne per dose\nAs MED Selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(MED+): minus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose below the tested range of doses was the MED.\n\n\nPr(MED+): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities that each dose was the MED. As Pr(MED) but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nPr(MED+): plus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose above the tested dose range is the MED.\n\n\nPost CE MED+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was selected as the minimum efficacious dose allowing for the possibility that the MED is at a dose below or above the range of tested doses.\n\n\nPost CE MED+: plus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after the results of the Cohort Expansion, that each dose was selected as the optimum selected dose allowing for the possibility that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation-box-and-whisker-plot",
    "href": "documentation/v71/userguides/crm.html#allocation-box-and-whisker-plot",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.1 Allocation Box and Whisker Plot",
    "text": "12.1 Allocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 75: Allocation box and whisker plot\n\n\n\nThis graph displays a box and whisker plot of the number of subjects allocated to each dose over the simulations. These plots show:\n\nThe mean allocation over all simulations at each dose plotted as a solid line.\n\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\n\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#response-and-subject-allocation",
    "href": "documentation/v71/userguides/crm.html#response-and-subject-allocation",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.2 Response and Subject Allocation",
    "text": "12.2 Response and Subject Allocation\n\n\n\n\n\n\nFigure 76: Response and subject allocation graph\n\n\n\nThis plot shows the mean subject allocation to each dose as a blue bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation and the mean fitted toxicity separately.\nIf efficacy is being simulated, then lines for the mean fitted efficacy and true efficacy are also shown.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#response-and-mtd-distribution",
    "href": "documentation/v71/userguides/crm.html#response-and-mtd-distribution",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.3 Response and MTD Distribution",
    "text": "12.3 Response and MTD Distribution\n\n\n\n\n\n\nFigure 77: MTD distribution and response graph\n\n\n\nThis plot shows the proportion of times each dose has been selected as the MTD as a brown bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nThere is an options to display the “MTD+” distribution. This is the proportion of times each dose has been selected as the MTD when a dose below the lowest dose and a dose above the highest dose is included.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately.\nIf efficacy is being simulated then there are versions of the graph showing the histograms showing the distribution of selection of MTD, MED, OSD, and TE targets. The plot of the MTD shows the true and mean fitted toxicity, the plot of the MED shows the true and mean fitted efficacy and the plots of the OSD and TE show both the true and mean fitted toxicity and efficacy.\n\n\n\n\n\n\nFigure 78: Response and TE Distribution for a group",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities",
    "href": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.4 Toxicity Interval Probabilities",
    "text": "12.4 Toxicity Interval Probabilities\n\n\n\n\n\n\nFigure 79: Toxicity interval probabilities graph\n\n\n\nThis plot shows the posterior probability for each dose that it’s toxicity rate lies in each of the four toxicity intervals as a stacked bar chart.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#observed-toxicity-and-allocation",
    "href": "documentation/v71/userguides/crm.html#observed-toxicity-and-allocation",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.5 Observed Toxicity and Allocation",
    "text": "12.5 Observed Toxicity and Allocation\n\n\n\n\n\n\nFigure 80: Observed Toxicities and Allocation Histogram\n\n\n\nThis graph shows the mean allocation to each dose and the mean number of toxicities observed at each dose. The total height of the bar shows the total allocation, and the red section of the bar shows the proportion that experienced toxicity.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sample-size-mtd-histogram",
    "href": "documentation/v71/userguides/crm.html#sample-size-mtd-histogram",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.6 Sample Size MTD Histogram",
    "text": "12.6 Sample Size MTD Histogram\n\n\n\n\n\n\nFigure 81: Sample Size MTD Histogram\n\n\n\nThis graph shows the number of times different sample sizes (number of subjects tested) were observed across the simulations. Each bar is shown as a stacked plot with each color indicating the proportion of times a particular dose was selected as the MTD in the simulations that ended with a particular sample size.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#alloc-history-summary",
    "href": "documentation/v71/userguides/crm.html#alloc-history-summary",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.7 Alloc History Summary",
    "text": "12.7 Alloc History Summary\n\n\n\n\n\n\nFigure 82: Allocation History Summary Graph\n\n\n\nThis graph overlays all the allocation histories of those simulations for which “cohorts” files have been output. The lines show the “route” the dose escalation followed and the circles show the dose selected as MTD at the end of the simulation. The lines are heavier and the circles darker the more simulations followed the same route or made the same selection.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nThis graph allows a quick appraisal of how well the design and priors allow the dose escalation to reach and then stop in the target band.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#per-sim-alloc-and-tox-history",
    "href": "documentation/v71/userguides/crm.html#per-sim-alloc-and-tox-history",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.8 Per Sim Alloc and Tox History",
    "text": "12.8 Per Sim Alloc and Tox History\n\n\n\n\n\n\nFigure 83: Allocation and toxicity history plot\n\n\n\nThis graph shows the allocation and toxicity history for an individual simulation. Each dot represents a subject, the dot is colored red if toxicity was observed, grey otherwise. The subjects’ outcomes are displayed left to right in the order in which they were dosed, and at a height corresponding to the dose they were given.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed. A history can only be displayed for trials for which a ‘cohorts’ file has been written out.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nOn the left hand side, a black triangle next to the y-axis indicates the highest cleared dose. Green/red triangles show the Model MTD/MED+ and finally the selected MTD/MED.\nIf the trial uses open enrolment this graph is slightly changed.\n\n\n\n\n\n\nFigure 84: Open Enrolment Allocation and Toxicity History graph\n\n\n\nIf the trial uses open enrolment then this graph has an “Interim” picker that shows the data available at a specific interim. Subjects whose outcome has not been observed at this interim are shown as grey squares. Subjects who were not included in the trial because there were already the maximum number of subjects treated but who had not attained their final toxicity / non-toxicity status are shown as yellow crosses at the level below the lowest dose. As the interim displayed is increased subjects symbol will change as their endpoint data becomes available.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#cohort-responses",
    "href": "documentation/v71/userguides/crm.html#cohort-responses",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.9 Cohort Responses",
    "text": "12.9 Cohort Responses\n\n\n\n\n\n\nFigure 85: Cohort response plot\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the fitted dose-toxicity model for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIt the trial uses open enrolment then subjects whose outcome has not been observed at the time of the interim being displayed are shown as a light grey part of the “allocated subjects” bar.\nIf trial simulates 2 groups then there are two graphs one for each group.\nIf the trial simulates efficacy, then the bar shows both toxicities and efficacies using half width bars of different colors.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED.\n\n\n\n\n\n\nFigure 86: Cohort Responses for one of two groups showing efficacy and toxicity",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#cohort-band-probabilities",
    "href": "documentation/v71/userguides/crm.html#cohort-band-probabilities",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.10 Cohort Band Probabilities",
    "text": "12.10 Cohort Band Probabilities\n\n\n\n\n\n\nFigure 87: Cohort band probabilities graph\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the posterior probabilities that the toxicity rate lies in each of the toxicity bands for each dose, for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#mtd-change-on-expansion",
    "href": "documentation/v71/userguides/crm.html#mtd-change-on-expansion",
    "title": "FACTS Dose Escalation CRM",
    "section": "12.11 MTD Change on Expansion",
    "text": "12.11 MTD Change on Expansion\n\n\n\n\n\n\nFigure 88: MTD change on expansion\n\n\n\nThis graph plots the dose selected as MTD before the results of the expansion cohort are available, against the dose selected as MTD after the expansion cohort results are available. Each simulation result is plotted as a ‘dot’, and the graph is divided up into a grid and dots located in the correct cell in the grid and then arranged within the cell depending on how many results fall into the cell. This allows the relative number of results in each cell to be appraised quickly by eye.\nThis graph allows the trial designer to see the degree of risk that the determination of the MTD will change as a result of the expansion cohort and whether the risk is predominantly that the determination will increase or decrease.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#selected-mtd-graph",
    "href": "documentation/v71/userguides/crm.html#selected-mtd-graph",
    "title": "FACTS Dose Escalation CRM",
    "section": "13.1 Selected MTD graph",
    "text": "13.1 Selected MTD graph\nThe Selected MTD “Across Scenarios” graph shows for each scenario and each variant a histogram of the proportion of times each dose was selected as the MTD at the end of the simulations in that scenario. The bars are colored to reflect the toxicity band that the “true” toxicity rate of the dose falls into in that scenario.\n\n\n\n\n\n\nFigure 90",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities-graph",
    "href": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities-graph",
    "title": "FACTS Dose Escalation CRM",
    "section": "13.2 Toxicity Interval Probabilities graph",
    "text": "13.2 Toxicity Interval Probabilities graph\nThe Toxicity Interval Probabilities “Across Scenarios” graph shows for each scenario and each variant a stacked bar chart of the posterior probability that the toxicity rate at each dose falls into one of the user defined 4 toxicity bands.\n\n\n\n\n\n\nFigure 91",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#fitted-toxicity-1",
    "href": "documentation/v71/userguides/crm.html#fitted-toxicity-1",
    "title": "FACTS Dose Escalation CRM",
    "section": "13.3 Fitted Toxicity",
    "text": "13.3 Fitted Toxicity\nThe Fitted Toxicity “Across Scenarios” graph shows for each scenario and each variant, the mean fitted toxicity and the 95-percentile spread of the fitted toxicities across the simulations.\n\n\n\n\n\n\nFigure 92",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation",
    "href": "documentation/v71/userguides/crm.html#allocation",
    "title": "FACTS Dose Escalation CRM",
    "section": "13.4 Allocation",
    "text": "13.4 Allocation\nThe Allocation “Across Scenarios” graph shows for each scenario and each variant, a box plot of the spread of the number of subjects allocated to each dose across the simulations. As N-CRM may only be allocating a small number of cohorts the number of subjects allocated to each dose is often not a smooth distribution, but somewhat discontinuous.\n\n\n\n\n\n\nFigure 93",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sample-size",
    "href": "documentation/v71/userguides/crm.html#sample-size",
    "title": "FACTS Dose Escalation CRM",
    "section": "13.5 Sample Size",
    "text": "13.5 Sample Size\nThe Sample Size “Across Scenarios” graph is the only “Across Scenario” graph that is not a trellis plot. It is a single graph with a line plotted per scenario of the mean sample size at each maximum sample size.\n\n\n\n\n\n\nFigure 94",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#entering-the-data-directly",
    "href": "documentation/v71/userguides/crm.html#entering-the-data-directly",
    "title": "FACTS Dose Escalation CRM",
    "section": "14.1 Entering the data directly",
    "text": "14.1 Entering the data directly\nThe data can be entered directly in the data grid – each row containing the date for a single subject.\nThe format of the grid is the same as though viewing the “subject.csv” file in a spreadsheet.\nFor historical reasons the data file format contains 5 columns that are now unused but have been retained. Thus the columns are:\n\nFirst column is the subject ID. This column can be left blank, FACTS does not used the value, it is there to allow the data to be cross-referenced to an external data source. If not required there is no harm simply entering ‘1’ on each row.\nThe next 5 columns are unused and can be left blank. Do not enter text containing comma’s in these fields, these will be read as column separators if the data is saved and read back in.\nCohort number, this should be an integer indicating which cohort the subject belonged to (and hence the order in which they entered the trial). This data is sometimes used when determining what doses the allocation rules permit to be used. The FACTS GUI now checks to ensure that this value has been entered.\nDose Strength, if explicit doses are being used this value must match the dose strength of one of the doses defined on the ‘Treatment Arms’ tab – as the dose escalation rules are defined in terms of “number of doses”. If doses have been defined using ‘finely spaced doses’ then this column can contain any value as dose escalation rules defined using “dose strength”. The value entered will be used as the strength of the dose the subject was administered.\nApart from the requirement that the dose strength corresponds to one of the planned doses if the design uses explicit doses, there is no need for the data entered to represent a dose escalation permitted by the design. The team can have been more or less cautious, and there can be more or less data, than originally planned.\nToxicity – if a binary endpoint is being used this must be 0 (not-toxicity) or 1 (toxicity), if Ordinal toxicity is being used then this must be 1, 2, 3 or 4; where 1 is now no toxicity, 2 mild toxicity, 3 toxicity, and 4 severe toxicity.\nEfficacy – this must be 0 (no efficacy) or 1 (efficacy), even if efficacy is not being modelled in the design. If it is not being modelled then whether the value is 0 or 1 is immaterial.\n\n\n\n\n\n\n\nFigure 98\n\n\n\nIf the data entered and then ‘Run Analysis’ clicked, the data is saved to a file called ‘subject.csv’ and the analysis results saved to a folder called ‘Analysis’ within the “_results” folder of the design.\n\n\n\n\n\n\nFigure 99\n\n\n\nThe ‘Save As’ button can be used to save the file with a different name, but it will still be saved within the _Results folder.\nA specific results folder is also created, called ‘Analysis_’.\n\n\n\n\n\n\nFigure 100\n\n\n\n\n\n\n\n\n\nFigure 101",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#loading-data-from-a-file",
    "href": "documentation/v71/userguides/crm.html#loading-data-from-a-file",
    "title": "FACTS Dose Escalation CRM",
    "section": "14.2 Loading Data From a File",
    "text": "14.2 Loading Data From a File\nAs well as entering the data via FACTS its possible to load the data from a ‘subject.csv’ file. These can be cerated within FACTS or outside of FACTS and once created can be edited FACTS or outside of FACTS.\nWe have tried to make it particularly easy to enter, modify and analyze data in N-CRM because this is a useful way to explore the properties of the design in addition to simulation.\n\n14.2.1 The subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nFor historic reasons the CRM subject.csv file format includes fields for patient identification data, however the FACTS design engine does not use this data, but does require that the columns contain data of the expected format. This is the first 6 columns: patient ID, Patient Initials, Year, Month, Day, Time & Cohort. The simplest thing to do is enter simple default values as shown below.\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Toxicity, Efficacy\n1, , , , , , 1, 12.5, 0, 0\nIn FACTS only the last four columns are ever significant: Cohort, Dose, Toxicity and Efficacy. “Dose” needs to contain the dose level (not the dose index)) of the dose given to the subject, For example if the dose levels were specified as 12.5, 25, 50, 100, 150, 200 and 250 so this column should contain one of these values.\nIf the toxicity endpoint is dichotomous then “Toxicity” needs to contain either a ‘1’ (to indicate toxicity observed) or a ‘0’ (for no toxicity). If its ordinal, then it needs to contain a ‘1’, ‘2’, ‘3’ or ‘4’ corresponding to the observed level of toxicity for that subject, where ‘1’ is now “no toxicity’ and ‘3’ is the ‘target toxicity’, ‘2’ is ‘mild toxicity’ and ‘4’ is ‘severe toxicity’.\nA value in the final “Efficacy” column is also required whether or not efficacy is being modelled in the design. The column should contain either a ‘1’ (to indicate efficacy observed) or a ‘0’ (for no efficacy).\nThus if the first cohort had been allocated the lowest dose ’12.5’ and no subjects experienced toxicity (‘0’), the subjects.csv file looks like this:\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Tox, Efficacy\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "href": "documentation/v71/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "title": "FACTS Dose Escalation CRM",
    "section": "14.3 Data file management on the Analysis tab",
    "text": "14.3 Data file management on the Analysis tab\nFrom FACTS 6.2 onwards FACTS now supports multiple subject data files and analysis folders.\nButtons that allow the subject data file to be changed:\n\n’Select File to Create New Analysis: launches a file browser that allows the user to select a new “.csv” file from any location. The selected file is copied to the “_Results” folder (retaining its current name) and made the current subject data file.\n‘Rename Current Analysis’ allows the name of the current subject data file to be changed.\n‘Select Difference Analysis’ allows a different subject data file that is in the “_Results” folder to be made the current subject data file.\n‘Delete Analysis’ allows any of the subject data files that are in the “_Results” folder to be deleted.\n\nThe name of the current subject data file and the name of the corresponding analysis folder are shown below the subject data file buttons.\nThere are five buttons that allow the currently loaded subject data file to be modified:\n\n‘Delete Row’ deletes the currently selected row in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Delete All’ clears all the data in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Reload data’ replaces the data in the data grid with the data that is still in the current subject data file.\n‘Save As’ saves the current data in the data grid to a new subject data file in the “_Results” folder, and makes that the current subject data file.\n“Save” saves the current data in the data grid to the current subject data file.\n\nRunning an analysis performs a ‘Save’ before running the analysis.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#running-an-analysis",
    "href": "documentation/v71/userguides/crm.html#running-an-analysis",
    "title": "FACTS Dose Escalation CRM",
    "section": "14.4 Running an Analysis",
    "text": "14.4 Running an Analysis\nOnce data has been loaded or entered, the user can click the ‘Run Analysis’ button.\nOnce the analysis has run, FACTS displays the recommendation, and a graph showing the data the fitted toxicity.\n\n\n\n\n\n\nFigure 102: Analysis tab - analysis results\n\n\n\nThe available analysis parameters are\n\nMCMC Burn-in: how many of the initial MCMC samples are discarded before accumulating samples to estimate the posterior distributions of the values of interest.\nNumber of samples: the number of MCMC samples to take in-order to estimate the posterior distributions of the values of interest.\nRandom Seed: the seed to be used initialize the random number sequence, with the same design data and random seed FACTS will return the same results.\nEdit command parameters. This allows the command line string to the design engine to modified, this is an advanced option. The command line options are described in the “FACTS DE User Guide for Trial Execution”.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#analysis-graphs",
    "href": "documentation/v71/userguides/crm.html#analysis-graphs",
    "title": "FACTS Dose Escalation CRM",
    "section": "14.5 Analysis Graphs",
    "text": "14.5 Analysis Graphs\nThree graphs are available, the first shows the subject allocation, observed toxicities and resulting fitted curve:\n\n\n\n\n\n\nFigure 103\n\n\n\nThe second shows the posterior probabilities for each dose that its toxicity rate falls in each of the 4 toxicity bands:\n\n\n\n\n\n\nFigure 104\n\n\n\nThe third simply shows the observed data:\n\n\n\n\n\n\nFigure 105\n\n\n\nA fourth graph is available if “Generate MCMC file” is checked before running the analysis, we can now view MCMC trace plots of the fitted parameters such as Alpha and Beta:\n\n\n\n\n\n\nFigure 106",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-summary",
    "href": "documentation/v71/userguides/crm.html#contents-of-summary",
    "title": "FACTS Dose Escalation CRM",
    "section": "15.1 Contents of summary.csv",
    "text": "15.1 Contents of summary.csv\nThe columns in summary.csv are common across all the FACST Dose Finding design engines.\nSome columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate summary files. The first summary file “summary.csv” contains the results for the first group, the second summary file “summary2.csv” contains the result for the second group.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber of Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nMean num subjects\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario (including any single patient run-in, but excluding any expansion cohort..\n\n\nPpn Tox\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn Eff\n1 – Efficacy only\nThis is the average proportion of the subjects recruited that experienced a efficacy in this scenario.\n\n\nSD Ppn Tox\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1 – Efficacy only\nThis is the standard deviation of the proportion of efficacy across the simulations.\n\n\nTrue Mean Tox\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile that we are simulating from.\n\n\nMean Beta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\ns.d.Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\nMean Alpha 3 Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\ns.d.Alpha 3 Tox3\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\nMean Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\ns.d.Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\nMean Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\ns.d.Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\nMTD Selection &lt;dose index&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study, as defined by the Selected MTD simulation results column. Index starts at 0 if a control arm is included.\n\n\nMED Selection &lt;dose index&gt;\nOne per dose – Efficacy only\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study, as defined by the Selected MED simulation results column. Index starts at 0 if a control arm is included.\n\n\nOSD Selection &lt;dose index&gt;\nOne per dose\nUnused [it contains values that are copies of the MTD selection. OSD = Optimum Safe Dose, it differs from the MTD only if there is an efficacy endpoint to take into account too]\n\n\nMean num Ph1\n1 – Efficacy only\nThis is the mean (over the simulations) of the number of subjects dosed during the first phase of the trial that targets the MTD.\n\n\nMean Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Fitted Efficacy &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Efficacy &lt;dose index&gt;\nOne per dose -\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Subj per dose&lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subj per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nMean Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nMean Eff per dose &lt;dose index&gt;\nOne per dose -Efficacy only\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\nMean Known per dose &lt;dose index&gt;\nOne per dose\nThe number of known patient outcomes for a dose. In N-CRM there is no simulation of drop-outs so this will always be the same as “Mean subj per dose”\n\n\nSD Known per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of known patient outcomes for each dose across the simulations. In N-CRM there is no simulation of drop-outs so this will always be the same as “SD subj per dose”\n\n\nNum subj 80%ile\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nTox Stopping 1\n1\nThe number of times that toxicity stopping rule 1 was true at the end of the simulation. This is the rule that the trial should stop when the specified minimum number of cohorts has been allocated to the dose currently selected as the MTD.\n\n\nTox Stopping 2\n1\nThe number of times that toxicity stopping rule 2 was true at the end of the simulation. This is the rule that the trial should stop when no more than the specified number of doses within the credible interval for the target toxicity.\n\n\nTox Stopping 3\n1\nThe number of times that toxicity stopping rule 3 was true at the end of the simulation. This is the rule that the trial should stop when a dose achieves the minimum posterior probability of having a toxicity rate within the target band/of being MTD.\n\n\nTox Stopping 4\n1\nThe number of times that toxicity stopping rule 4 was true at the end of the simulation. This is the rule that the trial should stop if testing another cohort and observing no toxicities does not change the dose selected as the MTD.\n\n\nTox stopping 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation.\n\n\nEff Stopping 1\n1 – Efficacy only\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nEff Stopping 2\n1 – Efficacy only\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nEff Stopping 3\n1 – Efficacy only\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Tox CI\n1 – MTD target only\nThe mean number of doses in the MTD CI\n\n\nSD Tox CI\n1 – MTD target only\nThe SD of the number of doses in the MTD CI\n\n\nMean Eff CI\n1 – Efficacy only\nThe mean number of doses in the MED CI\n\n\nSD Eff CI\n1 – Efficacy only\nThe SD of the number of doses in the MED CI\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe mean probability of the dose being MTD\n\n\nPr(MED)\nOne per dose – Efficacy only\nThe mean probability of the dose being the MED\n\n\nMTD+ &lt;1\n1\nThe number of times the MTD was deemed to be less than the lowest dose\n\n\nMTD+ &lt;dose index&gt;\nOne per dose\nThe number of times each dose was selected as the MTD\n\n\nMTD+ &gt;&lt;D&gt;\n1\nThe number of times the MTD was deemed to be above the highest dose\n\n\nMED+ &lt;1\n1 – Efficacy only\nThe number of times the MED was deemed to be less than the lowest dose\n\n\nMED+ &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of times each dose was selected as the MED\n\n\nMED+ &gt;&lt;D&gt;\n1 – Efficacy only\nThe number of times the MED was deemed to be above the highest dose\n\n\nOSD+ &lt;1\n1\nThe same as MTD+ &lt;1, can be ignored\n\n\nOSD+ &lt;dose index&gt;\nOne per dose\nThe same as MTD+ &lt;dose index&gt;, can be ignored\n\n\nOSD+ &gt;&lt;D&gt;\n1\nThe same as MTD+ &gt; &lt;D&gt;, can be ignored\n\n\nDE Version\n1\nThis is the version of the N-CRM design engine that simulated these trial results.\n\n\nGUI Version\n1\nThis is the version of the FACTS GUI that was used to specify the parameters for the trials to be simulated.\n\n\nProject\n1\nThe name of the project or design\n\n\nScenario\n1\nThe name of the scenario within the project or design\n\n\nDate/Time\n1\nThe date and time the simulations were started\n\n\nBest &lt;dose index&gt;\nOne per dose – Efficacy only\nThe probability that the dose is ‘the best’ – that is it has the highest probability of efficacy without toxicity.\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe mean probability that the MTD is below the lowest dose\n\n\nPr(MTD+) &lt;D+1&gt;\n1 – MTD target only\nThe mean probability that the MTD is above the highest dose\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe mean probability that the MED is below the lowest dose\n\n\nPr(MED+) &lt;D+1&gt;\n1 –Efficacy only\nThe mean probability that the MED is above the highest dose\n\n\nPr(Under) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPrior Mean ln(Beta) Tox\n1\nThe value used for the mean value for the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior s.d.(Beta) Tox\n1\nThe value used for the standard deviation of the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior Mean Alpha Tox\n1\nThe value used for mean value for the prior distribution of Alpha in the toxicity model for all the simulations\n\n\nPrior s.d.Alpha Tox\n1\nThe value used for the standard deviation of the prior distribution of Alpha in the toxicity model for all simulations\n\n\nPrior Mean Rho Tox\n1\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the toxicity model for all simulations\n\n\nPrior Mean ln(Beta) Eff\n1 – Efficacy only\nThe value used for the mean value for the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior s.d.(Beta) Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior Mean Alpha Eff\n1 – Efficacy only\nThe value used for mean value for the prior distribution of Alpha in the efficacy model for all the simulations\n\n\nPrior s.d.Alpha Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of Alpha in the efficacy model for all simulations\n\n\nPrior Mean Rho Eff\n1 – Efficacy only\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the efficacy model for all simulations\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration of the trial.\n\n\ns.d.Duration\n1\nThe SD (over the simulations) of the duration of the trial.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe actual toxicity rate being simulated for each dose\n\n\nMean Lost\n1 – Open Enrolment only\nThe mean (over the simulations) of the number of subjects who were available for treatment but could not be included in the trial because the number of treated subjects for whom the final result is not available equals the maximum queue length. This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\ns.d.Lost\n1 – Open Enrolment only\nThe SD (over the simulations) of the number of subjects ‘lost’ (see above). This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\nPostCE MTD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nAll Tox Stop\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nEarly Success\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nCap Stop\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nMean Fitted Tox Lower &lt;D&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the estimates of the toxicity rate across the simulations, at each dose.\n\n\nMean Fitted Tox Upper &lt;D&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the estimates of the toxicity rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "href": "documentation/v71/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "title": "FACTS Dose Escalation CRM",
    "section": "15.2 Contents of simulations.csv and cohortsNNN.csv",
    "text": "15.2 Contents of simulations.csv and cohortsNNN.csv\nMost of the columns are common to the two file types, but the first few are different.\nAs with the summary.csv file, some columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate simulation.csv and cohorts.csv files. The first simulations file “simulations.csv” contains the results for the first group, the second file “simulations2.csv” contains the result for the second group.\nFor the cohorts files if two groups are being simulated, the files names ‘cohortsNNN.csv’ contain the results for the first group, and the files ‘named cohorts2_NNN.csv’ contain the results for the second group\n\n15.2.1 simulations.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber\n1\nThe number of the simulation.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nNo.Subjects\n1\nThe number of subjects recruited in this trial in the small cohort run-in and in the cohorts used to locate the MTD, but not those in the expansion cohort.\n\n\nPpn Tox\n1\nThis is the proportion of the subjects recruited that experienced a toxicity in this trial\n\n\nPpn Eff\n1 – Efficacy only\nThis is the proportion of the subjects recruited that had an efficacious outcome in this trial\n\n\nTrue Mean Tox\n1\nThis is the average probability of toxicity for the subjects in the trial, given the doses they were treated with and the toxicity rate being simulated for each of those doses.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average probability of efficacy for the subjects in the trial, given the doses they were treated with and the efficacy rate being simulated for each of those doses.\n\n\nSeeds\n2\nThe two 32 bit numbers that make up the random number seed at the end of the simulation. To exactly re-simulate a specific simulation (e.g. in order to generate an mcmc file or cohorts file for that simulation) enter these values from the line above the simulation to be re-simulated.\n\n\n\n\n\n15.2.2 cohortsNNN.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe number of the cohort in the simulation.\n\n\nAlloc Dose\n1\nThe index of the dose assigned to that cohort. If using open enrolment, -2 means the subject was ‘lost’, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nNumToxic\n1\nThe number of subjects in that cohort that experienced a toxicity\n\n\n\n\n\n15.2.3 Common simulations.csv and cohortsNNN.csv columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nmean Beta Tox\n1\nThe mean fitted value for the toxicity model Beta parameter\n\n\ns.d.Beta Tox\n1\nThe standard deviation of the toxicity model fitted Beta parameter\n\n\nMean Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\ns.d.Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\nMean Alpha 3 Tox\n1\nThe mean fitted value for the toxicity model Alpha parameter for category 3 (or the only category) toxicity or higher\n\n\ns.d.Alpha 3 Tox\n1\nThe standard deviation of the toxicity model Alpha parameter for category 3 toxicity (or the only category) or higher\n\n\nMean Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 4 toxicity\n\n\ns.d.Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 4 toxicity\n\n\nMean Beta Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Beta parameter\n\n\ns.d.Beta Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model fitted Beta parameter\n\n\nMean Alpha Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Alpha parameter\n\n\ns.d.Alpha Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model Alpha parameter\n\n\nModel MTD\n1\nThe dose index of the model MTD\n\n\nModel MED\n1 – Efficacy only\nThe dose index of the model MED\n\n\nModel OSD\n1 [simulations only]\nThe index of the dose selected as the Optimal Safe Dose (OSD), in N-CRM this will always be the same as the MTD as there is no efficacy to take into consideration.\n\n\nHighest Cleared Dose\n1\nThe highest cleared dose\n\n\nSelected MTD\n1\nThe dose index of the selected MTD (minimum of the Highest Cleared Dose and the model MTD+)\n\n\nSelected MED\n1\nThe dose index of the selected MED (minimum of the Highest Cleared Dose and the model MED+)\n\n\nSelected OSD\n1 [simulations only]\nThe dose index of the selected OSD (minimum of the Highest Cleared Dose and the model OSD+)\n\n\nNo. Ph1\n1 – Efficacy only\nThe number of subjects enrolled during the first, MTD locating, phase, before switching to the MED locating phase\n\n\nToxicity &lt;dose index&gt;\nOne per dose\nThe mean of the final posterior estimate of the toxicity rate at each dose.\n\n\nEfficacy\nOne per dose – Efficacy only\nThe mean of the final posterior estimate of the efficacy rate at each dose.\n\n\nNo. Subj &lt;dose index&gt;\nOne per dose\nThe number of subjects that have been allocated to each dose.\n\n\nTox per dose &lt;dose index&gt;\nOne per dose\nThe number of toxicities that have observed at each dose\n\n\nEff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of efficacies that have observed at each dose\n\n\nKnown per dose &lt;dose index&gt;\nOne per dose\nThe number of subjects for whom final results are available at each dose.\n\n\nTox CI\n1 – MTD target only\nOnly used if targeting a single dose. It’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nEff CI\n1 – Efficacy only\nIt’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nFlags\n1\nA flag value comprising a number of (possible) flag values ’OR’d together to show the current allocation or stopping decisions. See the Flag Values table below for details\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe posterior probability for each dose that that dose is the MTD.\n\n\nPr(MED) &lt;dose index&gt;\nOne per dose – Efficacy only\nThe posterior probability for each dose that that dose is the MED.\n\n\n1st full size\n1 [simulations only]\nThe number of the first full sized cohort, this will be ‘1’ unless the small cohort run-in is enabled. If the small cohort run-in is enabled, then this is the index of the first full size cohort.\n\n\nCohort size\n1 [cohorts only]\nThe number of patients in the cohort.\n\n\nMTD+\n1\nThe dose index of the model MTD+, this is the model MTD using the dose range extended by one dose either end. Dose 0 will be selected if all doses are too toxic and dose D+1 will be selected if no doses are toxic enough.\n\n\nMED+\n1 – Efficacy only\nThe dose index of the model MED+, this is the model MED using the dose range extended by one dose either end. Dose 0 will be selected if all doses are efficacious enough and dose D+1 will be selected if no doses are efficacious enough.\n\n\nOSD+\n1 [simulations only]\nThe dose index of the model OSD+, will be the same as the model MED+ unless this is above the model MTD+, in which case it is same as the model MTD+. If there is no efficacy to take into consideration this is the same as the model MTD+..\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe probability that the model MTD+ is at a dose below the lowest tested doses.\n\n\nPr(MTD+) D+1\n1 – MTD target only\nThe probability that the model MTD+ is at a dose above the highest tested doses.\n\n\nPr(Good) &lt;dose index&gt;\n1 – Efficacy only\nThe posterior probability for each dose of observing efficacy without observing toxicity.\n\n\nBest\n1 – Efficacy only\nThe index of the dose with the highest Pr(Good)\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe probability that the model MED+ is at a dose below the lowest tested doses.\n\n\nPr(MED+) D+1\n1 – Efficacy only\nThe probability that the model MED+ is at a dose above the highest tested doses.\n\n\nPr(Under) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Under-dosing’ toxicity band.\n\n\nPr(Target) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Acceptable’ toxicity band.\n\n\nPr(Excess) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Excessive’ toxicity band.\n\n\nPr(Unacc) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Unacceptable’ toxicity band.\n\n\nDuration\n1 (simulations only)\nDuration of the trial in weeks.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe toxicity rate being simulated for that dose in the scenario.\n\n\nTrue Efficacy &lt;Dose&gt;\nOne per dose – Efficacy only\nThe efficacy rate being simulated for that dose in the scenario.\n\n\nRec Time\n1 - Cohorts only\nIf the trial is using open enrolment this column record the time the subject was available to be dosed.\n\n\nNum Lost\n1 – Open enrolment only (simulations only)\nIf the trial is using open enrolment this is the total number of subjects ‘lost’ during the simulation, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nPostCE MTD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MTD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED+\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPr(Tox) Lower &lt;Dose&gt;\nOne per dose\nThe lower bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Tox) Upper &lt;Dose&gt;\nOne per dose\nThe upper bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Eff) Lower &lt;Dose&gt;\nOne per dose – Efficacy only\nThe lower bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\nPr(Eff) Upper &lt;Dose&gt;\nOne per dose – Efficacy only\nThe upper bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\n\nThe ‘Flags’ column is a number comprised of a number of binary flags that are used to indicate the ‘state’ of the simulator at different analyses. These flags are:\n\n\n\n\n\n\n\n\nFlag Values\n\n\n\n\n\n\nBits used\nValue\nMeaning\n\n\n0x001F\n0x0001\nAllocating to MTD\n\n\n0x001F\n0x0002\nAllocating to MED\n\n\n0x001F\n0x0003\nAllocating to initial dose\n\n\n0x001F\n0x0004\nAllocating new single patient cohort\n\n\n0x001F\n0x0005\nAllocating 1st dose of 2nd sample to dose below MTD from 1st sample\n\n\n0x001F\n0x0006\nAllocating 1st dose of 2nd sample to MTD from 1st sample\n\n\n0x001F\n0x0007\nAllocating expansion cohort\n\n\n0x001F\n0x0008\nAllocating to MTD because can’t allocate to MED because its above MTD\n\n\n0x001F\n0x0009\nExpanding at the current dose\n\n\n0x001F\n0x000A\nExpanding at the dose below\n\n\n0x001F\n0x000D\nAllocating as a backfill\n\n\n0x001F\n0x000E\nAllocating as a frontfill\n\n\n0x001F\n0x000F\nAllocating to max/min for fixed probability\n\n\n0x001F\n0x0011\nStopping for early futility\n\n\n0x001F\n0x0012\nStopping because MED is found\n\n\n0x001F\n0x0013\nStopping because all doses are toxic\n\n\n0x001F\n0x0014\nStopping because MTD is found\n\n\n0x001F\n0x0015\nStopping because MTD is found but there is no MED\n\n\n0x0020\n0x0020\nReached max subjects on MTD\n\n\n0x0030\n0x0030\nReached min required subjects on MTD\n\n\n0x0040\n0x0040\nToxicity confidence interval test passed\n\n\n0x0080\n0x0080\nPr(MTD) test passed\n\n\n0x0100\n0x0100\nReached max subjects on MED\n\n\n0x0200\n0x0200\nEfficacy confidence interval test passed\n\n\n0x0400\n0x0400\nPr(MED) test passed\n\n\n0x1000\n0x1000\nNo MED so using maximum instead\n\n\n0x2000\n0x2000\nUsing maximum permitted dose in place of MTD\n\n\n0x4000\n0x4000\nUnable to escalate due to part filled dose\n\n\n0x8000\n0x8000\nUnable to allocate during open enrolment because maximum permitted subjects without final result reached\n\n\n0x10000\n0x10000\nReached max subjects on MTD\n\n\n0x20000\n0x20000\nUnable to allocate during open enrolment because reached max subjects on MTD, but not all subjects on MTD have final results, so trial is ‘paused’ – resuming if the final results move the MTD so the trial continues, or resuming to allocate an expansion cohort.\n\n\n0x40000\n0x40000\nMax subjects reached and no other stopping rules met\n\n\n0x80000\n0x80000\nMaximum cohorts used to determine MTD has been met when there is also an efficacy endpoint, switching to searching for the MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-mcmcnnnnn",
    "href": "documentation/v71/userguides/crm.html#contents-of-mcmcnnnnn",
    "title": "FACTS Dose Escalation CRM",
    "section": "15.3 Contents of MCMCNNNNN.csv",
    "text": "15.3 Contents of MCMCNNNNN.csv\nThe MCMC file, if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (excluding the burnin) and the samples from all the analyses (i.e from every cohort – or if using open enrollment, every subject) in the simulation are included. The first two columns are the cohort’s index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta\n1\nThe estimate of the slope of the logistic regression\n\n\nAlpha &lt;O&gt;\nO\nThe estimate of intercept of the logistic regression – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the second group.\n\n\na\n1\nIf a second group is included this is the estimate of the offset for the intercept of the cat-3 toxicity model.\n\n\n\n\n15.3.1 MCMC File if Efficacy is included\nIf an efficacy endpoint is included in the design, then all the model parameters columns described above are included suffixed with “Tox” and then duplicate, suffixed with “Eff”.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta Tox\n1\nThe estimate of the slope of the logistic regression of the toxicity model\n\n\nAlpha &lt;O&gt; Tox\nO\nThe estimate of intercept of the logistic regression of the toxicity model – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb Tox\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the toxicity model for the second group.\n\n\na Tox\n1\nIf a second group is included this is the estimate of the offset of the intercept for the cat-3 toxicity model.\n\n\nBeta Eff\n1\nThe estimate of the slope of the logistic regression of the efficacy model\n\n\nAlpha Eff\n1\nThe estimate of intercept of the logistic regression of the efficacy model.\n\n\nB Eff\n1\nIf a second group is included this is the estimate of ‘B’ the slope offset for the efficacy model for the second group.\n\n\nA Eff\n1\nIf a second group is included this is the estimate of the offset of the intercept for the efficacy model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#exporting-the-results",
    "href": "documentation/v71/userguides/crm.html#exporting-the-results",
    "title": "FACTS Dose Escalation CRM",
    "section": "15.4 Exporting the Results",
    "text": "15.4 Exporting the Results\nUsing the menu item File -&gt; Export Project, the .facts file and all the results files can be saved as a single zip file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Dose Escalation CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html",
    "href": "documentation/v71/userguides/installation.html",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document describes how to install and verify the installation of the FACTS (Fixed and Adaptive Clinical Trial Simulator) software. It is intended for anyone concerned with the installation of the system.\n\n\n\nThis document covers the installation and installation verification FACTS. It is only intended to demonstrate the usage of the software to the extent that it verifies the software will execute properly in the installed environment.\nFor parameter meanings or detail into the internal workings of the design engines or algorithm check the appropriate System Requirements Document. For appropriate guidance on usage check the User Guides.\n\n\n\nThis document is a guide to the FACTS installation, including both the desktop application and the grid components.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#purpose-of-this-document",
    "href": "documentation/v71/userguides/installation.html#purpose-of-this-document",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document describes how to install and verify the installation of the FACTS (Fixed and Adaptive Clinical Trial Simulator) software. It is intended for anyone concerned with the installation of the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#scope-of-this-document",
    "href": "documentation/v71/userguides/installation.html#scope-of-this-document",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document covers the installation and installation verification FACTS. It is only intended to demonstrate the usage of the software to the extent that it verifies the software will execute properly in the installed environment.\nFor parameter meanings or detail into the internal workings of the design engines or algorithm check the appropriate System Requirements Document. For appropriate guidance on usage check the User Guides.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#context-of-this-issue",
    "href": "documentation/v71/userguides/installation.html#context-of-this-issue",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document is a guide to the FACTS installation, including both the desktop application and the grid components.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#desktop-requirements",
    "href": "documentation/v71/userguides/installation.html#desktop-requirements",
    "title": "FACTS Installation Guide",
    "section": "3.1 Desktop Requirements",
    "text": "3.1 Desktop Requirements\nFACTS can be run on a standard system laptop or desktop running Windows 10 or 11 with the Windows .NET framework v4 or higher installed and at least 1 GB per core or more memory.\nIn addition:\n\nFACTS is expected to run on a display with a resolution of at least 1024x768 pixels and preferably greater.\n\nUser choice of non-default Windows styles/themes may result in unexpected and impractical foreground and background color combinations.\nFACTS 7.0 targets .NET Framework 4.8. Future versions of FACTS will target the latest available .NET Framework version.\nFACTS is now a 64-bit application and will be installed on the target machine’s Program Files area. Previous to FACTS 6.4 versions of FACTS were 32-bit applications installed on the target machine’s Program Files (x86) area.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#computation-requirements",
    "href": "documentation/v71/userguides/installation.html#computation-requirements",
    "title": "FACTS Installation Guide",
    "section": "3.2 Computation Requirements",
    "text": "3.2 Computation Requirements\nFACTS relies on running simulations and these simulations can be very computationally intensive. When running simulations, each simulation can be run separately (they do not depend on the results of other simulations) though to do so can be somewhat inefficient – repeatedly starting new processes and generating separate output files for every simulation that will need to be gathered together in a single “simulations.csv” file and then summarized. Thus FACTS allows the user to specify a “packet size” and the total number of simulations for each scenario to be simulated is divided by this packet size to create a set of independent jobs.\nIf the simulations are run on the users laptop or PC, FACTS will spawn a simulation thread for every core on the local machine up to the maximum number of simulation ‘packets’ that have been requested. The simulations are run at reduced priority so it is possible to continue to use the machine e.g. for email or Word whilst they run. Thus usually 2 or 4 sets of simulations are run in parallel depending on the processor in the laptop or PC.\nThere are a number of options for speeding up the running of FACTS simulations:\n\nThe simplest technically (and the approach we used to take at Berry Consultants) is to have a large multi-core server (say 32 core) remotely accessible to FACTS users and FACTs installed on it. To use, the user copies the “.facts” files to be simulated to a network shared directory which can be accessed from the server. Then after remotely logging into to the server, the user copies these files to a drive on the server, runs the simulations, zips up the results (within the FACTS GUI there is the FACTS File &gt; Export Project menu command to do this) and copies them back to the network shared drive and thence to their local machine.\nUse the FACTS network share folder “grid” interface, implemented using file transfers to and from a shared network drive. On a machine that can act as a client to a grid of compute nodes managed by one of the standard grid management packages (they used to be called “SunGrid” and “Condor” but have metamorphosed over the years) a “sweeper script” runs that transfers jobs to the grid. The jobs automatically transfer their results back to this shared drive. FACTS copies the job to a unique subfolder on the shared network location and then watches for a change in the lock file name - “submitted”, “running”, “complete” that are managed by the sweeper script. Once the simulations are complete FACTS copies the results back to the local machine. The fact that the simulations have been submitted to the grid are stored in the “.facts” file. Whenever that “.facts” file is open in FACTS, FACTS will poll the remote network drive to check if the simulations are complete.\nA more sophisticated FACTS grid interface that uses a web services to communicate between the FACTS client and a Linux server running a web-server (Apache Tomcat) and database (MySQL). The web service is used to submit jobs and they are stored in the database. A database process then submits them to the grid, once again managed by one of the standard grid management packages. The simulation results are then stored in the database for FACTS to download once complete. This provides a more robust and manageable interface, but it more work to set up. We can provide documentation and scripts and we can assist in setting this up. This is the form of grid that we now use in-house at Berry Consultants.\nTechnically as 3. (but for a fee) Berry Consultants can set and manage the grid for you in the cloud. Please contact us to discuss your requirements and for pricing. Therefore FACTS is able to offload the simulations from the desktop to be run by an external system. The interface describing the interactions with the external system is described in the FACTS Grid Interface document. With a FACTS Enterprise License, the command line executable files to run simulations externally under either Windows or Linux environments are available upon request.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#installation-instructions",
    "href": "documentation/v71/userguides/installation.html#installation-instructions",
    "title": "FACTS Installation Guide",
    "section": "4.1 Installation Instructions",
    "text": "4.1 Installation Instructions\nThe FACTS Desktop installation package consists of:\nsetup.exe a Windows installation program, Setup.msi the FACTS Microsoft Installer file Examples.zip a Zip file containing example FACTS projects, Documents.zip a Zip file containing the FACTS documentation. Config.xml an XML file containing the local configuration settings. These files are usually made available for download from Berry Consultants Microsoft App Center site. Download instructions are in a separate document. Versions of these files with the standard file extensions (.msi and .zip) modified are available it may have been these versions that were downloaded to circumvent firewall restrictions and these files will need to be renamed prior to use. Installation will take only a few moments. - Ensure that all the files have the correct file extension and are located on a local drive on the machine on which FACTS is to be installed. Windows can treat installs from networked drives as less trustworthy than installs from local drives and this can result in an incomplete installation. - Right click the setup.exe Windows installation program and select “Run as Administrator”. - Follow the instructions on the screen to complete the installation. - During FACTS installation you will have to option to enable FACTS to report Analytics back to the App Center. This allows to see how much FACTS is used and which features in FACTS are being used. It does NOT include any user or license information, we can’t see WHO is doing what, only WHAT is being done. Obviously we’d be grateful if you’d enable them. Analytics are off by default they will only be enabled of you enable them. Once installed analytics can be turned on or off from the FACTS “Settings” menu command.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#config.xml",
    "href": "documentation/v71/userguides/installation.html#config.xml",
    "title": "FACTS Installation Guide",
    "section": "4.2 Config.xml",
    "text": "4.2 Config.xml\nIncluded with the FACTS installation files is a configuration file that can be edited to local settings before the install files are distributed to users. It is also possible to provide an updated copy of the configuration file to users and ask them to update their default configuration, it is also possible for users to locally modify their configuration and revert to the installed configuration details.\nPrior to installation, a configuration file, ‘config.xml’, is available as one of the installation files. This file can be edited to set up a number of default settings for FACTS.\nThe settings are listed between the tags: &lt;configuration&gt; &lt;userSettings&gt; and &lt;/userSettings&gt; &lt;/configurations&gt;, for example:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;\n&lt;configuration&gt;\n  &lt;userSettings&gt;\n    &lt;GridLocation&gt;C:\\\\work\\\\grid&lt;/GridLocation&gt;\n    &lt;GridOpSys&gt;1&lt;/GridOpSys&gt;\n    &lt;GridListenerDelay&gt;10000&lt;/GridListenerDelay&gt;\n    &lt;LocalRVersions&gt;\n      &lt;value&gt;version=\"3.3.2\" path=\"C:\\\\Program Files\\\\R\\\\R-3.3.2\\\\bin\\\\R.exe\" active=\"1\"&lt;/value&gt;\n      &lt;value&gt;version=\"RStudio\" path=\"C:\\\\Program Files\\\\RStudio\\\\bin\\\\RStudio.exe\"&lt;/value&gt;\n    &lt;/LocalRVersions&gt;\n    &lt;FactsSimulationServicePortURL&gt;http://nowhere.com:8080/axis2/services/FactsSimulationServicePort&lt;/FactsSimulationServicePortURL&gt;\n    &lt;GridSimMethod&gt;0&lt;/GridSimMethod&gt;\n  &lt;/userSettings&gt;\n&lt;/configuration&gt;\nSpecifically, the following values may be adjusted, as desired:\n\nLocalRVersions – a list of available R (or RStudio) versions, each one bracketed by the tags  and  and composed of two parameters “version” which can contain any string to be used to identify that version of R and “path” which should contain location of “.exe” that should be run when the user requests R to be run or a Design Report to be generated.\nGridSimMethod – 0 or 1, Determines how FACTS tries to connect to the grid, 0 means the network file share & sweeper script method (option 2 above) is to be used, 1 means that the Web Service method (option 3 or 4 above) is to be used\nIf the network file share method is to be used to connect to the grid then:\n\nGridLocation – the network location of the network file share.\nGridOpSys – 0 or 1, the type of the operating system that is running on the nodes of the grid: 0 – Linux, 1 – Windows (the simulation engine executables have different names in the two environments).\nGridListenerDelay – the delay (in milliseconds) between each poll of the network file share for changes in the state of the simulation results.\n\nIf the Web Service grid access method is to be used to connect to the grid then:\n\nFactsSimulationServicePortURL – specifies the URL to the FACTS web-service endpoint.\n\n\nNote, this configuration file is only used on the initial load of FACTS – subsequently, a local user configuration file is created in a location under the AppData folder – e.g.:\nC:\\Users\\&lt;user_id&gt;\\AppData\\Local\\Berry_Consultants_Inc\\FACTS_File_Loader_Url_&lt;Windows unique file id &gt;\\6.1.6.17435\\user.config\nAny changes made to the configuration from the UI (under the ‘Settings’ menu) are saved to this local file. – and the original config file is only used if the options are reset.\nNB, these local configuration are FACTS version and build specific (note the version and build number in the directory) – which means that if a new install is run, local configuration modifications will be lost.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#notes-on-access-permissions",
    "href": "documentation/v71/userguides/installation.html#notes-on-access-permissions",
    "title": "FACTS Installation Guide",
    "section": "4.3 Notes on access permissions",
    "text": "4.3 Notes on access permissions\nFACTS uses the locations C:\\Program Data\\BerryConsultants and &lt;user&gt;\\AppData\\Local\\BerryConsultants, we have seen some IT departments set the default access permissions to deny access to these locations contrary to Microsoft’s intention and the access will need to be granted for FACTS to run. When FACTS runs simulations it spawns one or more simulations processes, and we have encountered environments where these processes do not get permission to write to network drives. If these permissions cannot be changed, it will be necessary for users to save their “.facts” file run in a directory on the local drive before running simulations, so the results can be written there and then copied/moved to the network drive once complete.\n\n4.3.1 License Installation\nWhen FACTS is first run, it may require the license to be entered. The user can choose the file when prompted, or cut and paste the information into the dialog box. Alternatively, the file can be dropped in the application folder and it will be picked up when needed. Note, depending on access permissions, it may be necessary to initially load FACTS with admin rights in order to load the license key from file.\nSubsequent runs, and subsequent installations of mod level updates, will not require the license to be re-entered.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/installation.html#installation-verification",
    "href": "documentation/v71/userguides/installation.html#installation-verification",
    "title": "FACTS Installation Guide",
    "section": "4.4 Installation Verification",
    "text": "4.4 Installation Verification\n\n\n\n\n\n\n\n\n\n\nAction\nExpected result\nActual result\nPass\nFail\n\n\nVerify that the FACTS shortcut appears in the Start Menu and on the user’s desktop\nApplication shortcuts located\n\n\n\n\n\nClick the application shortcut to launch FACTS (Enter license key as necessary)\nApplication opens.\n\n\n\n\n\nClick the Help &gt; About menu item\nAbout box appears.\n\n\n\n\n\nVerify the version number is the same as the version specified by Berry Consultants.\nVersion number is correct\n\n\n\n\n\nIf example projects were installed, select File &gt; Examples &gt; [example_file_name] menu item\nExample project opens.\n\n\n\n\n\nIf example projects were not installed select open and navigate to the location where an example file was saved\nExample project opens.\n\n\n\n\n\nClick the File &gt; Save As menu item.\nChoose a writeable location on the local drive and save a copy of the design.\nNew copy of project created.\n\n\n\n\n\nClick the Simulation tab, with “Locally” selected,\nClick “Select All”\nClick “Simulate”\nSimulations start.  After a short while, results are displayed in the GUI.\n\n\n\n\n\nClick the “View Graph” button.\nGraph window opens.  Displayed graph updates when graph title selected in list.\n\n\n\n\n\n\n\n\n\n\n\n\nThe “Design Report” feature in FACTS Core requires that R, R Studio (for mathjax & pandoc), and the R libraries “markdown”, “xtable” and “stringi” are installed. You will also need Microsoft Word installed to be able to open the generated “.docx” file.\n\n\n\n\n\n\nIn FACTS open the configuration settings and configure the location of R or RStudio on the computer. (See the Design Report User Guide for more details if required).\n\n\n\n\n\n\nIf the example file used above was not a “FACTS Core” example, repeat the steps above with a FACTS Core example, up to and including the “Run Simulations” step.\n\n\n\n\n\n\nPress the “Design Report” button. If you have “R” selected as your FACTS default “R”, this will run in batch mode.\nIf you have RStudio selected as our default “R”, RStudio will now start. In the terminal window you are shown the text of a function call. Copy and paste this text into the R command line and execute the function.\nOnce the function is complete you should have a Word file within the “_results” directory that corresponds to the current “.facts” file.\n\n\n\n\n\n\nOpen the World file and review the contents, it should describe the example you have selected and simulated.\n\n\n\n\n\n\nClick the File &gt; Exit menu item\nApplication closed.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "documentation/v71/examples/CRM/example1.html",
    "href": "documentation/v71/examples/CRM/example1.html",
    "title": "Example 1",
    "section": "",
    "text": "First example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "CRM",
      "Example 1"
    ]
  },
  {
    "objectID": "documentation/v71/examples/Staged/example1.html",
    "href": "documentation/v71/examples/Staged/example1.html",
    "title": "Example 1",
    "section": "",
    "text": "First example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "Staged",
      "Example 1"
    ]
  },
  {
    "objectID": "documentation/glossary.html",
    "href": "documentation/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page provides definitions of terms, acronyms, and abbreviations that are commonly used across FACTS documentation.\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nName\nDefinition\n\n\n\n\nActive Comparator\nA treatment arm where subjects are treated with some existing therapy which the user either wishes to compare the novel treatment to (in addition to control), or wishes to use to check that the study has been properly designed to show a difference from control.\n\n\nBaseline\nThe subject’s baseline is their endpoint value before treatment on the trial is started. In FACTS, the subject’s baseline is measured at their first visit at follow-up time 0, any prior visits (e.g. for screening to see if the patient is eligible for the trial) are not included in the simulation.\n\n\nCap\nA limit on the number of subjects recruited. In FACTS N-CRM users can specify a cap on the overall number of subjects to be recruited in the trial (the ‘Overall Cap’) and specify stopping rules to define when the trial should stop before it reaches cap.\n\n\nControl\nIs the treatment arm with which the novel treatment(s) are principally being compared. Control may be placebo, or some existing standard of care, or therapy, against which the novel treatment has to be benchmarked in order to determine its likely usefulness.\n\n\nCore\nFACTS Core: A mode of FACTS for designing trials where multiple treatments, (possibly different doses of a novel treatment) are tested against a control and optionally an active comparator.\n\n\nCRM\nContinual Reassessment Method – a dose escalation design where the dose-toxicity is estimated using a simple Bayesian model, and the resulting estimates used to control the dose escalation and estimate the Maximum Tolerated Dose (MTD).\n\n\nDE\nDose Escalation: a mode of FACTS where subjects are treated in cohorts and dose escalation is determined by the number of toxicities observed.\n\n\nDose Response Model\nA model used in the statistical analysis of the final response as a function of the treatment dose strength. FACTS includes both parametric and non-parametric models, including ‘no model’.\n\n\nED\nEnrichment Designs: a mode of FACTS for designing trials where the same treatment is testing in different settings for example different sub-populations or different but related indications.\n\n\nEndpoint\nAn endpoint is a measure of the subject’s health that is being analyzed in order to learn about the effects of the treatments being studied in the trial.\n\n\nFACTS\nFixed and Adaptive Clinical Trial Simulator.\n\n\nFinal Endpoint\nThe value, or state, of a subject’s endpoint at the last visit in the follow-up schedule.\n\n\nGroup\nThe very neutral term ‘groups’ is used in ED, because the ED design could be used in a number of different settings. The groups could be for instance different patient sub-populations with the same disease, different disease sub-populations such as different sarcoma types, or even different but related diseases that the same treatment might be effective against (migraine, dental pain and post-operative pain for instance). The key characteristic is that there is a need to study each independently (with their own control) but also look across the different groups for some purpose – this might be for analysis where there is an expectation of commonality in response on control, or in the difference from control on the treatment arms; or operationally where there is limited budget, drug supply or time across the groups; or for the purposes of decision making – where the treatment has to be successful in more than one group to be worth considering for further development.\n\n\nGUI\nGraphical User Interface, the visual part of the FACTS application that the user interacts with.\n\n\nHistorical Control\nA ‘historic control’ arm is used when no control arm is randomized to in the study, and the response on the arms where the novel treatment administered are compared to combined data from control arms from other already complete studies.\n\n\nInterim Visit\nA visit between the baseline visit and final visit, at which a subject’s endpoints are measured.\n\n\nIntermediate Endpoint\nThe value of a subject’s endpoint at a visit after their baseline visit, but before their final visit. Intermediate endpoint values can be used to estimate, using a longitudinal model, what the subject’s final endpoint will be (or would have been) thus providing additional information for analyses. Multiple imputation is used ensure that these estimated responses are included in the analysis with all due uncertainty. Sometimes called an early endpoint.\n\n\nLongitudinal Model\nAn analysis model that models the relationship between a subject’s interim endpoint values and their final endpoint value. In FACTS, all longitudinal models are simply multiple imputation models that can be used to impute a subjects final endpoint value when it is not available.\n\n\nMethod\nIn the FACTS documentation we try to reserve the term ‘method’ for the algorithms used in the simulation (as opposed to the analysis) part of the program. In the analysis part we use the term ‘Model’, see below.\n\n\nModel\nIn the FACTS documentation we try to reserve the term ‘model’ for the statistical models used in the analysis of the trial data (in the ‘design’ section of the FACTS user interface). Where mathematical algorithms are used for other purposes in FACTS (for instance in the generation of the simulated data) we try to use the term ‘method’. It is common for users to confuse the data generation and the trial implementation components of FACTS, and using distinct terminology may help to reduce this.\n\n\nMTD\nThe dose most likely to be the Maximum Tolerated Dose (MTD) – the dose with the highest Pr(MTD).\n\n\nMTD+\nThe dose most likely to be the MTD+ – the dose with the highest Pr(MTD+).\n\n\nMultiple Imputation\nWhen the Bayesian statistical models are fit to simulated data within FACTS, if longitudinal modeling has been included, then multiple imputation is used for missing subject final endpoint values. This includes subjects whose final endpoint data are missing due to the subject having dropped out and subjects who have not yet had enough follow-up time to observe their final visit response yet. Imputed final endpoint values are separately sampled at each iteration of the MCMC from the posterior distribution of the longitudinal model, given the subject’s interim endpoint values.\n\n\nPr(MTD)\nA dose’s probability of being MTD is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable toxicity band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. This estimate of MTD is constrained to select one of the available doses.\n\n\nPr(MTD+)\nA dose’s probability of being the MTD+ is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. Unlike Pr(MTD), Pr(MTD+) includes estimating whether a dose below or a dose above the range of those being tested is more likely to have a toxicity in the acceptable band than any of the doses in the range.\n\n\nProfile\nA profile is a specification of one aspect of a scenario. Examples of these aspects are Accrual Rate, VSR, Dropout Rate, and others. A scenario is made up of one profile of each of the required types for the type of trial being simulated. FACTS allows the user to specify multiple profiles of each type and then presents all the possible combinations of profiles as scenarios that can be used to drive simulations.\n\n\nQOI\nQuantity Of Interest - A value to be calculated because it is of interest to the proceeding of the simulated trials. Quantities may be of interest because they are to be used in an interim decision, to adapt allocation, for final evaluation, or to ensure it is value is written out in the FACTS results files for analysis outside of FACTS.\n\n\nResponse\nA synonym of endpoint value. May be the value of a subjects final endpoint, or a change in a subject’s endpoint compared to their baseline state.\n\n\nScenario\nA scenario is the complete specification of the unknown external factors that determine the data observed on the trial and its timing. The exact factors depend on the type of trial being simulated, but typically include:\n\nthe distribution of the final change from base line, or probability of response or rate of events in the different treatment groups\nthe properties of subjects’ early responses and the correlation with their final outcome\nthe rate at which subjects are recruited into the trial\nthe rate at which subjects drop out of the trial.\n\n\n\nSPEC\nThe Design Engine Specification document, describes the system algorithms, and meaning of parameters in a more technical context. SPEC documents have been deprecated as of FACTS 7.1.1\n\n\nResponse\nThe change in a subject’s endpoint compared to their baseline state.\n\n\nSubject\nAn entity recruited onto a clinical trial for the purposes of learning about the properties of a treatment. Depending on the type of trial they might be patients or they might be healthy volunteers.\n\n\nTreatment Arm\nAn arm being studied in a clinical trial. A treatment arm may refer to different doses of the same treatment or completely separate therapies. Subjects, upon entering a study, are randomized to a treatment arm.\n\n\nUG\nThe User Guide document - describes in detail how to use a FACTS engine.",
    "crumbs": [
      "Documentation",
      "Glossary"
    ]
  },
  {
    "objectID": "introduction/tutorials/tutorial1.html",
    "href": "introduction/tutorials/tutorial1.html",
    "title": "Tutorial 1",
    "section": "",
    "text": "First tutorial",
    "crumbs": [
      "Introduction",
      "Tutorials",
      "Tutorial 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Knowledge Hub",
    "section": "",
    "text": "Welcome to the Fixed and Adaptive Clinical Trial Simulator (FACTS) Knowledge Hub!\nFACTS is a powerful software tool that is trusted by half of the top 20 pharma companies to help them design efficient and successful clinical trials.\nWe are proud that also numerous academic, government and regulatory institutions trust FACTS."
  },
  {
    "objectID": "notes/posts/2024-10-12.html",
    "href": "notes/posts/2024-10-12.html",
    "title": "Post3",
    "section": "",
    "text": "TEXT TEXT\nTEXT"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "News",
    "section": "",
    "text": "Welcome to Field Notes — your hub for timely product news, upcoming features, and expert perspectives. Here, you can explore the latest announcements, gain industry insights, and see how others are using our software."
  },
  {
    "objectID": "notes/index.html#what-youll-find",
    "href": "notes/index.html#what-youll-find",
    "title": "News",
    "section": "What You’ll Find:",
    "text": "What You’ll Find:\n\nNews & Announcements: Key releases, product enhancements, and milestone events.\nExpert Commentary: Short analyses and tips from our team and community leaders.\nUse Cases: Inspiring stories and practical examples straight from the field.\n\nSubscribe via RSS to stay informed and ahead of the curve."
  },
  {
    "objectID": "concepts/bayes/bayes2.html",
    "href": "concepts/bayes/bayes2.html",
    "title": "Bayes 2",
    "section": "",
    "text": "Second Article on Bayesian Concepts",
    "crumbs": [
      "Concepts",
      "Bayesian Statistics",
      "Bayes 2"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/adaptiveDesigns2.html",
    "href": "concepts/adaptiveDesigns/adaptiveDesigns2.html",
    "title": "Adaptive Designs 2",
    "section": "",
    "text": "Second Article on Adaptive Designs",
    "crumbs": [
      "Concepts",
      "Adaptive Designs",
      "Adaptive Designs 2"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/adaptiveDesigns1.html",
    "href": "concepts/adaptiveDesigns/adaptiveDesigns1.html",
    "title": "Adaptive Designs 1",
    "section": "",
    "text": "First Article on Adaptive Designs",
    "crumbs": [
      "Concepts",
      "Adaptive Designs",
      "Adaptive Designs 1"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html",
    "href": "concepts/facts/LinearRegressionLMPriors.html",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "",
    "text": "Jump to widget\n\n\n\nClick to jump straight to prior specification application.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html#prior-specification-for-the-linear-regression-multiple-imputation-model",
    "href": "concepts/facts/LinearRegressionLMPriors.html#prior-specification-for-the-linear-regression-multiple-imputation-model",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "Prior Specification for the Linear Regression Multiple Imputation Model",
    "text": "Prior Specification for the Linear Regression Multiple Imputation Model\nThe default setting of “Same priors across all model instances and visits,” implies that each parameter \\(\\alpha\\), \\(\\beta\\), and \\(\\lambda\\) have the same prior for all visits \\(t\\). Estimation of the posterior distribution for these parameters is still done independently for each model instance.\n\nSame prior for all visits and model instances\nThe one prior across all model instance are formulated as: \\[\\alpha_t \\sim \\text{N}\\left(\\alpha_\\mu, \\alpha_{\\sigma}^2\\right)\\] \\[\\beta_t \\sim \\text{N}\\left(\\beta_\\mu, \\beta_{\\sigma}^2\\right)\\] \\[\\lambda_t^2 \\sim \\text{IG}\\left(\\frac{\\lambda_n}{2}, \\frac{\\lambda_\\mu^2 \\lambda_n}{2}\\right)\\]\n\n\nSame prior for all model instances, different prior per visit\nSince each visit will likely have a different estimated intercept and slope needed to accurately impute the final endpoint, the above prior formulation may not be desirable if specifying priors that are not extremely diffuse – especially on the \\(\\beta\\) parameters. Instead, selecting “Specify priors per visit across all model instances,” will share the prior specification across all instances of the model, but allows for different priors to be put on the parameters associated with each visit. The user inputted prior parameters are now subscripted with \\(t\\) to denote the visit they correspond to. These priors apply to all model instances:\n\\[\\alpha_t \\sim \\text{N}\\left(\\alpha_{\\mu_t}, \\alpha_{\\sigma_t}^2\\right)\\] \\[\\beta_t \\sim \\text{N}\\left(\\beta_{\\mu_t}, \\beta_{\\sigma_t}^2\\right)\\] \\[\\lambda_t^2 \\sim \\text{IG}\\left(\\frac{\\lambda_{n_t}}{2}, \\frac{\\lambda_{\\mu_t}^2 \\lambda_{n_t}}{2}\\right)\\]\n\n\nDifferent prior for all model instances and visits\nIt is also possible to specify priors “Per model instance and visit,” in which every visit has separate priors, and those differing priors vary across model instances. This is the most flexible prior specification method. The user inputted prior parameters are now subscripted by both t for visit and i for model instance. \\[\\alpha_{ti} \\sim \\text{N}\\left(\\alpha_{\\mu_{ti}}, \\alpha_{\\sigma_{ti}}^2\\right)\\] \\[\\beta_{ti} \\sim \\text{N}\\left(\\beta_{\\mu_{ti}}, \\beta_{\\sigma_{ti}}^2\\right)\\] \\[\\lambda_{ti}^2 \\sim \\text{IG}\\left(\\frac{\\lambda_{n_{ti}}}{2}, \\frac{\\lambda_{\\mu_{ti}}^2 \\lambda_{n_{ti}}}{2}\\right)\\]",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html#sec-priorSpecWidget",
    "href": "concepts/facts/LinearRegressionLMPriors.html#sec-priorSpecWidget",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "Prior Specification Widget",
    "text": "Prior Specification Widget\n\nInterpretation of parameters\n\n\\(\\alpha_{t}\\)\n\nThe expected response on the final endpoint when the early visit \\(t\\) has a response of 0\n\n\\(\\beta_{t}\\)\n\nIf \\(\\alpha=0\\), then \\(\\beta\\) is how many times larger the final endpoint response is than the early endpoint at visit \\(t\\). If \\(\\beta=0\\), then no matter what the early visit response is, the expectation for the final visit is \\(\\alpha\\). If \\(\\beta=1\\), then for any early response the expectation of the final response is the \\(\\text{early response} + \\alpha\\). A \\(\\beta \\lt 1\\) generally implies that the final endpoint is expected to regress towards 0 (when \\(\\alpha=0\\)), and a \\(\\beta \\gt 1\\) implies that the final response is expected to keep growing relative to the early visit response.\n\n\\(\\lambda_{t}\\)\n\nThe standard deviation around the expectation of the final visit response. This dictates how close the imputed final endpoint responses are to the mean response for a subject given \\(\\alpha\\) and \\(\\beta\\). Lower \\(\\lambda_t\\) implies higher correlation between the early visit response and final visit response.\n\n\n```vdhcylbewtxtn #| standalone: true #| viewerHeight: 1000\nlibrary(shiny) library(DT) library(ggplot2) library(htmltools)\nalignCenter &lt;- function(el) { htmltools::tagAppendAttributes(el, style=“margin-left:auto;margin-right:auto;” ) }\nsketch = htmltools::withTags(table( class = ‘display’, thead( tr( th(rowspan = 2, ’‘), th(rowspan = 2, style = “border-right: solid 1px;”,’Observed Visit Data’), th(colspan = 2, class=” dt-center”, style = “border-right: solid 1px;”,‘B1 priors’), th(colspan = 2, class=” dt-center”, style = “border-right: solid 1px;”,‘B2 priors’), th(colspan = 2, class=” dt-center”, style = “border-right: solid 1px;”,‘BB priors’) ), tr( th(“mean”), th(style = “border-right: solid 1px;”, “SD”), th(“mean”), th(style = “border-right: solid 1px;”, “SD”), th(“center”), th(style = “border-right: solid 1px;”, “weight”), ) ) ))\nui &lt;- fluidPage( tags\\(head(\n    # Note the wrapping of the string in HTML()\n    tags\\)style(HTML(” .my_col_class { align-content: center; }“) ) ),\ntitlePanel(h1(“Linear Regression LM Priors”, align = “center”)), alignCenter(sliderInput(“numVisits”, “Number of visits:”, min = 2, max = 20, value = 5, step = 1)), DTOutput(“dataInputTable”), h5(“Double click on a cell to edit.”, align = “center”), br(), titlePanel(h2(“Plot a subject’s prior predictive”, align = “center”)), fluidRow( #column(5, offset = 1, uiOutput(“slider”)), column(5, offset = 1, uiOutput(“slider”)), column(6, fluidRow( column(6, offset = 2, checkboxInput(“fixAlpha”, “Fix alpha at its mean?”, value = FALSE, width = “100%”)), column(6, offset = 2, checkboxInput(“fixBeta”, “Fix beta at its mean?”, value = FALSE, width = “100%”)), column(6, offset = 2, checkboxInput(“removePredictive”, “Remove endpoint prior predictive?”, value = FALSE, width = “100%”)) )) ), fluidRow( column(6, plotOutput(“visitToFinalPlot”)), column(6, plotOutput(“priorPredictive”)) )\n)\ngetLowerMedianUpper = function(earlyVisitVal, alpha = c(0,1), beta = c(0,1), lambda = c(1,1)) { distMeanFinal = c(alpha[1] + beta[1]earlyVisitVal, sqrt(alpha[2]^2 + beta[2]^2earlyVisitVal^2))\ndeviates = rnorm(10000) deviates = (deviates - mean(deviates))/(sd(deviates))\nif(any(is.na(lambda))) { samps = (deviatesdistMeanFinal[2] + distMeanFinal[1]) } else { samps = (deviatesdistMeanFinal[2] + distMeanFinal[1]) + rnorm(10000, 0, sd = sqrt(1/rgamma(10000, lambda[2]/2, lambda[1]^2*lambda[2]/2))) }\ndistValueFinal = c(mean(samps), sd(samps))\nreturn(list(“meanFinal” = data.frame(lower = distMeanFinal[1] + qnorm(.025)distMeanFinal[2], median = distMeanFinal[1], upper = distMeanFinal[1] + qnorm(0.975)distMeanFinal[2]), “predictionFinal” = data.frame(lower = quantile(samps,.025), median = median(samps), upper = quantile(samps,.975)))) }\nserver &lt;- function(input, output, session) {\ndf = data.frame(VisitResponse = c(2,5,3,7,11), alphaPriorMean = 0, alphaPriorSD = 2, betaPriorMean = 1, betaPriorSD = 2, lambdaPriorCenter = 5, lambdaPriorWeight = 3) df[5,] = c(5, NA, NA, NA, NA, NA, NA) row.names(df) = paste(“Visit”, 1:5)\n## Render DF to actually change output\\(dataInputTable = renderDT(datatable(df,\n                                             options = list(\n                                               pageLength = 20,\n                                               dom = \"t\",\n                                               autoWidth = TRUE,\n                                               columnDefs = list(list(className = 'dt-center', orderable = FALSE, width = '75px', targets = 0:7),\n                                                                 list(width = \"150px\", targets = 0:1))\n                                             ),\n                                             container = sketch,\n                                             rownames = TRUE,\n                                             # escape = FALSE,\n                                             selection = 'none',\n                                             editable = list(target = \"cell\")\n  ) |&gt; formatStyle(c(1,3,5,7), `border-right` = \"solid 1px\") |&gt;\n    formatRound(1, digits = 4) |&gt; formatRound(2:7, digits = 2) |&gt;\n    formatStyle(0,\n                target = \"row\",\n                backgroundColor = styleEqual(paste(\"Visit\",input\\)lastVisitWithData), “lightblue”, ‘white’)) )\n## Update from Conditional proxy = dataTableProxy(‘dataInputTable’)\nobserveEvent(input\\(dataInputTable_cell_edit, {\n    info = input\\)dataInputTable_cell_edit i = info\\(row\n    j = info\\)col v = info$value\nif(i &lt; nrow(df) | j == 1) {\n  df &lt;&lt;- editData(df, info)\n} else {\n  df[i,j] &lt;&lt;- NA\n}\nreplaceData(proxy, df) \n})\nobserve({ nv = input$numVisits if(nv &gt; nrow(df)) { tempd = df for(i in 1:(nv-nrow(df))) { tempd = rbind(tempd, setNames(data.frame(c(tempd[nrow(tempd),])), names(tempd))) rownames(tempd)[nrow(tempd)] = paste(“Visit”, nrow(tempd)) tempd[nrow(tempd)-1,-1] = tempd[nrow(tempd)-2,-1] } df &lt;&lt;- tempd } else if(nv &lt; nrow(df)) { df &lt;&lt;- df[1:nv,] df[nv,-1] &lt;&lt;- NA } replaceData(proxy, df) })\nsliderParams &lt;- reactiveValues(max = 5, value = 3) output\\(slider &lt;- renderUI({\n    sliderInput(\"lastVisitWithData\", \"Last complete visit:\", min = 1, max = sliderParams\\)max, value = sliderParams\\(value, step = 1)\n  })\n  observeEvent(input\\)numVisits, { sliderParams\\(max = input\\)numVisits if(!is.null(input\\(lastVisitWithData)) {\n      sliderParams\\)value &lt;- min(input\\(lastVisitWithData, input\\)numVisits) } else { sliderParams$value = 3 } })\noutput\\(priorPredictive = renderPlot({\n    req(input\\)lastVisitWithData) input$dataInputTable_cell_edit\nlvIndex = input$lastVisitWithData\nfinalVisitIndex = input$numVisits\ntempDF = df\ndataToPlot = getLowerMedianUpper(tempDF[lvIndex,1],\n                                 alpha = c(tempDF[lvIndex,2], ifelse(input$fixAlpha, 0, tempDF[lvIndex,3])),\n                                 beta = c(tempDF[lvIndex,4], ifelse(input$fixBeta, 0, tempDF[lvIndex,5])),\n                                 lambda = c(tempDF[lvIndex,6], tempDF[lvIndex,7]))\n\ntempDF$RowVisitIndex = 1:nrow(tempDF)\ntempDF$visitKnown = \"included\"\ntempDF$visitKnown[tempDF$RowVisitIndex &gt; lvIndex] = \"excluded\"\n\n# tempDF = rbind(setNames(data.frame(c(tempDF[1,])), names(tempDF)), tempDF)\n# tempDF[1,1] = 0\n# tempDF$RowVisitIndex[1] = 0\n# rownames(tempDF)[1] = \"Baseline\"\n\np1 = ggplot() + \n  geom_point(dat = tempDF, aes(x = RowVisitIndex, y = VisitResponse, color = visitKnown), size = 3) + \n  scale_color_manual(breaks = c(\"included\", \"excluded\"), values = c(\"black\", \"gray70\"), guide = \"none\") +\n  coord_cartesian(xlim = c(0, finalVisitIndex)) + \n  scale_x_continuous(breaks = 0:finalVisitIndex, labels = c(\"Baseline\", 1:finalVisitIndex)) +\n  xlab(\"Visit\") + ylab(\"Response\") + ggtitle(\"Predicting Final Endpoint of a Subject\") +\n  theme_bw() + \n  theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"left\", legend.position = \"bottom\", legend.direction = \"vertical\")\n\nif(lvIndex &lt; finalVisitIndex) {\n  if(!input$removePredictive) {\n    p1 = p1 + \n      geom_segment(data = dataToPlot[[2]], aes(x = finalVisitIndex, y = lower, yend = upper), color = \"darkgreen\", linewidth = 2.5) +\n      annotate(geom = \"point\", x = finalVisitIndex, y = dataToPlot[[2]]$median, color = \"darkgreen\", size = 3, shape = 18) +\n      geom_ribbon(data = NULL, aes(x = c(lvIndex, finalVisitIndex),\n                                   ymin = c(tempDF$VisitResponse[lvIndex], dataToPlot[[2]]$lower),\n                                   ymax = c(tempDF$VisitResponse[lvIndex], dataToPlot[[2]]$upper),\n                                   fill = \"preds\"),  color = NA, alpha = .2)\n  }\n  p1 = p1 +\n    geom_segment(data = dataToPlot[[1]], aes(x = finalVisitIndex, y = lower, yend = upper), color = \"darkblue\", linewidth = 1.5) +\n    annotate(geom = \"point\", x = finalVisitIndex, y = dataToPlot[[1]]$median, color = \"darkblue\", size = 3, shape = 18) +\n    annotate(geom = \"segment\", x = lvIndex, xend = finalVisitIndex, y = tempDF$VisitResponse[lvIndex], yend = dataToPlot[[1]]$median, linetype = \"dashed\", color = \"darkblue\")+\n    geom_ribbon(data = NULL, aes(x = c(lvIndex, finalVisitIndex),\n                                 ymin = c(tempDF$VisitResponse[lvIndex], dataToPlot[[1]]$lower),\n                                 ymax = c(tempDF$VisitResponse[lvIndex], dataToPlot[[1]]$upper)),\n                fill = \"darkblue\",  color = NA, alpha = .4)\n} else {\n  p1 = p1 + annotate(geom=\"text\", label = \"Final Visit Value Known\",\n                     alpha = .5, size = 10, x = (finalVisitIndex)/2, y = Inf, vjust = 1.3)\n}\n\np1 = p1 + scale_fill_manual(NULL, breaks = c(\"preds\"), limits = c(\"preds\"), values = c(\"darkgreen\"), labels = c(\"Prior predictive distribution for final endpoint of subject.\"))\n#  guides(fill = guide_legend(override.aes = list(limits = c(\"darkgreen\", \"darkblue\"), labels = c(\"Prior distribution for mean of imputed value.\", \"Prior predictive distribution for final endpoint of subject.\")))) \n\np1\n})\noutput\\(visitToFinalPlot = renderPlot({\n    input\\)dataInputTable_cell_edit req(input$lastVisitWithData)\nlvIndex = input$lastVisitWithData\nfinalVisitIndex = input$numVisits\n\ntempDF = df\n\nmin_s = ifelse(min(tempDF$VisitResponse, na.rm = TRUE) &lt; 0, (min(tempDF$VisitResponse, na.rm = TRUE)+1)*1.1, 0)\nmax_s = ifelse(max(tempDF$VisitResponse, na.rm = TRUE) &gt; 0, (max(tempDF$VisitResponse, na.rm = TRUE)+1)*1.1, 0)\n\ns = seq(min_s-5, max_s+5, length.out = 101)\n\nmeanDist_mean = tempDF$alphaPriorMean[lvIndex] + tempDF$betaPriorMean[lvIndex]*s\nif(!input$fixAlpha & !input$fixBeta) {\n  meanDist_sd = sqrt(tempDF$alphaPriorSD[lvIndex]^2 + tempDF$betaPriorSD[lvIndex]^2*s^2)\n} else if(input$fixAlpha & !input$fixBeta) {\n  meanDist_sd = sqrt(tempDF$betaPriorSD[lvIndex]^2*s^2)\n} else if(!input$fixAlpha & input$fixBeta) {\n  meanDist_sd = rep(sqrt(tempDF$alphaPriorSD[lvIndex]^2), length(s))\n} else {\n  meanDist_sd = rep(0, length(meanDist_mean))\n}\n\nplotDF = data.frame(earlyVis = s,\n                    lower = meanDist_mean + qnorm(0.025)*meanDist_sd,\n                    median= meanDist_mean,\n                    upper = meanDist_mean + qnorm(0.975)*meanDist_sd)\n\nif(lvIndex &lt; finalVisitIndex) {\n  if(!input$removePredictive) {\n    numSamps = 2500\n    \n    deviates = rnorm(numSamps)\n    deviates = (deviates - mean(deviates))/(sd(deviates))\n    normigsamps = rnorm(numSamps, 0, sd = sqrt(1/rgamma(numSamps, tempDF$lambdaPriorWeight[lvIndex]/2, tempDF$lambdaPriorCenter[lvIndex]^2*tempDF$lambdaPriorWeight[lvIndex]/2)))\n    \n    distVals = matrix(NA, ncol = 2, nrow = length(s))\n    for(i in 1:length(s)) {\n      distVals[i,] = quantile((deviates*meanDist_sd[i] + meanDist_mean[i] + normigsamps), c(0.025, 0.975))\n    }\n    \n    plotDF$lowerPred = distVals[,1]\n    plotDF$upperPred = distVals[,2]\n  }\n  \n  p2 = ggplot(data = plotDF) + geom_abline(aes(slope = tempDF$betaPriorMean[lvIndex], intercept = tempDF$alphaPriorMean[lvIndex]), color = \"darkblue\", linewidth = 1.5) + \n    geom_ribbon(aes(x = s, ymin = lower, ymax = upper, fill = \"means\"), color = NA, alpha = 0.4)\n  \n  if(!input$removePredictive) {\n    p2 = p2 + geom_ribbon(aes(x = s, ymin = lowerPred, ymax = upperPred), fill = \"darkgreen\", color = NA, alpha = 0.2) \n  }\n  p2 = p2 +\n    coord_cartesian(xlim = c(min_s, max_s)) +\n    xlab(\"Early Visit Known Value\") + ylab(\"Expectation of Final Visit Response\") + ggtitle(\"Expectation of Final Visit Given Early Visit\") +\n    theme_bw() + \n    theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"right\", legend.position = \"bottom\", legend.direction = \"vertical\")\n} else {\n  p2 = ggplot(data = NULL) + geom_abline(aes(fill = \"means\"), slope = 1, intercept = 0, color = \"darkblue\", linewidth = 1.5) + \n    coord_cartesian(xlim = c(min_s, max_s), ylim = c(min_s, max_s)) +\n    annotate(geom=\"text\", label = \"Final Visit Value Known\",\n             alpha = .5, size = 10, x = (max_s + min_s)/2, y = Inf, vjust = 1.3) +\n    xlab(\"Early Visit Known Value\") + ylab(\"Expectation of Final Visit Response\") + ggtitle(\"Expectation of Final Visit Given Early Visit\") +\n    theme_bw() + \n    theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"right\", legend.position = \"bottom\", legend.direction = \"vertical\")\n}\np2 = p2 + scale_fill_manual(NULL, breaks = c(\"means\"), limits = c(\"means\"), values = c(\"darkblue\"), labels = c(\"Prior distribution for mean of imputed value.\"))  \np2\n}) }\nshinyApp(ui = ui, server = server)```",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts620.html",
    "href": "releaseNotes/v6/facts620.html",
    "title": "FACTS 6.2.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.2.0\nBerry Consultants is delighted to announce that FACTS 6.2.0 is ready for release!\nBuilding on FACTS 6.1.0, FACTS 6.2.0 adds new features to “FACTS N-CRM”, the ability to generate a “Design Report” from FACTS Core designs and extending the ability to compute predictive probabilities to FACTS Core TTE and FACTS Staged TTE.\n\nFACTS N-CRM extensions. FACTS has had versions of the CRM with an efficacy endpoint, ordinal toxicity endpoint and 2 groups since its inception. But these were in separate engines and used the old CRM model for analysis. We have now added all these features as options to the N-CRM so they can be used with the 2 parameter Bayesian Logistic Regression model, targeting toxicity bands and the option to use overdose control. These features cannot only now but used with this better methodology, but can be used in combination with each other, and in combination with the other advanced features that were already included in the FACTS DE N-CRM simulator such as, run ins, stopping rules, escalation rules, fine grain dosing and open enrollment.\n\n\n\n\nNew N-CRM Graph\n\n\n\nFACTS Design Report. In FACTS Core there is now the ability to generate a “Design Report” as a MS Word file that describes the design and simulation results. The file is not intended as the final article but as something where the bulk of the straightforward text (and equations) have been provided and should just require polishing, particularly with the details of the indication and trial setting that FACTS is inevitably unaware of.\n\n\n\n\nNew Design Report\n\n\n\nFACTS 6.2 completes the implementation of predictive probabilities. Predictive probabilities in the current trial with a TTE endpoint are considerably more complex than predictive probabilities in the other endpoints. For the other endpoints the expected about of information after full enrollment and full follow-up is known, for time-to-event it can depend on multiple things such as accrual rate and the expected number of events so a degree of simulation within the simulation is required.\n\nFACTS 6.2.0 is fully backwards compatible with FACTS 6.1.0, 6.0.0 and 5 – it can load and run all your FACTS 6.0.0 and FACTS 5 designs – and then use new FACTS 6.2.0 features with those designs. You can have FACTS 6.2.0 and FACTS 6.1.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Dose Escalation, the N-CRM (also known as Bayesian Logistic Regression) now has options for:\n\nAn ordinal toxicity endpoint\nTo simulate a trial across 2 groups (e.g. Adults and Pediatrics)\nAn additional binary Efficacy endpoint\nThese options can be combined with each other and all the other N-CRM options.\n\nFACTS Core TTE\n\nThe ability to compute the predictive probability of success at the full enrollment of the current trial.\n\nFACTS Staged Design TTE\n\nThe ability to compute the predictive probability of success\n\nin Stage 1 at full enrollment\nof Stage 2 (whilst in Stage 1)\nin Stage 2 at full enrollment (whilst in Stage 2).",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.2.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts624.html",
    "href": "releaseNotes/v6/facts624.html",
    "title": "FACTS 6.2.4 Release Notes",
    "section": "",
    "text": "Berry Consultants would like to announce a new maintenance release, FACTS 6.2.4. FACTS 6.2.4 contains the following fixes to the FACTS 6.2.3 version:\nUpdating to 6.2.4 is recommended for those of you wishing to use predictive probabilities in FACTS Core TTE, in combination with a TTE predictor endpoint:\n\nIn FACTS Core with a Time-to-Event end point and a Time-to-Event predictor, the imputations of final event times for subjects with a predictor event but no final event during the estimation of “predictive probability of success at full enrolment” could produce an error in the prior version. There are two rare situations in FACTS 6.2.3 that uncover a bug in the dose escalation simulator and causes it to produce an error:\nIn FACTS Dose Escalation, in the N-CRM with only 3 doses the simulator could produce an error during some dose escalation decisions.\nIn FACTS Dose Escalation CRM (Toxicity) could produce an error when simulating 2 samples. The remaining, minor fixes in FACTS 6.2.4 are:\nIn FACTS N-CRM, the GUI was improved to handle the “Variant” options making is easier to change them once they were set.\nA fix to FACTS Dose Escalation 3+3 (!) – improved to handle the circumstance when the starting dose is not the lowest dose, and the dose assignment de-escalates to below the starting dose and validates the next lower dose.\n\nPlus we improved the labeling of a class of prior parameters:\n\nIn the FACTS GUI labels of parameters for prior with an Inverse-Gamma distribution the wording has been changed from “mean value” (which is technically incorrect) to “central value”.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.2.4 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts640.html",
    "href": "releaseNotes/v6/facts640.html",
    "title": "FACTS 6.4.0 Release Notes",
    "section": "",
    "text": "FACTS 6.4.0 is now available for official release. Please contact us regarding any questions.\nThe key features of this release are:\n\nThree new dose response models in FACTS (Staged) Core designs.\nAlternative parametrizations to Posterior Probability Quantities of Interest (QOIs) in FACTS (Staged) Core Dichotomous and Time-to-Event designs.\nThe ability to run FACTS from R and to run FACTS in command-line mode on Linux (Enterprise licensees only).\n\nIn detail the new features in FACTS 6.4.0 are:\n\nThree new dose response models have been added across all FACTS (Staged) Core designs. These new options will appear in the model selection dropdown on the Dose Response tab. The new models are as follows:\n\nThe Simple Hierarchical model – a model in which the mean responses for each of the arms in the design are drawn from a normal distribution, whose mean and variance are estimated by FACTS. The control arm can be included in the hierarchical model, or modeled separately, in which case it has its own prior mean and variance. The control arm cannot be included in this model for Time-to-Event designs.\nThe Simple Linear model – a linear model which assumes that the mean responses for each of the arms in the design are linear functions of the associated arm strength. In particular, the arm with the largest mean response is guaranteed to be either the largest dose or the smallest arm in this model. Note that the “2-Parameter Logistic” model in FACTS (Staged) Core Dichotomous designs has been replaced by the “Simple Linear model”. FACTS (Staged) Core Dichotomous designs making use of the 2-Parameter Logistic model will be automatically migrated to the Simple Linear model.\nThe Simple Hierarchical Linear model – a model which uses a linear model as a base dose-response structure but allows deviations from linearity in a manner similar to the Hierarchical Logistic dose response model. Given appropriate priors, if the data and prior distributions are consistent with linearity, the hierarchical variance parameter will be estimated to be small and the model fit will be essentially linear, but if the data is non-linear the variance parameter will be large allowing a significantly non-linear model fit.\n\nIn FACTS (Staged) Core Dichotomous and TTE designs, Posterior Probability QOIs with alternative parametrizations can be set when creating a new QOI. This can be achieved by selecting the appropriate option in the “Compare” dropdown of the QOI dialog. The options are as follows:\n\nFor FACTS (Staged) Core Dichotomous designs, Posterior Probability QOIs comparing the log-odds ratio of the response rate for each arm against that of a given arm can now be created. Previously, only the response rates could be compared.\nFor FACTS (Staged) Core TTE designs, Posterior Probability QOIs comparing the hazard rates of the response for each arm against that of a given arm can now be created. Previously, only the hazard ratios (HR) could be compared.\n\nEnterprise FACTS licensees will now be able to access and run FACTS Core and Enrichment Design (ED) analysis models from R via an R wrapper, the output of which is an MCMC file pertaining to the model. This can be used to simulate trials that require posterior quantities that FACTS does not include (e.g., probability that a dose has a treatment effect in a certain range) or simulate trials that make decisions that FACTS does not include (e.g., sample size re-assessment).\nEnterprise FACTS licensees will also now be able to run FACTS in command-line mode on Linux via a separate executable: FACTS Linux File Loader Lite (FLFLL). Mono 6.8.0+ is a pre-requisite for running FLFLL. Executing a valid FACTS design with FLFLL will generate the same results output as its Windows GUI counterpart; in particular, it will generate the simulations, summary, weeks and patients files. FLFLL can be used to automate the simulation of multiple (potentially related) FACTS designs and, more generally, can be used as a key component of a more complex trial design simulation pipeline.\n\nThe following features were also implemented in FACTS 6.4.0:\n\nThe control arm can now be modelled separately in TTE predictor dose response models within FACTS (Staged) Core TTE designs.\nFACTS Core designs will now report the time of the stopping decision of the trial through a new simulations output column named “EarlySuccess Time”.\nFACTS now computes lower and upper frequentist CI bounds, bias and coverage at the simulation level for all design types and summarized them in the associated summary file.\nA command line option for the number of samples per imputation called “samples-per-imputation” has now been added to FACTS when run in command-line mode. This applied to FACTS (Staged) Core and ED designs.\nThe analysis tab now accepts subject files with missing values for intermediate visits (denoted by -9999).\nThe analysis tab in Multiple Endpoint now accepts data files when the design includes visits where none of the endpoints are observed.\nThe “Interim vs Final” Scatter plot graph in the “Across Scenarios” now handles interactive selection of QOI and setting of thresholds, including the use of p-value QOIs.\nThe FACTS installer will now include an option to share basic, anonymous usage and crash data with the FACTS team. This option can also be enabled/disabled by going to Setting &gt; Options &gt; Analytics. Any change in this area will take effect the next time FACTS is loaded. By default, FACTS will NOT collect any usage/crash data. However, we strongly encourage licensees to enable this option to help the FACTS team proactively improve the software in the areas that matter the most. We take our licensee’s data privacy and security very seriously, so do not hesitate to get in touch if you have any questions about this feature.\nFACTS will now, by default, automatically calculate the simulation parallelization packet size based on the number of requested simulations. A manual parallelization packet size can be set instead by setting the “Parallelization packet size” checkbox on the Simulation tab. In FACTS command-line mode, the packet size is automatically set unless the user explicitly specifies the “-p” flag.\nInformation about the FACTS license, namely its expiry date, is now available in Help &gt; About.\n[Enterprise licensees only] FACTS will automatically retry any actions involving communication with the FACTS HPC server if initial communication fails (e.g., due to an intermittent connectivity). The following FACTS infrastructure changes were performed as a part of our roadmap to modernize FACTS to make use of the latest available tech stack. Please communicate the following information to your IT team as needed:\nFACTS 6.4.0 will now target .NET Framework 4.5.2. Previous versions of FACTS target .NET Framework 4. Future versions of FACTS will target the latest available .NET Framework version.\nFACTS 6.4.0 is now a 64-bit application and will be installed on the target machine’s Program Files area. Previous versions of FACTS were 32-bit applications installed on the target machine’s Program Files (x86) area. This release addresses some situations in FACTS 6.3.0 and older versions that could cause different simulation results. If any of your designs replicate these circumstances, you are recommended to upgrade to FACTS 6.4.0:\nThe “Pause accrual and wait for completers if stopping rules are met” option on the Stopping Criteria tab of FACTS Dose Escalation N-CRM designs making use of open enrollment did not have the correct behavior when the option was unchecked. This is fixed in FACTS 6.4.0.\nThe standard deviation (SD) of the number of subjects having observed a Cat 2 Toxicity in FACTS Dose Escalation N-CRM designs was calculated incorrectly. This is fixed in FACTS 6.4.0.\nFACTS Dose Escalation N-CRM designs simulations results differed between Windows and Linux (including Windows VMs running on top of Linux) when the Toxicity response Rho parameter was non-zero. The Linux results are now consistent with the Windows results in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims triggered by complete predictor data were not respecting any specified minimum information required on the number of predictor completers before an interim can be triggered. This is fixed in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims triggered by complete predictor data an addition interim at “full predictor data” was being simulated even if not asked for. This is fixed in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims in stage 2 by time, after full accrual a circumstance can arise when follow up stops prematurely and an “inconclusive” result declared. This is fixed in FACTS 6.4.0.\nFACTS Staged TTE with a predictor endpoint and stage 1 data included in stage 2, any stage 1 subjects who had not had their predictor observed by the end of stage 1 had their predictor outcome censored rather than observed in stage 2. This is fixed in FACTS 6.4.0. Finally, there are two unique situations and areas identified in FACTS 6.4.0 (and prior versions) that will be continued developed and improved in future releases:\nIn FACTS Staged Design TTE, where the data inclusion is: “included where we have neither observed an event or the predictor and they are on an arm that is kept in stage 2” and stage 2 interim timings are based on “complete predictor data” and “stage 2 and included stage 1 data”, then FACTS is failing to include the included stage 1 subjects in calculating the timings of the interims in stage 2.\nIn FACTS Stage Design TTE where events are censoring for predictor outcomes, this censoring is not taken into account in the timing of interims by “Predictor Complete”. Resulting in the interims being too early.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.4.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts650.html",
    "href": "releaseNotes/v6/facts650.html",
    "title": "FACTS 6.5.0 Release Notes",
    "section": "",
    "text": "1 Introduction\nFACTS 6.5.0 is now available for download via App Center. Please contact us regarding any questions.\nFACTS users can now:\n\nSpecify frequentist margins (“deltas”) in the calculation of p-value and predictive probability QOIs for FACTS Core and Staged designs (except Time-to-Event designs).\nCreate designs with interims triggered based on predictor events for FACTS Core and Staged Time-to-Event designs with a Time-to-Event predictor.\nCreate designs where the final event endpoint analysis can be performed without any imputation based on the predictor endpoint for FACTS Core and Staged Time-to-Event designs with a predictor endpoint.\nObserve significant improvements in the mixing of MCMC chains within the Bayesian Augmented Control and Active Comparator (BAC and BAAC respectively) hierarchical models for FACTS Core and Staged and Enrichment designs.\nGenerate design reports for FACTS Core Multiple Endpoint designs, FACTS Enrichment designs (Continuous, Dichotomous and Time-to-Event) and FACTS N-CRM designs.\n\n\n\n2 FACTS Core and Staged Improvements\n\nIn FACTS Core and Staged designs (except Time-to-Event designs), FACTS users can now globally specify a frequentist super-superiority/non-inferiority margin on the Quantities of Interest tab under the Standard Evaluation Variables area, which will be applied to the calculation of all p-value QOIs and “Current Trial” Predictive Probability QOIs. Note that this globally defined margin does not apply to “Future Trial” Predictive Probability QOIs, which can have their own separate margin defined. In addition, users now have the option on the “Frequentist Analysis” tab to use the frequentist super-superiority/non-inferiority margin in the frequentist analysis.\nIn FACTS Core and Staged Time-to-Event designs with a Time-to-Event predictor, users can now specify designs with interims triggered based on the number of predictor events that have been observed. In addition, and independently of how interims are triggered, users can now specify maximum event caps based on either Final events or Predictor events.\nIn FACTS Core and Staged Time-to-Event designs with a predictor endpoint, users can now specify the final endpoint analysis to not depend on any imputation from the predictor endpoint. This can be achieved by selecting the “No imputation” option within the “Imputation on Predictor” panel on the Design &gt; Predictor Model &gt; Relationship to Endpoint tab.\nIn FACTS Core and Staged designs, the mixing of MCMC chains within Bayesian Augmented Control and Active Comparator (BAC and BAAC respectively) hierarchical models has been improved significantly; in particular, when the prior on tau^2 in the hierarchical model is forced to be small.\nFACTS Core Multiple Endpoint now provides the ability to generate a design report once the design has been simulated. As a result, all FACTS Core design types can now generate design reports.\nFACTS Core and Staged designs now correctly display a trial as having stopped for futility if all arms have been dropped.\nFACTS Core and Staged designs now correctly prevent interims from being performed beyond full enrollment when the “Discontinue interim analysis beyond full enrolment” setting on the Interims tab is selected.\nFACTS Staged Time-to-Event designs now correctly handle interim timings in Stage 2 for the various data inclusion rules as specified on the Data Inclusion tab, and interim information based on “Just Stage 2 data” or “Stage 2 and included Stage 1 data”, as specified on the Stage 2 Interims tab.\nFACTS Staged Time-to-Event designs now correctly handles interim timings based on complete predictor data, when a predictor is included in the design and the “Primary endpoint is censoring for intermediate predictor” setting is selected.\nFACTS Core and Staged Time-to-Event designs now correctly handle predictor based imputation when using a dichotomous predictor endpoint.\nOn the Analysis tab in FACTS Core and Staged Time-to-Event designs, current trial predictive probabilities that estimate an accrual rate no longer require input data to be sorted by accrual time.\nFACTS Core Multiple Endpoint and FACTS Staged Dichotomous designs will now correctly output p-value trend test QOIs as a single output column, rather than one output column per dose, in summary files.\nFACTS Staged Multiple Endpoint designs will now correctly display the endpoint number when outputting p-value trend test QOIs in summary files.\nFACTS (Staged) Multiple Endpoint designs will now correctly display the posterior probability QOI comparison options (“Rates” and “Log-odds”) for dichotomous endpoints. Changing the endpoint from being dichotomous to continuous will delete posterior probability QOIs using the “Log-odds” comparison.\nFACTS Core and Staged Multiple Endpoint analyses will now correctly handle endpoints with visits schedules. Namely, when an endpoint contains only one visit schedule or when an endpoint’s visit schedule involved non-consecutive visits.\nFACTS Core and Staged Multiple Endpoint analyses will now correctly handle endpoints whose visit schedule contains missing data.\n\n\n\n3 FACTS Enrichment Design Improvements\n\nFACTS Enrichment designs (Continuous, Dichotomous and Time-to-Event) now provide the ability to generate design reports once the designs have been simulated. As a result, all FACTS Enrichment design types can now generate design reports.\nThe mixing of MCMC chains within Bayesian Augmented Control (BAC) hierarchical model has been improved significantly; in particular, when the prior on tau^2 in the hierarchical model is forced to be small.\nThe clustering in Enrichment designs has been improved for situations when the prior on tau^2 is chosen to be small (e.g. 0.01 with weight 1).\nThe patients file output from simulations (patients.csv) now correctly populates the dropout state of patients, and can now be used as subject data input on the Analysis tab without requiring modification.\nMCMC Trace plots are now available for all Enrichment design types when viewing simulation results graphs and when performing analyses. To view these graphs, at least one MCMC file needs to be generated. This can be done by going to the Simulations tab &gt; MCMC Settings.\nExternal data file validation has been improved.\n\n\n\n4 FACTS Dose Escalation Improvements\n\nN-CRM now provides the ability to generate design reports once the design has been simulated.\nIn N-CRM designs which include efficacy, the “Maximum cohorts used to determine MTD” setting on the Allocation Rule tab is now observed correctly.\nIn N-CRM designs, when deriving toxicity/efficacy priors from quantiles at the lowest and highest doses, the “middle” probability option now refers to the median dose rather than the reference dose and is now optional. When pre-6.5 FACTS files are loaded in FACTS 6.5, the “middle” probability option will be ignored and replaced by 0.5.\nIn N-CRM designs, the specification of at least two dose levels is now required when deriving toxicity/efficacy priors from specific quantiles. Previously, the specification of at least three dose levels was required.\n\nFACTS Command Line mode and FLFLL (FACTS Linux File Loader Lite) now correctly handle the processing of N-CRM designs whose prior was derived using the “Legacy Prior” option.\nIn N-CRM designs using open enrollment without accrual pausing, FACTS will now output entries in the cohort file for subjects that have been lost following stopping rules being met.\nIn N-CRM designs using open enrollment, FACTS will no longer prematurely apply stopping conditions when there are still subjects whose outcomes have been observed, but not yet processed.\nIn N-CRM designs using open enrollment, FACTS will now report the selected MTD/MED/OSD and associated state to be the dose where the stopping conditions were first met, unless the MTD has subsequently decreased post stopping rules being met.\nIn N-CRM designs using open enrollment, dose escalation rules when using fine-grained dosing will behave correctly.\nIn N-CRM designs using open enrollment and two groups, stopping rules and dose escalations rules will now apply to the correct group.\n\n\n\n5 General Improvements\n\nFACTS now targets .NET Framework 4.8, the latest major version of the .NET Framework.\nA new “Simulation Duration” table can be viewed when right-clicking on a simulation design scenario. The Simulation Duration table gives a granular view of simulation start and end times, as well as its total duration.\nSeveral major improvements to FLFLL (enterprise licensees only): in particular, the ability to process specific scenarios of a design, the ability to process all FACTS files contained within a specified directory, and the reporting of design scenario validation errors. See FLFLL documentation for details.\nIn FACTS Command Line mode and FLFLL, a new flag is available to specify the number of MCMC samples to generate for imputation purposes.\nIn FACTS Command Line mode, the ability to generate a design report has been added. This can be achieved by adding the -report flag and the -rpath flag, where the latter is used to specify the path to the R executable.\nFACTS now provides links to FACTS introductory videos hosted on YouTube via the Help menu.\nSimulation engine errors in FACTS are now displayed in the GUI more informatively.\nThe remaining time left on a FACTS license is now displayed correctly on the FACTS splash screen and Help menu.\nFACTS can now output up to 99,999 patients/weeks/frequentist/MCMC files. Previously, this was capped at 9,999 files.\nAll designs supporting design report generation can have their design report generated without having to perform an additional command execution step in RStudio, by selecting a valid R installation under Settings &gt; Options &gt; R Configuration.\nFACTS will now correctly handle the serialization/deserialization of text inputs involving the following characters: “&gt;”, “&lt;”, “&”, “ ’ ” and “\\”.\nWhen viewing FACTS simulation results through the GUI, estimates of responses, effects and hazard ratios (for Time-to-Event designs) will be display more obviously in the results column headers.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.5.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v6/facts616.html",
    "href": "releaseNotes/v6/facts616.html",
    "title": "FACTS 6.1.6 Release Notes",
    "section": "",
    "text": "FACTS 6.1.6 is a maintenance release for FACTS 6.1.0. The first product release of FACTS 6.1.0 was FACTS 6.1.3. Subsequent releases have introduced the following changes:\n\nFACTS 6.1.4: This was FACTS 6.1.3 with some additional logging when using the grid interface.\nFACTS 6.1.5: This was FACTS 6.1.4 with the Dose Escalation N-CRM re-compiled to allow a higher number (40) of dose strengths to be defined when using “Explicit Doses” rather than finely spaced doses.\nFACTS 6.1.6: This was FACTS 6.1.5 with 2 problems fixed in FACTS Core with a TTE endpoint & FACTS Staged Design with a TTE endpoint. In either engine the calculation of a “Current Trial Predictive Probability of Success at Current Enrollment” had 2 problems:\n\nThere was an error in the way timings of future events were simulated in the calculation of the predictive probability. The result was approximately correct, and erred on the conservative side, the error is more manifest if the trial has long follow-up times.\nThere was an error if the design also includes a predictor endpoint. This effects a much smaller set of designs, but the effect was much more marked and its impact was difficult to characterize in general. Our current advice is to not use predictive probability QOIs in combination with a “Predictor” endpoint using FACTS prior to FACTS 6.1.6.\n\n\nUpgrading FACTS 6.1.6 should introduce no changes to the simulation or analysis results relative to FACTS 6.1.3 except in designs using a time-to-event endpoint and a “Predictive Probability of Success in the Current Trial at Current Enrollment” Quantity of Interest.",
    "crumbs": [
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.1.6 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v7/facts700.html",
    "href": "releaseNotes/v7/facts700.html",
    "title": "FACTS 7.0.0 Release Notes",
    "section": "",
    "text": "1 Introduction\nFACTS 7.0.0 is now available for download via App Center. This release marks the addition of two new FACTS design types: Platform Trial Design – Continuous and Platform Trial Design – Dichotomous.\nPlease contact us regarding any questions.\n\n\n2 FACTS Platform Trial Features\nWithin these new Platform Trial design types, FACTS users can now:\n\nSimulate a platform trial, for a continuous/dichotomous endpoint, with various trial level participant and arm constraints. In particular, users can specify a maximum enrollment time, number of participants, successful treatments, participants per arm and concurrent treatments.\nSimulate a platform trial with treatments arriving at different times during the trial.\nSpecify simulated mean arm responses/effects to be a fixed value or sampled from a distribution.\nSimulate participant accrual, responses, and dropout rates as per FACTS Core.\nSpecify a constant proportion of participants allocated to the control arm, or an allocation dependent on the number of treatments currently in the trial, with the option of performing response adaptive randomization.\nAnalyze participant data and estimate mean treatment responses using a Bayesian independent arm model, or frequentist p-values, comparing treatment arms to a common control arm.\nSpecify “Trial Update” information and frequency, at which analyses are performed and allocation ratios may get updated.\nSpecify when to evaluate “Treatment Milestones”, at which decisions are made about treatment outcomes.\nSpecify success/futility criteria that apply to all treatments, or to specific treatments.\nClassify treatments as Good, Mediocre or Unacceptable to get summary statistics such as the proportion of ‘Good’ treatments that are successful/inconclusive/unsuccessful and similarly for the other classifications.\nView granular simulation and summary results of various Platform Trial operating characteristics.\nGenerate a Platform Trial design report outlining the characteristics of the simulated design in a Word document.\n\n\n\n3 FACTS Core and Staged Improvements\n\nIn FACTS Core and Staged designs (except Time-to-Event designs), users can now simulate single arm trials, with options for both Bayesian and Frequentist p-values to be calculated comparing the data on the experimental arm to an objective reference response/response rate specified on the QOI tab.\nFACTS Core and Staged designs (except Time-to-Event designs) will now correctly handle frequentist calculations when a control arm is not present and comparison is performed against an objective reference response/response rate.\np-value calculations have been updated to better accommodate their use at interims, with dropouts and incomplete subjects now handled differently. No incomplete subjects have a final endpoint imputed, but subjects that are known dropouts and have had the opportunity to complete are imputed/ignored according to the “Handle missingness” option for the p-value.\nLOCF behavior has been made consistent. LOCF will impute a participant’s baseline value as their final outcome if a baseline value is observed and no non-baseline visit data is observed.\nFACTS Staged designs will now correctly handle the mirroring of Stage 1 data in Stage 2 for the Dose Response and Longitudinal models.\n\n\n\n4 FACTS Enrichment Design Improvements\n\nIn FACTS Enrichment Time-to-Event designs, FACTS will now correctly apply the user specified alpha levels per group when calculating frequentist output summaries.\nIn FACTS Enrichment Time-to-Event designs, FACTS will now correctly calculate frequentist output when the underlying design has only specified one group.\nIn FACTS Enrichment designs, FACTS will now enforce group caps to be strictly greater than zero.\n\n\n\n5 FACTS Dose Escalation Improvements\n\nOn the Analysis tab, FACTS will now enforce the specification of the cohort number when uploading a subject data file to run an analysis.\nIn 2D-CRM, FACTS will now correctly handle a rare situation in the row-by-row run-in scheme.\nIn 2D-CRM, the engines when run in a Linux environment will have a correctly formatted simulation results output header.\n\n\n\n6 General Improvements\n\nBREAKING CHANGE: FACTS will now consistently handle the “Date” column in a patients file to be in weeks rather than days, making it consistent with the rest of FACTS. “Patients” files generated from FACTS simulations will report the “Date” column as “DateInWeeks” to avoid any ambiguity.\nBREAKING CHANGE: The “Date” column in Deterministic Accrual external data files will need to be manually updated to specify the date in weeks rather than days.\nBREAKING CHANGE: The “Date” column in subject data file provided when running a FACTS Analysis will need to be updated to specify the date in weeks rather than days. If performing FACTS Analysis via the GUI, the FACTS Analysis tab provides a “Convert Date from Days to Weeks” utility that does the conversion.\nThe precision of results output in FACTS will now consistently be up to 6 decimal places for all design types, except for Time-to-Event designs which will display output up to 8 decimal places.\nFACTS will now correctly handle interactions with the latest version of RStudio to date (2023.03.0). This includes the generation of design reports and the importing of FACTS results output to RStudio via the “Open in R” button on the Simulation tab. Note that FACTS will continue to support older version of RStudio.",
    "crumbs": [
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.0.0 Release Notes"
    ]
  },
  {
    "objectID": "releaseNotes/v7/facts711.html",
    "href": "releaseNotes/v7/facts711.html",
    "title": "FACTS 7.1.1 Release Notes",
    "section": "",
    "text": "1 Introduction\n\n\n2 FACTS Core and Staged Improvements\n\nIn Staged design, conditional power of current stage 2 when no control arm is carried to stage 2 is handled correctly."
  }
]