[
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html",
    "href": "concepts/facts/DropoutsDeepdive.html",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "For the continuous and dichotomous engines, and the multiple endpoint engine, the default dropout scenario is that no subjects drop out of the study before observing their final endpoint data. If dropouts are expected, the user can specify either the “Dropouts per Dose,” or “Dropouts per Dose per Visit.”\n\n\nIf “Dropouts per Dose” is selected, then each subject has a probability of not having an observable final endpoint value equal to the dropout rate of the dose that subject is randomized to. If each subject has multiple visits and “Dropouts per Dose” is selected, then the conditional probability of dropping out before each visit given that the subject had not dropped out up to the visit before rates are all equal. In other words, if the total dropout rate is π_D, the probability of dropping out between visits i and i+1 given that the subject had not dropped out at visit \\(i\\) is \\[\n1-\\left(1-\\pi_D\\right)^{(\\frac{1}{V})}\\text{ where } V \\text{ is the total number of visits.}\n\\] The following interactive widget takes in the desired dropout rate for a single dose and the number of visits a subject will have during their follow-up, and returns information about the dropout rate that FACTS will simulate by visit for that dose.\nThe columns in the dynamic output table are:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"Dropout Per Dose\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"dropoutRate\", \"Desired Dropout Rate:\", value = 0.1, min = 0.0, max = 1.0, step = 0.05),\n      sliderInput(\"numVisits\", \"Number of visits:\", \n                  min = 1, max = 20, value = 5)\n    ),\n    mainPanel(\n      tableOutput(\"visitsGuide\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  output$visitsGuide &lt;- renderTable({\n    \n    if(is.na(input$dropoutRate)) {stop(safeError(\"Please enter a dropout rate.\"))}\n    else if(input$dropoutRate &lt; 0 | input$dropoutRate &gt; 1) {stop(safeError(\"Dropout rate must be between 0 and 1.\"))}\n    else {\n      df = data.frame(Visits = 1:input$numVisits,\n                                                     ConditionalDropoutRate = 1-(1-input$dropoutRate)^(1/input$numVisits))\n      df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n      df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  \n      df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n      colnames(df) = c(\"Visit number\",\n                \"Conditional Visit Dropout Rate\",\n                \"Marginal Visit Dropout Rate\",\n                \"Cumulative Dropout Prob.\")\n    df}\n  }, digits = 4)\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\nIf “Dropouts per Dose per Visit” is selected, then each subject has a user specified probability of dropping out before a visit v that is specified as the conditional probability of dropping out before visit v given that that they had not dropped out by visit v-1. This leads to a total dropout rate π_D for a participant that is equal to:\n\\[\n\\pi_D = 1-\\Pi_{v=0}^V(1-\\pi_v)\n\\] When specifying Dropouts Per Dose Per Visit in FACTS, the user inputs the conditional visit dropout rate for each visit. This can make it not completely intuitive to get the exact desired cumulative dropout rate. The below application allows the user to enter either the conditional visit dropout rates or the marginal visit dropout rates for a single dose, and provides the implied dropout profile from those inputs. Again, the columns of the table represent:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(htmltools)\n\nalignCenter &lt;- function(el) {\n  htmltools::tagAppendAttributes(el,\n                                 style=\"margin-left:auto;margin-right:auto;\"\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(\n    # Note the wrapping of the string in HTML()\n    tags$style(HTML(\"\n      .my_col_class {\n         align-content: center;\n      }\")\n    )\n  ),\n  \n  fluidRow(\n    titlePanel(h1(\"Dropout Per Dose Per Visit\", align = \"center\")),\n    alignCenter(sliderInput(\"numVisits\", \"Number of visits:\", \n            min = 1, max = 20, value = 5)),\n    DTOutput(\"dataInputTable\")\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  df = data.frame(Visits = 1:5,\n                       ConditionalDropoutRate = 0.05)\n  df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n  df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n  colnames(df) = c(\"Visit number\",\n                   \"Conditional Visit Dropout Rate\",\n                   \"Marginal Visit Dropout Rate\",\n                   \"Cumulative Dropout Prob.\")\n  \n  ## Render DF to actually change\n  output$dataInputTable = renderDT(datatable(df, options = list(\n    pageLength = 20,\n    dom = \"t\",\n    autoWidth = TRUE,\n    columnDefs = list(list(width = '150px', targets = 1:4, orderable = FALSE))), selection = 'none', editable = \"cell\") |&gt; formatRound(2:4, digits = 4))\n  \n  ## Update from Conditional\n  \n  proxy = dataTableProxy('dataInputTable')\n  \n  observeEvent(input$dataInputTable_cell_edit, {\n    info = input$dataInputTable_cell_edit\n    str(info)\n    i = info$row\n    j = info$col\n    v = info$value\n    df &lt;&lt;- editData(df, info)\n    if(j == 2) {\n      ## Update other columns from Conditional\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    } else if(j == 3) {\n      ## Update other columns from Marginal\n      df[,4] &lt;&lt;- cumsum(df[,3])\n      df[,2] &lt;&lt;- c(df[1,3],\n                   df[-1,3]/(1-df[-nrow(df),4]))\n    } else if(j == 4) {\n      ## Update other columns from Cumulative\n      \n      df[info$row, 3] &lt;&lt;- df[info$row, 4] - ifelse(info$row == 1, 0, df[info$row-1, 4])\n      df[info$row, 2] &lt;&lt;- ifelse(info$row == 1, df[info$row,3], df[info$row,3]/(1-df[info$row-1,4]))\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    }\n    replaceData(proxy, df) \n  })\n  \n  observe({\n    input$numVisits\n    nv = input$numVisits\n    if(nv &gt; nrow(df)) {\n      if(nrow(df) == 0) {\n        df &lt;&lt;- data.frame(Visits = 1,\n                   ConditionalDropoutRate = 1-(1-.1)^(1))\n        df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n        df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n        df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n        colnames(df) = c(\"Visit number\",\n                         \"Conditional Visit Dropout Rate\",\n                         \"Marginal Visit Dropout Rate\",\n                         \"Cumulative Dropout Prob.\")\n      } else {\n        for(i in 1:(nv-nrow(df))) {\n          df &lt;&lt;- rbind(df, setNames(data.frame(matrix(c(1+nrow(df), 0.05, 0, 0), nrow = 1)), names(df)))\n        }\n        df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n        df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n      }\n    } else if(nv &lt; nrow(df)) {\n      df &lt;&lt;- df[1:nv,]\n    }\n    replaceData(proxy, df) \n  })\n}\n\nshinyApp(ui = ui, server = server)\nIn order to simulate dropouts from the below profile in FACTS, enter the Conditional Visit Dropout Rate into the FACTS Dropout Rate tab.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose",
    "href": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "If “Dropouts per Dose” is selected, then each subject has a probability of not having an observable final endpoint value equal to the dropout rate of the dose that subject is randomized to. If each subject has multiple visits and “Dropouts per Dose” is selected, then the conditional probability of dropping out before each visit given that the subject had not dropped out up to the visit before rates are all equal. In other words, if the total dropout rate is π_D, the probability of dropping out between visits i and i+1 given that the subject had not dropped out at visit \\(i\\) is \\[\n1-\\left(1-\\pi_D\\right)^{(\\frac{1}{V})}\\text{ where } V \\text{ is the total number of visits.}\n\\] The following interactive widget takes in the desired dropout rate for a single dose and the number of visits a subject will have during their follow-up, and returns information about the dropout rate that FACTS will simulate by visit for that dose.\nThe columns in the dynamic output table are:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"Dropout Per Dose\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"dropoutRate\", \"Desired Dropout Rate:\", value = 0.1, min = 0.0, max = 1.0, step = 0.05),\n      sliderInput(\"numVisits\", \"Number of visits:\", \n                  min = 1, max = 20, value = 5)\n    ),\n    mainPanel(\n      tableOutput(\"visitsGuide\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  output$visitsGuide &lt;- renderTable({\n    \n    if(is.na(input$dropoutRate)) {stop(safeError(\"Please enter a dropout rate.\"))}\n    else if(input$dropoutRate &lt; 0 | input$dropoutRate &gt; 1) {stop(safeError(\"Dropout rate must be between 0 and 1.\"))}\n    else {\n      df = data.frame(Visits = 1:input$numVisits,\n                                                     ConditionalDropoutRate = 1-(1-input$dropoutRate)^(1/input$numVisits))\n      df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n      df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  \n      df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n      colnames(df) = c(\"Visit number\",\n                \"Conditional Visit Dropout Rate\",\n                \"Marginal Visit Dropout Rate\",\n                \"Cumulative Dropout Prob.\")\n    df}\n  }, digits = 4)\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose-per-visit",
    "href": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose-per-visit",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "If “Dropouts per Dose per Visit” is selected, then each subject has a user specified probability of dropping out before a visit v that is specified as the conditional probability of dropping out before visit v given that that they had not dropped out by visit v-1. This leads to a total dropout rate π_D for a participant that is equal to:\n\\[\n\\pi_D = 1-\\Pi_{v=0}^V(1-\\pi_v)\n\\] When specifying Dropouts Per Dose Per Visit in FACTS, the user inputs the conditional visit dropout rate for each visit. This can make it not completely intuitive to get the exact desired cumulative dropout rate. The below application allows the user to enter either the conditional visit dropout rates or the marginal visit dropout rates for a single dose, and provides the implied dropout profile from those inputs. Again, the columns of the table represent:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(htmltools)\n\nalignCenter &lt;- function(el) {\n  htmltools::tagAppendAttributes(el,\n                                 style=\"margin-left:auto;margin-right:auto;\"\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(\n    # Note the wrapping of the string in HTML()\n    tags$style(HTML(\"\n      .my_col_class {\n         align-content: center;\n      }\")\n    )\n  ),\n  \n  fluidRow(\n    titlePanel(h1(\"Dropout Per Dose Per Visit\", align = \"center\")),\n    alignCenter(sliderInput(\"numVisits\", \"Number of visits:\", \n            min = 1, max = 20, value = 5)),\n    DTOutput(\"dataInputTable\")\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  df = data.frame(Visits = 1:5,\n                       ConditionalDropoutRate = 0.05)\n  df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n  df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n  colnames(df) = c(\"Visit number\",\n                   \"Conditional Visit Dropout Rate\",\n                   \"Marginal Visit Dropout Rate\",\n                   \"Cumulative Dropout Prob.\")\n  \n  ## Render DF to actually change\n  output$dataInputTable = renderDT(datatable(df, options = list(\n    pageLength = 20,\n    dom = \"t\",\n    autoWidth = TRUE,\n    columnDefs = list(list(width = '150px', targets = 1:4, orderable = FALSE))), selection = 'none', editable = \"cell\") |&gt; formatRound(2:4, digits = 4))\n  \n  ## Update from Conditional\n  \n  proxy = dataTableProxy('dataInputTable')\n  \n  observeEvent(input$dataInputTable_cell_edit, {\n    info = input$dataInputTable_cell_edit\n    str(info)\n    i = info$row\n    j = info$col\n    v = info$value\n    df &lt;&lt;- editData(df, info)\n    if(j == 2) {\n      ## Update other columns from Conditional\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    } else if(j == 3) {\n      ## Update other columns from Marginal\n      df[,4] &lt;&lt;- cumsum(df[,3])\n      df[,2] &lt;&lt;- c(df[1,3],\n                   df[-1,3]/(1-df[-nrow(df),4]))\n    } else if(j == 4) {\n      ## Update other columns from Cumulative\n      \n      df[info$row, 3] &lt;&lt;- df[info$row, 4] - ifelse(info$row == 1, 0, df[info$row-1, 4])\n      df[info$row, 2] &lt;&lt;- ifelse(info$row == 1, df[info$row,3], df[info$row,3]/(1-df[info$row-1,4]))\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    }\n    replaceData(proxy, df) \n  })\n  \n  observe({\n    input$numVisits\n    nv = input$numVisits\n    if(nv &gt; nrow(df)) {\n      if(nrow(df) == 0) {\n        df &lt;&lt;- data.frame(Visits = 1,\n                   ConditionalDropoutRate = 1-(1-.1)^(1))\n        df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n        df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n        df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n        colnames(df) = c(\"Visit number\",\n                         \"Conditional Visit Dropout Rate\",\n                         \"Marginal Visit Dropout Rate\",\n                         \"Cumulative Dropout Prob.\")\n      } else {\n        for(i in 1:(nv-nrow(df))) {\n          df &lt;&lt;- rbind(df, setNames(data.frame(matrix(c(1+nrow(df), 0.05, 0, 0), nrow = 1)), names(df)))\n        }\n        df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n        df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n      }\n    } else if(nv &lt; nrow(df)) {\n      df &lt;&lt;- df[1:nv,]\n    }\n    replaceData(proxy, df) \n  })\n}\n\nshinyApp(ui = ui, server = server)\nIn order to simulate dropouts from the below profile in FACTS, enter the Conditional Visit Dropout Rate into the FACTS Dropout Rate tab.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/InverseGammaDistribution.html",
    "href": "concepts/facts/InverseGammaDistribution.html",
    "title": "Inverse Gamma Distribution in FACTS",
    "section": "",
    "text": "The Inverse Gamma distribution is used as a prior for most variances in FACTS. The standard parameterization of the Inverse Gamma distribution using \\(\\alpha\\) and \\(\\beta\\) as the shape and scale parameter is not always intuitive for specifying a prior. In order to assist with prior specification, FACTS reparameterizes the Inverse Gamma distribution to be a function of the expected center of the standard deviation and a prior weight.\nThe application below is intended to help users of FACTS understand what the distribution they are specifying for the prior of a variance actually looks like.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\n\nqinvgamma = function (p, shape, rate = 1, scale = 1/rate, lower.tail = TRUE, \n                      log.p = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  qgamma(1 - p, shape, rate, lower.tail = lower.tail, log.p = log.p)^(-1)\n}\npinvgamma = function (q, shape, rate = 1, scale = 1/rate, lower.tail = TRUE, \n                      log.p = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  pgamma(1/q, shape, rate, lower.tail = !lower.tail, log.p = log.p)\n}\ndinvgamma = function (x, shape, rate = 1, scale = 1/rate, log = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  log_f &lt;- dgamma(1/x, shape, rate, log = TRUE) - 2 * log(x)\n  if (log) \n    return(log_f)\n  exp(log_f)\n}\nrinvgamma = function (n, shape, rate = 1, scale = 1/rate) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  1/rgamma(n, shape, rate)\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  tags$head(\n    tags$style(HTML(\"\n      #radioButtonDiv {\n      display: flex;\n      justify-content: center;\n      }\"\n    ))\n  ),\n  withMathJax(),\n  titlePanel(h1(\"Inverse Gamma Distribution in FACTS\", align = \"center\")),\n  h5('$$\\\\sigma^2 \\\\sim \\\\text{IG}\\\\left(\\\\alpha=\\\\frac{\\\\text{weight}}{2}, \\\\beta=\\\\frac{\\\\text{center}^2\\\\;*\\\\;\\\\text{weight}}{2}\\\\right)$$'),\n  sidebarLayout(\n    sidebarPanel(width = 3,\n                 style = \"border: 1px solid #000000\",\n                 titlePanel(h4(\"Center/Weight Parameterization\")),\n                 fluidRow(\n                   column(width = 10, offset = 1, numericInput(inputId = \"center\", label = \"Center of SD:\", value = 5, min = 0, max = Inf, step = \"any\")),\n                   column(width = 10, offset = 1, numericInput(inputId = \"weight\", label = \"Weight:\", value = 2, min = 0.001, max = Inf, step = \"any\")),\n                 ),\n                 titlePanel(h4(\"Alpha/Beta Parameterization\")),\n                 fluidRow(\n                   column(width = 10, offset = 1, numericInput(inputId = \"alpha\", label = \"Alpha:\", value = 1, min = 0.0005, max = Inf, step = \"any\")),\n                   column(width = 10, offset = 1, numericInput(inputId = \"beta\", label = \"Beta:\", value = 25, min = 0, max = Inf, step = \"any\"))\n                 )\n    ),\n    mainPanel(width = 9,\n              wellPanel(style = \"background: white; border: 1px solid #000000\",\n                        fluidRow(\n                          column(12,\n                                 div(\n                                   radioButtons(\"whichParam\", \n                                                \"Which parameter should be summarized?\", \n                                                choiceNames = c(\"Variance \\\\((\\\\sigma^2)\\\\)\", \"Std. Dev. \\\\((\\\\sigma)\\\\)\"), \n                                                choiceValues = c(\"sigma2\", \"sigma\"), \n                                                selected = \"sigma2\", \n                                                inline = TRUE),\n                                   id = \"radioButtonDiv\")\n                          )\n                        ),\n                        uiOutput(\"sectionTitle\"),\n                        fluidRow(\n                          column(width = 4, value_box(\"Mode\", value= uiOutput(\"mode\"), theme = value_box_theme(bg = \"#0b2545\"))),\n                          column(width = 4, value_box(\"Median\", value= uiOutput(\"median\"), theme = value_box_theme(bg = \"#ba5a31\"))),\n                          column(width = 4, value_box(\"Mean\", value= uiOutput(\"mean\"), theme = value_box_theme(bg = \"#06402b\")))\n                        ),\n                        br(),\n                        plotOutput(\"igDistributionPlot\")\n              )\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output, session) {\n  \n  update &lt;- reactiveVal(TRUE)\n  \n  observeEvent(input$center | input$weight, {\n    cat(\"CenterWeightChanged\\n\")\n    ctr = input$center\n    wgt = input$weight\n    \n    if(update() & !is.null(ctr) & !is.null(wgt) & !is.na(ctr) & !is.na(wgt) & ctr &gt; 0 & wgt &gt; 0) {\n      a = wgt/2\n      b = ctr^2*wgt/2\n      \n      updateNumericInput(session, \"alpha\", value = a)\n      updateNumericInput(session, \"beta\", value = b)\n      \n      update(FALSE)\n    } else {\n      update(TRUE)\n    }\n  })\n  observeEvent(input$alpha | input$beta, {\n    cat(\"AlphaBetaChanged\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(update() & !is.null(a) & !is.null(b) & !is.na(a) & !is.na(b) & a &gt; 0 & b &gt; 0) {\n      wgt = 2*a\n      ctr = sqrt(b/a)\n      \n      updateNumericInput(session, \"center\", value = ctr)\n      updateNumericInput(session, \"weight\", value = wgt)\n      \n      update(FALSE)\n    } else {\n      update(TRUE)\n    }\n  })\n  \n  meanHolder = reactiveVal(NA)\n  \n  output$mean = renderText({\n    cat(\"CalcMean\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(a &gt; 1) {\n      if(input$whichParam == \"sigma2\") {\n        meanHolder(b/(a-1))\n        return(round(b/(a-1), 2))\n      } else {\n        tmp = mean(sqrt(rinvgamma(10000, a, b)))\n        meanHolder(tmp)\n        return(round(tmp, 2))\n      }\n    } else {\n      meanHolder(NA)\n      return(\"-\")\n    }\n  })\n  output$median = renderText({\n    cat(\"CalcMedian\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(input$whichParam == \"sigma2\") {\n      return(round(qinvgamma(0.5, shape = a, rate = b), 2))\n    } else {\n      return(round(sqrt(qinvgamma(0.5, shape = a, rate = b)),2))\n    }\n  })\n  \n  output$mode = renderText({\n    cat(\"CalcMode\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(input$whichParam == \"sigma2\") {\n      return(round(b/(a+1),2))\n    } else {\n      lmode = 0\n      hmode = max(b/(a+1), 2)\n      smode = seq(lmode, hmode, length.out = 100001)\n      dmode = dinvgamma(smode, a, b)*(1/(2*sqrt(smode)))\n      calcMode = sqrt(smode[which.max(dmode)])\n      \n      return(round(calcMode, 2))\n    }\n  })\n  \n  output$sectionTitle = renderUI({\n    cat(\"ChangeHeader\\n\")\n    if(input$whichParam == \"sigma2\") {\n      return(h4(\"Characteristics of the Variance\"))\n    } else {\n      return(h4(\"Characteristics of the Standard Deviation\"))\n    }\n  })\n  \n  output$igDistributionPlot = renderPlot({\n    cat(\"MakePlot\\n\")\n    a = input$alpha\n    b = input$beta\n    wchParam = input$whichParam\n    isolate({\n      if(!is.null(a) & !is.null(b) & !is.null(input$center) & !is.null(input$weight) &\n         a &gt; 0 & b &gt; 0 & input$center &gt; 0 & input$weight &gt; 0) {\n        if(wchParam == \"sigma2\") {\n          lowerbound = 0\n          upperbound = qinvgamma(.995, a, b)\n          upperbound = min(1e15, upperbound)\n          \n          sq0to1 = seq(1e-10, 1-(1e-10), length.out = 1000)\n          sq = qinvgamma(sq0to1, a, b)\n          #  sq = seq(1e-10,upperbound, length.out = 1001)\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b))\n          \n          modeDensity = dinvgamma(b/(a+1), a, b)\n          \n          if(df$density[sum(is.finite(df$density))] &gt; modeDensity*.01) {\n            maxPlot = sq[length(sq)]\n          } else {\n            maxPlot = sq[min(which(df$density &lt; modeDensity*.01 & sq &gt; sq[which.max(df$density)]))]\n          }\n          maxPlot = max(maxPlot, 2*(b/(a+1)))\n          \n          sq0to1 = c(10^seq(-17, -2, length.out = 51), seq(.0101, pinvgamma(maxPlot*1.1, a, b), length.out = 1001))\n          sq = qinvgamma(sq0to1, a, b)\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b))\n          \n          p1 = ggplot(data = df) + geom_line(aes(x = sq, y = density)) + geom_area(aes(x = sq, y = density), fill = \"gray80\") +\n            theme_bw() +\n            xlab(\"Variance\") + ylab(\"Density\") +\n            scale_y_continuous(expand = c(0, 0)) +\n            ggtitle(\"Probability Distribution of the Variance\") +\n            coord_cartesian(xlim = c(0, maxPlot), ylim = c(0, modeDensity*1.3)) +\n            theme(text = element_text(size = 18))\n          \n          if(a &gt; 1) {\n            meantmp = meanHolder()\n            p1 = p1 + geom_vline(xintercept = meantmp, color = \"#06402b\") + annotate(geom=\"label\", color = \"white\", fill = \"#06402b\", x = meantmp, y = modeDensity*1.15, label = \"mean\", angle=90, size = 4.5)\n          }\n          p1 = p1 + geom_vline(xintercept = b/(a+1), color = \"#0b2545\") + annotate(geom=\"label\", color = \"white\", fill = \"#0b2545\", x = b/(a+1), y = modeDensity*1.15, label = \"mode\", angle=90, size = 4.5)\n          \n          if(is.finite(qinvgamma(.5, a, b))) {\n            p1 = p1 + geom_vline(xintercept = qinvgamma(.5, a, b), color = \"#ba5a31\") + annotate(geom=\"label\", color = \"white\", fill = \"#ba5a31\", x = qinvgamma(.5, a, b), y = modeDensity*1.15, label = \"median\", angle=90, size = 4.5)\n          }\n          \n          p1 = p1 + geom_vline(xintercept = b/a) + annotate(geom=\"label\", x = b/a, y = modeDensity*1.15, label = \"center\", angle=90, size = 4.5)\n          \n          p1\n        }\n        else {\n          #df = data.frame(sq = sq,\n          #density = dinvgamma(sq, a, b)*(1/(2*sqrt(sq))))\n          \n          lowerbound = 0\n          upperbound = qinvgamma(.995, a, b)\n          upperbound = min(1e15, upperbound)\n          \n          sq0to1 = seq(1e-10, 1-(1e-10), length.out = 1000)\n          sq = qinvgamma(sq0to1, a, b)\n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b)*(2*sqrt(sq)))#(1/(2*sqrt(sq))))\n          \n          lmode = 0\n          hmode = max(b/(a), 2)\n          smode = seq(lmode, hmode, length.out = 1001)\n          dmode = dinvgamma(smode, a, b)*(2*sqrt(smode))#(1/(2*sqrt(smode)))\n          wchMax = which.max(dmode)\n          if(wchMax &gt; 1) {\n            smode2 = seq(smode[wchMax-1], smode[wchMax + 1], length.out = 1001)\n            dmode2 = dinvgamma(smode2, a, b)*(2*sqrt(smode2))#(1/(2*sqrt(smode2)))\n            wchMax = which.max(dmode2)\n          } else {\n            smode2 = seq(0, smode[2], length.out = 1000)\n            dmode2 = dinvgamma(smode2, a, b)*(2*sqrt(smode2))#(1/(2*sqrt(smode2)))\n            wchMax = which.max(dmode2)\n          }\n          calcMode = sqrt(smode2[wchMax])\n          \n          modeDensity = dinvgamma(calcMode^2, a, b)*(2*calcMode)#(1/(2*calcMode))\n          \n          if(df$density[sum(is.finite(df$density))] &gt; modeDensity*.01) {\n            maxPlot = sq[length(sq)]\n          } else {\n            maxPlot = sq[min(which(df$density &lt; modeDensity*.01 & sq &gt; sq[which.max(df$density)]))]\n          }\n          maxPlot = max(maxPlot, 2*smode2[wchMax])\n          \n          sq = (seq(1e-10,sqrt(maxPlot*1.1), length.out = 1001))^2\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b)*(2*sqrt(sq)))#(1/(2*sqrt(sq))))\n          \n          p1 = ggplot(data = df) + geom_line(aes(x = sqrt(sq), y = density)) + geom_area(aes(x = sqrt(sq), y = density), fill = \"gray80\") +\n            theme_bw() +\n            xlab(\"Standard Deviation\") + ylab(\"Density\") +\n            scale_y_continuous(expand = c(0, 0)) +\n            ggtitle(\"Probability Distribution of the Standard Deviation\") +\n            coord_cartesian(xlim = c(0, sqrt(maxPlot)), ylim = c(0, modeDensity*1.3)) +\n            theme(text = element_text(size = 18))\n          \n          if(a &gt; 1) {\n            meantmp = meanHolder()\n            p1 = p1 + geom_vline(xintercept = meantmp, color = \"#06402b\") + annotate(geom=\"label\", color = \"white\", fill = \"#06402b\", x = meantmp, y = modeDensity*1.15, label = \"mean\", angle=90, size = 4.5)\n          }\n          \n          p1 = p1 + geom_vline(xintercept = calcMode, color = \"#0b2545\") + annotate(geom=\"label\", color = \"white\", fill = \"#0b2545\", x = calcMode, y = modeDensity*1.15, label = \"mode\", angle=90, size = 4.5)\n          if(is.finite(sqrt(qinvgamma(.5, a, b)))) {\n            p1 = p1 + geom_vline(xintercept = sqrt(qinvgamma(.5, a, b)), color = \"#ba5a31\") + annotate(geom=\"label\", color = \"white\", fill = \"#ba5a31\", x = sqrt(qinvgamma(.5, a, b)), y = modeDensity*1.15, label = \"median\", angle=90, size = 4.5)\n          }\n          p1 = p1 + geom_vline(xintercept = sqrt(b/a)) + annotate(geom=\"label\", x = sqrt(b/a), y = modeDensity*1.15, label = \"center\", angle=90, size = 4.5)\n          \n          p1\n        }\n      }\n    })\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Inverse Gamma Distribution in FACTS"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/index.html",
    "href": "concepts/adaptiveDesigns/index.html",
    "title": "Adaptive Trials",
    "section": "",
    "text": "To understand adaptive trials it may be easiest to look at a fixed trial. Traditionally a fixed sample size is specified with fixed allocation and fixed entry criteria. The data is not analyzed until trial completion, which is typically years after the trial began. This 75-year-old design approach creates very restrictive design types – and really forces the design team to guess at the doses, range, patient population, duration and frequency of treatment, etc, because a fixed design requires linear effort spent for each treatment arm.\nAn adaptive trial is like driving with your eyes open! It allows the pre-specification of flexible components to the major aspects of the trial, like the treatment arms used (dose, frequency, duration, combinations, etc), the allocation to the different treatment arms, the patient population used, and the sample size. An adaptive design can learn from the accruing data what the most therapeutic doses or arms are, allowing the design to hone in on the best arms. This allows the design to start with a wider range of doses – say 8 instead of 3 – with using a smaller number of patients. The result is a smart design, using resources (including time) much more efficiently, at the same time increasing the scientific precision. Well constructed adaptive designs can be better for all involved – better learning, more efficient, better treatment for subject in a trial, better information for regulators and the medical community.\nThe one drawback is that adaptive designs are more work to construct. It involves clinical trial simulation to make sure the design works well, meets regulatory scrutiny and is efficient. Creating an adaptive design means getting all parties involved in the process – having statisticians work with clinicians, marketing, regulatory experts, execution teams, drug supply, etc, to construct a highly efficient design.\nAdaptive Designs are not restricted to phase I, but rather all stages of development of drugs and devices. The following list of phases/stages is where we have constructed adaptive designs:\nPhase I: Sample size, Dose escalation, Combination of arms, Seamless phase I-II.\nPhase II/Pilot: Sample size, Dose allocation, Introduce/Drop arms, Histology investigation, Prediction of Phase III, Seamless Phase II-III.\nPhase III/Confirmatory: Sample size, Multiple arms, Accrual Interim Analysis, Futility Analyses, Timing of Conclusions.\nPhase IV: Sample size, Timing of Conclusions, Indications",
    "crumbs": [
      "Concepts",
      "Adaptive Trials"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/index.html#what-is-an-adaptive-trial",
    "href": "concepts/adaptiveDesigns/index.html#what-is-an-adaptive-trial",
    "title": "Adaptive Trials",
    "section": "",
    "text": "To understand adaptive trials it may be easiest to look at a fixed trial. Traditionally a fixed sample size is specified with fixed allocation and fixed entry criteria. The data is not analyzed until trial completion, which is typically years after the trial began. This 75-year-old design approach creates very restrictive design types – and really forces the design team to guess at the doses, range, patient population, duration and frequency of treatment, etc, because a fixed design requires linear effort spent for each treatment arm.\nAn adaptive trial is like driving with your eyes open! It allows the pre-specification of flexible components to the major aspects of the trial, like the treatment arms used (dose, frequency, duration, combinations, etc), the allocation to the different treatment arms, the patient population used, and the sample size. An adaptive design can learn from the accruing data what the most therapeutic doses or arms are, allowing the design to hone in on the best arms. This allows the design to start with a wider range of doses – say 8 instead of 3 – with using a smaller number of patients. The result is a smart design, using resources (including time) much more efficiently, at the same time increasing the scientific precision. Well constructed adaptive designs can be better for all involved – better learning, more efficient, better treatment for subject in a trial, better information for regulators and the medical community.\nThe one drawback is that adaptive designs are more work to construct. It involves clinical trial simulation to make sure the design works well, meets regulatory scrutiny and is efficient. Creating an adaptive design means getting all parties involved in the process – having statisticians work with clinicians, marketing, regulatory experts, execution teams, drug supply, etc, to construct a highly efficient design.\nAdaptive Designs are not restricted to phase I, but rather all stages of development of drugs and devices. The following list of phases/stages is where we have constructed adaptive designs:\nPhase I: Sample size, Dose escalation, Combination of arms, Seamless phase I-II.\nPhase II/Pilot: Sample size, Dose allocation, Introduce/Drop arms, Histology investigation, Prediction of Phase III, Seamless Phase II-III.\nPhase III/Confirmatory: Sample size, Multiple arms, Accrual Interim Analysis, Futility Analyses, Timing of Conclusions.\nPhase IV: Sample size, Timing of Conclusions, Indications",
    "crumbs": [
      "Concepts",
      "Adaptive Trials"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/platformTrials.html",
    "href": "concepts/adaptiveDesigns/platformTrials.html",
    "title": "Platform Trials",
    "section": "",
    "text": "A traditional trial typically takes two years to design, create the infrastructure, and begin enrolling patients. One therapy and a control are enrolled and then the trial is over, and all the infrastructure built for the trial is obsolete and the trial addressed a single question. A platform trial is built, with a master protocol, to allow multiple therapies to be investigated in the single trial, being added continuously, all with shared infrastructure. A platform trial allows multiple arms to share a common control, share the common infrastructure of the trial, and it allows the better treatment of patients in the trial. The platform trial is a win-win for all stakeholders – smaller sample size, reduced patients on placebo, reduced costs, reduced timelines, and better inferences for regulators.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Platform Trials"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/platformTrials.html#overview",
    "href": "concepts/adaptiveDesigns/platformTrials.html#overview",
    "title": "Platform Trials",
    "section": "",
    "text": "A traditional trial typically takes two years to design, create the infrastructure, and begin enrolling patients. One therapy and a control are enrolled and then the trial is over, and all the infrastructure built for the trial is obsolete and the trial addressed a single question. A platform trial is built, with a master protocol, to allow multiple therapies to be investigated in the single trial, being added continuously, all with shared infrastructure. A platform trial allows multiple arms to share a common control, share the common infrastructure of the trial, and it allows the better treatment of patients in the trial. The platform trial is a win-win for all stakeholders – smaller sample size, reduced patients on placebo, reduced costs, reduced timelines, and better inferences for regulators.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Platform Trials"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/platformTrials.html#our-experience",
    "href": "concepts/adaptiveDesigns/platformTrials.html#our-experience",
    "title": "Platform Trials",
    "section": "Our experience",
    "text": "Our experience\nScientists from Berry Consultants are internationally renowned leaders in the creation, design, and implementation of adaptive platform trials. Platform trials designed by the team at Berry Consultants are either ongoing or being initiated in oncology, infectious diseases, pulmonary and critical care, neurology, and several rare diseases. In addition, the statisticians from Berry Consultants have extensive experience working with clinical investigators and scientific domain experts in both academic and for-profit settings.\nBerry Consultants has been involved in the design and implementation of the following Adaptive Platform Trials:\n\nHealey ALS Platform Trial (ALS)\niSpy-2 (Breast Cancer)\nDIAN-TU (Alzheimer’s Disease)\nIMI-EPAD (Alzheimer’s Disease)\nPrecision Promise (Pancreatic Cancer)\nGBM-AGILE (Glioblastoma Multiforme)\nACTIV-4, REMAP-CAP, PRINCIPLE, DNDi ANTICOV (COVID-19 adaptive platform trials)\nARLG ADAPT (Antibiotics for Drug Resistant Infections)\nPREPARE REMAP-CAP (Community Acquired Pneumonia)\nPREPARE-ALIC4E (Influenza)\nEBOLA (Gates Foundation Funded Platform)\n\nIf you have a need for a platform trial or would like to learn more about its benefits, please contact us.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Platform Trials"
    ]
  },
  {
    "objectID": "concepts/bayes/index.html",
    "href": "concepts/bayes/index.html",
    "title": "The Bayesian Approach",
    "section": "",
    "text": "The Bayesian approach provides a mathematically rigorous and principled methodology for making decisions under arbitrarily complex scenarios. It provides a powerful framework for determining optimal behavior in the face of uncertainty. The Bayesian approach is ideal for many adaptive designs because it provides a naturally sequential learning framework, and allows the efficient and transparent integration of complex clinical trial and external data and natural prediction of future events (e.g., clinical trial results).\nIn contrast to traditional methods, the Bayesian approach itself is very flexible. It is naturally sequential, and can create updated distributions based on the information from a trial – and can be done continuously, without constraints that traditional methods pose. Essentially doing complex adaptive trials from a traditional approach is impossible – whereas from a Bayesian perspective is quite natural and straightforward.\nThe flexibility of modeling is also a huge advantage in flexible designs. Bayesian methods provide a flexible, coherent, and transparent method for the creation and evaluation of disparate information. The approach has computational advantages, the ability to model early predictors and biomarkers, the ability to incorporate prediction in to trial design, combining multiple endpoints together, and synthesizing possibly related populations in to combined analyses.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach"
    ]
  },
  {
    "objectID": "concepts/bayes/index.html#trial-design",
    "href": "concepts/bayes/index.html#trial-design",
    "title": "The Bayesian Approach",
    "section": "",
    "text": "The Bayesian approach provides a mathematically rigorous and principled methodology for making decisions under arbitrarily complex scenarios. It provides a powerful framework for determining optimal behavior in the face of uncertainty. The Bayesian approach is ideal for many adaptive designs because it provides a naturally sequential learning framework, and allows the efficient and transparent integration of complex clinical trial and external data and natural prediction of future events (e.g., clinical trial results).\nIn contrast to traditional methods, the Bayesian approach itself is very flexible. It is naturally sequential, and can create updated distributions based on the information from a trial – and can be done continuously, without constraints that traditional methods pose. Essentially doing complex adaptive trials from a traditional approach is impossible – whereas from a Bayesian perspective is quite natural and straightforward.\nThe flexibility of modeling is also a huge advantage in flexible designs. Bayesian methods provide a flexible, coherent, and transparent method for the creation and evaluation of disparate information. The approach has computational advantages, the ability to model early predictors and biomarkers, the ability to incorporate prediction in to trial design, combining multiple endpoints together, and synthesizing possibly related populations in to combined analyses.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach"
    ]
  },
  {
    "objectID": "concepts/bayes/index.html#modeling-and-meta-analysis",
    "href": "concepts/bayes/index.html#modeling-and-meta-analysis",
    "title": "The Bayesian Approach",
    "section": "Modeling and Meta-Analysis",
    "text": "Modeling and Meta-Analysis\nIn addition to the advantages of using the Bayesian approach in trial design, a key strength of the Bayesian approach is hierarchical modeling. The approach is critical for evaluating the sufficiency and reliability of evidence for supporting a treatment guideline or a payers reimbursement decision—is that the degree of evidence integration or borrowing across multiple information sources is not defined a priori but, instead, depends on the consistency of evidence across the information sources. This provides a quantitative and rigorous methodology that allows firm conclusions to be drawn when the evidence is concordant and, just as important, avoids such conclusions, capturing the increased uncertainty when the heterogeneity in the information is large.\nBayesian methods allow for a formal synthesis of information from clinical trials that ask the same or related questions and are uniquely suited to comparative effectiveness research. There are three important aspects to the modeling. The first is the ability to model study-to-study heterogeneity, the second is the ability to handle indirect treatment comparisons, also called mixed treatment comparisons, and the third is the ability to model nested treatments and combinations of treatments.\nDifferences between trials include varying study designs, eligibility criteria, patient populations, treatment regimens, and outcome measures. Bayesian hierarchical models view the trials included in the meta-analysis as sampled from a larger population of trials. A Bayesian hierarchical model is a random effects model that explicitly accounts for trial heterogeneity and allows for “borrowing of strength” across the trials. If results of trials are similar, there will be greater borrowing of information and this will increase the precision of the estimates. If trial results differ substantially, then there will be less borrowing and appropriately increased uncertainty regarding the conclusions of the meta-analysis. The amount of borrowing across trials is not specified in advance, but is determined by the heterogeneity of the available data.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach"
    ]
  },
  {
    "objectID": "blog/TomParke.html",
    "href": "blog/TomParke.html",
    "title": "Tom Parke",
    "section": "",
    "text": "ADMTP 2025 Conference Presentation\n\n\nOptimizing Group Sequential Designs using Machine Learning\n\n\n\nTom Parke\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADMTP 2025 Conference Presentation\n\n\nThe Properties of Multi-Arm Response Adaptive Designs\n\n\n\nTom Parke\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-03-06.html",
    "href": "blog/posts/2025-03-06.html",
    "title": "ADMTP 2025 Conference Presentation",
    "section": "",
    "text": "Traditionally, phase I dose escalation designs aim to find the maximum tolerated dose (MTD), usually the highest dose whose probability to cause dose-limiting toxicities (DLTs) stays below a certain target toxicity level (TTL), and an adequate dosing scheme. In practice, however, this focus on the MTD when selecting doses to take into registration trials often leads to exposing many trial participants to doses that either produce more toxicity without increased efficacy or severe toxicities that could both limit the options for receiving benefits or lead to premature discontinuation and missed opportunity for continued benefit. Recently, FDA launched Project Optimus with the aim of educating and innovating all stakeholders to, among other goals, move towards designing dose escalation trials that attempt to find “optimal” doses, where optimality comprises safety, tolerability and efficacy.\nIn this talk, we draw from our in-house experiences of designing first-in-human oncology trials to present a seamless phase 1 / phase 2a dose escalation design using open enrollment guided by the continual reassessment method (CRM) that evaluates both safety and efficacy. We introduce advanced CRM adaptations such as open enrollment, target toxicity intervals and escalation with overdose control (EWOC), early stopping rules, backfilling and frontfilling, ad-hoc rules, etc., that not only vastly improve the performance and customizability of CRM, but also help identify “optimal” doses to take forward into registration trials.\nClick here to download the slides. Below is a suggested reading list to get started with dose escalation designs and CRM:\n\n(Le Tourneau, Lee, and Siu 2009)\n(O’Quigley, Pepe, and Fisher 1990)\n(O’Quigley, Shen, and Gamst 1999)\n(Neuenschwander, Branson, and Gsponer 2008)\n(Broglio et al. 2015)\n(Dehbi, O’Quigley, and Iasonos 2021)\n\n\n\n\n\n\n\nFigure 1: Exemplary individual simulation run illustrating the behavior of the Phase I part of the CRM dose escalation design from inception to the declaration of an MTD based on the selected stopping rules.\n\n\n\n\n\n\n\nReferences\n\nBroglio, Kristine R, Larissa Sandalic, Tina Albertson, and Scott M Berry. 2015. “Bayesian Dose Escalation in Oncology with Sharing of Information Between Patient Populations.” Contemporary Clinical Trials 44: 56–63.\n\n\nDehbi, Hakim-Moulay, John O’Quigley, and Alexia Iasonos. 2021. “Controlled Backfill in Oncology Dose-Finding Trials.” Contemporary Clinical Trials 111: 106605.\n\n\nLe Tourneau, Christophe, J Jack Lee, and Lillian L Siu. 2009. “Dose Escalation Methods in Phase i Cancer Clinical Trials.” JNCI: Journal of the National Cancer Institute 101 (10): 708–20.\n\n\nNeuenschwander, Beat, Michael Branson, and Thomas Gsponer. 2008. “Critical Aspects of the Bayesian Approach to Phase i Cancer Trials.” Statistics in Medicine 27 (13): 2420–39.\n\n\nO’Quigley, John, Margaret Pepe, and Lloyd Fisher. 1990. “Continual Reassessment Method: A Practical Design for Phase 1 Clinical Trials in Cancer.” Biometrics, 33–48.\n\n\nO’Quigley, John, Larry Z Shen, and Anthony Gamst. 1999. “Two-Sample Continual Reassessment Method.” Journal of Biopharmaceutical Statistics 9 (1): 17–44."
  },
  {
    "objectID": "blog/posts/2025-03-04.html",
    "href": "blog/posts/2025-03-04.html",
    "title": "ADMTP 2025 Conference Presentation",
    "section": "",
    "text": "I gave a talk at ADMTP 2025 on using Machine Learning to optimize a trial design. Click here to download the slides. As trials get more complex, they acquire more design parameters that need setting. For many of these there are no obvious values that should be used, and there is a considerable parameter space to be explored, using simulations if we are to find the best. Can this be done for us automatically using the latest Machine Learning (ML) software?\nUsing a simple group sequential design as an example, we find that it can. Using the python “botorch” package we can efficiently find when the first interim should be, along with the shape of both the success and futility boundaries, in order to optimize a weighted combination of the expected sample size and maximum sample size over a range of scenarios. Once found, we look at whether this is a maximum by manually running situations at neighboring parameter values and find that actually the maximum area is relatively flat. I think this is encouraging - optimization is going to be easier of we have a big fat (and flat) target to hit!\nWe then looked at how the location of the maximum changes as we changed the weight coefficient in the utility function, and found that it was relatively insensitive to these changes. I think this is also relatively encouraging for the idea of optimization, as different stakeholders may differ in their utility weighting, and this says that as long as they are not too far apart, the same choice of maximizing parameters will satisfy them all.\nWith Thanks to: Dr Matthew Darlington, Dr Luke Rhodes-Leader and Dr Peter Jacko (Lancaster University)!\n\n\n\n\n\n\nFigure 1: Scatterplot of simulated parameter sets"
  },
  {
    "objectID": "blog/posts/2025-04-22.html",
    "href": "blog/posts/2025-04-22.html",
    "title": "Response-Adaptive Randomization in Clinical Trials",
    "section": "",
    "text": "Response adaptive randomization adjusts allocation probabilities over the course of a trial, based on accumulating data. RAR increases allocation to arms performing well, and decreases allocation to arms performing poorly. In multiple arm trials, this has advantages over fixed randomization.\nImagine you have 4 arms, and in truth 2 are pretty bad, one is good, one is great. Ideally you might start allocating equally, with RAR eventually focusing on the good and great arms, and finally determining the great one is the best. Most patients get treated on the good or great arms. Ideally. A typical implementation is to compute the probability each arm is the best at each interim, and assign patients in proportion to that probability (this dates to the 1930s!). You can add bells and whistles like being more or less aggressive, completely eliminating arms with low probabilities, etc. Thus, if you have 4 arms at a interim, with their probability of being the best at 2%, 5%, 70%, and 23%, you would allocate about 70% to the third arm and so on. These probabilities change over time as data accumulates, with the true best arm eventually getting more and more probability.\nIf done correctly, patients within the trial are, on average, treated better (more of them are allocated to the better arms). If your endpoint is mortality, more patients will live in an RAR trial, on average, than live in a trial with fixed randomization. An RAR trial is more likely to correctly identify the best arm, and to correctly determine whether that best arm beats control. The reason is increased sample sizes on the good arms, allowing more accurate comparisons among the good arms and to control (Viele et al. 2020).\nCorrectly is important. There are many papers pointing out pitfalls. You can’t be too aggressive, for example. That could cause you to overreact to a good arm having early bad luck… you lower its allocation too much early and the arm never recovers during the trial. Don’t do this. If you want comparisons to a control arm, you need to maintain enrollment to control throughout the trial (estimating a treatment effect requires estimating control parameters, so you need control data). You need to do this even if early results indicate the treatment beats control. RAR creates different allocation ratios at different times in the trial. If there are time trends in your data, you need to model these time trends correctly (the reference below is for platform trials, but RAR has qualitatively similar issues) (Bofill Roig et al. 2022).\nI’m a little down on RAR right now due to one of our recent papers. To be clear, RAR works well, but some arm dropping designs work equivalently well, and those designs are simpler. Note this paper doesn’t cover platform trials, where the results COULD differ (L. R. Berry et al. 2024). RAR has been done in practice, successfully, in a number of settings. Multiple indications, multiple funding mechanisms, multiple regulatory settings (S. M. Berry and Viele 2023). A pet peeve… a lot of papers CORRECTLY show variants of RAR that perform poorly, but then generalize those results to all RAR, in my opinion incorrectly. RAR is a nuanced methodology. Definitely read what doesn’t work, but beware of overly broad conclusions. Just avoid the pitfalls!\nObviously this is a complex topic, and I haven’t done it justice, but I do love talking about such things if anyone has comments!\n\n\n\n\n\n\nFigure 1: Figure 2 from (L. R. Berry et al. 2024). Simulated Type I error rates in the null scenario without covariate adjustment for time. The five time trend scenarios are shown on the x-axis.\n\n\n\n\n\n\n\nReferences\n\nBerry, Lindsay R, Elizabeth Lorenzi, Nicholas S Berry, Amy M Crawford, Peter Jacko, and Kert Viele. 2024. “Effects of Allocation Method and Time Trends on Identification of the Best Arm in Multi-Arm Trials.” Statistics in Biopharmaceutical Research 16 (4): 512–25.\n\n\nBerry, Scott M, and Kert Viele. 2023. “Comment: Response Adaptive Randomization in Practice.” Statistical Science 38 (2): 229–32.\n\n\nBofill Roig, Marta, Pavla Krotka, Carl-Fredrik Burman, Ekkehard Glimm, Stefan M Gold, Katharina Hees, Peter Jacko, et al. 2022. “On Model-Based Time Trend Adjustments in Platform Trials with Non-Concurrent Controls.” BMC Medical Research Methodology 22 (1): 228.\n\n\nViele, Kert, Kristine Broglio, Anna McGlothlin, and Benjamin R Saville. 2020. “Comparison of Methods for Control Allocation in Multiple Arm Studies Using Response Adaptive Randomization.” Clinical Trials 17 (1): 52–60."
  },
  {
    "objectID": "get/index.html",
    "href": "get/index.html",
    "title": "Get FACTS",
    "section": "",
    "text": "FACTS (Fixed and Adaptive Clinical Trial Simulator) is the industry’s most powerful tool for adaptive and fixed trial design. It enables biostatisticians to design, simulate, and optimize trials with speed and precision, reducing risk and driving innovative, data-driven decisions. Over half of the top 20 largest pharmaceutical companies in the world and more than 30 academic institutions trust FACTS to assist them in the design, simulation, and implementation of trials.\n\nIndustry\nWe offer a 3-months free FACTS Evaluation License to showcase the power and features of our FACTS simulation tool. Please contact us to get a free demo, or learn more about this offer and our regular licenses.\n\n\nAcademia / Charities / Regulatory Bodies / Government\nTo academic and other non-profit research institutions and regulatory bodies, we will generally offer a free FACTS license under certain conditions. Please contact us to see if your organization qualifies.",
    "crumbs": [
      "Get FACTS"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html",
    "href": "introduction/gettingStarted/installation.html",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document describes how to install and verify the installation of the FACTS (Fixed and Adaptive Clinical Trial Simulator) software. It is intended for anyone concerned with the installation of the system.\n\n\n\nThis document covers the installation and installation verification FACTS. It is only intended to demonstrate the usage of the software to the extent that it verifies the software will execute properly in the installed environment.\nFor parameter meanings or detail into the internal workings of the design engines or algorithm check the appropriate System Requirements Document. For appropriate guidance on usage check the User Guides.\n\n\n\nThis document is a guide to the FACTS installation, including both the desktop application and the grid components.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#purpose-of-this-document",
    "href": "introduction/gettingStarted/installation.html#purpose-of-this-document",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document describes how to install and verify the installation of the FACTS (Fixed and Adaptive Clinical Trial Simulator) software. It is intended for anyone concerned with the installation of the system.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#scope-of-this-document",
    "href": "introduction/gettingStarted/installation.html#scope-of-this-document",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document covers the installation and installation verification FACTS. It is only intended to demonstrate the usage of the software to the extent that it verifies the software will execute properly in the installed environment.\nFor parameter meanings or detail into the internal workings of the design engines or algorithm check the appropriate System Requirements Document. For appropriate guidance on usage check the User Guides.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#context-of-this-issue",
    "href": "introduction/gettingStarted/installation.html#context-of-this-issue",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document is a guide to the FACTS installation, including both the desktop application and the grid components.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#desktop-requirements",
    "href": "introduction/gettingStarted/installation.html#desktop-requirements",
    "title": "FACTS Installation Guide",
    "section": "Desktop Requirements",
    "text": "Desktop Requirements\nFACTS can be run on a standard system laptop or desktop running Windows 10 or 11 with the Windows .NET framework v4 or higher installed and at least 1 GB per core or more memory.\nIn addition:\n\nFACTS is expected to run on a display with a resolution of at least 1024x768 pixels and preferably greater.\n\nUser choice of non-default Windows styles/themes may result in unexpected and impractical foreground and background color combinations.\nFACTS 7.0 targets .NET Framework 4.8. Future versions of FACTS will target the latest available .NET Framework version.\nFACTS is now a 64-bit application and will be installed on the target machine’s Program Files area. Previous to FACTS 6.4 versions of FACTS were 32-bit applications installed on the target machine’s Program Files (x86) area.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#computation-requirements",
    "href": "introduction/gettingStarted/installation.html#computation-requirements",
    "title": "FACTS Installation Guide",
    "section": "Computation Requirements",
    "text": "Computation Requirements\nFACTS relies on running simulations and these simulations can be very computationally intensive. When running simulations, each simulation can be run separately (they do not depend on the results of other simulations) though to do so can be somewhat inefficient – repeatedly starting new processes and generating separate output files for every simulation that will need to be gathered together in a single “simulations.csv” file and then summarized. Thus FACTS allows the user to specify a “packet size” and the total number of simulations for each scenario to be simulated is divided by this packet size to create a set of independent jobs.\nIf the simulations are run on the users laptop or PC, FACTS will spawn a simulation thread for every core on the local machine up to the maximum number of simulation ‘packets’ that have been requested. The simulations are run at reduced priority so it is possible to continue to use the machine e.g. for email or Word whilst they run. Thus usually 2 or 4 sets of simulations are run in parallel depending on the processor in the laptop or PC.\nThere are a number of options for speeding up the running of FACTS simulations:\n\nThe simplest technically (and the approach we used to take at Berry Consultants) is to have a large multi-core server (say 32 core) remotely accessible to FACTS users and FACTs installed on it. To use, the user copies the “.facts” files to be simulated to a network shared directory which can be accessed from the server. Then after remotely logging into to the server, the user copies these files to a drive on the server, runs the simulations, zips up the results (within the FACTS GUI there is the FACTS File &gt; Export Project menu command to do this) and copies them back to the network shared drive and thence to their local machine.\nUse the FACTS network share folder “grid” interface, implemented using file transfers to and from a shared network drive. On a machine that can act as a client to a grid of compute nodes managed by one of the standard grid management packages (they used to be called “SunGrid” and “Condor” but have metamorphosed over the years) a “sweeper script” runs that transfers jobs to the grid. The jobs automatically transfer their results back to this shared drive. FACTS copies the job to a unique subfolder on the shared network location and then watches for a change in the lock file name - “submitted”, “running”, “complete” that are managed by the sweeper script. Once the simulations are complete FACTS copies the results back to the local machine. The fact that the simulations have been submitted to the grid are stored in the “.facts” file. Whenever that “.facts” file is open in FACTS, FACTS will poll the remote network drive to check if the simulations are complete.\nA more sophisticated FACTS grid interface that uses a web services to communicate between the FACTS client and a Linux server running a web-server (Apache Tomcat) and database (MySQL). The web service is used to submit jobs and they are stored in the database. A database process then submits them to the grid, once again managed by one of the standard grid management packages. The simulation results are then stored in the database for FACTS to download once complete. This provides a more robust and manageable interface, but it more work to set up. We can provide documentation and scripts and we can assist in setting this up. This is the form of grid that we now use in-house at Berry Consultants.\nTechnically as 3. (but for a fee) Berry Consultants can set and manage the grid for you in the cloud. Please contact us to discuss your requirements and for pricing. Therefore FACTS is able to offload the simulations from the desktop to be run by an external system. The interface describing the interactions with the external system is described in the FACTS Grid Interface document. With a FACTS Enterprise License, the command line executable files to run simulations externally under either Windows or Linux environments are available upon request.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#installation-instructions",
    "href": "introduction/gettingStarted/installation.html#installation-instructions",
    "title": "FACTS Installation Guide",
    "section": "Installation Instructions",
    "text": "Installation Instructions\nThe FACTS Desktop installation package consists of:\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\nsetup.exe\na Windows installation program\n\n\nSetup.msi\nthe FACTS Microsoft Installer file\n\n\nExamples.zip\na Zip file containing example FACTS projects\n\n\nDocuments.zip\na Zip file containing the FACTS documentation.\n\n\nConfig.xml\nan XML file containing the local configuration settings.\n\n\n\nThese files are usually made available for download from Berry Consultants Microsoft App Center site. Download instructions are in a separate document.\nVersions of these files with the standard file extensions (.msi and .zip) modified are available it may have been these versions that were downloaded to circumvent firewall restrictions and these files will need to be renamed prior to use.\nInstallation will take only a few moments.\n\nEnsure that all the files have the correct file extension and are located on a local drive on the machine on which FACTS is to be installed. Windows can treat installs from networked drives as less trustworthy than installs from local drives and this can result in an incomplete installation.\nRight click the setup.exe Windows installation program and select “Run as Administrator”.\nFollow the instructions on the screen to complete the installation.\nDuring FACTS installation you will have to option to enable FACTS to report Analytics back to the App Center. This allows to see how much FACTS is used and which features in FACTS are being used. It does NOT include any user or license information, we can’t see WHO is doing what, only WHAT is being done. Obviously we’d be grateful if you’d enable them.\nAnalytics are off by default they will only be enabled of you enable them.\nOnce installed analytics can be turned on or off from the FACTS “Settings” menu command.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#config.xml",
    "href": "introduction/gettingStarted/installation.html#config.xml",
    "title": "FACTS Installation Guide",
    "section": "Config.xml",
    "text": "Config.xml\nIncluded with the FACTS installation files is a configuration file that can be edited to local settings before the install files are distributed to users. It is also possible to provide an updated copy of the configuration file to users and ask them to update their default configuration, it is also possible for users to locally modify their configuration and revert to the installed configuration details.\nPrior to installation, a configuration file, ‘config.xml’, is available as one of the installation files. This file can be edited to set up a number of default settings for FACTS.\nThe settings are listed between the tags: &lt;configuration&gt; &lt;userSettings&gt; and &lt;/userSettings&gt; &lt;/configurations&gt;, for example:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;\n&lt;configuration&gt;\n  &lt;userSettings&gt;\n    &lt;GridLocation&gt;C:\\\\work\\\\grid&lt;/GridLocation&gt;\n    &lt;GridOpSys&gt;1&lt;/GridOpSys&gt;\n    &lt;GridListenerDelay&gt;10000&lt;/GridListenerDelay&gt;\n    &lt;LocalRVersions&gt;\n      &lt;value&gt;version=\"3.3.2\" path=\"C:\\\\Program Files\\\\R\\\\R-3.3.2\\\\bin\\\\R.exe\" active=\"1\"&lt;/value&gt;\n      &lt;value&gt;version=\"RStudio\" path=\"C:\\\\Program Files\\\\RStudio\\\\bin\\\\RStudio.exe\"&lt;/value&gt;\n    &lt;/LocalRVersions&gt;\n    &lt;FactsSimulationServicePortURL&gt;http://nowhere.com:8080/axis2/services/FactsSimulationServicePort&lt;/FactsSimulationServicePortURL&gt;\n    &lt;GridSimMethod&gt;0&lt;/GridSimMethod&gt;\n  &lt;/userSettings&gt;\n&lt;/configuration&gt;\nSpecifically, the following values may be adjusted, as desired:\n\nLocalRVersions – a list of available R (or RStudio) versions, each one bracketed by the tags  and  and composed of two parameters “version” which can contain any string to be used to identify that version of R and “path” which should contain location of “.exe” that should be run when the user requests R to be run or a Design Report to be generated.\nGridSimMethod – 0 or 1, Determines how FACTS tries to connect to the grid, 0 means the network file share & sweeper script method (option 2 above) is to be used, 1 means that the Web Service method (option 3 or 4 above) is to be used\nIf the network file share method is to be used to connect to the grid then:\n\nGridLocation – the network location of the network file share.\nGridOpSys – 0 or 1, the type of the operating system that is running on the nodes of the grid: 0 – Linux, 1 – Windows (the simulation engine executables have different names in the two environments).\nGridListenerDelay – the delay (in milliseconds) between each poll of the network file share for changes in the state of the simulation results.\n\nIf the Web Service grid access method is to be used to connect to the grid then:\n\nFactsSimulationServicePortURL – specifies the URL to the FACTS web-service endpoint.\n\n\nNote, this configuration file is only used on the initial load of FACTS – subsequently, a local user configuration file is created in a location under the AppData folder – e.g.:\nC:\\Users\\&lt;user_id&gt;\\AppData\\Local\\Berry_Consultants_Inc\\FACTS_File_Loader_Url_&lt;Windows unique file id &gt;\\6.1.6.17435\\user.config\nAny changes made to the configuration from the UI (under the ‘Settings’ menu) are saved to this local file. – and the original config file is only used if the options are reset.\nNB, these local configuration are FACTS version and build specific (note the version and build number in the directory) – which means that if a new install is run, local configuration modifications will be lost.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#notes-on-access-permissions",
    "href": "introduction/gettingStarted/installation.html#notes-on-access-permissions",
    "title": "FACTS Installation Guide",
    "section": "Notes on access permissions",
    "text": "Notes on access permissions\nFACTS uses the locations C:\\Program Data\\BerryConsultants and &lt;user&gt;\\AppData\\Local\\BerryConsultants, we have seen some IT departments set the default access permissions to deny access to these locations contrary to Microsoft’s intention and the access will need to be granted for FACTS to run. When FACTS runs simulations it spawns one or more simulations processes, and we have encountered environments where these processes do not get permission to write to network drives. If these permissions cannot be changed, it will be necessary for users to save their “.facts” file run in a directory on the local drive before running simulations, so the results can be written there and then copied/moved to the network drive once complete.\n\nLicense Installation\nWhen FACTS is first run, it may require the license to be entered. The user can choose the file when prompted, or cut and paste the information into the dialog box. Alternatively, the file can be dropped in the application folder and it will be picked up when needed. Note, depending on access permissions, it may be necessary to initially load FACTS with admin rights in order to load the license key from file.\nSubsequent runs, and subsequent installations of mod level updates, will not require the license to be re-entered.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#installation-verification",
    "href": "introduction/gettingStarted/installation.html#installation-verification",
    "title": "FACTS Installation Guide",
    "section": "Installation Verification",
    "text": "Installation Verification\n\n\n\n\n\n\n\n\n\n\nAction\nExpected result\nActual result\nPass\nFail\n\n\nVerify that the FACTS shortcut appears in the Start Menu and on the user’s desktop\nApplication shortcuts located\n\n\n\n\n\nClick the application shortcut to launch FACTS (Enter license key as necessary)\nApplication opens.\n\n\n\n\n\nClick the Help &gt; About menu item\nAbout box appears.\n\n\n\n\n\nVerify the version number is the same as the version specified by Berry Consultants.\nVersion number is correct\n\n\n\n\n\nIf example projects were installed, select File &gt; Examples &gt; [example_file_name] menu item\nExample project opens.\n\n\n\n\n\nIf example projects were not installed select open and navigate to the location where an example file was saved\nExample project opens.\n\n\n\n\n\nClick the File &gt; Save As menu item.\nChoose a writeable location on the local drive and save a copy of the design.\nNew copy of project created.\n\n\n\n\n\nClick the Simulation tab, with “Locally” selected,\nClick “Select All”\nClick “Simulate”\nSimulations start.  After a short while, results are displayed in the GUI.\n\n\n\n\n\nClick the “View Graph” button.\nGraph window opens.  Displayed graph updates when graph title selected in list.\n\n\n\n\n\n\n\n\n\n\n\n\nThe “Design Report” feature in FACTS Core requires that R, R Studio (for mathjax & pandoc), and the R libraries “markdown”, “xtable” and “stringi” are installed. You will also need Microsoft Word installed to be able to open the generated “.docx” file.\n\n\n\n\n\n\nIn FACTS open the configuration settings and configure the location of R or RStudio on the computer. (See the Design Report User Guide for more details if required).\n\n\n\n\n\n\nIf the example file used above was not a “FACTS Core” example, repeat the steps above with a FACTS Core example, up to and including the “Run Simulations” step.\n\n\n\n\n\n\nPress the “Design Report” button. If you have “R” selected as your FACTS default “R”, this will run in batch mode.\nIf you have RStudio selected as our default “R”, RStudio will now start. In the terminal window you are shown the text of a function call. Copy and paste this text into the R command line and execute the function.\nOnce the function is complete you should have a Word file within the “_results” directory that corresponds to the current “.facts” file.\n\n\n\n\n\n\nOpen the World file and review the contents, it should describe the example you have selected and simulated.\n\n\n\n\n\n\nClick the File &gt; Exit menu item\nApplication closed.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html",
    "href": "introduction/gettingStarted/downloadInstructions.html",
    "title": "Download Instructions",
    "section": "",
    "text": "This document provides download instructions for all new releases of Berry software; including, the FACTS desktop app installer, FACTS engine executable, FACTS HPC app, ADDPLAN Classic, ADDPLAN Neo and QUOTES. This assumes that you currently have a software license agreement with Berry Consultants, or that you have already gotten in touch with Berry Consultants for a software evaluation. If this is not the case, please get in touch with us by clicking here. Details about all the software we provide can be found here.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html#purpose-and-scope-of-this-document",
    "href": "introduction/gettingStarted/downloadInstructions.html#purpose-and-scope-of-this-document",
    "title": "Download Instructions",
    "section": "",
    "text": "This document provides download instructions for all new releases of Berry software; including, the FACTS desktop app installer, FACTS engine executable, FACTS HPC app, ADDPLAN Classic, ADDPLAN Neo and QUOTES. This assumes that you currently have a software license agreement with Berry Consultants, or that you have already gotten in touch with Berry Consultants for a software evaluation. If this is not the case, please get in touch with us by clicking here. Details about all the software we provide can be found here.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html#registration-email",
    "href": "introduction/gettingStarted/downloadInstructions.html#registration-email",
    "title": "Download Instructions",
    "section": "Registration email",
    "text": "Registration email\nAll Berry software releases are hosted on FACTS Cloud, our cloud-hosted web application. You will first be receiving an email from noreply@berryconsultants.net to register with FACTS Cloud as shown below.\n\n\n\n\n\n\nFigure 1\n\n\n\nThe link in the email will remain valid for up to a day, and can only be used once. Note that this is a one time operation: once registered with FACTS Cloud, you will never need to re-register again. Please do not reply to this registration email, as replies are not tracked.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html#set-password",
    "href": "introduction/gettingStarted/downloadInstructions.html#set-password",
    "title": "Download Instructions",
    "section": "Set password",
    "text": "Set password\nWhen clicking on the registration email link, you will be navigated to the FACTS Cloud password setting screen as shown below.\n\n\n\n\n\n\nFigure 2\n\n\n\nWithin the email address field, as an additional security measure, you will need to re-enter exactly the same email address that the FACTS Cloud registration email was sent to. The password will need to be at least 8 characters long, contain lower and upper case characters, contain digits and non-alphanumeric characters.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/webinars.html",
    "href": "introduction/webinars.html",
    "title": "Videos and Webinars",
    "section": "",
    "text": "Introduction to FACTS\nDose Escalation N-CRM Example\nCore Dichotomous Simple Two-Arms Example\nEnrichment Designs: Basket Trial Example\nFACTS Platform Trial Intro",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#introductions",
    "href": "introduction/webinars.html#introductions",
    "title": "Videos and Webinars",
    "section": "",
    "text": "Introduction to FACTS\nDose Escalation N-CRM Example\nCore Dichotomous Simple Two-Arms Example\nEnrichment Designs: Basket Trial Example\nFACTS Platform Trial Intro",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#training-sessions",
    "href": "introduction/webinars.html#training-sessions",
    "title": "Videos and Webinars",
    "section": "Training Sessions",
    "text": "Training Sessions\n\nFACTS Core Introduction\nFACTS Core Dose Response Models\nFACTS Core Predictive Probabilities\nFACTS Core Arm Dropping and RAR\nFACTS Core Multiple Endpoint\nFACTS Core and Enrichment Designs Longitudinal Modeling\nFACTS Core Time-to-Event\nFACTS Core Time-to-Event Predictors\nFACTS Enrichment Designs Introduction\nFACTS Enrichment Designs Hierarchical Modeling\nFACTS Enrichment Designs Borrowing information on control\nFACTS Core and Enrichment Designs Hierarchical Prior for Control\nFACTS Dose Escalation Introduction\nFACTS Dose Escalation N-CRM Setting the Prior\nFACTS Dose Escalation N-CRM Open enrollment, pseudo subjects, expansion cohort, fine spaced doses\nFACTS Dose Escalation N-CRM Ordinal toxicity, 2 samples, efficacy\nFACTS Dose Escalation 2D-CRM\nFACTS Staged Design Introduction\nFACTS Staged Design First Example\nFACTS Staged Design Advanced Topics\nFACTS Dose Escalation - Hands On Training\nFACTS Staged Design - Hands on Training",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#introduction-to-adaptive-design-using-facts-webinar-series",
    "href": "introduction/webinars.html#introduction-to-adaptive-design-using-facts-webinar-series",
    "title": "Videos and Webinars",
    "section": "Introduction to Adaptive Design Using FACTS Webinar Series",
    "text": "Introduction to Adaptive Design Using FACTS Webinar Series\n\nPhase 1 Dose Escalation\nArm Dropping and RAR\nBasket Trials\nGroup Sequential\nMAMS and Platform Trials\nSeamless Phase 2-3",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#introductory-topics",
    "href": "introduction/webinars.html#introductory-topics",
    "title": "Videos and Webinars",
    "section": "Introductory Topics",
    "text": "Introductory Topics\n\nThe Uses of Trial Simulation in Consulting\nFACTS Core Design Exploration - part 1\nFACTS Core Design Exploration - part 2\nStopping for Futility\nN-CRM Case Study\nIntroduction to Staged Designs\n\n\nDiscussion of FDA Guidance\n\nDiscussion of the new FDA Guidance on Adaptive Designs",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#response-adaptive-randomization",
    "href": "introduction/webinars.html#response-adaptive-randomization",
    "title": "Videos and Webinars",
    "section": "Response Adaptive Randomization",
    "text": "Response Adaptive Randomization\n\nThe pros and cons of Response Adaptive Randomization (RAR)\nRAR and arm dropping in FACTS\nAdaptive Randomization Works\nResponse Adaptive Randomization (RAR) in multi-arm trials seeking to find the best arm",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#case-studies",
    "href": "introduction/webinars.html#case-studies",
    "title": "Videos and Webinars",
    "section": "Case Studies",
    "text": "Case Studies\n\nThe Evolution of a trial from FACTS Core to Staged Design and back\nUsing a biomarker endpoint to trigger phase 3\nDesigning an Adaptive Multiple Dose Trial with FACTS\nPapers on Bayesian Group Sequential Designs\nThe ENRICH trial an Enrichment Design case study\nThe ENRICH Trial re-visited\nA seamless phase 2/3 trial in type-2 diabetes\nA two arm study with adherence model driven effect size and irregular interims\nA Phase 1 dose escalation study using open enrolment\nA successful dose finding study in Alzheimer’s\nA two arm non-inferiority MACE safety study",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#advance-topics",
    "href": "introduction/webinars.html#advance-topics",
    "title": "Videos and Webinars",
    "section": "Advance Topics",
    "text": "Advance Topics\n\nCalibrating hierarchical model priors\nPost Processing FACTS output with R\nUsing FACTS to analyze Ordinal Data\nFACTS predictive Power and Predicting the Trial Outcomes\nDose escalation designs with open enrollment\nFACTS Core TTE Predictors\nEarly decision making PFS and OS - part 1\nEarly decision making PFS and OS - part 2",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#optimization",
    "href": "introduction/webinars.html#optimization",
    "title": "Videos and Webinars",
    "section": "Optimization",
    "text": "Optimization\n\nPhase 2 sources of efficiency\nPicking the Winner - how to compare trial designs\nAn early look at QUOTES and Estimating eNPV\nApplying Bayesian Optimization to FACTS Trial designs and choosing stopping boundaries\nTrial Design Optimization through the lens of expected Net Present Value",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "contact/index.html",
    "href": "contact/index.html",
    "title": "Contact",
    "section": "",
    "text": "Get FACTS\nTo directly apply for a free 3-months FACTS Evaluation license or a regular license, please visit the FACTS product webpage.\nTo enquire about a free demo or if you are unsure about your needs, feel free to contact us directly via email at facts@berryconsultants.com.\n\n\nHelp using FACTS\nPlease contact us directly via e-mail at facts@berryconsultants.com.\n\n\nGeneral Inquiries about Berry Consultants\nPlease use the Berry Consultants online form.",
    "crumbs": [
      "Contact"
    ]
  },
  {
    "objectID": "documentation/v72/examples/Staged/example2.html",
    "href": "documentation/v72/examples/Staged/example2.html",
    "title": "Example 2",
    "section": "",
    "text": "Second example"
  },
  {
    "objectID": "documentation/v72/examples/CRM/example2.html",
    "href": "documentation/v72/examples/CRM/example2.html",
    "title": "Example 2",
    "section": "",
    "text": "Second example"
  },
  {
    "objectID": "documentation/v72/userguides/de.html",
    "href": "documentation/v72/userguides/de.html",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document describes how to install and use the Rule-Based Dose Escalation (DE) Fixed and Adaptive Clinical Trial Simulator (FACTS) software (from now on referred to as Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software). It is intended for all end users of the system.\n\n\n\nThis document covers the Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software by describing the user interface. It covers the 3+3 and mTPI. It does not cover the CRM design engine which has its own User Guide.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS, 5 or later if changed, installed on Windows 7 and Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nThis document is a guide to the version 7.1 release of Dose Escalation FACTS.\nThere have been no changes to these elements of FACTS since FACTS 6.1.\n\n\n\nPlease cite FACTS wherever applicable using this citation.\n\n\n\nAn overview of the acronyms and abbreviations used in this document can be found here."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#purpose-of-this-document",
    "href": "documentation/v72/userguides/de.html#purpose-of-this-document",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document describes how to install and use the Rule-Based Dose Escalation (DE) Fixed and Adaptive Clinical Trial Simulator (FACTS) software (from now on referred to as Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software). It is intended for all end users of the system."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#scope-of-this-document",
    "href": "documentation/v72/userguides/de.html#scope-of-this-document",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document covers the Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software by describing the user interface. It covers the 3+3 and mTPI. It does not cover the CRM design engine which has its own User Guide.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS, 5 or later if changed, installed on Windows 7 and Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#context-of-this-issue",
    "href": "documentation/v72/userguides/de.html#context-of-this-issue",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document is a guide to the version 7.1 release of Dose Escalation FACTS.\nThere have been no changes to these elements of FACTS since FACTS 6.1."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#citing-facts",
    "href": "documentation/v72/userguides/de.html#citing-facts",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#definition-of-terms",
    "href": "documentation/v72/userguides/de.html#definition-of-terms",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "An overview of the acronyms and abbreviations used in this document can be found here."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#facts-7.1-changes-to-dose-escalation",
    "href": "documentation/v72/userguides/de.html#facts-7.1-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 7.1 Changes to Dose Escalation",
    "text": "FACTS 7.1 Changes to Dose Escalation\nThe CRM (Toxicity), CRM (Ordinal), CRM (Efficacy), bCRM are now considered deprecated. Existing designs will still work as expected, but users starting a new design are encouraged to switch over to the new CRM engine if possible."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#facts-7.0-changes-to-dose-escalation",
    "href": "documentation/v72/userguides/de.html#facts-7.0-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 7.0 Changes to Dose Escalation",
    "text": "FACTS 7.0 Changes to Dose Escalation\nIn FACTS 7.0 the only change was on the Analysis tab, when entering subject data manually, the FACTS GUI now ensures that a cohort number is entered for every subject."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#facts-6.5-changes-to-dose-escalation",
    "href": "documentation/v72/userguides/de.html#facts-6.5-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.5 Changes to Dose Escalation",
    "text": "FACTS 6.5 Changes to Dose Escalation\nIn FACTS 6.5 there were no changes to the Dose Escalation simulators."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#facts-6.4-changes-to-dose-escalation",
    "href": "documentation/v72/userguides/de.html#facts-6.4-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.4 Changes to Dose Escalation",
    "text": "FACTS 6.4 Changes to Dose Escalation\nIn FACTS 6.4 there were no changes to the Dose Escalation simulators."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#facts-6.3-changes-to-dose-escalation",
    "href": "documentation/v72/userguides/de.html#facts-6.3-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.3 Changes to Dose Escalation",
    "text": "FACTS 6.3 Changes to Dose Escalation\nIn FACTS 6.3 there were no changes to Dose Escalation except in N-CRM which is described in its own User Guide."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#facts-6.1-changes-to-dose-escalation",
    "href": "documentation/v72/userguides/de.html#facts-6.1-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.1 Changes to Dose Escalation",
    "text": "FACTS 6.1 Changes to Dose Escalation\nIn FACTS 6.1:\n\nTwo new types of design have been added:\n\nThe mTPI design - described in this document.\nThe 2D-CRM for phase 1 dose escalation trials of combinations of multiple doses of 2 drugs described in the FACTS DE 2D-CRM User Guide.\n\nA ‘design variant’ facility has been added to the N-CRM engine that allows the user to easily simulate and evaluate an N-CRM design at different sample sizes. See the updated FACTS DE N-CRM User Guide for details.\nNote that from FACTS 6.0 onwards the default MCMC sampling length for Dose Escalation designs was increased from 2500 to 25000. This was because it was found that with the shorter sampling length the results were too inconsistent. It has however significantly slowed the speed with which FACTS DE designs are run."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#introduction-1",
    "href": "documentation/v72/userguides/de.html#introduction-1",
    "title": "3+3 / mTPI",
    "section": "Introduction",
    "text": "Introduction\nTo open the application, select FACTS Dose Escalation from your Start menu. At different points in the FACTS GUI, the user is required to make decisions about how to model or simulate various aspects of a clinical trial. To simplify data entry, FACTS shows the user only the information that is relevant to the current decisions.\nFACTS has a tabbed design to allow entry of different categories of information about the design.\nThe File menu allows the user to create a new design, open an existing design, save a design, or copy a design to a new name (Save as). The saved design includes all parameters entered as well as simulation results, if they have been produced.\nThe Settings menu provides the ability to configure how simulation jobs should be submitted to the Grid. Selecting whether to execute simulations locally or on the Grid is done from the Simulation tab.\nThe Help menu allows the user to learn more about the FACTS software and verify the application’s version number.\nFinally, note that in all tabs of the application, red exclamation points\n\n\n\n\n\n\nFigure 2\n\n\n\nindicate errors in data entry from the user that must be corrected. Moving the cursor over the exclamation point causes a pop-up help text indicating what the error is, helping the user remedy the error."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#selecting-a-design-model",
    "href": "documentation/v72/userguides/de.html#selecting-a-design-model",
    "title": "3+3 / mTPI",
    "section": "Selecting a Design Model",
    "text": "Selecting a Design Model\nOnce the FACTS application has been opened, the user may select to model a clinical trial using any of the following design engines from the introduction screen or the File &gt; New &gt; Dose Escalation menu:\n\n\n\n\n\n\nFigure 3\n\n\n\nOnce a design has been selected, the associated form of the GUI will be displayed, the contents of the tabs and sub-tabs of FACTS displaying the information relevant to the selected model. Additionally, the model type selected will appear in the title bar of FACTS to verify the user’s choice of design."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#study-info",
    "href": "documentation/v72/userguides/de.html#study-info",
    "title": "3+3 / mTPI",
    "section": "Study Info",
    "text": "Study Info\nThe Study Info sub-tab provides basic study parameters such as Trial Size and Cohort options, as relevant to the type of design. Depending on the choice of design engine, the Study info tab may also allow the user to enter specification for a two-sample population, Toxicity and/or Efficacy targets, response category options, or Joint Efficacy and Toxicity options.\n\nRecruitment, like N-CRM the mTPI design includes the option to use Open Enrollment instead of recruitment by cohorts. A phase 1 trial using open enrolment recruits ‘all comers’ with a cap on the number of subjects who can be treated but have not yet had a final result. Subjects that arrive whilst this cap has been reached are deemed to be ‘lost’ to the study. If selected the user specifies the following additional parameters.\n\nMaximum study size (subjects): the maximum number of subjects who can be recruited into the study.\nMean recruitment ate: for simulation purposes this is the average number of subjects who will be expected to be available for recruitment during each time period. Recruitment is simulated using a standard Poisson process.\nTime until final result: the time for each subject’s final result to become available. This is simulated as a fixed length of time that is the same for all subjects.\nMaximum subjects without a final result: the cap on the number of subjects who can be recruited but not yet completed. This is also known as the ‘maximum queue length’. Subjects arriving and available for recruitment while the number of subjects treated but not yet completed is a the cap are dropped and assumed not available for recruitment once the current subjects complete but the study has to await further new subjects to become available.\n\nStudy Size – (unless an mTPI design using open enrollment) for all designs this is specified in terms of the maximum number of cohorts.\n\nIf the design is CRM(Toxicity) or CRM(Efficacy) and the option to include a “Two sample population” analysis is included then the Study Size applies to the first sample and with an additional parameter the user specifies the maximum size of the second sample, also in cohorts.\n\nCohort - (unless an mTPI design using open enrollment and if not a “3+3” design where cohorts are always of size 3 and must always complete before the next cohort is recruited) then the user can specify:\n\nCohort size: The number of subjects in each cohort. The maximum sample size for the trial is simply: Max Trial Size (cohorts) * Cohort Size.\nCohort information delay: Normally left at zero, this allows for the trial to be simulated with cohorts recruited more quickly than if waiting for the last cohort to complete before treating the next cohort. This allows the trial to complete in shorter time, but is likely to lead to a larger incidence of toxicities. If the delay is set to 1, the second cohort will be recruited before the results for the first cohort are available, the third cohort will be recruited after the results of the first cohort are available but before the results for the second cohort are available, and so on.\n\nJoint Efficacy and Toxicity – (bCRM only) – there is a flag that the user can set to indicate that a subject cannot experience both toxicity and efficacy (toxicity censors or prevents efficacy).\n\n\n\nSingle subject run-in – (CRM(Toxixity), bCRM, CRM(Ordinal) only) – this allows the user to specify that the trial is to start with a single subject run-in. With a single subject run-in, subjects are allocated in cohorts of one, incrementing by one dose strength each cohort until a toxicity is observed. The user has the option to specify whether, on observing the first toxicity, the first full cohort is then allocated at that dose, or at the dose below.\nTarget – (mTPI only) – somewhat similar to the N-CRM, in mTPI the target is defined as toxicity bands rather than single toxicity rate. The user specifies the lower and upper bounds of the target toxicity band as well as a target toxicity rate within that band.\nThe parameters to specify the Toxicity target are displayed for CRM(Toxicity), bCRM and CRM(Ordinal) designs. These allow the user to specify the maximum tolerated toxicity target and choose which dose should be selected based one of the following criteria:\n\nnearest dose to target\nnearest dose above the target (except for ordinal CRM)\nnearest dose below the target\nIf a control arm is included with the Treatment arms then there is the option to specify that the target toxicity rate is relative to control, not absolute.\n\nSimilarly the parameters to specify the Efficacy target are displayed for CRM(Efficacy) and bCRM. These allow the user to specify the maximum tolerated toxicity target and choose which dose should be selected based one of the following criteria:\n\nnearest dose to target\nnearest dose above the target (except for ordinal CRM)\nnearest dose below the target\nIf a control arm is included with the Treatment arms then there is the option to specify that the target efficacy rate is relative to control, not absolute.\n\nCohort Expansion – (all designs except CRM(Efficacy) The user can specify that after the dose escalation phase of the trial has completed, the simulation is to include an ‘expansion cohort’ allocated at the final value of the target dose, and how large that expansion cohort will be.\n\nIf a Control arm has been included, then there can be a specific allocation to Control as well (these are taken from the overall cohort size, they are not additional to it).\nIn bCRM the user selects whether the expansion cohort is allocated to the MTD, MED or OSD.\n\n\nFigure 4 displays the Study Info sub-tab for a bCRM design. The functionality and look of the tab is similar for other CRM designs and mTPI.\n\n\n\n\n\n\nFigure 4: Study Info (bCRM)"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#treatment-arms",
    "href": "documentation/v72/userguides/de.html#treatment-arms",
    "title": "3+3 / mTPI",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nThe Treatment Arms sub-tab (Figure 5) provides an interface for specifying the various dose levels, and (except in “3+3”) a Control treatment arm.\nIf a control arm is included:\n\nTarget rates can be specified to be relative to control (otherwise they are absolute).\nA specific number of subjects are specified to be allocated to control in each cohort\nThe response on the Control arm can be included in the dose response model, or modeled separately (using a simple beta-binomial model) in which case monotonicity is not enforced.\n\nThe user may add doses either explicitly or by auto-generation, as depicted below. The user may also edit the Dose Names within the table by double clicking on any existing dose name, however the index cannot be edited.\nNote that the CRM designs use transformed dose levels, and unlike the Dose Finding designs do not have a relative dose level specified.\n\n\n\n\n\n\nFigure 5: Treatment Arms"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#explicitly-defined",
    "href": "documentation/v72/userguides/de.html#explicitly-defined",
    "title": "3+3 / mTPI",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\n\nToxicity [3+3, CRM(Toxicity), mTPI and bCRM]\nThe Toxicity sub-tab provides an interface for specifying one or more Toxicity profiles.\nToxicity profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 6. Toxicity values are entered directly into the Probability of Toxicity column of the table, and the graphical representation of these toxicity values updates accordingly. The graph may be modified by plotting the log of the dose strength as the x-axis, and by plotting the logit or the probability of toxicity as the y-axis.\nThis graph – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right-clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\n\n\n\n\n\n\nFigure 6: Toxicity Virtual Subject Response (bCRM)\n\n\n\nWhen using a bCRM design engine, if the Toxicity and Efficacy profiles are specified separately, then the results are simulated without correlation. To simulate correlation in the results it is necessary to specify joint profiles.\n\n\nToxicity with 2 Samples [CRM(Toxicity)]\nIf utilizing a CRM (Toxicity) design, the user has the option to model two sample populations (this option may be selected on the Study Info tab, as described here). If the user elects to enable the modeling of two samples, then the Toxicity sub-tab allows the user to input probabilities of Toxicity for each sample, as displayed in Figure 7 below.\n\n\n\n\n\n\nFigure 7: Toxicity Virtual Subject Response with 2 samples (CRM (Toxicity))\n\n\n\n\n\nOrdinal Toxicity [CRM(Ordinal)]\nFor CRM (Ordinal) designs, the user must specify the probability of toxicity at or above each category. Ordinal designs can use either three or four categories, with a category three toxicity corresponding to a toxic response in a CRM (Toxicity) design. Toxicity data must be entered for each category greater than 1 and must be monotonically decreasing with category (Figure 8).\n\n\n\n\n\n\nFigure 8: Toxicity Virtual Subject Response (CRM (Ordinal))\n\n\n\n\n\nEfficacy [CRM (Efficacy) and bCRM]\nSimilar to the Toxicity sub-tab, the Efficacy sub-tab provides an interface for specifying one or more Efficacy profiles.\nEfficacy profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen.\nEfficacy values are entered directly into the Probability of Efficacy column of the table, and the graphical representation of these efficacy values updates accordingly.\nIf utilizing a bCRM design, in which both Toxicity and Efficacy profiles are utilized, the user must ensure that profile names are unique. FACTS does not allow the user to enter Toxicity profiles and Efficacy profiles which are named identically.\nFinally, if utilizing a CRM (Efficacy) design, the user has the option to model two sample populations (this option may be selected on the Study Info tab, as described here). If the user elects to enable the modeling of two samples, then the Efficacy sub-tab allows the user to input probabilities of Efficacy for each sample, just as in the case of CRM (Toxicity) illustrated in Figure 9.\n\n\n\n\n\n\nFigure 9: Efficacy Virtual Subject Response (CRM Efficacy)\n\n\n\n\n\nJoint Efficacy / Toxicity [bCRM]\nThe Joint Efficacy / Toxicity sub-tab (Figure 10) allows the user to specify the probability of efficacy, toxicity, and success at each dose.\nJoint profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in (Figure 10). The user must ensure that any profile name entered on this tab is unique from all other profile names in the application, whether on this tab, or on the Toxicity sub-tab or the Efficacy sub-tab.\nProbabilities of toxicity, efficacy, and success are entered directly into the table, and the graphical representation of these probabilities updates accordingly. Success is the probability of observing efficacy without toxicity, thus the rate for success is naturally bounded:\n\nThe probability of success at a dose cannot exceed the probability of efficacy at that dose.\nThe probability of success at a dose cannot exceed (1 – the probability of toxicity) at that dose.\nThe probability of success cannot be less than the probability of efficacy minus the probability of toxicity at that dose.\nIf the user enters probabilities violating these limit, then FACTS mark them as invalid and will refuse to simulate the scenarios including the profile. FACTS reports to the user, on the Simulation tab, that the following error has been found: “True toxicity and Efficacy curve is not in range [0,1]” and will ask the user to resolve the error before running simulations.\n\nJoint Efficacy / Toxicity profiles may also be generated from models, rather than explicitly. This way of specifying Joint Efficacy / Toxicity profiles is described in this section.\n\n\n\n\n\n\nFigure 10: Joint Efficacy"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#external",
    "href": "documentation/v72/userguides/de.html#external",
    "title": "3+3 / mTPI",
    "section": "External",
    "text": "External\nSubject response data may be simulated from a PK-PD model in place of or in addition to choosing a response model from FACTS. The importing of subject response data (which must be in the form specified in the System Requirements Document) may be done from the External Files sub-tab depicted below (Figure 11).\nTo import an external file, the user must first add a profile to the table. After adding a profile, the user must click “Browse” to locate the externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 11: External Data"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#parametric-bcrm-sec-parametric-bcrm",
    "href": "documentation/v72/userguides/de.html#parametric-bcrm-sec-parametric-bcrm",
    "title": "3+3 / mTPI",
    "section": "Parametric [bCRM] (#sec-parametric-bcrm)",
    "text": "Parametric [bCRM] (#sec-parametric-bcrm)\nThe Parametric sub-tab allows the user to specify the Joint Efficacy / Toxicity profiles as modeled by Cox or Gumbel functions. This allowance is in contrast to the explicit definition of Joint Efficacy / Toxicity profiles, as may be done in the Joint Efficacy / Toxicity sub-tab of the Explicitly Defined Response tab (see this section).\nThe interface of the Parametric tab is displayed below (Figure 12). As with other Virtual Response sub-tabs, profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 12.\n\n\n\n\n\n\nFigure 12: bCRM Parametric Joint Toxicity & Efficacy VSR - Cox Model\n\n\n\n\n\n\n\n\n\nFigure 13: bCRM Parametric Joint Toxicity & Efficacy VSR - Gumbel Model"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#toxicity-and-efficacy-response-tabs-crm-efficacy-crm-toxicity-and-bcrm",
    "href": "documentation/v72/userguides/de.html#toxicity-and-efficacy-response-tabs-crm-efficacy-crm-toxicity-and-bcrm",
    "title": "3+3 / mTPI",
    "section": "Toxicity and Efficacy Response tabs [CRM (Efficacy), CRM (Toxicity) and bCRM]",
    "text": "Toxicity and Efficacy Response tabs [CRM (Efficacy), CRM (Toxicity) and bCRM]\nOn the Toxicity and Efficacy Response sub-tabs, the user may choose toxicity and efficacy models from among a list available for this release. The supported models for modeling toxicity and efficacy response in this release are logistic, tanh, and power for one sample studies, and tanh (x-b) for two sample studies.\nThe user has the option to set the Minimum and Maximum parameter values, which define the toxicity/efficacy asymptotes. These values are used to rescale the probabilities and calculated the scaled dose values, X^ (X-hat). These default to 0 and 1, but if the endpoint being observed is expected to have a background rate (that would be observed even if placebo were administered) or a natural maximum rate (that would not be exceeded whatever the strength of dose used), then the model fitting will be improved\nThe Toxicity Response tab is depicted below in Figure 14; the Efficacy Response tab has a similar appearance.\n\n\n\n\n\n\nFigure 14: Toxicity Response (bCRM)\n\n\n\nThe user selects the model type:\n\nLogisitic, in which case the fixed value for the Alpha parameter is specified (usually this is set to 3 – giving a toxicity rate of 0.953 at a dose with a transformed dose strength of 0. Effective dose strengths should typically be in the range (-8, -1)\nTanh in which case the effective dose strengths should typically be in the range (-2, 1)\nPower in which case the effective dose strengths should typically be in the range (0,1)\n\nThe user specified the prior distribution of the estimated model parameter (Beta) either as an Exponential (1) distribution or a uniform distribution (0,U) where U is specified by the user.\nThe effective dose strengths of the doses are specified in the “Model Priors” table. They can be specified in one of two ways:\n\nBy defining a prior probability of toxicity for each dose. The corresponding transformed dose strength is then calculated as the value that would yield that toxicity rate from the model with Beta=1. The corresponding transformed dose strength is shown in the right hand column. Care needs to be taken with this approach that the resulting transformed dose strengths are well spaced out – this can be checked on the graph that shows the transformed dose strengths, and the corresponding toxicity rate on the graph of the Beta=1 model.\nBe specifying the effective dose strength explicitly. Care needs to be taken with this approach, depending on the model this may yield nonsensical toxicity rates, but once the correct range for the transformed doses is understood, it easier to ensure a good spacing of the transformed dose strengths."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#toxicity-response-tab-crmordinal",
    "href": "documentation/v72/userguides/de.html#toxicity-response-tab-crmordinal",
    "title": "3+3 / mTPI",
    "section": "Toxicity Response tab [CRM(Ordinal)]",
    "text": "Toxicity Response tab [CRM(Ordinal)]\n\n\n\n\n\n\nFigure 15: CRM(Ordinal) Toxicity Response tab"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#design-tab-mtpi",
    "href": "documentation/v72/userguides/de.html#design-tab-mtpi",
    "title": "3+3 / mTPI",
    "section": "Design tab [mTPI]",
    "text": "Design tab [mTPI]\nThe mTPI method has two parameters that can be specified\n\nThe first is an option to prevent re-escalating to a dose (the “DU” or Do not Use category in the diagram) if the posterior probability that the toxicity rate is above the target toxicity plus the delta: pT+εU (on the Study &gt; Study Info tab this is the “Upper Bound” parameter) exceeds a specified threshold (0.95 is the default).\nThe second is an option that allows an early stopping rule to be specified that if the dose to be allocated next already has the maximum number of subjects on it then it is declared the MTD. Setting this value shrinks the size of the table displayed.\n\nGiven the target toxicity rate and the boundaries specified on the Study &gt; Study Info tab, and the parameters specified here the table of mTPI dosing decisions is displayed. This shows the dosing decision given the number of subject treated at the current dose and the number of toxicities observed:\n\nE Escalate\nS Stay\nD De-escalate\nDU De-escalate and do not revisit\n\n\n\n\n\n\n\nFigure 16: mTPI Design tab"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#allocation-rule",
    "href": "documentation/v72/userguides/de.html#allocation-rule",
    "title": "3+3 / mTPI",
    "section": "Allocation Rule",
    "text": "Allocation Rule\nThe Allocation Rule sub-tab is depicted below (Figure 17); it allows the user to set basic allocation parameters, including Maximum Dose Increment, Minimum Cohorts on Dose prior to Increment, and Maximum number of Cohorts used to determine MTD (bCRM only).\nThe dose range may be split with a greater maximum increment allowed in the upper part of the range than in the lower.\nWhen specifying the maximum dose increment, the user has two options of how to apply that increment; relative to the current dose or relative to the highest dose with a specified number of cohorts. In other words, the largest possible value for the next allocation at any point in the trial is either the current dose level plus the Maximum Dose Increment, or the highest dose that has been allocated to Max Subjects Before Incrementing plus the Maximum Dose Increment.\n\n\n\n\n\n\nFigure 17: Design & Allocation Rule tab (CRM Toxicity)\n\n\n\nAlso on this tab, the user may specify sampling rule parameters, namely the initial dose level for sampling. If using a “3+3” design, the sampling rule parameters will be the only parameters on this screen (and hence the tab will be called “Sampling Rule” in place of “Allocation Rule”), since allocation does not apply to the “3+3” design engine.\n\n\n\n\n\n\nFigure 18: Design & Allocation Rule tab (bCRM)\n\n\n\nIf utilizing a CRM (Toxicity) design, and if the option to model two sample populations has been enabled (this option may be selected on the Study Info tab, as described in this section), the user also has the option to set the initial dose for the second population.\nAdditionally, a single subject run-in can also be specified on the Allocation Rule tab. A single subject run-in starts at the specified starting dose and allocates a single subject to each dose until either a toxicity is observed or the maximum dose is reached. This option is available for CRM (Toxicity), CRM (Ordinal), and bCRM only. Note that subjects in the single subject run-in are not included in the “max trial size”, so with a single subject run-in, it is possible to end a simulation with more subjects that were specified in the max trial size.\nIf a Control treatment arm has been included, the user must also specify the number of subjects per cohort that should be allocated to Control. The Control dose is treated differently than the active doses, and cannot ever be found as the MTD or MED. Thus, it will never be assigned a cohort based on the fitted model, and must have subjects assigned per cohort.\nFinally, if utilizing a CRM (Efficacy) design, the user also has the option on this tab to allocate extra subjects to the min or max dose, as shown in Figure 19. “Probability gamma” is used to ensure that the numbers of subjects allocated to the maximum and minimum stay close to their specified values. Setting gamma (γ) to 0 turns this off, so the actual number allocated may drift away from the number expected. 2 is a good value to enable this correction.\nThe formula used is:\n\\(p = p_T^{\\gamma (p_O - p_T)}\\)\nwhere pT is the target probability as entered above, pO is the observed probability measured from the subjects allocated so far and p is corrected the probability that will be used in allocation. To prevent wild allocations, p is subject to the restrictions, p&lt;0.5 and pT/4 &lt; p &lt; 4pT.\n\n\n\n\n\n\nFigure 19: Allocation Rule (CRM (Efficacy))\n\n\n\nWhen a Control treatment arm is included, it is treated as the minimum dose, and gives the user another option of how to allocate to it. In this case, the user can specify a probability of allocating whole cohorts to the Control, or a number of subjects per cohort allocated to Control. The two options cannot be combined."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#stopping-criteria-crm-efficacy-crm-toxicity-crm-ordinal-n-crm-and-bcrm-only",
    "href": "documentation/v72/userguides/de.html#stopping-criteria-crm-efficacy-crm-toxicity-crm-ordinal-n-crm-and-bcrm-only",
    "title": "3+3 / mTPI",
    "section": "Stopping Criteria (CRM (Efficacy), CRM (Toxicity), CRM (Ordinal), N-CRM and bCRM only)",
    "text": "Stopping Criteria (CRM (Efficacy), CRM (Toxicity), CRM (Ordinal), N-CRM and bCRM only)\nFor any design, including “3+3”, study simulation will always stop when the maximum number of subjects has been achieved.\nThe study simulation may also stop early when the following conditions that have been enabled by the user are met:\n\nMTD finding (CRM (Toxicity), N-CRM and bCRM only)\n\nthe user-specified minimum number of subjects are allocated to the MTD\nAND the following optional rules either ANDed or ORed together.\n\nThe number of dose levels in the specified confidence interval meets the specified the threshold\nThere is a dose with a probability of being the MTD that is greater than the specified threshold.\nThe minimum number of cohorts have been accrued\n\n\nMED finding (CRM (Efficacy) and bCRM only)\n\nthe user-specified maximum number of subjects are allocated to the MED\nthe number of dose levels in the confidence interval meets the specified number\nthere is a dose with a probability of being MED that is greater than the specified threshold\nThe minimum number of cohorts have been accrued\n\nmTPI when the user specified number of subjects are already allocated to the next dose to be allocated – this is specified on the mTPI Design tab.\n\nWhen studying two samples, stopping rules will be evaluated and applied separately for each sample.\nThese parameters are set on the Stopping Criteria sub-tab as depicted below in Figure 20 Stopping tab CRM Toxicity.\n\n\n\n\n\n\nFigure 20: Stopping tab CRM Toxicity\n\n\n\nFor bCRM the stopping rules become the rules for stopping the MTD phase and starting the MED phase. There are then similar rules that can be used for stopping the MED phase early.\n\n\n\n\n\n\nFigure 21: Stopping Criteria (bCRM)\n\n\n\nFinally, if utilizing a CRM (Efficacy) design, the user may specify an additional set of parameters for stopping for futility, as displayed at the bottom of Figure 22. These allow the trial to be stopped early for futility. If the user enable these rules then the user can specify:\n\nThat a minimum number of cohorts must have been allocated to the minimum and maximum doses.\nThat the trial will stop unless there is a minimum difference in the mean response between the minimum and maximum doses.\n\n\n\n\n\n\n\nFigure 22: Stopping Criteria (CRM (Efficacy))"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#to-run-simulations",
    "href": "documentation/v72/userguides/de.html#to-run-simulations",
    "title": "3+3 / mTPI",
    "section": "To run simulations",
    "text": "To run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user may not modify any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#how-many-simulations-to-run",
    "href": "documentation/v72/userguides/de.html#how-many-simulations-to-run",
    "title": "3+3 / mTPI",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%).\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios.\n\nSimulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nAll: A window containing all the summary results columns\nHighlights: (all) a separate window with the results shown on the main tab\nAllocation, Observed: (all) summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity/efficacy/response: (all CRM) summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc / Pr(MED) etc: (all CRM) summary results of the posterior probabilities of the properties of interest. In bCRM these values are organised in three groupings called “Probabilities”, “Toxicity” and “Efficacy”.\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\nTwo Sample results: (CRM Toxicity)\n\nView Graph: opens the FACTS built in graph utility displaying the results for the currently selected scenario. See Section 11 below for a description of the graphs.\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options:\n\nOpen results folder: Opens a file browser in the results folder of the scenario, allowing swift access to any of the results files.\nSimulation results: Opens a window displaying the individual simulation results for each simulation of the currently selected scenario\nOpen in R: opens a control that will launch R, first loading the selected files in the results folder as data frames.\nView Graphs: launches the graph viewer to view the results of the currently selected scenario.\n\n\n\n\nMCMC Settings\n\n\n\n\n\n\nFigure 24\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nIf the Number of MCMC samples to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output, allowing the user to check convergence.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#facts-grid-simulation-settings",
    "href": "documentation/v72/userguides/de.html#facts-grid-simulation-settings",
    "title": "3+3 / mTPI",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#detailed-simulation-results",
    "href": "documentation/v72/userguides/de.html#detailed-simulation-results",
    "title": "3+3 / mTPI",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 25) will reveal more detailed results about the simulation.\n\n\n\n\n\n\nFigure 25: Detailed Simulation Results\n\n\n\nRight-clicking on a row displays a context menu from which the user can also view the corresponding cohort results:\n\n\n\n\n\n\nFigure 26: bCRM Parametric Joint Toxicity & Efficacy VSR - Gumbel Model"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#aggregation",
    "href": "documentation/v72/userguides/de.html#aggregation",
    "title": "3+3 / mTPI",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 27\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose, plus two extra rows for ‘below lowest’ and ‘above highest’.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nIf there is no value for the below lowest or above highest rows, they will be left blank.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. cohortsNNN.csv files are aggregated into a single agg_[pivot_]cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#highlights-all",
    "href": "documentation/v72/userguides/de.html#highlights-all",
    "title": "3+3 / mTPI",
    "section": "Highlights (All)",
    "text": "Highlights (All)\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nEngines\nDescription\n\n\n\n\nSelect\n1\nAll\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nAll\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nAll\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nNum Sims\n1\nAll\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nAll\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nAll\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPPn. Tox\n1\nAll except CRM Efficacy\nThis is the average proportion of the subjects recruited that experienced a toxicity in the simulations of this scenario.\n\n\nSD Ppn. Tox\n1\nAll except CRM Efficacy\nThis is the standard deviation of the proportion of toxicity across the simulations of this scenario.\n\n\nPpn Eff\n1\nCRM Efficacy & bCRM\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nCRM Efficacy & bCRM\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Tox\n1\nAll except CRM Efficacy\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nTrue Ppn Eff\n1\nCRM Efficacy & bCRM\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile of the scenario\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nFor each dose, this is proportion of the simulations where it was selected as the MTD (Maximum Tolerated Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MTD (closest dose to having the target toxicity rate – or “nearest below” or “nearest above” as selected by the user on the Study Info tab.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nCRM Efficacy & bCRM\nFor each dose, this is proportion of the simulations where it was selected as the MED (Minimum Efficacious Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MED (“closest” dose to having the target efficacy rate – or “nearest below” or “nearest above” as selected by the user on the Study Info tab.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nbCRM only\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. The dose chosen as the OSD will be the MED if that is below the MTD, otherwise it will be the MTD.\n\n\nPpn(All Tox)\n1\nAll\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nPpn(early Success)\n1\nAll\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nPpn(Cap)\n1\nAll\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#allocations-observed-all",
    "href": "documentation/v72/userguides/de.html#allocations-observed-all",
    "title": "3+3 / mTPI",
    "section": "Allocations, Observed (All)",
    "text": "Allocations, Observed (All)\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nEngines\nDescription\n\n\n\n\nScenario\n1\nAll\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Subj.\n1\nAll\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Tox\n1\nAll except CRM Efficacy\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nSD PPn. Tox\n1\nAll except CRM efficacy\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nPpn, Eff\n1\nCRM Efficacy and bCRM only\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nCRM Efficacy and bCRM only\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Tox\n1\nAll except CRM Efficacy\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nTrue Ppn Eff\n1\nCRM Efficacy and bCRM only\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile of the scenario\n\n\nNum Phase 1\n1\nbCRM Only\nThe mean (over the simulations) of the number of subjects allocated during phase 1 (the MTD finding part) of the trial.\n\n\nSubj. Per Dose: &lt;dose&gt;\nOne per dose\nAll\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subjects: &lt;dose&gt;\nOne per dose\nAll\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox. Per Dose: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nEff. Per Dose: &lt;dose&gt;\nOne per dose\nCRM Efficacy and bCRM only\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff. Per Dose: &lt;dose&gt;\nOne per dose\nCRM Efficacy and bCRM only\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\n80% Num Subj\n1\nAll\nThis is the 80th centile of the overall number of subjects recruited in each simulation."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#fitted-toxicity-crm-toxicity-crm-ordinal-bcrm",
    "href": "documentation/v72/userguides/de.html#fitted-toxicity-crm-toxicity-crm-ordinal-bcrm",
    "title": "3+3 / mTPI",
    "section": "Fitted Toxicity (CRM Toxicity, CRM Ordinal, bCRM)",
    "text": "Fitted Toxicity (CRM Toxicity, CRM Ordinal, bCRM)\n\n\n\nColumn Title\nNumber of columns\nEngines\n\n\n\n\nScenario\n1\nAll\n\n\nMean Beta 1\n1\nAll\n\n\nSD Beta 1\n1\nAll\n\n\nMean Beta 2\n1\nbCRM only\n\n\nSD Beat 2\n1\nbCRM only\n\n\nPsi\n1\nbCRM only\n\n\nSD Psi\n1\nbCRM only\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nAll\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nTrue Toxicity: &lt;Dose&gt;\nOne per dose\nAll\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nMean Fit Tox Lower: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fit Tox Upper: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fit Eff Lower: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nMean Fit Eff Upper: &lt;dose&gt;\nOnse per dose\nbCRM only"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#fitted-efficacy-crm-efficacy",
    "href": "documentation/v72/userguides/de.html#fitted-efficacy-crm-efficacy",
    "title": "3+3 / mTPI",
    "section": "Fitted Efficacy (CRM Efficacy)",
    "text": "Fitted Efficacy (CRM Efficacy)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Beta 1\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response\n\n\nSD Beta 1\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy model for each dose\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nMean Fit Eff Lower: &lt;dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.\n\n\nMean Fit Eff Upper: &lt;dose&gt;\nOnse per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#prmtd-etc.-crm-toxicity-crm-ordinal-probabilities-etc.-bcrm",
    "href": "documentation/v72/userguides/de.html#prmtd-etc.-crm-toxicity-crm-ordinal-probabilities-etc.-bcrm",
    "title": "3+3 / mTPI",
    "section": "Pr(MTD) etc. (CRM Toxicity, CRM Ordinal, “Probabilities etc.” bCRM)",
    "text": "Pr(MTD) etc. (CRM Toxicity, CRM Ordinal, “Probabilities etc.” bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study. The dose chosen will be the dose with the highest posterior probability of having a toxicity rate that is closest to / nearest below / nearest above the target toxicity rate.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nFor each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose selected will be the MED if that is below the MTD, otherwise it will be the MTD.\n\n\nNum Stop Rule 1\n1\nNumber of times the minimum subjects on MTD was met.\n\n\nNum Stop Rule 2\n1\nNumber of times the number of doses in the credible interval of the estimate of the MTD was met.\n\n\nNum Stop Rule 3\n1\nNumber of times a dose met the required threshold for Pr(MTD).\n\n\nNum Eff Stop Rule 1\n1 (bCRM only)\nNumber of times the minimum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1 (bCRM only)\nNumber of times the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1 (bCRM only)\nNumber of times a dose met the required threshold for Pr(MED).\n\n\nMean Tox CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MTD.\n\n\nSD Tox CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MTD.\n\n\nMean Eff CI\n1 (bCRM only)\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1 (bCRM only)\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MTD): &lt;dose&gt;\nOne per dose\nThe mean (over the simulations) of posterior probability that a dose is the dose nearest / closest below / closest above the target toxicity rate.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose+&gt;\n1\nAs MTD Selection, but allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nMED+ Selection: minus\n1 (bCRM only)\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1 (bCRM only)\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nOSD+ Selection: minus\n1 (bCRM only)\nThe proportion of simulations where a dose below the tested range of doses is the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1 (bCRM only)\nThe proportion of simulations where a dose above the tested range of doses is the optimum selected dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being both at or above the MED and at or below the MTD.\n\n\nPr(MTD+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MTD.\n\n\nPr(MTD+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MTD.\n\n\nPost CE MTD+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as maximum tolerated dose.\n\n\nPost CE MTD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the maximum tolerated dose, allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nPost CE MTD+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as maximum tolerated dose.\n\n\nPost CE MED+: minus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of simulations where, after cohort expansion, the dose was selected as the minimum efficacious dose, allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nPost CE MED+: plus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of simulations where, after cohort expansion, the dose was selected as the optimum selected dose, allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nPost CE OSD+: plus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as optimum selected dose."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#prmed-etc.-crm-efficacy",
    "href": "documentation/v72/userguides/de.html#prmed-etc.-crm-efficacy",
    "title": "3+3 / mTPI",
    "section": "Pr(MED) Etc. (CRM Efficacy)",
    "text": "Pr(MED) Etc. (CRM Efficacy)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study. The dose chosen will be the dose with the highest posterior probability of having an efficacy rate in closest to / nearest below o/ nearest above the target efficacy rate..\n\n\nNum Eff Stop Rule 1\n1\nNumber of times the maximum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times a dose met the required threshold for Pr(MED).\n\n\nMean Eff CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MED): &lt;dose&gt;\n\nThe mean (over the simulations) of posterior probability that a dose is MED.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose+&gt;\nOne per dose\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nPpn Early Futility\n1\nThe proportion of simulations where the trials stopped because the early futility rule was met.\n\n\nPr(MED+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MED.\n\n\nPr(MED+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MED."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#toxicity-bcrm",
    "href": "documentation/v72/userguides/de.html#toxicity-bcrm",
    "title": "3+3 / mTPI",
    "section": "Toxicity (bCRM)",
    "text": "Toxicity (bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nPpn Tox\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nSD Ppn Tox\n1\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nTrue Ppn Tox\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nMean Beta 1\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the toxicity response.\n\n\nSD Beta 1\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the toxicity response.\n\n\nPsi\n1\nThe mean estimate over the simulations of the overall probability of observing both a toxicity and efficacy response in a subject.\n\n\nSD Psi\n1\nThe standard deviation of the estimate of Psi over the simulations.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD (Maximum Tolerated Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MTD (closest dose to having the target toxicity rate – or nearest below or above as selected by the user.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. This will be the MED if it is below the MTD, otherwise it will be the MTD.\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nNum Stop Rule 1\n1\nNumber of times simulations stopped because maximum subjects on MTD was met.\n\n\nNum Stop Rule 2\n1\nNumber of times simulations stopped because the number of doses in the credible interval was met.\n\n\nNum Stop Rule 3\n1\nNumber of times simulations stopped because Pr(MTD) was met.\n\n\nMean Tox CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MTD.\n\n\nSD Tox CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MTD.\n\n\nPr(MTD): &lt;dose&gt;\nOne per dose\nThe posterior probability that a dose is the dose nearest / closest below / closest above the target toxicity rate.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose+&gt;\n1\nAs MTD Selection, but allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nOSD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose+&gt;\nOne per dose\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was the optimum selected dose.\n\n\nPpn Best: &lt;dose&gt;\nOne per dose\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being at or above the MED and at or below the MTD.\n\n\nPr(MTD+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MTD.\n\n\nPr(MTD+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MTD.\n\n\nTrue Toxicity: &lt;dose&gt;\nOne per dose\nThis is the true simulated toxicity rate for each dose in the scenario"
  },
  {
    "objectID": "documentation/v72/userguides/de.html#efficacy-bcrm",
    "href": "documentation/v72/userguides/de.html#efficacy-bcrm",
    "title": "3+3 / mTPI",
    "section": "Efficacy (bCRM)",
    "text": "Efficacy (bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nPpn Eff\n1\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Eff\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nMean Beta 2\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response.\n\n\nSD Beta 2\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response.\n\n\nPsi\n1\nThe mean estimate over the simulations of the overall probability of observing both a toxicity and efficacy response in a subject.\n\n\nSD Psi\n1\nThe standard deviation of the estimate of Psi over the simulations.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED (Minimum Efficacious Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MED (closest dose to having the target toxicity rate – or nearest below or above as selected by the user.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. This will be the MED if it is below the MTD, otherwise it will be the MTD.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy model for each dose\n\n\nSD Fitted Effiacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nEff. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\nNum Eff Stop Rule 1\n1\nNumber of times simulations stopped because the maximum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times simulations stopped because the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times simulations stopped because a dose met the required threshold for Pr(MED).\n\n\nMean Eff CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MED): &lt;dose&gt;\nOne per dose\nThe mean (over the simulations) of the posterior probability that a dose is the dose nearest / closest below / closest above the target efficacy rate.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose+&gt;\nOne per dose\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nOSD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose+&gt;\nOne per dose\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was the optimum selected dose.\n\n\nPpn Best: &lt;dose&gt;\nOne per dose\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being both at or above the MED and at or below the MTD.\n\n\nPr(MED+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MED.\n\n\nPr(MED+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MED.\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nPost CE MED+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the minimum efficacious dose, allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nPost CE MED+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the optimum selected dose, allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nPost CE OSD+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as optimum selected dose."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#allocation-box-and-whisker-plot-all",
    "href": "documentation/v72/userguides/de.html#allocation-box-and-whisker-plot-all",
    "title": "3+3 / mTPI",
    "section": "Allocation Box and Whisker plot (All)",
    "text": "Allocation Box and Whisker plot (All)\n\n\n\n\n\n\nFigure 29: Allocation Box and Whisker plot\n\n\n\nThis graph displays a box and whisker plot of the number of subjects allocated to each dose over the simulations. These plots show:\n\nThe mean allocation over all simulations at each dose plotted as a solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#resp-and-subject-alloc-all-crm",
    "href": "documentation/v72/userguides/de.html#resp-and-subject-alloc-all-crm",
    "title": "3+3 / mTPI",
    "section": "Resp and Subject Alloc (all CRM)",
    "text": "Resp and Subject Alloc (all CRM)\n\n\n\n\n\n\n\nCRM Toxicity, CRM Ordinal\nCRM Efficacy\n\n\n\n\n\n\n\n\nbCRM\n\n\n\n\n\n\n\n\nThese graphs show the mean subject allocation to each dose as a blue bar, along with, as appropriate lines showing the mean estimated toxicity/efficacy and the simulated ‘true’ toxicity/efficacy. The ‘error’ bars on the mean estimated toxicity/efficacy are the 95% interval of the mean estimates across the simulations.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#distribution-of-mtd-med-osd-and-te",
    "href": "documentation/v72/userguides/de.html#distribution-of-mtd-med-osd-and-te",
    "title": "3+3 / mTPI",
    "section": "Distribution of MTD, MED, OSD and TE",
    "text": "Distribution of MTD, MED, OSD and TE\nMTD: Maximum Tolerated Dose\nMED: Minimum Effective Dose\nOSD: Optimum Selected Dose – if the MED is below the MTD then the OSD is the MED, otherwise it is the MTD.\nTE: Tolerated and Effective – the probability the dose is both below the MTD and above the MTD. In the results file this is under the column heading “Pr(Good)”.\n\n\n\n\n\n\n\n3+3 version\nCRM Toxicity, CRM Ordinal, bCRM - MTD\n\n\n\n\n\n\n\n\nCRM Efficacy, bCRM - MED\nbCRM – OSD and TE\n\n\n\n\n\n\n\nThese graphs show the proportion of times each dose has been selected as a particular target dose as a brown bar, along with lines showing the mean estimated toxicity/efficacy and the simulated ‘true’ toxicity/efficacy. The ‘error’ bars on the mean estimated toxicity/efficacy are the 95% interval of the mean estimates across the simulations.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#toxicity-and-allocation-obs-tox-and-alloc-obs-efficacy-and-allocation",
    "href": "documentation/v72/userguides/de.html#toxicity-and-allocation-obs-tox-and-alloc-obs-efficacy-and-allocation",
    "title": "3+3 / mTPI",
    "section": "“Toxicity and Allocation” / “Obs Tox and Alloc” / “Obs Efficacy and Allocation”",
    "text": "“Toxicity and Allocation” / “Obs Tox and Alloc” / “Obs Efficacy and Allocation”\n\n\n\n\n\n\n\n3+3 version, CRM Toxicity, CRM Ordinal, bCRM\nCRM Efficacy, bCRM\n\n\n\n\n\n\n\n\n\nThese graph show the mean allocation to each dose and the mean number of toxicities/efficacies observed at each dose.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#mtd-credible-interval",
    "href": "documentation/v72/userguides/de.html#mtd-credible-interval",
    "title": "3+3 / mTPI",
    "section": "MTD Credible Interval",
    "text": "MTD Credible Interval\n\n\n\n\n\n\nFigure 30\n\n\n\nCurrently only available for CRM Toxicity, this histogram shows the distribution of the final number of doses in the MTD credible interval.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#sample-size-mtd-histogram-all-crm",
    "href": "documentation/v72/userguides/de.html#sample-size-mtd-histogram-all-crm",
    "title": "3+3 / mTPI",
    "section": "Sample Size MTD Histogram (All CRM)",
    "text": "Sample Size MTD Histogram (All CRM)\n\n\n\n\n\n\nFigure 31\n\n\n\nThis graph plots the distribution of the final sample sizes a a stacked bar chart with different shades of color indicating the proportion of simulations with that sample size had selected the different doses as MTD."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#simulation-allocation-history",
    "href": "documentation/v72/userguides/de.html#simulation-allocation-history",
    "title": "3+3 / mTPI",
    "section": "Simulation Allocation History",
    "text": "Simulation Allocation History\n\n\n\n\n\n\n\n\n\n\n\n\n\n3+3 version, CRM Toxicity, bCRM\nCRM Ordinal\n\n\n\n\n\n\nCRm Efficacy, bCRM\n\n\n\n\nThis graph shows the allocation and toxicity/efficacy history for an individual simulation. Each dot represents a subject, the dot is colored red if toxicity was observed, light pink for a mild toxicity, blue for efficacy and grey otherwise. The subjects’ outcomes are displayed left to right in the order in which they were dosed, and at a height corresponding to the dose they were given.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed. A history can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#cohort-responses-sample-all-crm-except-efficacy",
    "href": "documentation/v72/userguides/de.html#cohort-responses-sample-all-crm-except-efficacy",
    "title": "3+3 / mTPI",
    "section": "Cohort Responses Sample (All CRM except Efficacy)",
    "text": "Cohort Responses Sample (All CRM except Efficacy)\n\n\n\n\n\n\n\nCRM Toxicity\nCRM Ordinal\n\n\n\n\n\n\n\n\nbCRM\n\n\n\n\n\n\n\n\nThis graph shows the dose allocation and resulting toxicities/efficacies along with the fitted dose-toxicity/efficacy model for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample."
  },
  {
    "objectID": "documentation/v72/userguides/de.html#mtd-change-on-expansion",
    "href": "documentation/v72/userguides/de.html#mtd-change-on-expansion",
    "title": "3+3 / mTPI",
    "section": "MTD Change on Expansion",
    "text": "MTD Change on Expansion\n\n\n\n\n\n\nFigure 32: MTD change on expansion graph\n\n\n\nThis graph plots the dose selected as MTD before the results of the expansion cohort are available, against the dose selected as MTD after the expansion cohort results are available. Each simulation result is plotted as a ‘dot’, and the graph is divided up into a grid and dots located in the correct cell in the grid and then arranged within the cell depending on how many results fall into the cell. This allows the relative number of results in each cell to be appraised quickly by eye.\nThis graph allows the trial designer to see the degree of risk that the determination of the MTD will change as a result of the expansion cohort and whether the risk is predominantly that the determination will increase or decrease.\nThere are similar graphs showing the change in the estimate of the MED and OSD after the expansion cohort."
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html",
    "href": "documentation/v72/userguides/FACTSfromR.html",
    "title": "FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate."
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "href": "documentation/v72/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "title": "FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate."
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html#software-prerequisites",
    "href": "documentation/v72/userguides/FACTSfromR.html#software-prerequisites",
    "title": "FACTS from R",
    "section": "Software prerequisites",
    "text": "Software prerequisites\nTo call FACTS from R, you will need the following installed on your Windows machine on which you are using FACTS:\n\nA reasonably recent version of R (v3.5.3+).\nFACTS v6.4 or later, the command line executable versions of the FACTS simulation engines – these are currently available to Enterprise licensees.\nThe supplied R file: factR.R"
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html#factr.r",
    "href": "documentation/v72/userguides/FACTSfromR.html#factr.r",
    "title": "FACTS from R",
    "section": "factR.R",
    "text": "factR.R\nProvides an ‘R’ wrapper for accessing FACTS analysis models for:\n\nCore and Enrichment Design, allowing you to use the following FACTS analysis features:\n\nDose Response models\nLongitudinal models\nHierarchical Prior on Control (borrowing from historical data)\nTTE predictor endpoint\nBAC\n\nInputs\n\nFACTS param file with trial info and model specifications\nData file\n\nOutput\n\nMCMC file"
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "href": "documentation/v72/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "title": "FACTS from R",
    "section": "Steps for calling FACTS from R",
    "text": "Steps for calling FACTS from R\nTo call FACTS from R, you will need to do the following sequence of steps:\n\nCreate a (non-adaptive) FACTS project for your Engine type with the general study info: Number of Arms, number and timing of Visits (if using), Dose response (& longitudinal if using) model specification and MCMC setup.\nConfigure VSR and Execution profiles to allow a simple simulation run.\nRun 1 simulation to produce:\n\nA ‘param’ file which will be passed as an input to the R function.\nA ‘patients’ file. This may be useful to illustrate data file format for the input data. See FACTS Execution Guides for details.\nAn ‘mcmc’ file. This will show you what to expect in the output MCMC file.\n\nIf using FACTS to analyze a data set, then\n\nput the data set into the required format\nwrite an R script to call FACTS with the data set\nprocess the MCMC output\n\nIf using FACTS within a simulation framework, then:\n\nWrite an R script that generates the data you wish to simulate and pass to FACTS to analyze\nWrite a loop that\n\ngenerates the data for a simulation\ncalls FACTS with generated data\nprocess the MCMC output\naccumulate the statistics for the overall operating characteristics to be computed\n\nOutput the resulting OCs"
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html#runfacts-usage-notes",
    "href": "documentation/v72/userguides/FACTSfromR.html#runfacts-usage-notes",
    "title": "FACTS from R",
    "section": "runFACTS() Usage Notes",
    "text": "runFACTS() Usage Notes\n\nRun FACTS MCMC Model from R\nrunFACTS(\n engine, \n data.file = “patients.dat”, \n param.file = “nuk1_e.param, \n mcmc.file.num = 0, \n rng.seed = 1, \n exec.path = getwd()\n)\nReturn Value: runFACTS returns a TRUE/FALSE to indicate a successful/failed execution. In case of errors, R error messages may be printed and in case of a FACTS execution error, a file called ‘error.txt’ will be output, containing the error description.\nArguments:\n\nengine: Name of the FACTS engine to use. Can be one of the following:\n\nFor Core Engines: “contin”, “dichot”, “ME”, “TTE”\nFor Enrichment Design Engines: “ed_contin”, “ed_dichot”, “ed_tte”\n\ndata.file: Name of the input data file. Default is “patients.dat”. This file format should exactly match the file format of the ‘patients’ file corresponding to the ones produced by FACTS for the design you setup in FACTS to specify the analysis model. (See the FACTS Execution Guide under the FACTS Help menu for details.)\nparam.file: Name of the FACTS ‘.param’ file that specifies the model setup. Default is ‘nuk1_e.param’.\nmcmc.file.num: The MCMC output is written to a file named ’mcmcNNNNN.csv. This argument set the NNNNN. Therefore, mcmc.file.num = 1 will create an MCMC output file called mcmc00001.csv. Default value is 0.\nrng.seed: Integer-valued random number generator seed. Will use the value from the ‘.param’ file if unspecified.\nexec.path The path to the directory where the FACTS executable program is available. Default is the current working directory.\n\n\n\nSet Up Files and Folders\nIt is important to pass files and parameters correctly, as there is not much in the way of helpful error messaging. Setting up the required folder and files is not hard but should be done carefully. The following example shows how."
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "href": "documentation/v72/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "title": "FACTS from R",
    "section": "The “core.dichot.example.facts”",
    "text": "The “core.dichot.example.facts”\nIn this example we wish to use FACTS to fit a simple NDLM dose response model across 6 arms (control & 5 doses) with a dichotomous endpoint.\nWe have entered the following parameters:\n\nStudy:\n\nStudy Info:\n\nNon-adaptive\nRecruit subjects continuously\nMax subjects: 300\nResponse is a positive outcome\nTime to final endpoint: 4 weeks\n\nTreatment Arms:\n\nControl and 5 doses with strengths 1, 2, … 5\n\n\nVirtual Subject Response\n\nExplicitly defined\n\nDose Response\n\nresponses: 0.1, 0.1, 0.125, 0.15, 0.2, 0.25\n\n\n\nExecution\n\nAccrual\n\n1 region with mean accrual of 5 subjects per week\n\nDropout\n\nNo dropouts\n\n\nQuantities of Interest\n\nPosterior probability: Pr(P_d &gt; P_Control)\nProbability of being target: Pr(Max)\nDecision Quantity: Pr(P_d &gt; P_Contorl); d=Greatest Pr(Max)\n\nDesign\n\nDose Response\n\nSimple NDLM\n\nInitial Dose ~N(0,22)\nTau IG(1,1) “central value”, “weight”\n\n\nRequentist analysis: none\nAllocation: 1:1:1:1:1:1\nSuccess/Futility Criteria\n\nSuccess: Pr(P_d &gt; P_Control); d= Greatest Pr(Max) &gt; 0.9\n\n\n\nNot all these parameters will effect our analysis, but we have to enter sufficient parameters to be able to run a simulation and get a bin1_e.param file. This can be found in the scenario simulation results folder. We only need to run 1 simulation on order to have one written out. This file is copied to our “Example” directory. If we want to change something in the analysis – the model or the prior for example, we can modify this facts file, re-run one simulation, and copy the new bin1_e.param file."
  },
  {
    "objectID": "documentation/v72/userguides/FACTSfromR.html#dichot-demo.r",
    "href": "documentation/v72/userguides/FACTSfromR.html#dichot-demo.r",
    "title": "FACTS from R",
    "section": "dichot-demo.R",
    "text": "dichot-demo.R\nWe start by setting the current working directory to the “Example” folder, and setting up some file locations and sourcing the factR.R file.\n## Set up Folders and Paths\n\n# This is the directory where the parameter file and patient data must be located\n# It will be where the MCMCM files are written\nsetwd(\"Z:/FACTS test/FACTS 6 Training/FACTS R interface/Example\")\n\n# This must be the location of the factR.R file\nFactR.src = \"../factR.R\"\n\n# This must be the location of the executable files\nExec.dir = \"../WindowsExecutables\"\n\n# Load runFACTS\nsource(FactR.src)\nWe can then copy an example patients file from the simulation results and check that we can run facts.\n# Test to check its working\n# Copy an example patients file from the simulations results to this folder before running.\nsystem.time(\n  runFACTS(\n    engine='dichot', \n    data.file = 'patients00001.csv', \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 1, \n    rng.seed = 1, \n    exec.path = Exec.dir\n  )\n)\n\ngenBinaryData()\nWe now define a simple function to generate the data for a single run of FACTS.\n# generates a data frame that can be used to drive a FACTS analysis\n# dichotomous endpoint\n# no visits\n# n.per.arm: int, the number of subjects to be simulated for each arm\n# rates: int[], the response rate to be simulated for each arm\n# the length if rates defines the number of arms\n# returns a dataframe with n.per.arm * length(rates) simulated subjects\n\ngenBinaryData &lt;- function(nPerArm, rates) {\n  \n  patientID &lt;- 1:(nPerArm * length(rates)) # Generate a list of patients\n  region &lt;- rep(1, nPerArm * length(rates)) # all patients come from region 1\n  date &lt;- 1:(nPerArm * length(rates)) # Generate a list of enrolment dates - here simply one per day\n  doseAlloc &lt;- rep(1:length(rates), each = nPerArm) # Allocate patients equally to each dose\n  lastVisit &lt;- rep(1, nPerArm * length(rates)) # all patients have last visit data\n  dropout &lt;- rep(0, nPerArm * length(rates)) # no patients have dropped out\n  baseline &lt;- rep(-9999, nPerArm * length(rates)) # not simulating baseline\n  visit1 &lt;- rep(0, nPerArm * length(rates)) # create the outcome vector\n\n  for (d in 1:length(rates)){ # get responses for each dose\n  ix &lt;- which(doseAlloc == d) # get indices of patients on dose d\n  # assign them a final response based on the rate to simulate for dose d\n  if (length(ix) &gt; 0) {\n  visit1[ix] &lt;- \n    sample(\n     c(0,1), \n     size = length(ix), \n      replace = TRUE, \n      prob = c(1-rates[d], rates[d])\n    )\n  }\n  \n}\n\ndat &lt;- data.frame(\n  SubjectID = patientID, \n  Region = region, \n  Date = date, \n  Dose = doseAlloc, \n  LastVisit = lastVisit, \n  Dropout = dropout, \n  Baseline = baseline, \n  Visit1 = visit1, \n  row.names = NULL\n)\n\nreturn(dat)\n}\n\n\nrunSims\n########### Toy Example Trial Sim ##########\n### Constants\nDATAFILE = \"patients.csv\"\nMCMCFILE = \"mcmc00000.csv\"\n# function to simulate an example data set with dichotomous endpoint\n# nSims - the number of sims to run\n# nBurnin - the number of MCMC smaples to discard\n# (the number of MCMC samples is specified in the parameter file)\n# details - a boolean. If TRUE the function returns a data frame with\n# the results of each individual simulation,\n# otherwise just the win proportion and probabilities of being control\n\nrunSims &lt;- function(\n    nSims = 10, \n    nBurnin = 1000, \n    rates = c(0.1, 0.1, 0.125, 0.15, 0.2, 0.25),\n    details = FALSE\n) {\n\nwinPpn = 0\npr.gt.ctl.sum &lt;- rep(0, length(rates) - 1)\nif (details) {\nperSim &lt;- data.frame(Sim = 1)\n}\n\nfor(sim in 1:nSims) {\ndat = genBinaryData(nPerArm = 50, rates = rates)\nwrite.csv(dat,DATAFILE, row.names = FALSE)\nif (details) {\nperSim[sim, \"Sim\"] &lt;- sim\n# record true rates and observed rates\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"sim.rate.\", d, sep=\"\")] &lt;- rates[d]\n}\n\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"obs.rate.\", d, sep=\"\")] &lt;- mean(dat[dat[,\"Dose\"]==d, \"Visit1\"])\n}\n}\n\ncat(\"run FACTS: \", sim, \"\\n\")\n\nret &lt;- \n  runFACTS(\n    engine = 'dichot', \n    data.file = DATAFILE, \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 0, \n    rng.seed = sim, \n    exec.path = Exec.dir\n  )\n\ndat = read.csv(MCMCFILE, skip = 1)\n\n# discard burnin rows and just estimates of rate - the \"Pi\" columns\ndat = dat[(nBurnin + 1):nrow(dat), grep(\"Pi\", names(dat))]\n\nif (details) {\n# record est rate\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"est.rate.\", d, sep=\"\")] &lt;- mean(dat[,paste(\"Pi.\", d, sep=\"\")])\n}\n}\n\n# success if the first dose is not in the top 2 .. i.e. the resposnse on any 2 doses is &gt; control\n\nsuccess &lt;- apply(dat, 1, FUN = function(x) {ifelse(length(x) - which(order(x)==1) &gt;= 2, 1, 0)})\n\nif (details) {\nperSim[sim, \"Pr.Success\"] &lt;- mean(success)\nperSim[sim, \"Success.flag\"] &lt;- ifelse(mean(success) &gt; 0.9, 1,0)\n}\n\nwinPpn = winPpn + ifelse(mean(success) &gt; 0.9, 1,0)\n\n# example: calc pr(theta_d &gt; theta_ctl)\ngt.ctl.flag &lt;- apply(dat, 1, FUN = function(x){x[2:length(x)] &gt; x[1]})\npr.gt.ctl &lt;- apply(gt.ctl.flag,1,sum)\npr.gt.ctl &lt;- pr.gt.ctl / length(gt.ctl.flag[1,])\npr.gt.ctl.sum &lt;- pr.gt.ctl.sum + pr.gt.ctl\n\nif (details) {\nfor (d in 1:length(pr.gt.ctl)) {\nperSim[sim, paste(\"Pr.pi.\", d+1, \"&gt;pi_ctl\", sep=\"\")] &lt;- pr.gt.ctl[d]\n    }\n  }\n}\n\ncat(\"win proportion: \", winPpn/nSims, \"\\n\")\n\nif (details) {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims, perSim))\n} else {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims))\n}\n}\n\n\nRunning the Example\nHaving sourced the above functions and variables holding paths and fllenames we can run the simulations with the default scenario:\nrunSims(details=FALSE)\n\n&gt; run FACTS: 1\n&gt; run FACTS: 2\n&gt; run FACTS: 3\n&gt; run FACTS: 4\n&gt; run FACTS: 5\n&gt; run FACTS: 6\n&gt; run FACTS: 7\n&gt; run FACTS: 8\n&gt; run FACTS: 9\n&gt; run FACTS: 10\n&gt; win proportion: 0.3\n\n&gt; [[1]]\n&gt; [1] 0.3\n\n&gt; [[2]]\n&gt; Pi.2 Pi.3 Pi.4 Pi.5 Pi.6\n&gt; 0.30572 0.45824 0.51936 0.77944 0.92916\nIf “details” is set to TRUE then the list of results has a dataframe at the end that contains a row per simulation and details of that simulations results.\nHopefully this is sufficient to get you started."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/index.html",
    "href": "documentation/v72/userguides/enrichment/index.html",
    "title": "Enrichment Designs",
    "section": "",
    "text": "This document covers the design options that are common across the three FACTS Enrichment Design Engines: Continuous, Dichotomous and Time-to-Event. Some design elements are shared across design engines, in which case there is only a single description of them. Others differ based on the endpoint used, in which case separate pages have been created to describe each.\nThe screenshots provided are specific to a particular installation and  may not reflect the exact layout Screenshots from earlier versions of FACTS 6 are still used only when the tabs they show are unchanged in FACTS 7.1.  of the information seen by any particular user. They were taken from FACTS V7 & V6 installed on Windows 10 or 11. If FACTS is installed on different versions of Windows, or with different Windows ‘themes’ there will be some differences in appearance introduced by Windows. The contents of each tab, however, will generally be consistent with the screenshots in this document.\n\n\nThis is the version of the user guide for inclusion with the FACTS 7.1 release.\n\n\n\nPlease cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/index.html#facts-version",
    "href": "documentation/v72/userguides/enrichment/index.html#facts-version",
    "title": "Enrichment Designs",
    "section": "",
    "text": "This is the version of the user guide for inclusion with the FACTS 7.1 release."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/index.html#citing-facts",
    "href": "documentation/v72/userguides/enrichment/index.html#citing-facts",
    "title": "Enrichment Designs",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis is the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Odds Ratio: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of odds ratio for the study treatment in each group.\n\n\nMean Odds Ratio: Across groups\n1\nThis is the mean (over the simulations) of the across groups odds ratio (the estimate across the groups of the odds ratio between response rate on the study treatment and the historic control rate or the mean response rate on the control arm).\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response rate for the control arm in each group.\n\n\nMean Ctrl Response: Across Group\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response rate on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the response rate on the treatment arms for each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the average treatment response rates for each group.\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response rate for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response rate for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response rate for the scenario\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response rate in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations that each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups’ success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups’ futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\n\n\n\n\nThis is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall OR\n1\nThe mean (over the simulations) overall Odds Ratio\n\n\nSE Mean Overall OR\n1\nThe SE (over the simulations) of the mean overall Odds Ratio\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Chi-Square test in each group.\n\n\nppn Significant(Fisher Exact) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Fisher’s exact test in each group.\n\n\nPpn Significant (Binomial) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Binomial test in each group.\n\n\nMean Trt Effect (By Group) &lt;group&gt;\nG\nThe mean (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nSE Mean Trt Effect (by Group) &lt;group&gt;\nG\nThe SE (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\npSignificant (Ctl)\n1\nThe proportion of simulations with an overall significant control effect.\n\n\npSignificant (CMH)\n1\nThe proportion of simulations with an overall significant effect using Cochran-Mantel-Haenszel test.\n\n\npSignificant (Breslow-Day)\n1\nThe proportion of simulations with an overall significant effect using Breslow-Day test."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#highlights",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#highlights",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis is the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#allocation",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#allocation",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#response",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#response",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Odds Ratio: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of odds ratio for the study treatment in each group.\n\n\nMean Odds Ratio: Across groups\n1\nThis is the mean (over the simulations) of the across groups odds ratio (the estimate across the groups of the odds ratio between response rate on the study treatment and the historic control rate or the mean response rate on the control arm).\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response rate for the control arm in each group.\n\n\nMean Ctrl Response: Across Group\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response rate on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the response rate on the treatment arms for each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the average treatment response rates for each group.\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response rate for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response rate for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response rate for the scenario\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response rate in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#observed",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#observed",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#probabilities",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#probabilities",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#stopping-rules",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#stopping-rules",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#evaluation-rules",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#evaluation-rules",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations that each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups’ success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups’ futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#hierarchical-prior-parameters",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#hierarchical-prior-parameters",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#simulation-results",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#simulation-results",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "This is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#frequentist-results",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#frequentist-results",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall OR\n1\nThe mean (over the simulations) overall Odds Ratio\n\n\nSE Mean Overall OR\n1\nThe SE (over the simulations) of the mean overall Odds Ratio\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Chi-Square test in each group.\n\n\nppn Significant(Fisher Exact) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Fisher’s exact test in each group.\n\n\nPpn Significant (Binomial) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Binomial test in each group.\n\n\nMean Trt Effect (By Group) &lt;group&gt;\nG\nThe mean (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nSE Mean Trt Effect (by Group) &lt;group&gt;\nG\nThe SE (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\npSignificant (Ctl)\n1\nThe proportion of simulations with an overall significant control effect.\n\n\npSignificant (CMH)\n1\nThe proportion of simulations with an overall significant effect using Cochran-Mantel-Haenszel test.\n\n\npSignificant (Breslow-Day)\n1\nThe proportion of simulations with an overall significant effect using Breslow-Day test."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-summary.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-summary.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Project\n1\nThe name of the “.facts” file in the FACTS GUI that was used to generate the simulations.\n\n\nScenario\n1\nThe name of the scenario – this is the various profile names that make up the scenario, concatenated together.\n\n\nTimestamp\n1\nThe date and time when the simulations started.\n\n\nVersion\n1\nThe version number of the FACTS GUI that ran the simulations.\n\n\nNsim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\n80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nMean Alloc &lt;Group&gt; \n2*G\nThe mean number of subjects recruited into each arm in each group. If a control arm is not included the column is still present, with value ‘0’.\n\n\nMean Odds Ratio &lt;Group&gt;\nG\nThe mean of the response odds ratio estimates between treatment and control.\n\n\nMean Odds Ratio (Overall)\n1\nThe mean of the means of the response odds ratio estimates across all groups.\n\n\nSE Odds Ratio &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of the response odds ratio in each group.\n\n\nSE Odds Ratio (Overall)\n1\nThe standard error, over the simulations, of the mean of the mean estimates of the response odds ratio across all groups.\n\n\nMean Trt Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response rate on the study treatment arm in each group.\n\n\nMean Trt Effect (Overall)\n1\nThe mean of the mean estimate of the across groups treatment response rate.\n\n\nSE Trt Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response rate on the study treatment arm in each group.\n\n\nSE Trt Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups treatment response rate.\n\n\nMean Control Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response rate on the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\nThe mean of the average of the control response rate across the groups.\n\n\nSE Control Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response rate on the control arm in each group.\n\n\nSE Avg. Control Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups control response rate.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\nTrue mean treatment response for the scenario.\n\n\nTrue Mean Control Resp &lt;Group&gt;\nG\nTrue mean control response for the scenario.\n\n\nMean Mu Theta\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean log-odds study treatment response rate from control in each group.\n\n\nSE Mu Theta\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean log-odds study treatment response rate from control in each group.\n\n\nMean Tau Theta\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the mean log-odds study treatment response rate from control in each group.\n\n\nSE Tau Theta\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the log-odds study treatment response rate from control in each group.\n\n\nMean Mu Gamma\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nSE Mu Gamma\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nMean Tau Gamma\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nSE Tau Gamma\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nMean Pr Ph3 Success &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of success in phase 3 of the study treatment in each group.\n\n\nMean Pr Ph3 Success 99\n1\nThe mean, across the simulations, of the mean probability of success in phase 3 of the across groups treatment difference.\n\n\nMean Pr CSD (Success) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for success in each group.\n\n\nMean Pr CSD (Success) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for success.\n\n\nMean Pr CSD (Futility) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for futility in each group.\n\n\nMean Pr CSD (Futility) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for futility.\n\n\nPpn CSD Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD success criterion.\n\n\nPpn Ph3 Success &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility &lt;Group&gt;\nG\nThe proportion of simulations that each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility 99\n1\nThe proportion of simulations that the across groups treatment effect met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success &lt;Group&gt;\nG\nThe proportion of simulations that each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success 99\n1\nThe proportion of simulations that the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations that each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThe proportion of simulations where the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations that each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThe proportion of simulations where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn CSD Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility (Final) 99\n1\nThe proportion of simulations in which after all data has been collected the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its futility criterion in each group.\n\n\nPpn Ph3 Futility (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its CSD success criterion.\n\n\nPpn Ph3 Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its success criterion in each group.\n\n\nPpn Ph3 Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility (Final) 99\n1\nThe proportion of simulations where the across groups treatment effect after all data has been collected met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success (Final) 99\n1\nThe proportion of simulations where after all data has been collected the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where after all data has been collected the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations after all data has been collected where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn Outcome 1\n1\nThe proportion of simulations that stopped early for success.\n\n\nPpn Outcome 2\n1\nThe proportion of simulations that reached full accrual and declared success on final evaluation.\n\n\nPpn Outcome 3\n1\nThe proportion of simulations that reached full accrual and declared futility on final evaluation.\n\n\nPpn Outcome 4\n1\nThe proportion of simulations that stopped early for futility.\n\n\nPpn Outcome 5\n1\nThe proportion of simulations that stopped early for success but were deemed futile on final evaluation.\n\n\nPpn Outcome 6\n1\nThe proportion of simulations that stopped early for futility but were deemed successful on final evaluation.\n\n\nPpn Outcome 7\n1\nThe proportion of simulations that reached full accrual and were inconclusive.\n\n\nMean Study Accrual Stop Week\n1\nThe mean study duration of accrual – from start of accrual to last patient first visit.\n\n\nMean BAC Mu &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Mu &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nMean BAC Tau &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Tau &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-summary_freq.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-summary_freq.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall OR\n1\nThe mean (over the simulations) overall Odds Ratio\n\n\nSE Mean Overall OR\n1\nThe SE (over the simulations) of the mean overall Odds Ratio\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Chi-Square Test in each group.\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Fisher exact test in each group.\n\n\nPpn Significant (Binomial) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Binomial test in each group.\n\n\nMean Trt Effect (By Group) &lt;group&gt;\nG\nThe mean (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nSE Mean Trt Effect (by Group) &lt;group&gt;\nG\nThe SE (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\npSignificant (Ctl)\n1\nThe proportion of simulations with an overall significant control effect.\n\n\npSignificant (CMH)\n1\nThe proportion of simulations with an overall significant effect using Cochran-Mantel-Haenszel test.\n\n\npSignificant (Breslow-Day)\n1\nThe proportion of simulations with an overall significant effect using Breslow-Day test."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n#Sim\n1\n✔\n\nSimulation number\n\n\nWeeks (Duration)\n1\n✔\n\nThe week of final analysis – the total duration of the simulation.\n\n\n#Week\n1\n\n✔\nWeek\n\n\nNo.Subj\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nAlloc &lt;Group&gt; \n2 * G\n✔\n✔\nThe number of subjects allocated to each arm in each group.\n\n\nMean Odds Ratio\nG\n✔\n✔\nThe estimated mean response odds ratio between the treatment and control arms\n\n\nMean Trt Resp &lt;Group&gt;\nG\n✔\n✔\nThe estimated mean response of the study treatment in the group.\n\n\nTrt Effect (Overall)\n1\n✔\n✔\nThe estimated mean treatment difference across the groups.\n\n\nSD Mean Trt Resp &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the study treatment in the group.\n\n\nSD Trt Effect (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the mean treatment difference across the groups.\n\n\nTrt Resp Lower\nG\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the treatment response.\n\n\nTrt Resp Lower (Overall)\n1\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the treatment response across the groups\n\n\nTrt Resp Upper\nG\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the treatment response.\n\n\nTrt Resp Upper (Overall)\n1\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the treatment response across the groups\n\n\nMu Theta\n1\n✔\n✔\nThe mean of the hierarchical distribution of the treatment differences over all the groups.\n\n\nTau Theta\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the treatment differences over all the groups.\n\n\nMean Control Resp &lt;Group&gt;\nG\n✔\n✔\nThe estimated mean response of the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\n✔\n✔\nThe average overall mean response of the control arm over all the groups. (Note: this is not part of the response model, but estimated separately).\n\n\nSD Mean Control Resp &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the control arm in each group.\n\n\nSD Avg. Control Resp (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the average response over all the control arms.\n\n\nControl Resp Lower\nG\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the control response.\n\n\nAvg. Control Resp Lower (Overall)\n1\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the control response across the groups\n\n\nControl Resp Upper\nG\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the control response.\n\n\nAvg. Control Resp Upper (Overall)\n1\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the control response across the groups\n\n\nMu Gamma\n1\n✔\n✔\nThe mean of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nTau Gamma\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\n✔\n✔\nTrue mean treatment response for the scenario\n\n\nTrue Mean Control Resp &lt;Group&gt;\nG\n✔\n✔\nTrue mean control response for the scenario\n\n\nNum Completed &lt;Group&gt; \n2 * G\n✔\n✔\nThe number of subjects completed (final endpoint available) in each arm in each group.\n\n\nComplete Info &lt;Group&gt; \n2 * G\n✔\n✔\nThe amount of complete information in each arm in each group – however that has been defined in the definition of interim timing – subjects enrolled, subjects complete at a certain visit or subjects who had the opportunity to complete at a certain visit.\n\n\nMean Raw Response &lt;Group&gt; \n2 * G\n✔\n✔\nThe mean raw response\n\n\nSE Raw Response &lt;Group&gt; \n2 * G\n✔\n✔\nThe standard deviation of the raw response\n\n\nNum Dropouts &lt;Group&gt; \n2 * G\n✔\n✔\nNumber of dropouts seen\n\n\nPr Ph3 Success &lt;Group&gt;\nG\n✔\n✔\nThe probability of success in phase 3 of the study treatment for each group.\n\n\nPr Ph3 Success 99\n1\n✔\n✔\nThe probability of success in phase 3 of the across groups treatment difference.\n\n\nPr CSD (Success) &lt;Group&gt;\nG\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the success NIM) of the study treatment for each group.\n\n\nPr CSD (Success) 99\n1\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the NIM) of the across groups treatment difference.\n\n\nPr CSD (Futility) &lt;Group&gt;\nG\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the study treatment for each group.\n\n\nPr CSD (Futility) 99\n1\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the across groups treatment difference.\n\n\nCSD Futility &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Futility 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Success 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nCSD Futility (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are met for the final evaluation for futility for each group, 0 = they are not all met. Whether the across group final futility condition is also met if required is not included in this flag.\n\n\nCombined Futility (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success for each group, 0 = they are not all met. Whether the across group final success condition is also met if required is not included in this flag.\n\n\nCombined Success (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met (Final) &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met (Final) &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nFutile Study\n1\n✔\n✔\nA flag: 1 = the study was futile overall, 0 = otherwise.\n\n\nSuccessful Study\n1\n✔\n✔\nA flag: 1 = the study was successful overall, 0 = otherwise.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome: 1. Early success; 2. Late success; 3. Late futility; 4. Early futility; 5. Success to futility flip-flop; 6. Futility to success flip-flop; 7. Inconclusive\n\n\nGroup Outcome\nG\n✔\n✔\nA flag categorizing the group outcome: 1. Early success; 2. Late success; 3. Late futility; 4. Early futility; 5. Success to futility flip-flop; 6. Futility to success flip-flop; 7. Inconclusive\n\n\nGroup Stop Type &lt;Group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: 8. Success; 9. Group Cap; 10. Futility; 11. Other (group stopped because study stopped early); 12. Study cap\n\n\nGroup Stop Week &lt;Group&gt;\nG\n✔\n✔\nThe week the group stop decision was taken. There may be further follow-up time before the group analysis was completed.\n\n\nStudy Accrual Stop Week\n1\n✔\n✔\nThe week the study stop decision was taken. There may be further follow-up time before the study analysis was completed.\n\n\nBAC Mu &lt;Group&gt;\nG\n✔\n✔\nThe mean estimated value of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Mu &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nBAC Tau &lt;Group&gt;\nG\n✔\n✔\nThe mean estimated value of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Tau &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nAlpha0  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Alpha for state 0 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nAlphaS  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Alpha for stable state (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nAlpha1  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Alpha for state 1 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nProb0  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Transition probability to state 0 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nProbS  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Probability of remaining stable (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nProb1  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Transition probability to state 1 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nBeta Binom Alpha0  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial alpha parameter for the probability that the final result will be 1 if the result at the visit is 0\n\n\nBeta Binom Beta0  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial beta parameter for the probability that the final result will be 1 if the result at the visit is 0\n\n\nProb0  \nLM*V\n✔\n✔\nIf using the Beta Binomial or Logistic Regression longitudinal models: the probability of 1 being the final result if the result at the visit is 0\n\n\nBeta Binom Alpha1  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial alpha parameter for the probability that the final result will be 1 if the result at the visit is 1\n\n\nBeta Binom Beta1  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial beta parameter for the probability that the final result will be 1 if the result at the visit is 1\n\n\nProb1  \nLM*V\n✔\n✔\nIf using the Beta Binomial or Logistic Regression longitudinal models: the probability of 1 being the final result if the result at the visit is 1"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations_freq.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations_freq.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nOverall (Chisq) Group p-value\n1\nThe overall Chi-Squared p-value\n\n\nOverall OR\n1\nThe overall Odds Ratio\n\n\nExp[SD Overall log(OR)]\n1\nThe estimated SD of the Overall Log Odds Ratio\n\n\nOverall Grp in Trt p-value\n1\nThe overall treatment p-value\n\n\nGroup in Ctl p-value\n1\nThe treatment-group interaction p-value\n\n\nGroup (ChiSq) p-values\nG\nThe Chi-Squared p-value for each group\n\n\nGroup (Fisher Exact) p-values\nG\nThe Fisher Exact p-value for each group\n\n\nGroup (Binomial) p-values\nG\nThe Binomial p-value for each group\n\n\nGrp Trt Effects\nG\nThe treatment difference in rates compared to Control per group\n\n\nCMH p-value\n1\nCochran-Mantel-Haenszel test p-value\n\n\nBreslow-Day p-value\n1\nBreslow-Day test p-value\n\n\nAgresti Score Upper CI\n1\nUpper bound of Agresti Score interval\n\n\nAgresti Score Loser CI\n1\nLower bound of Agresti Score interval"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-patientsnnnnn.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nRegion index\n\n\nDateInWeeks\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nGroup\n1\nThe index number of the group the subject belongs to.\n\n\nArm\n1\nA flag indicating the arm the subject was randomized to: 0 = Control, 1 = Study Treatment.\n\n\nLastVisit#\n1\nThe index of the last visit for which subject data is available\n\n\nVisit &lt;visit&gt;\nV\nThe subject’s endpoint score for that visit: -9999 = not available."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/dichotomous.html#contents-of-mcmcnnnnn.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContent if using a conventional Dichotomous endpoint\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nPi1 &lt;group&gt;\nG\nThe estimate of the response rate on the treatment arm in each group\n\n\nOverallTheta\n1\nThe overall treatment difference in log-odds\n\n\nPi0 &lt;group&gt;\nG\nThe estimate of the response rate on the control arm in each group.\n\n\nOverallGamma &lt;group&gt;\nG\nThe log odds of the response rate on control in each group\n\n\n\n\nLongitudinal parameters – the number and names of the longitudinal parameters will depend on which longitudinal model is being used in the analysis (if any) and how many model instances are being used. The reported parameter names can be matched up against the symbols used on the Design &gt; Longitudinal Model tab\n\n\n\n\n\nContents if using a Restricted Markov endpoint\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nPi1 &lt;group&gt;\nG\nThe estimate of the response rate on the treatment arm in each group\n\n\nOverallTheta\n1\nThe overall treatment difference in log-odds\n\n\nPi0 &lt;group&gt;\nG\nThe estimate of the response rate on the control arm in each group.\n\n\nOverallGamma &lt;group&gt;\nG\nThe log odds of the response rate on control in each group\n\n\nAlpha0 &lt;model&gt; &lt;visit&gt;\nM * V\nThe Dirichlet parameter: the number of observed fails in this model at this visit\n\n\nAlphaS &lt;model&gt; &lt;visit&gt;\nM * V\nThe Dirichlet parameter: the number of observed stable in this model at this visit\n\n\nAlpha1 &lt;model&gt; &lt;visit&gt;\nM * V\nThe Dirichlet parameter: the number of observed responses in this model at this visit\n\n\nProb0 &lt;model&gt; &lt;visit&gt;\nM * V\nThe probability of becoming a fail in this model in this visit\n\n\nProbS &lt;model&gt; &lt;visit&gt;\nM * V\nThe probability of remaining stable in this model in this visit\n\n\nProb0 &lt;model&gt; &lt;visit&gt;\nM * V\nThe probability of becoming a responder in this model in this visit"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ -&gt; Fut Flip Flop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut -&gt; Succ Flip Flop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nMean Events &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment in each group.\n\n\nMean Events Across groups, &lt;segement&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment.\n\n\nMean Exposure &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment in each group.\n\n\nMean Exposure Across groups, &lt;segment&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Trt.: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the hazard ratio on the treatment arms for each group.\n\n\nMean Trt.: Across groups\n1\nThis is the mean (over the simulations) of the average treatment hazard ratio across the groups.\n\n\nSD Trt : &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the estimate of the treatment hazard ratio in each group.\n\n\nSD Trt : Across groups\n1\nThis is the standard deviation (over the simulations) of the average of the estimates of the treatment hazard ratio across the groups.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response for the scenario\n\n\nMean Mu_theta\n1\nThis is the mean (over the simulations) of the estimate of the mean of hierarchical distribution of the log hazard ratios of the treatment arms in the groups.\n\n\nSD Mu_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the mean of the hierarchical distribution of the log hazard ratios in the groups.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios each group.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios in each group.\n\n\nAlpha: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Alpha: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nBeta: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Beta: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the mean (over the simulations) of the estimate of the control event rate in each group over each hazard model time segment.\n\n\nTrue Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the true mean control event rate for each group and each hazard time segment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSHRD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for success.\n\n\nProb. CSHRD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for success.\n\n\nProb. CSHRD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for futility.\n\n\nProb. CSHRD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for futility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn. CSHRD (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD success criteria.\n\n\nPpn. CSHRD (Success) Met: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSHRD success criteria.\n\n\nPpn. CSHRD (Futility) met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD futility criteria.\n\n\nPpn. CSHRD (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the CSHRD futility criteria.\n\n\nPpn. P3 (Futility) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria.\n\n\nPpn. P3 (Success) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\n\n\n\n\nThis is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nMean Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSE Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nCox PPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Cox proportion hazard test between treatment and control in each group was significant.\n\n\nLog Rank Ppn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Log Rank test between treatment and control in each group was significant"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#highlights",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#highlights",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ -&gt; Fut Flip Flop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut -&gt; Succ Flip Flop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#allocation-observed",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#allocation-observed",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nMean Events &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment in each group.\n\n\nMean Events Across groups, &lt;segement&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment.\n\n\nMean Exposure &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment in each group.\n\n\nMean Exposure Across groups, &lt;segment&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#response",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#response",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Trt.: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the hazard ratio on the treatment arms for each group.\n\n\nMean Trt.: Across groups\n1\nThis is the mean (over the simulations) of the average treatment hazard ratio across the groups.\n\n\nSD Trt : &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the estimate of the treatment hazard ratio in each group.\n\n\nSD Trt : Across groups\n1\nThis is the standard deviation (over the simulations) of the average of the estimates of the treatment hazard ratio across the groups.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response for the scenario\n\n\nMean Mu_theta\n1\nThis is the mean (over the simulations) of the estimate of the mean of hierarchical distribution of the log hazard ratios of the treatment arms in the groups.\n\n\nSD Mu_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the mean of the hierarchical distribution of the log hazard ratios in the groups.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios each group.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios in each group.\n\n\nAlpha: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Alpha: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nBeta: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Beta: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the mean (over the simulations) of the estimate of the control event rate in each group over each hazard model time segment.\n\n\nTrue Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the true mean control event rate for each group and each hazard time segment"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#probabilities",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#probabilities",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSHRD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for success.\n\n\nProb. CSHRD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for success.\n\n\nProb. CSHRD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for futility.\n\n\nProb. CSHRD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for futility."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#stopping-rules",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#stopping-rules",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn. CSHRD (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD success criteria.\n\n\nPpn. CSHRD (Success) Met: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSHRD success criteria.\n\n\nPpn. CSHRD (Futility) met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD futility criteria.\n\n\nPpn. CSHRD (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the CSHRD futility criteria.\n\n\nPpn. P3 (Futility) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria.\n\n\nPpn. P3 (Success) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#evaluation-rules",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#evaluation-rules",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#hierarchical-prior-parameters",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#hierarchical-prior-parameters",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#simulation-results",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#simulation-results",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "This is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#frequentist-results",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#frequentist-results",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nMean Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSE Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nCox PPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Cox proportion hazard test between treatment and control in each group was significant.\n\n\nLog Rank Ppn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Log Rank test between treatment and control in each group was significant"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-summary.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-summary.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Project\n1\nThe name of the “.facts” file in the FACTS GUI that was used to generate the simulations.\n\n\nScenario\n1\nThe name of the scenario – this is the various profile names that make up the scenario, concatenated together.\n\n\nTimestamp\n1\nThe date and time when the simulations started.\n\n\nVersion\n1\nThe version number of the FACTS GUI that ran the simulations.\n\n\nNsim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\n80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nMean Alloc &lt;group&gt; &lt;arm&gt;\n2*G\nThe mean number of subjects recruited into each arm in each group. If a control arm is not included the column is still present, with value ‘0’.\n\n\nMean Trt Resp &lt;group&gt;\nG\nThe mean of the estimates of hazard ratio on the study treatment arm in each group.\n\n\nMean Trt Effect (Overall)\n1\nThe mean of the estimate of the across groups treatment hazard ratio.\n\n\nSE Trt Resp &lt;group&gt;\nG\nThe standard error, over the simulations, of the estimate of hazard ratio on the study treatment arm in each group.\n\n\nSE Trt Effect (Overall)\n1\nThe standard error, over the simulations, of the estimate of the across groups treatment hazard ratio.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\nTrue mean treatment response for the scenario\n\n\nMean Pr Ph3 Success &lt;group&gt;\nG\nThe mean, over the simulations, of the mean probability of success in phase 3 of the study treatment in each group.\n\n\nMean Pr Ph3 Success 99\n1\nThe mean, across the simulations, of the mean probability of success in phase 3 of the across groups treatment difference.\n\n\nMean Pr CSD (Success) &lt;group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm hazard ratio being better than the CSHRD for success in each group.\n\n\nMean Pr CSD (Success) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups hazard ratio being better than the CSHRD for success.\n\n\nMean Pr CSD (Futility) &lt;group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm hazard ratio being better than the CSHRD for futility in each group.\n\n\nMean Pr CSD (Futility) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups hazard ratio being better than the CSHRD for futility.\n\n\nPpn CSD Futility &lt;group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSHRD futility criterion in each group.\n\n\nPpn CSD Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSHRD futility criterion.\n\n\nPpn Ph3 Futility &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success &lt;group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSHRD success criterion in each group.\n\n\nPpn CSD Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSHRD success criterion.\n\n\nPpn Ph3 Success &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility &lt;group&gt;\nG\nThe proportion of simulations where each group met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success &lt;group&gt;\nG\nThe proportion of simulations that each group met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met &lt;group&gt;\nG\nThe proportion of simulations where each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThe proportion of simulations where the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met &lt;group&gt;\nG\nThe proportion of simulations where each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile\n1\nThe proportion of simulations where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn CSD Futility (Final) &lt;group&gt;\nG\nThe proportion of simulations in which the estimate after all the data has been collected of the response on the study treatment arm met its CSHRD futility criterion in each group.\n\n\nPpn CSD Futility (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its CSHRD futility criterion.\n\n\nPpn Ph3 (Final) Futility &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability after all the data has been collected of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success (Final) &lt;group&gt;\nG\nThe proportion of simulations in which the estimate after all the data has been collected of the response on the study treatment arm met its CSHRD success criterion in each group.\n\n\nPpn CSD Success (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its CSHRD success criterion.\n\n\nPpn Ph3 Success (Final) &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability after all the data has been collected of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility (Final) &lt;group&gt;\nG\nThe proportion of simulations where each group after all the data has been collected met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility (Final) 99\n1\nThe proportion of simulations where the across groups treatment effect after all the data has been collected met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success (Final) &lt;group&gt;\nG\nThe proportion of simulations where after all the data has been collected each group met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success (Final) 99\n1\nThe proportion of simulations where after all the data has been collected the across groups treatment effect met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met (Final) &lt;group&gt;\nG\nThe proportion of simulations where after all the data has been collected each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where after all the data has been collected the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met (Final) &lt;group&gt;\nG\nThe proportion of simulations where after all the data has been collected each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where after all the data has been collected the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn Outcome 1\n1\nThe proportion of simulations that stopped early for success.\n\n\nPpn Outcome 2\n1\nThe proportion of simulations that reached full accrual and declared success on final evaluation.\n\n\nPpn Outcome 3\n1\nThe proportion of simulations that reached full accrual and declared futility on final evaluation.\n\n\nPpn Outcome 4\n1\nThe proportion of simulations that stopped early for futility\n\n\nPpn Outcome 5\n1\nThe proportion of simulations that stopped early for success but were deemed futile on final evaluation.\n\n\nPpn Outcome 6\n1\nThe proportion of simulations that stopped early for futility but were deemed successful on final evaluation.\n\n\nPpn Outcome 7\n1\nThe proportion of simulations that reached full accrual and were inconclusive.\n\n\nMean Study Accrual Stop Week\n1\nThe mean study duration of accrual – from start of accrual to last patient first visit.\n\n\nMean Mu Theta\n1\nThe mean, over the simulations, of the estimate of the mean of hierarchical distribution of the mean log hazard ratio in each group.\n\n\nSE Mu Theta\n1\nThe standard error, over the simulations, of the estimate of the mean of the hierarchical distribution of the mean log hazard ratio in each group.\n\n\nMean Tau Theta\n1\nThe mean, over the simulations, of the estimate of the standard deviation of the hierarchical distribution of the mean log hazard ratio in each group.\n\n\nSE Tau Theta\n1\nThe standard error, across the simulations, of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratio in each group.\n\n\nMean Alpha &lt;segment&gt;\nS\nThis is the mean (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSE Alpha &lt;segment&gt;\nS\nThis is the standard deviation (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean Beta &lt;segment&gt;\nS\nThis is the mean (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSE Beta &lt;segment&gt;\nS\nThis is the standard deviation (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean BAC Mu &lt;group&gt;\nG\nThe mean, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nSE BAC Mu &lt;group&gt;\nG\nThe standard error, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nMean BAC Tau &lt;group&gt;\nG\nThe mean, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nSE BAC Tau &lt;group&gt;\nG\nThe standard error, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nLambda &lt;group&gt; &lt;segment&gt;\nG * S\nThe mean, over the simulations, of the estimate of the event rate on the control arm in each group in each time segment of the hazard model.\n\n\nTrue Lambda &lt;group&gt; &lt;segment&gt;\nG * S\nThe true mean event rate on the control arm in each group in each time segment of the Control Hazard VSR profile.\n\n\nMean Events &lt;group&gt; &lt;segment&gt; &lt;arm&gt;\nG * S * A\nThe mean, over the simulations, of the number of events on each arm in each group in each time segment of the hazard model.\n\n\nMean Events 99 &lt;segment&gt; &lt;arm&gt;\nS * A\nThe mean, over the simulations, of the number of events on each arm across all the groups in each time segment of the hazard model.\n\n\nMean Exposure &lt;group&gt; &lt;segment&gt; &lt;arm&gt;\nG * S *A\nThe mean, over the simulations, of the subject weeks exposure time on each arm in each group in each time segment of the hazard model.\n\n\nMean Exposure 99 &lt;segment&gt; &lt;arm&gt;\nS * A\nThe mean, over the simulations, of the subject weeks exposure time on each arm across all groups in each time segment of the hazard model"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-summary_freq.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-summary_freq.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Overal Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nMean Trt Effect in Grp &lt;group&gt;\nG\nThe mean, over the simulations, of the estimate of the mean treatment effect in each group from the fit of an overall Cox model that includes a Group X Treatment interaction.\n\n\nSE Trt Effect in Grp &lt;group&gt;\nG\nThe standard error, over the simulations, of the estimate of the mean treatment effect in each group from the fit of an overall Cox model that includes a Group X Treatment interaction.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations in which the treatment effect was significant.\n\n\nPpn Significnat Str. LogRank\n1\nThe proportion of simulations significant by the stratified log rank test\n\n\npSignificant (GrpxTrt)\n1\nThe proportion of simulations with a significant group and treatment effect.\n\n\nMean Trt Effect &lt;group&gt;\nG\nThe mean treatment effect per group.\n\n\nSE Trt Effect &lt;group&gt;\nG\nThe standard error of the treatment effect per group\n\n\nCox ppn significant\nG\nThe proportion of simulations with significant effect by Cox proportional hazards\n\n\nLogRank Significant &lt;group&gt;\nG\nThe proportion of simulations with significant effect by the Log Rank test"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n#Sim\n1\n✔\n\nSimulation number\n\n\n# Weeks (Duration)\n1\n✔\n\nThe week of final analysis – the total duration of the simulation.\n\n\n#Week\n1\n\n✔\nWeek\n\n\nNo.Subj\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nAlloc &lt;group&gt; \n2 * G\n✔\n✔\nThe number of subjects allocated to each arm in each group.\n\n\nMean Trt Resp (HR) &lt;group&gt;\nG\n✔\n✔\nThe estimated hazard ratio of the study treatment in the group.\n\n\nTrt Effect (HR) (Overall)\n1\n✔\n✔\nThe estimated mean hazard ratio across the groups.\n\n\nSD Mean Trt Resp &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the hazard ratio of the study treatment in the group.\n\n\nSD Trt Effect (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the hazard ratio difference across the groups.\n\n\nMu Theta\n1\n✔\n✔\nThe mean of the hierarchical distribution of the hazard ratios over all the groups.\n\n\nTau Theta\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the hazard ratios over all the groups.\n\n\nLambda &lt;group&gt; \nS * G\n✔\n✔\nThe estimate of the control event rate in each group in each time segment of the hazard model\n\n\nAlpha \nS\n✔\n✔\nThe estimate of the alpha parameter of the hierarchical gamma distribution of the control event rates across the groups, in each time segment of the hazard model\n\n\nBeta \nS\n✔\n✔\nThe estimate of the alpha parameter of the hierarchical gamma distribution of the control event rates across the groups, in each time segment of the hazard model.\n\n\nTrue Lambda &lt;group&gt; \nG * S\n✔\n✔\nThe true control even rate in each group in each time segment of the Hazard Rate VSR profile\n\n\nPr Ph3 Success &lt;group&gt;\nG\n✔\n✔\nThe probability of success in phase 3 of the study treatment for each group.\n\n\nPr Ph3 Success 99\n1\n✔\n✔\nThe probability of success in phase 3 of the across groups treatment difference.\n\n\nPr CSD (Success) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the success CSHRD (or as good as control by the success NIHRM) of the study treatment for each group.\n\n\nPr CSD (Success) 99\n1\n✔\n✔\nThe probability of being better than control by the success CSHRD (or as good as control by the NIHRM) of the across groups treatment difference.\n\n\nPr CSD (Futility) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the futility CSHRD (or as good as control by the futility NIHRM) of the study treatment for each group.\n\n\nPr CSD (Futility) 99\n1\n✔\n✔\nThe probability of being better than control by the futility CSHRD (or as good as control by the futility NIHRM) of the across groups treatment difference.\n\n\nCSD Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSHRD was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSHRD was below the threshold for early stopping for futility for the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSHRD was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSHRD was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Futility 99\n1\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for success for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Success 99\n1\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s success criteria were met for the final evaluation, 0 = otherwise.\n\n\nSuccess Criteria Met (Final) 99\n1\n✔\n✔\nA flag: 1 = the Across Group success criteria were met for the final evaluation, 0 = otherwise\n\n\nFutility Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s futility criteria were met for the final evaluation, 0 = otherwise.\n\n\nFutility Criteria Met (Final) 99\n1\n✔\n✔\nA flag: 1 = the Across Group futility criteria were met for the final evaluation, 0 = otherwise\n\n\nFutile Study\n1\n✔\n✔\nA flag: 1 = the study was futile overall, 0 = otherwise.\n\n\nSuccessful Study\n1\n✔\n✔\nA flag: 1 = the study was successful overall, 0 = otherwise.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome: 1. Early success; 2. Late success; 3. Late futility; 4. Early futility; 5. Success to futility flip-flop; 6. Futility to success flip-flop; 7. Inconclusive\n\n\nGroup Outcome &lt;group&gt;\nG\n✔\n✔\nA flag categorizing final group outcome, using the same codes as for the study outcome (above)\n\n\nGroup Stop Type &lt;group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: 1. Success; 2. Group Cap; 3. Futility; 4. Other (group stopped because study stopped early); 5. Study cap\n\n\nGroup Stop Week &lt;group&gt;\nG\n✔\n✔\nThe week the group stop decision was taken. There may be further follow-up time before the group analysis was completed.\n\n\nStudy Stop Week\n1\n✔\n✔\nThe week the study stop decision was taken. There may be further follow-up time before the study analysis was completed.\n\n\nBAC Mu &lt;group&gt;\nG\n✔\n✔\nThe estimated value of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Mu &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nBAC Tau &lt;group&gt;\nG\n✔\n✔\nThe estimated value of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Tau &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nEvents &lt;group&gt;  \nG * S * A\n✔\n✔\nThe number of events observed in each group, in each time segment of the hazard model, on each arm.\n\n\nEvents 99  \nS * A\n✔\n✔\nThe overall events observed over all the groups, in each time segment of the hazard model, on each arm.\n\n\nExposure &lt;group&gt;  \nG * S * A\n✔\n✔\nThe number of exposure in subject weeks in each group, in each time segment of the hazard model, on each arm.\n\n\nExposure 99  \nS * A\n✔\n✔\nThe overall exposure in subject weeks over all the groups, in each time segment of the hazard model, on each arm."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-simulations_freq.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-simulations_freq.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nOverall Group p-value\n1\nThe overall group effect p-vale\n\n\nOverall Trt Estimate\n1\nThe overall treatment effect estimate (cox)\n\n\nOverall Trt SD\n1\nThe SD of the overall treatment effect (Cox)\n\n\nEst Trt Effect [exp(coeff)] in Group &lt;group&gt;\nG\nThe estimate of the group treatment effect from fitting the full Cox model with Group x Treatment interaction\n\n\nSD Trt Effect [exp(SD)] in Group &lt;group&gt;\nG\nThe SD of the estimate of the group treatment effect from fitting the full Cox model with Group x Treatment interaction\n\n\nOverall Trt p-val\n1\nThe overall treatment effect p-value (Cox)\n\n\nStr. LogRank Statistic\n1\nStratified LogRank statistic\n\n\nStr. LogRank p-val\n1\nStratified LogRank p-value\n\n\nGroup x Trt p-value\n1\nGroup * Trt interaction p-value\n\n\nEst Trt Effect &lt;group&gt;\nG\nEstimated Trt Effect (Cox) or Exponential rate if control not included\n\n\nSD Trt Effect &lt;group&gt;\nG\nThe SD of the estimated treatment effect\n\n\nCox or Exponential MLE p-value &lt;group&gt;\nG\nTrt Effect (Cox) or rate if control not included – p-value\n\n\nLogRank p-value &lt;group&gt;\nG\np-value based on log-rank test statistic"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nThe accrual region index – where this subject was recruited.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nGroup\n1\nThe index number (1, …) of the group the subject belongs to.\n\n\nArm\n1\nA flag indicating the arm the subject was randomized to: 0 = Control, 1 = Study Treatment.\n\n\nDuration\n1\nThe time of observation of the subject (in weeks)\n\n\nOutcome\n1\nWhether an event was observed (1) or not (0)"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#exporting-the-results",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#exporting-the-results",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Exporting the Results",
    "text": "Exporting the Results\nUsing the menu item File -&gt; Export Project, the .facts file and all the results files can be saved as a single zip file."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nHR\nG\nThe estimate of the Hazard Ratio in each group\n\n\nOverall HR\n1\nThe estimate of the overall Hazard Ratio\n\n\nLambda &lt;group&gt; &lt;seg&gt;\nG * S\nThe estimate of the event rate in the control arm in each of the Hazard Model observation segments in each group\n\n\nOverall Lambda &lt;group&gt;\nG\nThe estimate over all the observation segments of the event rate in the control arm in each group."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/continuous.html",
    "href": "documentation/v72/userguides/enrichment/vsr/continuous.html",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual subject responses generated from an external response model."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/continuous.html#explicitly-defined",
    "href": "documentation/v72/userguides/enrichment/vsr/continuous.html#explicitly-defined",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\n\nBaseline\nThis tab is present only if “Include baseline data” was selected on the Study Info tab.\nBaseline profiles may be added, removed or renamed using the buttons and list on the left hand side of the screen. The baseline for each group is specified as a normal distribution, which may be truncated. If a truncated baseline is used, then samples will be drawn from the underlying normal and re-sampled if they fall outside the specified upper or lower bounds. [Note: clearly this will result in an observed baseline distribution with a different mean and SD from the underlying one].\n\n\n\n\n\n\nFigure 1\n\n\n\nOptionally we can simulate a baseline effect on subject’s responses. This will be an adjustment to the “change from baseline” or “final endpoint value” depending on which definition of Response is being used.\nIf adjusting the final response based on baseline, then the user selects “Use Baseline Adjustment for Subject Response” and supplies 3 parameters for each group:\n\nBeta - a coefficient that reflects the degree of influence of baseline on final score and the degree of variability in the final score due to baseline.\nC – a centering offset, typically the expected mean of the observed baseline scores\nS – a scaling element, typically set to the expected SD of the baseline.\n\nExample – in the above screenshot for the severe group, a baseline of mean 55 and SD 5 has been specified – so a centering of 55 and scaling of 5 is used. Wishing to simulate an overall SD of 5 in the final change from baseline and apportion two-thirds the variance to baseline, Beta has been set as follows:\n\nThe desired final variance is 25 (52), divided between 2/3rd dose response and 1/3rd baseline effects.\nThe SD of the simulated response is set to 4.08 \\(= \\sqrt{\\left( 25*\\frac{2}{3} \\right)}\\)\nThe SD of the scaled baseline score is 1, so to contribute one third the final variance of 25, Beta is set to 2.89 \\(= \\sqrt{\\left( 25*\\frac{1}{3} \\right)}\\)\nNote that when simulating a baseline effect in this way, limiting the range of baseline by specifying upper and lower cut-offs – which might be natural limits of the endpoint, or due to inclusion / exclusion criteria in the protocol – can significantly reduce the variance in the final endpoint due to the baseline effect.\n\n\n\nGroup Response\nResponse profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 2. Mean response, the change from baseline (unless baseline is included in the simulation and the “Response is: Final endpoint value” option has been selected on the Study &gt; Study Info, in which case the response simulated is the final endpoint value), values are entered for the treatment and control arms (if present) for each group directly into the “Treatment Response”/“Treatment change from baseline” and “Control Response”/“Control change from baseline” columns of the table. The graphical representation of these values updates accordingly.\nAs well as the mean of each distribution to sample responses from, it is necessary to specify the variance, by specifying the standard deviation of the observations, termed here the ‘SD response’/‘SD change from baseline’.\n\n\n\n\n\n\nNote\n\n\n\nThe ‘SD response’/‘SD change from baseline’ needs to be specified for each profile, initially for each new profile it will be set to the default value, which is 12. It is very easy to overlook setting this value! If unexpected operating characteristics are seen for any response profile in ED, it is advisable to first check first that ‘SD response’/‘SD change form baseline’ has been set correctly for the profile before looking for more sophisticated reasons.\n\n\nIn addition it is possible for the user to specify if a group “Should Succeed” using the “Should succeed” checkbox on each row. This is then used in the summary of the simulation results to compute how often the simulated trial was successful and groups that ‘Should Succeed’ were successful (reported in the column “Ppn Correct Groups”) and how often the simulated trial was successful and groups that were not marked ‘Should Succeed’ were successful (reported in the column “Ppn Incorrect Groups”).\n\n\n\n\n\n\nFigure 2: Virtual Subject Response – Explicitly Defined - Group Response sub-tab\n\n\n\nThe graph that shows the response to simulate that has been specified – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\nInstead of specifying a single value for the SD of all responses, it is possible to specify (still on a per profile basis) a different SD for the responses for each of the treatment and control arms:\n\n\n\n\n\n\nFigure 3\n\n\n\nOr a different SD for treatment and control in each group:\n\n\n\n\n\n\nFigure 4: Specifying different SD’s of the outcome measure buy group and arm\n\n\n\n\n\nLoad Scenario Means From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses.\nEach individual simulation uses one set of mean responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 5: Virtual Subject Response - Loading group response means from an external file\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual mean responses and the overall mean response over all the VSRs.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. The format is:\n\nIf a control arm is being used: Each line should contain columns [MT1, MT2, … , MTG, ST1, ST2, … , STG, MC1, MC2, … , MCG, SC1, SC2, … , SCG] giving the true Mean responses and SD’s for the Treatment arm in each of the G groups, followed by the Mean responses and SD’s in the Control arms.\nWithout a control arm, using Objective Control: Each line should contain columns [MT1, MT2, … , MTG, ST1, ST2, … , STG] giving the true Mean responses and SD’s for the Treatment arm in each of the G groups.\n\nFor example:\n# Alzheimer’s Example, 3 groups including control, SD of 5 for all arms\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.1, 0.2, 0.4, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.2, 0.4, 0.8, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.3, 0.6, 1.2, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.4, 0.8, 1.6, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.5, 1.0, 2.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n\n\nLongitudinal\nIf ‘Use longitudinal modeling’ has been checked on the ‘Study Info’ tab, and explicitly defined group responses have been specified, then it will be necessary to specify how to simulate the subjects’ responses at intermediate visits. This is done by selecting the correlation method to be combined with the response profiles to generate the intermediate results for subjects on treatment and control in all the groups.\nThe methods are parameterized so that their inclusion does not affect the mean and variance of the final endpoint to be simulated, and they scale automatically to suit each group response profile.\nThree longitudinal methods are provided:\n\nCorrelated: is specified in terms of the ‘amount of noise’ carried forward from the last visit and the amount that is generated ‘a new’.\nHierarchical: visits are correlated through the use of a subject-specific random effect.\nITP: responses follow an exponential model over time with a subject specific random effect.\n\nAll methods allow the fraction of the final response and the fraction of the final variance observed at each visit to be specified.\nBoth methods can be parameterized across the whole trial or separately parameterized for each group.\nClick here for an overview of longitudinal models for continuous endpoints in the FACTS Core engine.\n\nCorrelated Method\nThis method is defined by 3 values for each visit:\n\nft the fraction of the final mean response seen at visit t.\nφt the fraction of the final sigma seen at visit t.\nρt the visit-to-visit correlation in the noise from visit t-1 to t.\n\nThe method can be parameterized for each group individually (as shown) or with one set of parameters for all groups.\n\n\n\n\n\n\nFigure 6: Simulated Longitudinal Results - correlated method\n\n\n\nThe equations currently use a subscript of ‘1’ for the first visit, corresponding to the values shown for the index of each visit shown in the table where the values are entered.\nFor the first visit, for a subject in group g, treatment arm a, the mean response is given by: \\(\\mu_{g,\\ a,1} = f_{1}\\mu_{g,a}\\),\nand the observed variance by \\(\\sigma_{g,a,1}^{2} = \\varphi_{1}^{2}\\sigma_{g,a}^{2}\\)\nthus the expected response \\(y_{g,a,1}\\) for a subject is \\(y_{g,a,1} \\sim\\mu_{g,a,1} + N\\left( 0,\\sigma_{g,a,1}^{2} \\right)\\)\nWhere the final mean \\(\\mu_{g,a}\\), and variance \\(\\sigma_{g,a}\\), are as defined on the group response tab.\nFor the second visit, the mean response is given by: \\(\\mu_{g,a,2} = f_{2}\\mu_{g,a}\\),\nand the observed variance by \\({\\rho_{g,1}\\left( y_{i,1} - \\mu_{g,a,1} \\right)\\frac{\\sigma_{g,a,2}^{2}}{\\sigma_{g,a,1}^{2}} + \\sqrt{1 - \\rho_{g,2}^{2}}N\\left( 0,\\sigma_{g,a,2}^{2} \\right)}_{}^{}\\)\n(where \\(y_{i,1}\\) is the observed response at the ith subject’s first visit response) thus the variance is \\(\\sigma_{g,a,2}^{2} = \\varphi_{2}^{2}\\sigma_{g,a}^{2}\\) but made up of a component that has already been observed at the previous visit, scaled by the correlation factor and relative variance at the second visit compared to the first, plus a new suitably scaled random component.\n\n\nHierarchical Method\nThis method is defined by 2 values for each visit:\n\nft the fraction of the final mean response seen at visit t.\nφt the fraction of the final sigma seen at visit t.\n\nAnd an overall (or per group) fraction \\(\\nu_{g}\\) for how much of the final variance is due to inter-subject variance.\nThe method can be parameterized for each group individually (as shown) or with one set of parameters for all groups.\n\n\n\n\n\n\nFigure 7: Simulated Longitudinal Results - hierarchical method\n\n\n\nFor the visit t, for subject i, in group g, treatment arm a\nthe expected response \\(y_{g,a,t}\\) for a subject is \\(y_{g,a,t} \\sim f_{g,t}\\left( \\mu_{g,a} + \\delta_{i} \\right) + N\\left( 0,\\kappa_{g,a,t}^{2} \\right)\\)\nwhere \\(\\delta_{i}\\sim N\\left( 0,\\tau_{g,a}^{2} \\right)\\)\nand \\(\\tau_{g}^{2} = \\upsilon_{g}\\sigma_{g,a}^{2}\\),\nthe variance of \\(y_{g,a,t}\\) will be \\(\\kappa_{g,a}^{2} + {f_{g,t}^{2}\\tau}_{g}^{2}\\), and \\(\\kappa_{g,a}^{2}\\), will be set so that this \\(= \\varphi_{t}^{2}\\sigma_{g,a}^{2}\\)\nThus there is an inter-subject component to the variance \\(\\delta_{i}\\), and an intra-subject component \\(\\kappa_{g,a}^{2}\\).\n\n\nITP\nThis method is defined by 2 values:\n\nk which controls how quickly the response changes with visit.\nν the fraction of the final variance that is independent of visit.\n\nThese values may be specified per arm or per group and arm.\n\n\n\n\n\n\nFigure 8\n\n\n\nThe subject variability σ2 as specified on the Group Response tab is divided into a per subject component, νσ2 and a component which varies between visits (1-ν)σ2.\nThe response for each subject (including both noise terms and any baseline adjustment term) is scaled at each visit as:\n\\[\ny_{g,a,1} \\propto \\left( \\frac{1 - \\exp\\left( kv_{t} \\right)}{1 - \\exp\\left( kv_{T} \\right)} \\right)\n\\]\nwhere vt is the week of visit t and T is the final visit. The effect of some k values are shown below:\n\n\n\nK\nWeek 1\nWeek 2\nWeek 3\nWeek 4\n\n\n\n\n-1\n64%\n88%\n97%\n100%\n\n\n-0.5\n45%\n73%\n90%\n100%\n\n\n1\n3%\n12%\n36%\n100%\n\n\n\nThe later the final visit, the closer to zero interesting values of k will be, non-interesting values will be:\n\ntoo +ve: the scaling factor is close to 0 until the final visit, or\ntoo –ve and the scaling factor is almost 1 from the first visit onwards."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/continuous.html#external",
    "href": "documentation/v72/userguides/enrichment/vsr/continuous.html#external",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subjects’ responses within FACTS, they can be simulated externally, from a PK-PD model for instance, and imported into FACTS, and the supplied responses are sampled from (with replacement) to provide the subject responses in the simulation. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below.\nTo import an external file, the user must first add a profile to the table. After adding the profile, a file selector window is opened and the user must select the file of externally simulated data. To change the selection click the ‘Browse’ button.\n\n\n\n\n\n\nFigure 9: External Data\n\n\n\n\nRequired Format of Externally Simulated Detail\nThe supplied data should be in the following form: an ascii file with data in comma separated value format with the following columns:\n\nPatient id (must be positive and change from subject to subject)\nGroup index (1, 2, 3, … )\nArm Index (1 = Control, 2 = Treatment)\nVisit Id (1, 2, 3, …)\nResponse\n\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file. Note that all visits for each subject must be grouped together. Thus all the data for the first subject comes before that of the second, and so on.\n#Patient ID, Group Index, Arm Index, Visit, Response\n1, 1, 1, 1, 0.5\n1, 1, 1, 2, 0.8\n1, 1, 1, 3, 1.2\n1, 1, 1, 4, 0.9\n2, 1, 2, 1, 0.4\n2, 1, 2, 2, 0.6\n2, 1, 2, 3, 0.8\n2, 1, 2, 4, 0.9\n3, 2, 1, 1, 0.45\n3, 2, 1, 2, 0.55\n3, 2, 1, 3, 0.6\n3, 2, 1, 4, 0.5\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values. The treatment response analysis in FACTS will be based on the values supplied – if you require the analysis to be on patients change from baseline, then you should supply change from baseline values as the subjects’ responses in the file. Conversely if the analysis should be on the absolute value of the response then these are the values that should be supplied as the subject’s response.\nIf baseline is included, each subject must include a visit 0 for the baseline, and the absolute values of the subject responses should be supplied, the option on the Study &gt; Study Info tab to include baseline allows the specification of whether the analysis should be on\n\nthe absolute values,\nor change from baseline values and FACTS will calculate these and perform the analysis on them. For example the above data might become ….\n\n#Patient ID, Group Index, Arm Index, Visit, Response\n1, 1, 1, 0, 6.2\n1, 1, 1, 1, 6.7\n1, 1, 1, 2, 7.0\n1, 1, 1, 3, 7.4\n1, 1, 1, 4, 7.1\n2, 1, 2, 0, 5.4\n2, 1, 2, 1, 5.8\n2, 1, 2, 2, 6.0\n2, 1, 2, 3, 6.2\n2, 1, 2, 4, 6.3\n3, 2, 1, 0, 6.9\n3, 2, 1, 1, 7.35\n3, 2, 1, 2, 7.45\n3, 2, 1, 3, 7.5\n3, 2, 1, 4, 7.4"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/tte.html",
    "href": "documentation/v72/userguides/enrichment/vsr/tte.html",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual subject response from externally simulated PK/PD data. When simulations are executed, they will be executed for each profile specified by the user."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/tte.html#explicitly-defined",
    "href": "documentation/v72/userguides/enrichment/vsr/tte.html#explicitly-defined",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nUnlike other endpoints where simply a response (mean change from base line or rate) is specified, specification of subject responses for a time-to-event endpoint is by separately specifying a piecewise exponential event rate for the control population and then hazard ratios for the treatment arms. For simplicity and consistency, this means of specifying the simulated event rates is also used when no control arm is present and the comparison is with historic control rates.\n\nControl Hazard Rates\nControl hazard rate profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 1. Event rates values are entered for control subjects in each group. By specifying segment breakpoints (in weeks) different event rates for different periods of follow-up can be specified. The graphical representation of these values updates accordingly.\nTo remove an unwanted segment breakpoint, select the row where the segment starts with that breakpoint and click on ‘Delete’. Note that segments are always defined in weeks. The selection of the unit time only applies to the specification of the event rate on this page.\n\n\n\n\n\n\nFigure 1: Control Hazard Rates tab\n\n\n\n\n\nGroup Response\nResponse profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 2. Hazard ratios are entered for the study treatment arm for each group directly into the Hazard ratio column of the table. The graphical representation of these values updates accordingly.\nIn addition it is possible for the user to specify if a group “Should Succeed” using the “Should succeed” checkbox on each row. This is then used in the summary of the simulation results to compute how often the simulated trial was successful and groups that ‘Should Succeed’ were successful (reported in the column “Ppn Correct Groups”) and how often the simulated trial was successful and groups that were not marked ‘Should Succeed’ were successful (reported in the column “Ppn Incorrect Groups”).\n\n\n\n\n\n\nFigure 2: Virtual Subject Response – Explicitly-Defined - Group Response sub-tab\n\n\n\nThis graph that shows the response to simulate that has been specified – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right-clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\n\n\nLoad Scenario Response From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses.\nEach individual simulation uses one set of responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\nFor TTE the user must supply 2 files – one for the Control Hazard Rates and one for the Hazard Ratios in each group. MVSR hazard rates are only combined with MVSR control hazard rates. MVSR hazard rates are only combined with MVSR control hazard rates. The lines from each file are paired up for each simulation, so the first control hazard rate is used with the first group response hazard ratio, the second control hazard rate is used with the second dose response hazard ratio and so on. There must be the same number of lines in each file.\n\n\n\n\n\n\nFigure 3: Virtual Subject Response - Loading scenario hazard rates from a MVSR file\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual control hazard rates for each group.\n\n\n\n\n\n\nFigure 4: Virtual Subject response – Loading group response using an MVSR file\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual hazard ratios for each group.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. The VSR parameters are provided in two separate files, (the number of lines in the files must be the same for the two files). The formats are:\n\nThe Control Hazard Rate File: Each line should contain columns [L1,1, L1,2, … , L1,S, L2,1, L2,2, … , L2,S, … LG,1, LG,2, … , LG,S] giving the true control hazard rates (lambda) for each of the S segments for each of the G groups. (Note: FACTS assumes that the unit of time is weeks)\nThe Hazard Ratio File: Each line should contain columns [HR1, HR2, … , HRG] giving the true treatment arm Mean Hazard Ratios for each of the G groups.\n\nFor example:\n# TTE control hazard rates, 8 groups 1 segment\n# Same number of rows as in Group Response MVSR file\n#G1\n0.082, 0.087, 0.083, 0.085, 0.091, 0.090, 0.087, 0.087\n0.083, 0.086, 0.085, 0.083, 0.090, 0.091, 0.088, 0.087\n0.084, 0.085, 0.087, 0.082, 0.089, 0.088, 0.089, 0.087\n0.085, 0.084, 0.082, 0.087, 0.088, 0.089, 0.090, 0.087\n0.086, 0.083, 0.084, 0.086, 0.087, 0.086, 0.091, 0.087\n0.087, 0.082, 0.086, 0.084, 0.086, 0.087, 0.086, 0.087\n0.088, 0.091, 0.088, 0.090, 0.085, 0.084, 0.085, 0.087\n0.089, 0.090, 0.090, 0.088, 0.084, 0.085, 0.084, 0.087\n0.090, 0.089, 0.089, 0.091, 0.083, 0.083, 0.083, 0.087\n0.091, 0.088, 0.091, 0.089, 0.082, 0.082, 0.082, 0.087\n# TTE 8 Groups, HR for each group\n# 5 Null cases\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n# increasing mixed strong XX\n1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.3\n1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 1.0, 0.2\n1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4\n1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5\n1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/tte.html#external",
    "href": "documentation/v72/userguides/enrichment/vsr/tte.html#external",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subject response within FACTS they can be simulated externally, from a PK-PD model for instance, and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below.\nTo import an external file, the user must first add a profile to the table. After adding the profile a file selector window is opened and the user must select the file of externally simulated data. To change the selection, click the ‘Browse’ button .\n\n\n\n\n\n\nFigure 5: External Data\n\n\n\n\nRequired Format of Externally Simulated Detail\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id\nGroup index (1, 2, 3,… )\nArm Index (1 = Control, 2 = Treatment)\nUncensored time to event in weeks\n\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n#Patient ID, Group Index, Arm Index, Time to Event\n1, 1, 1, 8.87\n2, 1, 2, 9.34\n3, 2, 1, 6.78\n4, 2, 1, 10.23\n5, 2, 1, 9.96\n6, 2, 2, 5.6\n7, 1, 1, 37.01\n8, 1, 2, 28.67\n9, 1, 1, 39.70\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/dichotomous.html",
    "href": "documentation/v72/userguides/enrichment/study/dichotomous.html",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement, and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether a response is subject improvement or subject worsening. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a response indicates subject improvement, then success criteria will look for the response rate of subjects in the treatment arm to be greater than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will look for the response rate in the treatment arm failing to exceed the response rate in the control arm by more than the Target Rate Difference for Futility (TRDF).\nIf a response indicates subject worsening, then success criteria will look for a response rate of subjects in the treatment arm to be less than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS). Futility criteria will look for a response rate in the treatment arm in failing to be less than the response rate of the control arm less the Target Rate Difference for Futility (TRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 0.5 for success corresponds to lowering the rate by 0.5”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\n\nIn ‘Special Longitudinal Features’ the user specifies whether to use the restricted Markov model for longitudinal aspects of the design. This special longitudinal model determines both the method used to simulate subject response data as well as the model used in the design analysis. See SPEC for more information about the model details.\n\nWithin the restricted Markov model, the user must specify whether subjects with a final assessment of “stable” should be counted as a success or failure.\n\n\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/dichotomous.html#study-info",
    "href": "documentation/v72/userguides/enrichment/study/dichotomous.html#study-info",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement, and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether a response is subject improvement or subject worsening. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a response indicates subject improvement, then success criteria will look for the response rate of subjects in the treatment arm to be greater than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will look for the response rate in the treatment arm failing to exceed the response rate in the control arm by more than the Target Rate Difference for Futility (TRDF).\nIf a response indicates subject worsening, then success criteria will look for a response rate of subjects in the treatment arm to be less than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS). Futility criteria will look for a response rate in the treatment arm in failing to be less than the response rate of the control arm less the Target Rate Difference for Futility (TRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 0.5 for success corresponds to lowering the rate by 0.5”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\n\nIn ‘Special Longitudinal Features’ the user specifies whether to use the restricted Markov model for longitudinal aspects of the design. This special longitudinal model determines both the method used to simulate subject response data as well as the model used in the design analysis. See SPEC for more information about the model details.\n\nWithin the restricted Markov model, the user must specify whether subjects with a final assessment of “stable” should be counted as a success or failure.\n\n\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/dichotomous.html#sec-groups",
    "href": "documentation/v72/userguides/enrichment/study/dichotomous.html#sec-groups",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Group Info",
    "text": "Group Info\nOn this tab values to define the analysis for each group are specified.\n\nTrials to show Superiority\nIn a trial to show superiority, for each group the Target Clinically Significant Difference can be set). Optionally the parameters for a phase 3 may be specified to allow the predictive probability of subsequent success in such a phase 3 trial to be used as decision criteria.\n\nWith a dichotomous endpoint the Clinically Significant Difference is in terms of Target Rate Difference – that is a difference between the probability of response in the treatment arm and the probability of response in the control arm. Separate differences can be set for determining success and futility, and separate differences can be set for each group and for the across groups analysis. Their use in practice is specified on the “Design &gt; Stopping Criteria” and “Design &gt; Evaluation Criteria” tabs.\n\n\n\n\n\n\n\nFigure 3: The Group Info sub-tab.\n\n\n\n\nThe phase 3 success criteria are:\n\nPhase 3 total number of subjects per arm\nThe required one-sided alpha\nWhether the phase 3 is to use a test for superiority or non-inferiority (set independently from whether the ED trial is for superiority or non-inferiority)\nA super-superiority margin / non-inferiority margin (depending on whether the phase 3 trial is for superiority or non-inferiority), this margin is independent from any margins specified for the ED trial itself.\n\nGiven these criteria FACTS calculates the predicted probability of success in such a trial for each treatment arm given the estimate of the treatment difference, integrated over the uncertainty in that estimate. The conventional expected power of the specified phase 3 is calculated for the treatment effect in each MCMC sample and then averaged. The resulting predicted probability of success in phase 3 can then be used in the stopping criteria and final evaluation criteria.\n\nSeparate ‘Target Rate Differences’ can be set for success and for futility, specifying the TRD for success and TRD for futility. We use the terms Target Rate Difference here as it makes it clearer that the value to be entered should be positive. Elsewhere (in column headings for instance) the more conventional term CSD may be used.\n\nIf the “Posterior probability” criteria is used for stopping a group for success or judging if a group is successful in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\pi_{d} - \\pi_{0} &gt; CSD\\ for\\ success \\right) &gt; Success\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target difference in rates or success is greater than a specified threshold (set on the Design tabs).\nIf the “Posterior probability” criteria is used for stopping a group for futility or judging if a group is futile in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\pi_{d} - \\pi_{0} &gt; CSD\\ for\\ futility \\right) &lt; Futility\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target mean difference for futility by is less than a specified threshold (set on the Design tabs).\nIf the endpoint is such that a response means subject worsening, then the comparison is reversed (the treatment difference becomes \\(\\pi_{0} - \\pi_{d}\\)) and if the trial is non-inferiority then the test for “being greater than the CSD” is replaced with testing for “being less than the NIM” (Non-Inferiority Margin). Thus the meaning of the user specified Difference or Margin is interpreted taking into account both whether a response means ‘better’ or ‘worse’ and whether the trial aim is ‘superiority’ or ‘non-inferiority’. The result is that for normal usage the value entered will be +ve, as the following diagrams should make clear:\n\n\n\n\n\n\nFigure 4\n\n\n\n\nNote that in Superiority trials the required TRD for success will be greater than or equal to the TRD for futility, whereas in the Non-Inferiority trials the required NIM for futility will be greater than or equal to the NIM for success.\n\n\nNotes on setting Target Mean Differences\nTarget Rate Differences (TRD) are also referred to as Clinically Significant Differences (CSD). A “standard” hypothesis test for demonstrating superiority to control uses a CSD of 0. Testing with a non-zero CSD is different, and the implications need to be carefully understood.\nThe first mistake to avoid is setting the target rate difference for success too large. The decisions for success are in terms of the estimated posterior probability that the rate difference of the response on the treatment arm from the response on the control arm is greater than the target. If the CSD is set to what might be the treatment difference in the “alternate hypothesis” of a standard hypothesis test, we could only expect on average to have a posterior probability that the treatment difference is greater than the CSD of 50%.\nTo achieve posterior probabilities of &gt; 50%, we must set a CSD that we expect the treatment to exceed. To achieve the desired power by instead lowering the required posterior probability threshold would be a mistake, as posterior probability thresholds of &lt; 50% have the undesirable characteristic that the criteria can be met in circumstances where it can be seen that if further data was gathered consistent with what had been seen already, it would lead the threshold no longer being met. The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a target rate difference for success that, should the treatment have the value that it is hoped to achieve, we would expect to see some &gt;50% probability of being greater than it. Thus rather than using what might be termed ‘the target value’ for the TRDS, it is better to use ‘the minimum acceptable value’.\nThe same target difference can be used to decide futility, requiring a &lt;&lt; 50% confidence that the difference of the response rate on the treatment arm from response rate on the control arm is greater than the target. However, particularly if there are other endpoints not being explored in the simulation, or other properties of the treatment (such as convenience, compliance, cost, tolerability etc.) that might justify continued development even if it is not an outright winner on the primary endpoint, it may be that a lower target rate difference needs to be set as the threshold for determining futility – for example sufficient to demonstrate that development even on the basis of non-inferiority on the primary endpoint is likely to fail. Hence FACTS allows separate CSDs to be set for assessing success and futility.\n\n\nNon-inferiority\nIn a trial to show non-inferiority, the tab is the same except that ‘Target Rate Differences’ are now ‘Target Non-inferiority Margins’.\n\n\n\n\n\n\nFigure 5: Group Info tab, in a non-inferiority design"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/dichotomous.html#visits",
    "href": "documentation/v72/userguides/enrichment/study/dichotomous.html#visits",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Visits",
    "text": "Visits\nIf longitudinal modeling is not being used, then simply the time to the final visit is specified:\n\n\n\n\n\n\nFigure 6: Time to endpoint\n\n\n\nOtherwise the visit schedule needs to be specified. The last visit in the schedule is taken to be when the final endpoint is observed. Visits can be specified one at a time by entering the week of the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week of the visit and the visit index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 7: Visit schedule"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/dichotomous.html#variants",
    "href": "documentation/v72/userguides/enrichment/study/dichotomous.html#variants",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of subjects).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different maximum subjects for each group that has had a cap specified on the Study &gt; Study Info tab and maximum “Total Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\nIn FACTS Enrichment Designs, as well as trial Success and Failure rates, a major Operating Characteristic that we often wish to estimate is the ability of the design (if the trial is successful) to select the ‘right’ groups, depending of course on the scenario being simulated. To enable FACTS to report this the user must specify on the Virtual Subject Response &gt; Explicitly Defined &gt; Group Response profiles which of the groups “Should succeed”, that is, it would constitute a ‘correct selection’ by the design in that scenario.\n\n\n\n\n\n\nFigure 8: The Variants tab, specifying 3 variants"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/execution.html",
    "href": "documentation/v72/userguides/enrichment/execution.html",
    "title": "Execution Tab",
    "section": "",
    "text": "The Execution tab allows the user to specify profiles for subject accrual rates and dropout rates."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/execution.html#accrual",
    "href": "documentation/v72/userguides/enrichment/execution.html#accrual",
    "title": "Execution Tab",
    "section": "Accrual",
    "text": "Accrual\nThe Accrual sub-tab provides an interface for specifying accrual profiles; these define the mean recruitment rate week by week during the trial. During the simulation, the simulator uses a Poisson process to simulate the random arrival of subjects with the specified mean accrual rate.\nAccrual profiles are displayed in left-most table on the screen, as depicted below. These accrual profiles may be renamed by double-clicking on them, and typing a new profile name. After creating a profile the user must create at least one recruitment region. Early in the trial design process, detailed simulation of the expected accrual pattern is typically not necessary and a single region with a simple mean accrual rate is sufficient.\n\n\n\n\n\n\nTip\n\n\n\nIf using a hierarchical model across groups and you know the group accruals are going to be staggered, this should be incorporated in the simulations earlier rather than later.\n\n\nTo model more accurately the expected accrual rates over the trial, the user may specify multiple regions for each accrual profile and separately parameterise them. Regions are added via the table in the center of the screen (Figure 8‑1). Within this table, the user may specify:\n\nthe peak, mean weekly recruitment rate,\nthe start date (in weeks from the start of the trial) for this recruitment region,\nwhether the region will have a ramp up phase and if so when the ramp up will be complete (in weeks from the start of the trial),\nwhether the region will have a ramp down, and if so when the ramp down start and when the ramp down will complete (in weeks from the start of the trial).\n\nRamp up and ramp down define periods of simple linear increase and decrease in the mean recruitment rate from the start to the end of the ramp. Note that simulation of accrual is probabilistic but ramp downs are defined in terms of time, so even if ramp downs are planned so that at the average accrual rate they will occur as the trial reaches cap, there is a risk in simulations with slow accrual, that ramp downs occur before the full sample size is reached. It is advisable to have at least one region that doesn’t ramp down to prevent simulations being unable to complete.\nA graph of the overall recruitment rate profile of and the currently selected region is displayed. As the recruitment parameters are changed, the graph will update to show the time at which full accrual is reached. An accrual profile that does not reach full accrual is invalid and cannot be used to run simulations.\nIn ED, as well as the overall recruitment rate, the relative recruitment into each group must be defined. This is interpreted as defining the relative proportion of the total population that would fall into each particular group. It is also possible to specify a delay (in weeks from the start of the trial) in accepting subjects of a particular group.\nWhen a group is not being recruited – either because recruitment in that group has not yet started or because that group is no longer being recruited – then the overall recruitment rate is reduced by the fraction corresponding to the relative proportion of the population constituted by that group.\n\n\n\n\n\n\nFigure 1: Accrual\n\n\n\nIn the screenshot above you can see\n\nThe two step ramp up in accrual from the two regions – one starting later than the other.\nIn the group accrual that the mild group (who constitute 50%of the population) will complete accrual first at about week 65.\nThe overall recruitment shows a drop after about week 65 because after then only the Moderate and Severe sub-populations will be recruited as the Mild subgroup should have reached its cap.\n\nNote that the accrual profile graph is only the mean expectation; actual accrual is simulated using exponential distributions for the intervals between subjects, derived from the mean accrual profile specified here. Thus some simulated trials will recruit more quickly than this and some more slowly.\nThere are commands to import and export region details from/to simple external XML files. When importing, the regions defined in the external file are added to the regions already defined, they don’t replace them. Import/export allows an accrual profile with a large specification of regions to be copied to another FACTS design by exporting it from this design and importing it into the other.\nThe simple XML format means that these region definitions could be shared with other software. This is an example of a simple region file defining with 4 regions:\nRegion 1: has a peak rate of 1 and starts on week 0\nRegion 2: has a peak rate of 1 and starts on week 4, with a ramp up that completes on week 8\nRegion 3: has a peak rate of 1 and starts on week 6, with a ramp down starting week 40 that completes on week 50\nRegion 4: has a peak rate of 2 and starts on week 10, with a ramp up that completes on week 14 and a ramp down starting week 45 that completes on week 55.\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;regions&gt;\n&lt;region&gt;\n&lt;name&gt;Region 1&lt;/name&gt;\n&lt;rate&gt;1&lt;/rate&gt;\n&lt;start&gt;0&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;region&gt;\n&lt;name&gt;Region 2&lt;/name&gt;\n&lt;rate&gt;1&lt;/rate&gt;\n&lt;start&gt;4&lt;/start&gt;\n&lt;ramp-up&gt;\n&lt;ramp-complete&gt;8&lt;/ramp-complete&gt;\n&lt;/ramp-up&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;region&gt;\n&lt;name&gt;Region 3&lt;/name&gt;\n&lt;rate&gt;1&lt;/rate&gt;\n&lt;start&gt;6&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down&gt;\n&lt;ramp-start&gt;40&lt;/ramp-start&gt;\n&lt;ramp-complete&gt;50&lt;/ramp-complete&gt;\n&lt;/ramp-down&gt;\n&lt;/region&gt;\n&lt;region&gt;\n&lt;name&gt;Region 4&lt;/name&gt;\n&lt;rate&gt;2&lt;/rate&gt;\n&lt;start&gt;10&lt;/start&gt;\n&lt;ramp-up&gt;\n&lt;ramp-complete&gt;14&lt;/ramp-complete&gt;\n&lt;/ramp-up&gt;\n&lt;ramp-down&gt;\n&lt;ramp-start&gt;45&lt;/ramp-start&gt;\n&lt;ramp-complete&gt;55&lt;/ramp-complete&gt;\n&lt;/ramp-down&gt;\n&lt;/region&gt;\n&lt;/regions&gt;\n\n\n\n\n\n\nFigure 2: Accrual"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/execution.html#drop-out-rates",
    "href": "documentation/v72/userguides/enrichment/execution.html#drop-out-rates",
    "title": "Execution Tab",
    "section": "Drop-out Rates",
    "text": "Drop-out Rates\nThe Dropout Rate sub-tab provides an interface for specifying dropout profiles; these define the probability of subjects dropping out during the trial.\nDropout profiles are listed on the left of the screen, as depicted below. These profiles may be renamed by double-clicking on them, and typing a new profile name. The default dropout scenario is that no subjects drop out of the study before observing their final endpoint data.\nThe continuous and dichotomous engines allow specification of dropout rate slightly differently than the time-to-event engine.\n\nContinuous/DichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 3: Dropout Rate specification tab for a continuous or dichotomous endpoints.\n\n\n\nIn the Continuous and Dichotomous engines, the following options for dropouts exist:\n\nHaving no dropout modelling (“No Dropouts”);\nSpecifying dropouts as a fraction of all patients per visit (“Dropouts per visit”);\nSpecifying dropouts for each arm and visit (“Dropouts per arm and visit”).\nSpecifying dropouts for each group, arm and visit (“Dropouts per group, arm and visit”).\n\nMissing Data - A subject that drops out after the first visit will contribute to the Bayesian modeling and frequentist analysis. Final response estimates are imputed based on the completed visits and the selected longitudinal model for Bayesian modeling, and LOCF for the frequentist analysis. However, a subject that drops out before the first visit will be included in subject randomization counts, but excluded from the frequentist analysis and will make no net contribution to the Bayesian modeling. When the longitudinal modeling option has been disabled, no intermediate visits are simulated and for all dropouts the latter approach applies.\n\n\n\n\n\n\n\n\nFigure 4: Dropout Rate specification for time-to-event endpoints.\n\n\n\nIn the Time-to-Event engine, the following options for dropouts exist:\n\nHaving no dropouts included in the simulation (“No Dropouts”);\nSpecifying dropouts by group;\nSpecifying dropouts by group and visit;\nSpecifying dropouts for each group, and segment.\n\nDropout rates can be specified either by the mean time or a hazard rate (in weeks). When specified by group and segment, a set of dropout specific time segments can be entered; these do not have to match the segments used in defining the control hazard rate (or segments used in the design).\nMissing Data – Only subjects that drop out before experiencing an event matter, such a subject’s data becomes censored, contributing only to the overall exposure time."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html",
    "href": "documentation/v72/userguides/crm.html",
    "title": "CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system.\n\n\n\nThis document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nPlease cite FACTS wherever applicable using this citation.\n\n\n\nAn overview of the acronyms and abbreviations used in this document can be found here."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-purpose",
    "href": "documentation/v72/userguides/crm.html#sec-purpose",
    "title": "CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-scope",
    "href": "documentation/v72/userguides/crm.html#sec-scope",
    "title": "CRM",
    "section": "",
    "text": "This document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-citing",
    "href": "documentation/v72/userguides/crm.html#sec-citing",
    "title": "CRM",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-definitions",
    "href": "documentation/v72/userguides/crm.html#sec-definitions",
    "title": "CRM",
    "section": "",
    "text": "An overview of the acronyms and abbreviations used in this document can be found here."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "href": "documentation/v72/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 7.1 Changes to N-CRM",
    "text": "FACTS 7.1 Changes to N-CRM\nIn FACTS 7.1 there were new features added to N-CRM:\n\nIt is now possible to backfill to the current escalation dose (also known as “frontfilling”).\nIt is now possible to specify a third queue concept – maximum number of patients in their DLT period on the current MTD estimate.\nIt is now possible to define the concept of “near” target/MTD as part of stopping rules, for both fine-grained and regular dosing."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "href": "documentation/v72/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 7.0 Changes to N-CRM",
    "text": "FACTS 7.0 Changes to N-CRM\nIn FACTS 7.0 the only change was on the Analysis tab, when entering subject data manually, the FACTS GUI now ensures that a cohort number is entered for every subject."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "href": "documentation/v72/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.5 Changes to N-CRM",
    "text": "FACTS 6.5 Changes to N-CRM\nIn FACTS 6.5 there was a new feature added to N-CRM:\n\nIt is now possible to generate a design report – a Word document describing design - once the design has been simulated. In FACTS 6.5 there was two small changes to the functionality:\nWhen deriving toxicity/efficacy priors from quantiles at the lowest and highest doses, the “middle” probability option now refers to the median dose rather than the reference dose and is now optional. When pre-6.5 FACTS files are loaded in FACTS 6.5, the “middle” probability option will be ignored and replaced by 0.5.\nWhen deriving toxicity/efficacy priors from specific quantiles the specification of at least two dose levels is now required whereas previously the specification of at least three dose levels was required.\n\nIn FACTS 6.5 there were some improvements in the simulated behavior:\n\nDesigns which include efficacy, the “Maximum cohorts used to determine MTD” parameter on the Allocation Rule tab is now observed, in FACTS 6.4 and earlier it was ignored.\nIn N-CRM designs using open enrollment without accrual pausing, FACTS will now output entries in the cohort file for subjects that have been lost following stopping rules being met.\nIn N-CRM designs using open enrollment, FACTS will no longer prematurely apply stopping conditions when there are still subjects whose outcomes have been observed, but not yet processed.\nIn N-CRM designs using open enrollment, FACTS will now report the selected MTD/MED/OSD and associated state to be the dose where the stopping conditions were first met, unless the MTD has subsequently decreased post stopping rules being met. This is to prevent a dose above the one selected when the stopping conditions were met being reported as the MTD when it is very likely that there is insufficient data on this higher dose to justify its selection. If rather than reporting the MTD at the point when the stopping rules where met, you would like the trial to resume if the dose selected as MTD has changed (and this the stopping rules possibly no longer met), ensure that the ”Pause accrual and wait for completers” option is selected on the “Stopping Criteria” tab. This allows the trial to resume if the recruitment cap has not been met.\nFACTS Command Line mode and FLFLL (FACTS Linux File Loader Lite) now correctly handle the processing of N-CRM designs whose prior was derived using the “Legacy Prior” option."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "href": "documentation/v72/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.4 Changes to N-CRM",
    "text": "FACTS 6.4 Changes to N-CRM\nIn FACTS 6.4 there were no changes to the Dose Escalation simulators."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "href": "documentation/v72/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.3 Changes to N-CRM",
    "text": "FACTS 6.3 Changes to N-CRM\nIn FACTS 6.3 a number of changes were made to improve facilities in N-CRM, or improve the way existing facilities were implemented. These were:\n\nNew run-in options: the existing run-in scheme is available as “simple run-in”, “custom run-in” allows a specific sequence of doses and number of subjects to test at each dose to be specified, “small cohort pre-escalation” allows a run that uses a smaller cohort size but follows the dose escalation rules and over dose control.\nNew “backfill” options in open enrolment. Backfill allows subjects that become available at a time when they can’t be allocated to the current dose (because the maximum number of subjects without final results have already been allocated to the current dose).\nImproved handling of “maximum subjects without final results” in open enrolment. In earlier versions of FACTS this was a “global” maximum, which led to a suboptimal allocation pattern and overly cautious rejection of subjects that became available. The new model applies a maximum “per dose” so that once the trial has escalated to a new dose strength, any subjects without final results on lower doses do not block allocation to the new dose, in addition it is possible to specify two different maximums – one for when a dose has just been escalated to but has not been “cleared” (typically smaller and more cautious), and one when a dose has been cleared but we continue to allocate to it because it is the target dose (typically larger and more confident). This method is such an improvement that we recommend moving any design using open enrolment to this new version of FACTS.\nImproved Ordinal Toxicity model – the way the likelihood is calculated has been improved – reducing the uncertainty in the model fit. Any design using an ordinal model will need to re-calibrate the prior if you move the design to FACTS 6.3. If you have a design already complete, or in execution we recommend you remain using the earlier version of FACTS for that trial."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "href": "documentation/v72/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.2 Changes to N-CRM",
    "text": "FACTS 6.2 Changes to N-CRM\nIn FACTS 6.2 features available separately in the other FACTS CRM engines (CRM (Toxicity), bCRM & CRM Ordinal) were all incorporated into N-CRM. This allowed these features to be used in conjunction with N-CRM’s target toxicity band methodology, overdose control and open enrollment features, as well as in conjunction with each other for the first time.\nThe new features are:\n\nFrom CRM (Toxicity) the option to specify that the data is coming from ‘two groups’ and for the toxicity experienced in the two groups to be modelled with a joint model [CRM 2 Sample]. This allows a trial where there are two patient populations (such as adults and children) or where there are two versions of the treatment to be simulated.\nFrom bCRM the option to model a second binary efficacy endpoint [bCRM] and the for dose allocation to proceed in two stages – the first to establish an MTD and the second to establish an MED.\nFrom CRM Ordinal the option for the toxicity endpoint to be modelled not as binary endpoint, but one with different categories of toxicity, and with a joint model applied to the different categories [CRM Ordinal]. The endpoint can be to model either 3 or 4 categories of toxicity:\n\ncategory 1 is “no toxicity”,\ncategory 2 is “mild toxicity”,\ncategory 3 is “toxicity”\ncategory 4 (if included) is “severe toxicity”\n\n\nAll decision making is made in terms of the probability of observing a category 3 (or worse) toxicity."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "href": "documentation/v72/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.1 Changes to N-CRM",
    "text": "FACTS 6.1 Changes to N-CRM\nIn FACTS 6.1 N-CRM has the new feature of ‘design variants’ that allow the user to easily simulate and evaluate an N-CRM design at different sample sizes. This change includes 4 elements:\n\nUnder the ‘Study’ tab the user can now specify the number of design variants, and for each variant the maximum study size in Cohorts.\nOn the simulation tab FACTS will display a copy of each simulation scenario for each variant.\nThe simulation results now include the Ppn of trials that stopped for each stopping reason: stopping because all doses are too toxic (the toxicity estimates exceed the overdose criteria), because a stopping rule was met or because the study cap was reached.\nThere are now a set of cross variant graphs that show trellis plots of the key summary graphs by design variant and scenario."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#overdose-control",
    "href": "documentation/v72/userguides/crm.html#overdose-control",
    "title": "CRM",
    "section": "Overdose Control",
    "text": "Overdose Control\nOverdose control can be specified on the Study &gt; Toxicity tab. Overdose control specifies a limit on the probability that a dose has a toxicity rate above a certain level. After fitting the Bayesian logistic regression model, all doses for which the posterior probability that their toxicity rate lies above the specified level exceeds the specified limit are ineligible for allocation. Because the Bayesian Logistic regression is monotonic, this means that after every analysis either all doses are permitted for allocation or there will be a dose level above which no dose is permitted for allocation.\n\n\n\n\n\n\nFigure 1: Setting the overdose control limit\n\n\n\nThe overdose control is specified in terms of the “toxicity bands” (concept of allowing ranges for the target toxicity, excess toxicity, unacceptable toxicity and under-dosing explained in more detail in this section) and can either be in terms of the “excess and unacceptable toxicity bands” or just the “unacceptable toxicity band”. The “excess and unacceptable toxicity band” is every toxicity rate above the upper bound of the target band. Care should be taken when setting the permitted threshold for this joint band. If set below 0.5, it will likely exclude doses whose mean expected toxicity rate is within the target band with the risk that this makes the escalation decision in the design too cautious. Initially it might be recommended to just use the “unacceptable band” for specifying the overdose control. This allows an overdose control that is more strict – for example: “exclude any dose where the probability that the toxicity rate is above 0.6, is greater than 20%“. The lower bound for the unacceptable band can be set wherever desired, its only role is in defining this band for overdose control. It is also possible to specify that the limit changes over the course of the trial, allowing the overdose control to become stricter as more information becomes available. For example, one could reduce the permitted probability of a dose having a toxicity rate in the unacceptable band from 50% to 25% in steps of 2.5% after every cohort."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#dose-escalation-rules",
    "href": "documentation/v72/userguides/crm.html#dose-escalation-rules",
    "title": "CRM",
    "section": "Dose Escalation Rules",
    "text": "Dose Escalation Rules\nThe dose escalation could be solely controlled by the overdose control (as originally proposed in (Neuenschwander, Branson, and Gsponer 2008)), however this means that the escalation behavior is very dependent on the interplay between the prior and the observed data. Usually, teams prefer to have a fixed set of rules in place ensuring the escalation behavior is sufficiently cautious. FACTS has an option to just use overdose control or to use a combination of overdose control and a set of fixed escalation rules. In the latter case, the following rules can be set in the Design &gt; Allocation Rule &gt; Allocation tab:\n\n\n\n\n\n\nFigure 2: Dose escalation rules\n\n\n\nWe introduce the notion of whether a dose has been “cleared”. A dose is cleared once we have sufficient data on it (usually, but not necessarily, the results of one cohort, but if the cohort size is small, for example 2 subjects, perhaps more than one cohort will be required). This can be supplemented by a rule that if the observed raw toxicity rate at the dose exceeds a certain limit, then the dose is not counted as cleared (this rule is usually unnecessary if overdose control limits have been set). Once a dose has been cleared, it stays cleared, meaning there is “maximum cleared dose”. The number of dose increments or the factor of dose strength above the current cleared dose that can be allocated to is then specified. For example, with doses of 12.5, 25, 50, 100, 150, 200, 250, we might allow escalation at two dose increments a time. In the figure below, you see the combination of settings used to achieve this behaviour alongside the “Fastest Possible Dose Escalation” plot on the right:\n\n\n\n\n\n\nFigure 3: Escalation by number of dose increments\n\n\n\nAlternatively, we can specify the permitted escalation as a ratio, for example we might allow the dose strength to be at most tripled at each escalation, which, with the example dose strengths, makes the initial escalation more cautious:\n\n\n\n\n\n\nFigure 4: Escalation by dose strength factor\n\n\n\nThe escalation rules can be adjusted so that instead of a single increment rule, there are different increments depending on the dose, or depending on the number of observed toxicities. To modify our earlier example, we can allow escalation by 2 dose levels while no toxicities have been observed, but limit it to only one dose level once one or more toxicities have been observed:\n\n\n\n\n\n\nFigure 5: Escalation increment varying by number of toxicities\n\n\n\nLastly escalation can be relative to the highest cleared dose, or relative to the last dose allocated.\nTo summarize the allocation procedure:\n\nThe current maximum cleared dose is identified.\nThe current data is analyzed using the Bayesian Logistic Regression model.\nThe overdose rules are evaluated and all doses exceeding the overdose control limit are excluded from this escalation selection.\nFrom the remaining doses, the dose best meeting the target MTD or target toxicity interval objective based on the model is selected as the “target dose” (TD).\nIf the TD is at or below the current maximum cleared dose, the next cohort is allocated to the TD.\nIf the TD is within the escalation rules of the current maximum cleared dose, the next cohort is allocated to the TD.\nOtherwise, the next cohort is allocated to the highest dose above the current maximum cleared dose as allowed by the escalation rules."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#initial-run-in",
    "href": "documentation/v72/userguides/crm.html#initial-run-in",
    "title": "CRM",
    "section": "Initial Run-in",
    "text": "Initial Run-in\nThe purpose of defining a run-in is to define a fixed allocation behavior to be followed up to the first toxicity being observed. The specified number of subjects to allocate to each dose in the run-in and which doses to test are specified. This scheme is followed until a toxicity is observed or we reach the end of this fixed scheme.\nThree forms of run-in specification are available:\n\nSimple: allocates a small cohort to every defined dose in ascending order (unless fine grain doses - see this section – have been specified, in which case the escalation rules are followed).\nCustom: allocates a defined number of subjects (possibly varying by dose) to selected doses in ascending order.\nSmall cohort pre-escalation: allocates a small cohort, but follows the escalation rules assuming just a single small cohort is required to clear a dose.\n\nAll run-in schemes can be modified in a number of ways:\n\nSpecifying a maximum dose at which the run-in stops if no toxicities are observed until that dose.\nIf ordinal toxicities are being simulated, the run-in may should at the first observed category 2 toxicity (rather than a category 3 toxicity)\nWhether the subjects used in the run-in should be counted towards the trial sample size or not.\nWhen a toxicity is observed the standard behavior is to allocate to the minimum of: the last dose tested in the run-in, the current TD or the highest dose that can be allocated to by the overdose rules. This can be replaced by expanding the allocation on the current dose to make it a full cohort as specified in Study &gt; Study Info tab (this option is particularly useful in conjunction with stopping for a category 2 toxicity)."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#two-groups",
    "href": "documentation/v72/userguides/crm.html#two-groups",
    "title": "CRM",
    "section": "Two Groups",
    "text": "Two Groups\nFACTS has the option to model the subjects in the trial as belonging to two different groups, these can be either:\n\nTwo groups distinguished by a baseline property of the subjects, for example adults and paediatrics.\nTwo groups separated by a difference in treatment (and selected randomly), for example the study drug alone or in combination with an additional drug.\n\nThere are options for when group 2 starts enrolling:\n\nThey can be recruited sequentially – group 1 then group 2.\nThey can be recruited in parallel\nThe second group can be started when the allocation to the first group reaches a particular dose\nThe second group can be started when the number of subjects allocated to group 1 reaches a particular threshold.\n\nA joint model is fitted to the two groups.\nThe first group is modeled:\n\\[\nlogit(p_{1j}) = \\alpha + \\beta \\hat{x}_j\n\\]\nThe second group is modeled:\n\\[\nlogit(p_{2j}) = (\\alpha + a) + (\\beta + b) \\hat{x}_j\n\\] With separate priors and some optional constraints on \\(a\\) and \\(b\\). Dose escalation and stopping are judged independently for the two groups."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-concepts-efficacy",
    "href": "documentation/v72/userguides/crm.html#sec-concepts-efficacy",
    "title": "CRM",
    "section": "Efficacy",
    "text": "Efficacy\nFACTS has the option to additionally model an efficacy endpoint. There are currently two limitations in simulation:\n\nOnly a binary efficacy endpoint can be simulated\nThe efficacy endpoint is assumed to be available at the same time as the toxicity endpoint.\n\nThe efficacy and toxicity endpoints are modelled separately. There are options to specify early stopping rules for finding the MTD, and to specify a cap on the sample size that can be spent finding the MTD. Once these rules are met, then allocation is towards the Minimum Efficacious Dose (MED) – if this is below the MTD. If the estimated MED lies at or above the estimated MTD, the allocation is at the estimated MTD.\nIf while allocating to the estimated MED further toxicity results change the estimate of the MTD, and if there is now insufficient information on the MTD as specified by the early stopping rules for finding the MTD, allocation switches back to allocating to the estimated MTD, if the sample size cap for finding the MTD allows."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-concepts-fgd",
    "href": "documentation/v72/userguides/crm.html#sec-concepts-fgd",
    "title": "CRM",
    "section": "Fine Grain Dosing",
    "text": "Fine Grain Dosing\nIn some settings, e.g. when the drug is delivered in solution by IV or when manufacturing allows any dose in a range from say 100mg to 400mg in steps of 10mg, dose strengths need not be restricted to just a small number of pre-defined levels. FACTS has a feature that allows this to be simulated, not with a continuous range of doses, but with “fine grain” dosing.\nFACTS supports the specification of a range of doses from a minimum to a maximum with doses either equally spaced or spaced with equal ratio. Using dose ratio makes most sense it you want to use the dose strength whilst believing the effect will be roughly log-dose. Using dose ratios, it’s necessary to accept FACTS reporting dose strengths only close to those desired. As an example, if the main doses followed a dose doubling scheme: 12.5, 25, 50, etc., one might use fine grain dosing with dose space ratios of approximately the 4th root of 2 (1.1892). The resulting doses are 12.5, 14.865, 17.677, 21.022, 24.999, 29.729, 35.354, 42.043, 49.998, etc., which means there are three dose levels between each of the original doses.\nThere are two alternatives:\n\nUse nominal dose strengths 1, 2, 3, 4, … (i.e. assuming the dose spacing is linear in expected effect) and label the doses according to their actual strength.\nUse a fixed dose interval (e.g. 12.5 resulting in doses of 12.5, 25, 37.5, 50, 62.5, etc.) so the lower doses (of the original scheme) have fewer (or no) intermediate doses and the higher doses have many more. The dose escalation rules can be specified in terms of dose strength ratio to achieve the required escalation, for example allowing dose escalation with a dose strength ratio of 2 will result in the initial escalation using doses 12.5, 25, 50, 100, etc.\n\nAs well as possibly adjusting the dose escalation step size to accommodate the new dose levels on the Design &gt; Allocation Rule tab, there are two other rules that may need modification:\n\nTo count a dose as “cleared”, we might now count cohorts on nearby doses to count towards the required clearing total. This is specified as the “Max ratio of dose strengths considered as near” (if dose allocation rules apply to ratio of dose strength) or “Delta in dose strength considered as near” (if dose allocation rules apply to dose strength) on the Design &gt; Allocation Rule tab.\n\nFor example, if we have doses at roughly 4th root of 2 intervals, we might count any dose within a ratio of 1.2 as “near” so that any cohorts allocated to immediate neighbor doses count towards clearing a dose.\nAlternatively, if we have doses every 12.5mg from 12.5 to 400, counting any dose within a ratio of 1.1 will mean that from dose 125 and above, immediate neighbor doses (within 12.5) count towards clearing a dose, and from dose 250 and above, doses within 25mg (two immediate neighbor doses) count towards clearing a dose.\n\nThe concept of “near doses” in fine grained dosing allows us to skip certain doses in the escalation phase, which might make sense if there is reason to believe that doses of similar dose strengths behave similarly and don’t provide enough additional information to justify assigning more cohorts to.\n\n\n\n\n\n\nFigure 6: Doses from 12.5 to 400mg, with fixed spacing of 12.5. Showing dose escalation by dose doubling.\n\n\n\nWhen requiring a certain number of cohorts to have been allocated to the estimated MTD before the trial can stop / to allow the trial to stop, we might now count cohorts on doses near the estimated MTD as counting towards that total. This is set on the Design &gt; Stopping Criteria tab. In considering which doses are near, the same logic as on the Design &gt; Allocation Rule tab regarding Dose Strength or Ratio of Dose Strength will be used.\n\nIf Dose Strength is used, then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength. For example, by +/- 12.5 mg:\n\n\n\n\n\n\nFigure 7\n\n\n\nIf ratio of Dose Strength is used then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength ratio. For example, by +/- 10%:\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nNote that with Fine Grain dosing, if a band is specified for a dose to count as cleared, then the maximum cleared dose will be the maximum dose within that band, and if incrementing relative to the Maximum cleared dose, then the maximum permitted increment will be relative to the maximum dose within the cleared band."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-concepts-oe",
    "href": "documentation/v72/userguides/crm.html#sec-concepts-oe",
    "title": "CRM",
    "section": "Open Enrollment",
    "text": "Open Enrollment\nOpen enrollment (Broglio et al. 2015) can be used instead of cohort enrollment. Cohort enrolment enrolls a fixed number of subjects to a given dose, then waits until their treatment and follow up is complete (so their final status – whether they suffered a DLT (Dose Limiting Toxicity) or not – is known) before deciding on the next dose to allocate to and then recruiting the next cohort. This likely means that subjects become available for inclusion in the trial, but have to be turned away as the trial waits for the current cohort to complete. Open enrollment attempts to address this by allowing subjects to be enrolled whilst the current “cohort” is completing. However, this may come with some risk – more than a cohort’s worth of subjects may now be exposed to a new dose before we have any estimate of its DLT rate. To allow this risk to be managed, open enrollment introduces two new concepts:\n\nWhen allocating to an uncleared dose, a cap can be set on the number of subjects that can be allocated to that dose who have not yet got their final results (“OE cap 1”). For example, if this number is set to 3 (to be the same as a common cohort size), after 3 subjects have been recruited and allocated to the current dose, no more subjects will be allocated to this dose until at least one of these subjects has completed. Until then, potential subjects that become available will be turned away unless backfilling is enabled (see next point below). But unlike cohort enrolment, as soon as the first of the subjects on the dose completes, a subject that comes available could now be allocated to the dose, depending on further rules explained below (frontfilling) – unless of course that subject’s result has changed the estimated MTD. Note that the trial won’t escalate beyond the current dose until the required number of subjects to clear the dose have completed. By default, the trial won’t allocate more than the number of subjects required to clear the dose until the dose is cleared, meaning if 3 subjects are required to clear a dose and 3 subjects have been allocated to this dose, even when 1 or 2 of these subjects have their final results and a new subjects is enrolled, they won’t be allocated to this dose. If this is regarded as over cautious, it can be modified by enabling frontfilling, allowing 3 subjects without final results simultaneously. In the above example, this would mean we could place a fourth subject on the dose when the result of the first subject has come in and a fifth subject as soon as the result of the second subject has come in.\nWhen the cap on the number of subjects without final results on an uncleared dose has been reached (“OE cap 1”) new potential subjects will be turned away, unless backfilling is enabled. Enabling backfilling allows these subjects to instead be included, allocating them to a lower dose that has already been cleared. Whilst such an allocation may not contribute as much to identifying the MTD as allocating to the current dose would, it can still contribute by:\n\nIncreasing the information on the next lower dose can inform the estimate of toxicity on the current dose through the Bayesian logistic model.\nProviding additional information on a dose that it may be necessary to de-escalate too if the current dose turns out to be too toxic.\n\nIt can also contribute information on other endpoints (such as efficacy). Once backfilling has been enabled, it is also possible to enable frontfilling. For more information on backfilling and frontfilling, see this section.\n\nAssume at a given point in time we want to allocate a subject to a specific dose, denoted by “candidate dose”. FACTS allows 3 different caps to be specified on how many subjects who have not yet got their final results (i.e. are not yet complete) can be allocated to this candidate dose:\n\n\n\n\n\n\nFigure 9\n\n\n\n\nMaximum subjects without final results if dose is uncleared: As described early in this section, we encounter this cap during escalation when the candidate dose is not yet cleared. This cap takes into account subjects not yet complete on the candidate dose and any higher dose (“OE cap 1”).\nMax subjects without final results if dose is cleared and below MTD: We encounter this cap when the candidate dose is cleared and below the estimated MTD (which can happen if the estimated MTD is beyond the range of available doses, when backfilling, or when allocating during the efficacy phase of a toxicity plus efficacy trial). This cap takes into account subject not yet complete on the candidate dose and any lower doses (“OE cap 2”).\nMax subjects without final results if dose is cleared and at MTD: We encounter this cap when the candidate dose is cleared and the current model estimated MTD (which can happen when after clearing a dose we decide not to escalate, or after de-escalating) (“OE cap 3”)."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-concepts-bf",
    "href": "documentation/v72/userguides/crm.html#sec-concepts-bf",
    "title": "CRM",
    "section": "Backfilling and Frontfilling",
    "text": "Backfilling and Frontfilling\nAs described in the preceding section, Backfilling is the allocation of subjects to a lower dose when, due to restrictions, it is not possible to allocate a subject who comes available to the current dose (Dehbi, O’Quigley, and Iasonos 2021). FACTS provides a number of options to configure how backfilling behaves. Backfilling can be enabled in the Study &gt; Study Info tab. The total sample size can be divided between the subjects allocated as part of conventional dose escalation and those allocated using backfill. When backfill is enabled, it is important to increase the total sample size and then limit the number that can be allocated using backfill, as subjects allocated using backfilling will not contribute to the escalation and the confirmation of the MTD and it’s usually important to retain sufficient sample size to achieve this aim.\n\n\n\n\n\n\nFigure 10\n\n\n\nWhen enabling backfilling, several options can be specified in the Design &gt; Backfill Allocation tab.\n\n\n\n\n\n\nFigure 11\n\n\n\n\nTwo maximum caps can be specified on the number of subjects that are assigned in the process of backfilling to a given dose:\n\nan overall cap on subjects per dose that cannot be exceeded by backfill, counting also subjects that were assigned to that dose through regular allocation\na cap on the number of subjects per dose that were allocated by backfill, counting only subjects that were assigned to that dose using backfilling.\n\nHow many dose levels below the current dose can be allocated to when backfilling. Backfilling will always be to the highest dose possible (which might be the current dose if frontfilling is enabled, otherwise it will be below the current dose). Allocation to the next highest dose might be limited either by an open enrolment cap if there are already subjects allocated to that dose who have not yet completed, or it might be limited by the backfill caps described above. If allocation to the dose below the current dose is not possible, backfilling will by default look at the dose below that (two levels below the current dose) and so on. Using this option can ensure no backfilling happens to doses that are too far below the current dose.\nThe lowest dose that can be allocated to when backfilling. This option is particularly useful when there is reason to believe doses below a certain level will not be effective.\nWhether frontfilling is allowed – frontfilling allows allocating more subjects to uncleared doses than the number required to clear that dose (see this section)."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "href": "documentation/v72/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "title": "CRM",
    "section": "Open Enrollment, Backfilling and Fine Grain Dosing",
    "text": "Open Enrollment, Backfilling and Fine Grain Dosing\nWhen using open enrolment and fine grain dosing, the interval defined on the Design &gt; Allocation Rule tab “Delta in dose strength considered as near +/-“, or “Max ratio of dose strengths considered as near” is crucial: it is used to define the range of doses where subjects allocated to any of them count towards clearing a dose.\n\nIf dose allocation rules are selected to apply to “Dose strength”, the interval is defined “Delta in dose strength considered as near +/-“. Thus, for example, if this is set to 2, then subjects complete on doses with strength in the range 4-8 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 8.\nIf dose allocation rules are selected to apply to “Ratio of strength”, the interval is defined “Max ratio of dose strengths considered as near“. Thus, for example if this is set to 1.5, then subjects complete on doses with strength in the range 4-9 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 9. These ranges also apply when assessing “OE cap 1-3”, and how many subjects have been allocated overall, or via backfilling. In backfilling, FACTS checks each dose strength and the doses in its “near” interval range, at (if frontfilling) or below the current dose, until the first dose strength is found where backfilling can take place."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-studyinfo",
    "href": "documentation/v72/userguides/crm.html#sec-studyinfo",
    "title": "CRM",
    "section": "Study Info",
    "text": "Study Info\nThe Study Info sub-tab provides parameters for specifying:\n\nWhether the trial has an Efficacy endpoint as well as a Toxicity one.\nWhether recruitment is in Cohorts or uses Open Enrolment,\nWhether the trial data is being analyzed as a single population (single group) or two groups (which could be 2 different patient types, or 2 different treatment types).\nThe option to specify that the trial should include an expansion cohort once the MTD has been identified.\nThe option (if using open enrolment) to specify the use of backfill.\n\nIncluding an efficacy endpoint – this allows the trial to include a binary efficacy outcome that is observed at the same time as the toxicity endpoint. Once the MTD has been sufficiently determined further cohorts are allocated to determine the MED (Minimum Effective Dose) as long as that is below the MTD, until the maximum sample size or MED stopping rules have been reached.\nCohort versus Open enrolment: cohort enrolment is the standard way of running a phase 1 trial, a cohort of subjects of pre-determined size are treated at the current dose and the trial pauses until all the subjects in the cohort are complete, then the dose for the next cohort is determined. A phase 1 trial using Open Enrolment recruits ‘all comers’ with a cap on the number of subjects who can be treated but have not yet had a final result. Subjects that arrive whilst this cap has been reached are deemed to be ‘lost’ to the study.\nIf Open Enrolment and Efficacy Endpoint options are being used together, then subjects who arrive who cannot be allocated to the MTD (because the cap number of subjects awaiting a final result has been reached), can be allocated to the MED (as long as that is below the MTD).\nIf the trial is analyzing 2 Groups then a joint statistical model is used with options to constrain the group 2 difference in the intercept term to be +ve or -ve, and options as to whether a common or separate estimates of the slope term are used.\nIf an expansion cohort is included, the this is a single cohort (or one per group, if 2 groups is being used) typically much larger than used during the dose escalation, that is assigned at the end of the study to the target dose. FACTS simulates the results that arise from this cohort and a final analysis.\nIf open enrolment is being used, the further option to use backfill becomes available. The parameters on this tab for backfill, are to specify the maximum number of subjects that can be allocated for escalation, and the maximum that can be allocated in backfill. These two maximums should not total less than the overall “Max subjects” that can be enrolled. If adding backfill to a trial, usually the previous “Max subjects” becomes the “Max study allocation for escalation”, and an additional sample is allowed for backfill and added to the overall Max subjects.\nIf the trial has two groups the backfill maximums are the sum of the subjects in the two groups.\nFor Cohort enrolment, the parameters are:\n\nMaximum Study Size, in cohorts: the maximum number of cohorts the trial can use, though designs can include conditions that cause them to stop earlier.\nIf the trial has two groups, the maximum number of cohorts of the second group.\nCohort size: The number of subjects in each cohort. The maximum sample size for the trial is simply: Max Trial Size (cohorts) * Cohort Size.\nCohort information delay: Normally left at zero, this allows for the trial to be simulated with cohorts recruited more quickly than if waiting for the last cohort to complete before treating the next cohort. This allows the trial to complete in shorter time, but is likely to lead to a larger incidence of toxicities. If the delay is set to 1, the second cohort will be recruited before the results for the first cohort are available, the third cohort will be recruited after the results of the first cohort are available but before the results for the second cohort are available, and so on.\nExecution rate: the time taken to recruit, treat and complete the observation of each cohort (in weeks). The value of this parameter does not affect the behavior of the simulations, but it allows a nominal “duration” of each simulation to be calculated. Unlike other FACTS simulations, this duration is not simulated stochastically, it is simply the number of cohorts * this duration. Its purpose is to give a figure to compare with open enrolment designs of the same trial.\n\n\n\n\n\n\n\nFigure 15: Study Info - Cohort Enrolment\n\n\n\n\n\n\n\n\n\nFigure 16\n\n\n\nIf rather than Cohorts, subjects are recruited using open enrolment, the parameters are:\n\nMax subjects: the maximum number of subjects who can be recruited into the study.\nTime unit – this is a text string that will change the “units” label for time on graphs. This allows data to be more easily entered when the natural time unit is not “weeks”, but “days” or “months”.\nMean recruitment rate: for simulation purposes this is the average number of subjects who will be expected to be available for recruitment during each time period. Recruitment is simulated using a standard Poisson process.\nTime until final result: the time for each subject for their final result to become available. This is simulated as a fixed length of time that is the same for all subjects.\nMaximum subjects without a final result: the cap on the number of subjects on the current dose or a backfill dose (if backfill is enabled) who can be recruited but not yet completed. This is also known as the ‘maximum queue length’. Subjects arriving and available for recruitment while the number of subjects on this dose who have been treated but have not yet completed is at this maximum, are dropped and assumed no longer available for recruitment. Once the current subjects complete the study has to await further new subjects to become available. There are three caps:\n\nMaximum subjects without final results if the dose is uncleared. This allows the design to be cautious when a new dose is used for the first time.\nMaximum subjects without final results if dose is cleared and below MTD. This allows a larger number of subjects without final results to be recruited at backfill doses or at the current escalation dose if backfill to the current escalation dose (“frontfilling”) is enabled, or at the MED in the efficacy phase of a trial that includes an efficacy endpoint.\nMaximum subjects without final results if dose is cleared and at MTD. This allows us to be more cautious if the model thinks all doses are toxic or if we are allocating at the model MTD and don’t want to expose too many subjects.\n\nBackfill – this can be enabled. Backfilling is the allocation of subjects to a dose below the current target dose, if the number of subjects allocated to the current target dose without final results has reached the maximum. Further parameters for backfill are set on the “Backfill” tab under the “Design” tab. On this tab, if backfill is enabled, two sub-maximums can optionally be specified:\n\nthe maximum number of subjects who can be allocated as part of usual allocation for escalation and MTD determination (and MED if efficacy is included in the trial),\nand the maximum number of subjects who can be allocated as part of “backfill”.\n\n\n\n\n\n\n\n\nFigure 17: Study Info - Open Enrolment\n\n\n\nGroups: a trial can be analyzed as a “single group” or as “two groups”. If analyzed as a single group, then all subjects are assumed to be the same and treated the same (except for the difference in the dose strength). If analyzed as two groups this allows either:\n\nThe subjects can be simulated as coming from two similar but distinct groups such as: adults and children, first line or recurrent, having some concomitant treatment or not. The separation into the two groups is based on some property of the subject.\nOr the subjects can be simulated as having been allocated (possibly randomized) to one of two versions of the treatment, with the same rang of dose strengths and differing in some other way such as dosing schedule, treatment duration or combination with an additional treatment. The separation of the subjects into the two groups is under the control of the protocol.\n\nIn either case the same analysis options are available (hence we use the generic term “groups” to describe this feature).\nIf enrolment is by cohort, the there are two separate “maximum study sizes” in cohorts – one for each group.\nThe Group 2 recruitment, while it overlaps in time with the Group 1 recruitment, is simulated as being in lock-step and the recruitment of the cohort in each group is concurrent and analyzed when both are complete. In the ‘cohorts.csv’ files that are output, the cohort numbers indicates which cohorts were concurrent. The options are:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the first group1 cohort has been allocated the specified dose (if cohorts can be accrued before the cohort before has completed, the group 2 is accrued too – it does not wait until the group 1 cohort completes unless the next group 1 also waits).\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the specified number of subjects had been recruited into group 1.\n\nIf enrolment is open then there are options similar to the cohort enrolment to control when enrolment into group 2 starts:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 subject can be recruited after the first group1 subject has been allocated the specified dose.\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 subject can recruited after the specified number of subjects had been recruited into group 1.\n\nBut in addition the user specifies the maximum number of subjects in Group 2 and how the recruitment is to be simulated:\n\nWith the group membership a property of the subject – along with a mean recruitment rate for group 2.\nWith subjects randomized between the two groups (the randomization is fixed at 1:1).\n\nThe user specifies the three “Maximum subjects without final results …” for the second group.\nIf backfill is enabled, the backfill totals apply to the total of the subjects on both groups.\n\n\n\n\n\n\nFigure 18: Study info - 2 group options with open enrolment\n\n\n\nEnable Final Expansion Cohort: if this is enabled a final cohort of specified size will be allocated the dose selected as MTD at the end of the N-CRM phase of the study:\n\nIf the study includes a control arm, the number of subjects in this expansion cohort to be allocated to control is also specified.\nIf the study has two groups, two separate expansion cohorts will be allocated, their sizes are set separately.\nIf the study includes observing efficacy then the target dose can be changed from MTD to MED or OSD.\n\n\n\n\n\n\n\nFigure 19\n\n\n\nSimulating an additional efficacy outcome is simply specified by checking the “include efficacy” checkbox.\n\n\n\n\n\n\nFigure 20: Study tab with “Include efficacy” checked\n\n\n\nSimulating an efficacy endpoint can be combined with all the other features (two groups, open enrolment, backfilling) already discussed, as well as with ordinal toxicity and fine grain dosing that are described below.\nCurrently there are two significant limitations to the simulation of an efficacy endpoint:\n\nThe endpoint is assumed to be dichotomous.\nThe endpoint is assumed to be observable at the same time as toxicity.\n\nWe hope to lift these restrictions in a later version of FACTS."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#toxicity",
    "href": "documentation/v72/userguides/crm.html#toxicity",
    "title": "CRM",
    "section": "Toxicity",
    "text": "Toxicity\nThe Toxicity sub-tab provides parameters for specifying:\n\nWhether toxicity should be simulated as dichotomous or ordinal. If simulated as Ordinal, toxicity can be simulated as a 1-3 scale or 1-4 scale. In both cases ‘1’ is “no-toxicity”, 2 is “mild toxicity” and 3 is the toxicity of interest (from the point of view of defining the MTD, target toxicity band and Overdose Control. If a four category scale is selected, 4 is “sever toxicity” or death. Choosing whether to model the ordinal response is a separate option and is present on the Design &gt; Toxicity Response tab.\nType of target: this allows the user to specify whether the dose selection targets “the dose who’s estimated toxicity rate is closest to a specified target rate” or “the dose with the highest posterior probability of having a toxicity rate in a target band”. The former is the target rule used in the original CRM papers ((O’Quigley, Shen, and Gamst 1999), (deMoor et al. 1996), and the latter rule was introduced in (Neuenschwander, Branson, and Gsponer 2008).\nToxicity target (only displayed if the type of target is “a single dose”): this allows the target toxicity rate to be specified and whether the target dose is the one nearest, the one nearest but with a lower rate or the one nearest but with a higher rate.\nTarget: this panel allows the target toxicity bands to be specified along with overdose control limits. The panel is displayed and enabled even if the target type is “a single dose” to allow overdose control limits to be specified.\nType of Target: controls the selection of the dose for the next cohort – this can be to target a single dose (to replicate the original CRM behavior, see this section) or to target the dose with the highest probability that its toxicity rate lies in a target band.\n\n\n\n\n\n\n\nFigure 21: Toxicity tab targeting a toxicity band\n\n\n\n\nTargeting a Toxicity Interval\nTargeting a toxicity band or interval is an innovation introduced with the N-CRM design, unlike other CRM designs that select the dose that is expected to have a toxicity response closest to the desired tolerated limit, the N-CRM selects the dose that has the highest posterior probability of having a toxicity rate in a target toxicity band. This has the advantage of a) having a clearer probability statement and b) having in addition probability statements about the probability of under and overdosing (the toxicity rate being below or above the target toxicity band).\n\nThe uncertainty in the estimate of toxicity at each dose is expressed by calculating the posterior distribution of the estimate of the rate of toxicity at each dose and calculating the proportion of that distribution that falls in to each of 4 bands of toxicity: ‘Under-dosing’ (toxicity so low that it is likely that a higher dose could be used), ‘Target’ toxicity (we want to select doses whose toxicity rate is most likely to be in this band), ‘Excess’ toxicity (toxicity higher than desired) and ‘Unacceptable’ toxicity.\n\nUnder-dosing: this band always starts at 0.0; the user specifies the upper bound.\nTarget band: this band always starts at the upper-bound of the under-dosing band; the user specifies the upper bound.\nExcess toxicity: this band always starts at the upper-bound of the target band; the user specifies the upper bound.\nUnacceptable toxicity: this band always starts at the upper-bound of the target band, with an upper bound of 1.0. The graph shows the width of the different bands using a simple, fixed, example posterior probability distribution of a toxicity rate.\n\nIntervals are relative to control: if a control arm is included in the study, then toxicity bands can be defined as the difference in toxicity rate relative to control. Negative differences (a lower toxicity rate than control) are always treated as under-dosing. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\nLimit max excess/unacceptable toxicity: the ‘overdose control’ in terms of a maximum allowed posterior probability that a dose’s toxicity rate lies in either the ‘Excess’ or the ‘Unacceptable’ toxicity bands. Any dose with a posterior probability of having a toxicity rate in either of these two bands that is higher than the specified limit, cannot be selected for allocation to the next cohort, nor selected as MTD at the end of the study (this gives rise to slightly different results compared to those in (Neuenschwander, Branson, and Gsponer 2008) where the overdose control was not applied to final selection).\n\nIt is possible to have the overdose control limit vary with the number of cohorts allocated. In particular this can be used to reduce the overdose limit as the number of cohorts (and the amount of information) grows. For example for a particular prior and final level of overdose control, it may be that initial escalation is excessively constrained, one way to allow early escalation in this setting is to use these parameters to allow an higher initial overdose control limit and gradually reduce it over time to the final desired limit. Tuning the parameters will require some iteration and simulation. A varying limit is specified by the specifying amount to change the limit by per cohort and the final limit. The amount to change by is always entered as a number in the range (0,1), whether this is an increment or decrement depends on whether the target limit is greater or less than the initial limit. Leaving the change in limit at its default of 0 means the limit does not vary.\nLimit max unacceptable toxicity: as for the previous parameter, but here the overdose control is only in terms of the posterior probability that a dose’s toxicity rate lies in the ‘Unacceptable’ toxicity band.\nAs or the limit on excess/unacceptable toxicity it is possible to have the overdose control limit vary with the number of cohorts, see the description above.\n\n\n\nTargeting a single dose\nIt is possible to use the N-CRM design engine with a conventional CRM allocation strategy - to “allocate to the nearest / highest dose below the maximum tolerated toxicity”; this allows conventional CRM design to be simulated with some of the additional features of N-CRM:\n\nOverdose control\nEstimate both parameters of the 2 parameter logistic\nThe “Recommender” to analyze a specific data set.\n\nThe target is calculated by:\n\nIn the MCMC sampling loop finding the dose that meets the target criteria, a doses probability of being the target is then the proportion of times that dose meets the target criteria across the MCMC sampling.\nRather than selecting the dose with the highest probability, the dose at the 50% quantile is used. The cumulative probability of being the target is calculated over the doses in ascending dose strength, and the dose when the cumulative probability passes 50% is selected. This addresses some problems that can arise when very little data is available: that the dose with the highest probability is at one end of the dose range, but that probability is not that high, or that doses are not evenly spaced and a dose close to both its immediate neighbors may never have greater probability than both of them.\n\nSetting the Type of Target option to Target a single dose, modifies the tab thus:\n\n\n\n\n\n\nFigure 22: N-CRM, targeting a single dose, not a toxicity band\n\n\n\nWhen targeting a single dose FACTS allows the user to specify:\n\nThe target toxicity rate\nWhether to allocate to the dose with the mean estimate of its toxicity rate nearest the target, highest dose with a mean estimate of its toxicity rate below the target or lowest dose with a mean estimate of its toxicity rate above the target.\nAn option to use the toxicity rate relative to control, rather than the default of the absolute toxicity rate. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\n\nThe definition of the boundaries of the toxicity bands is still included in order to allow the specification of overdose control limits. These are calculated and applied in exactly the same way as when targeting a toxicity interval."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#efficacy",
    "href": "documentation/v72/userguides/crm.html#efficacy",
    "title": "CRM",
    "section": "Efficacy",
    "text": "Efficacy\nIf include efficacy has been checked on the Study Info tab, a simple additional input page is included:\n\n\n\n\n\n\nFigure 23: The Efficacy tab\n\n\n\nIf simulating and modelling an efficacy endpoint is included there are two items to be specified on this tab:\n\nWhether a subject experiencing a toxicity can also count towards efficacy or not. If unchecked patients outcomes are simply sampled separately and a patient can both have a toxicity and an efficacy response. If checked, a patient’s toxicity outcome is sampled first, and only if there is no toxicity is an efficacy outcome sampled.\nThe efficacy target – this consists of:\n\nThe target efficacy rate required for the Minimum Efficacy Dose (MED).\nWhether the target dose is the nearest dose to the MED rate, the lowest dose above the MED rate or the highest dose below the MED rate.\nIf a control arm has been included, whether the target rate is absolute or relative to the observed rate on the control arm."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#treatment-arms",
    "href": "documentation/v72/userguides/crm.html#treatment-arms",
    "title": "CRM",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nOn this tab the number of treatment arms (doses) available to the study is specified. The user can either define a set of specific doses that can be used or a continuous dose range with some granularity.\nSelecting Explicit Doses allows the user to specify the specific doses that can be used on the trial:\n\nA single new dose or multiple doses can be added either by clicking “Add” or “Generate”. Initially each dose is defined by a simple integer name and level. The dose levels and dose names can then be edited on by clicking on them and entering the desired value. The dose level can also be set later on in the Design &gt; Toxicity Response tab.\nThere is also the option to include a control arm. Including a control arm allows the toxicity rate to be relative to the control arm.\n\n\n\n\n\n\n\nFigure 24: The Treatment Arms tab specifying explicit doses\n\n\n\n\nFinely Spaced Doses\nSelecting Finely Spaced Doses allows the user to specify the dose range that can be used on the trial:\n\nThe minimum and maximum dose strength to be used\nThe ‘granularity’ of the actual dose used, either as a fixed delta (Fixed spacing) or a dose ratio (the ratio specified must be greater than 1) (Ratio spacing).\nThe number of ‘bins’ or ‘doses for which to report’ – this is because FACTS will still produce summary statistics in columns, many with a “column per dose” – it is possible to use more doses than it is practical to report on (and a limit in MS Windows of 32K pixels for the width of a table means that the GUI can only display simulation results for a maximum of ~40 doses). However this limitation is only for reporting summary statistics; the dose strengths modeled and allocated in the simulations are unaffected.\n\nSelecting ‘Finely Spaced Doses’ will also affect how some of the other parameters are specified in the FACTS GUI.\n\n\n\n\n\n\nFigure 25: The Treatment Arms tab, specifying a finely spaced dose range"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-variants",
    "href": "documentation/v72/userguides/crm.html#sec-variants",
    "title": "CRM",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of cohorts).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with different maximum numbers of cohorts.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Max Cohorts” for each variant.\nThese will then appear on the simulations tab.\nIf open enrolment is being used, then the enrolment cap is specified by the number of subjects.\nIf there are two groups then separate caps are specified for each group.\n\n\n\n\n\n\nFigure 26: The Variants tab, specifying 5 variants"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#explicitly-defined",
    "href": "documentation/v72/userguides/crm.html#explicitly-defined",
    "title": "CRM",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nToxicity profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 28. The user enters the toxicity rate to simulate at each dose into the Probability of Toxicity column of the table, and the graphical representation of these toxicity values updates accordingly.\nThis form of toxicity profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter toxicity rates for all of them. When using “finely spaced” doses the toxicity rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 27: Explicitly defined toxicity - binary endpoint\n\n\n\nIf the design is using ordinal toxicity, the toxicity response rates can be specified either:\n\nOn the “Toxicity” tab by specifying the category 3 or greater toxicity rate at each dose and then two offsets – one for the category 2 or greater rate and one for the category 4 rate.\nOn the “Ordinal Toxicity” tab by separately specifying the toxicity rates for each category of toxicity at each dose.\n\nSpecifying offsets: to ensure that the specified category 3 rate plus the category 2 offset doesn’t sum to more than 1, or the category 3 rate plus the -ve category 4 offset sum to less than 0, the offsets are applied to the logit of the category 3 toxicity rate.\nThus for the category 2+ rate:\n\\[\nln(\\frac{p_{2+}}{1-p_{2+}}) = ln(\\frac{p_{3}}{1-p_{3}} + \\Delta_2)\n\\]\nwhere:\n\n\\(p_{2+}\\) is the probability of observing a category 2 or greater toxicity at a dose\n\\(p_3\\) is the probability of observing a category 3 or greater toxicity at a dose\n\\(\\Delta_2\\) is the difference in the log odds between the two probabilities\n\nThe offset is defined at the lowest dose and highest dose and then varied linearly with dose strength at the intermediate doses. The plot of the curve can either use Pr(Tox) or Log-odds(Tox) as the y-axis and dose strength or log(dose strength) as the x-axis. A graph is displayed of the toxicity rates that have been entered, and the category 2+ and category 4 toxicity rates if applicable. This graph, as with all graphs in the application, may be copied to the clipboard or to a file using the “right-click” menu.\n\n\n\n\n\n\nFigure 28: Virtual Subject Response – Explicitly-Defined – ordinal endpoint\n\n\n\n\nExplicitly defined – Ordinal Toxicity\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 29: Virtual Subject Response – Explicitly-Defined: Ordinal Toxicity tab\n\n\n\n\n\nExplicitly defined toxicity – when simulating 2 groups\nIf simulating toxicity as a binary outcome, when simulating 2 groups, the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group.\n\n\n\n\n\n\nFigure 30: Explicitly defined toxicity - 2 groups\n\n\n\nIf simulating 2 groups and ordinal toxicity, then on the explicitly defined tab once again the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group. As for a single group the Category 2 toxicity and Category 4 (if using) toxicity rates are defined by defining log odds offsets at the lowest and highest dose. Specification is limited to a single set of offsets that are applied to both groups.\n\n\n\n\n\n\nFigure 31: Explicitly defined toxicity - 2 groups and ordinal offsets\n\n\n\nAs in the single group case in addition to the Category 3 toxicity rates that are editable, columns showing the Pr(Tox 2+) and Pr(Tox 4) are shown, but these are not editable and derived from the Pr(Tox) rates and the offsets that have been specified. The ordinal toxicity rates are only shown for group 1.\n\n\nExplicitly defined – Ordinal Toxicity with 2 groups\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 32: Explicitly Defined, Ordinal Toxicity with 2 Groups\n\n\n\n\n\nEfficacy response profiles\nEntering efficacy response profiles is very similar to entering toxicity profiles. FACTS will construct scenarios to simulate of every combination of toxicity and efficacy response profiles.\n\n\nExplicitly Defined – Efficacy\nEfficacy profiles may be added, deleted, and renamed just like toxicity profiles. The user enters the efficacy rate to simulate at each dose into the Probability of Efficacy column of the table, and the graphical representation of these efficacy values updates accordingly.\nAs with toxicity this form of efficacy profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter efficacy rates for all of them. When using “finely spaced” doses the efficacy rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 33: Efficacy virtual subject response, explicitly defined\n\n\n\n\n\nExplicitly Defined – Efficacy with two groups\nIf the design included 2 groups, when explicitly defining an efficacy response profile, there is simply a second column of efficacy response rates to enter:\n\n\n\n\n\n\nFigure 34: Explicity defined efficacy response profile with 2 groups"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-parametric",
    "href": "documentation/v72/userguides/crm.html#sec-parametric",
    "title": "CRM",
    "section": "Parametric",
    "text": "Parametric\nToxicity scenarios may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen. The user selects the model to use to determine the toxicity rate to simulate at each dose, and specifies the values of the model’s parameters. The graphical representation of these toxicity values updates accordingly.\nThe graph may be copied using the context menu functionality described in the previous section.\nFour models are available:\n\nLogistic: the probability of toxicity at dose x is given by: \\(P_x=\\frac{1}{1+e^{-s(x-x_{50})}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with linear effective doses \\(\\hat{x}=x-x_{ref}\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*(x_{ref}-x_{50})\\)\nEmax: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{x}{x+x_{50}}\\) with user specified parameter \\(x_{50}\\).\nLog Logistic: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{1}{1+e^{-s(ln(x)-ln(x_{50}))}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with log effective doses \\(\\hat{x}=ln(\\frac{x}{x_{ref}})\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*ln(\\frac{x_{ref}}{x_{50}})\\)\nPiecewise linear: with the probability of toxicity specified at a series of knots, with the probability linearly interpolated between knots.\n\n\n\n\n\n\n\nFigure 35: Virtual Subject Response - Parametric Toxicity tab\n\n\n\nIf Ordinal toxicity is being simulated then the category 2 and greater toxicity rates and category 4 toxicity rates are specified using the logit offset methods as on the Explicitly-Defined &gt; Toxicity tab.\n\n\n\n\n\n\nFigure 36: Virtual Subject Response - Parametric Toxicity tab with Ordinal toxicity\n\n\n\nIf Ordinal Toxicity and 2 groups are being simulated then both the Cat 2+ and Cat 4 toxicities and the Group 2 toxicities are defined using the logit offset methods.\n\n\n\n\n\n\nFigure 37: Parametric definition of ordinal toxicity response with 2 groups\n\n\n\n\nParametric efficacy response\nParametric efficacy response profiles function exactly like toxicity profiles, with the same parametric models to choose from and if 2 groups are present the response of the second group is again defined by 2 log-odds offsets, one at the lowest dose and one at the highest.\n\n\n\n\n\n\nFigure 38: Parametric efficacy response profile"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#external",
    "href": "documentation/v72/userguides/crm.html#external",
    "title": "CRM",
    "section": "External",
    "text": "External\nSubject response data may be simulated from a PK-PD model in place of, or in addition to, choosing a response model from FACTS. The importing of subject response data (which must be in the form specified in the System Requirements Document) may be done from the External Files sub-tab depicted below (Figure 39).\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id\nDose index (1, 2, 3,… if a Control is to be included it should be index 0) this is not the user settable dose name or dose level\nToxicity (0,1)\nEfficacy (0. 1) even if efficacy not being simulated this value must be present\nGroup (*1, 2) only required if groups are being simulated\n\nThe GUI requires that the file name has a “.dat” suffix.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.\nTo import an external file, the user must first add a scenario to the table. After adding a scenario, the user must click “Browse” to locate the externally simulated data via a standard file browser dialog.\n\n\n\n\n\n\nFigure 39: External Data"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#x-hats",
    "href": "documentation/v72/userguides/crm.html#x-hats",
    "title": "CRM",
    "section": "X-Hats",
    "text": "X-Hats\nOn this tab the user specifies the reference dose \\(d^*\\) for use in calculating the adjusted dose value (the “x-hat” values). The default value to use is Median dose is reference, this uses the median of the dose range for the reference dose, minimizing the correlation in the sampled values of \\(\\alpha\\) and \\(\\beta\\). Note though that when allocation is restricted to explicit doses it is also recommended that the value of the reference dose is not the same as an actual dose that can be used (at this dose \\(\\hat{x}\\) will be 0 and the data on this dose can have undue weight on the estimate of \\(\\alpha\\)).\nDifferent reference doses can easily be used however – between the two doses thought most likely to be MTD, just below the lowest dose, just above the highest dose. The bi-variate Normal prior for \\((\\alpha, ln(\\beta))\\) will need to be recalibrated to take the change into account.\nX-Hats are log(dose strength) allows the user to select between:\n\nlinear effective dose \\(\\hat{x}_j = d_j - d^*\\)\n\\(log(\\hat{x}_j) = ln(\\frac{d_j}{d^*})\\)\n\nIf you have entered linear dose strengths for the doses (1, 2, 3, 4, … or 100, 150, 200, 250, …) then use the linear effective dose. If however the dose strengths that have been entered are non-linear (12.5, 25, 50, 100, …) but expected to be roughly linear in effect, then use the log of the dose ratio.\n\n\n\n\n\n\nFigure 40: Specifying the dose transformation - the “x-hats”.\n\n\n\n\nThe Pro’s & Con’s of using the median dose as the reference dose\nThe reason the median dose is recommended as the reference is that this minimizes the correlation in the fit of \\(\\alpha\\) and \\(ln(\\beta)\\), the parameters of the BLRM, and it maximises the flexibility of the fit of the model over the dose range.\nHowever care needs to be taken that the prior on \\(\\alpha\\) is not more restrictive than that on \\(ln(\\beta)\\) in order to avoid a phenomena observed when preparing tutorials: observing “no toxicities” below the reference dose resulted in a model with increased probability of toxicity above the reference dose compared to observing a toxicity below the reference dose. For a given value of \\(\\alpha\\), higher values of \\(ln(\\beta)\\) correspond to lower toxicity below the reference dose – as the \\(\\hat{x}\\)̂ values are -ve below the reference dose. The fitted curve thus “pivots” about the value of \\(\\alpha\\) at the reference dose.\nThere are two solutions to this:\n\nmove the reference dose, which involves a choice between two options\n\nmoving it to the first dose or below (normally allowing a relatively constrained prior around a low value for \\(\\alpha\\)),\nor to the highest dose or above (with a relatively uninformative prior).\n\nWe have seen both solutions perform well against the chosen scenarios – but the choice needs checking and refining with a full range of scenarios that represent the full uncertainty in the true response.\nor modify the priors on \\(\\alpha\\) and \\(ln(\\beta)\\) making the prior on \\(\\alpha\\) less informative (in particular increase the probability of low values) and make the prior on \\(ln(\\beta)\\) more informative (in particular lower the probability of high values less). Because the prior distribution on \\(\\beta\\), is on \\(ln(\\beta)\\), it is easy to make large values of \\(\\beta\\) more probable than intended."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#toxicity-response",
    "href": "documentation/v72/userguides/crm.html#toxicity-response",
    "title": "CRM",
    "section": "Toxicity Response",
    "text": "Toxicity Response\nThe parameters that can be specified on this page are:\n\nThe parameters of the bivariate Normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\). Specifying the mean and standard deviation of \\(\\alpha\\) \\((\\mu_{\\alpha}, \\sigma_{\\alpha})\\), and \\(ln(\\beta)\\) \\((\\mu_{ln(\\beta)},\\sigma_{ln(\\beta)})\\) and the correlation coefficient \\(\\rho\\).\n\nIf ordinal toxicity is being simulated, it is possible to model the ordinal toxicity, specifying the mean and standard deviation of \\(\\alpha_2\\) and \\(\\alpha_4\\). These priors are separate from the \\(\\alpha_3\\) and \\(ln(\\beta)\\) prior, there is no correlation term in the prior. There is the constraint in the model that \\(\\alpha_2 &gt; \\alpha_3 &gt; \\alpha_4\\).\nUse fixed Alpha: the value of Alpha can be fixed to allow the N-CRM model to behave like the traditional CRM models. [Where \\(\\alpha\\) was set to 3 and the reference dose is set above the top of the available dose range]\n\n\nRather than entering the priors directly, they can be derived based on indirect prior information or beliefs, see ‘Deriving the Prior’ below.\n\nThe Minimum and Maximum rates that the model is to be fitted too. The model fits the range \\((0,1)\\), asymptotically approaching each limit as the adjusted dose value tends to \\(-\\infty\\) or \\(+\\infty\\). By specifying an alternative minimum and maximum, inside the range \\((0,1)\\), the user can have the model scaled to fit data to fit event rates where the asymptotic rates are not \\(0\\) or \\(1\\). For instance if the event being observed has a non-zero background rate (probability of being observed in placebo treated subjects), then the model may fit better if the minimum is set to the lower limit of this expected rate. Similarly if, even at the most toxic dose the event being observed is only expected to effect a proportion of subjects, the model may fit better if the maximum is set to the upper limit of this expected rate.\nIf a control arm is present, the user can specify to have this modelled separately, and if so the user specifies the parameters for a prior Beta distribution – in terms of numbers of prior observations on control of subjects with and without a toxicity.\nGroup 2 priors: if a second ‘Group’ is being simulated – whether this is a subset of subjects, or a modified treatment that subjects can be randomized to, then the BLRM is jointly fitted to the responses for both groups, with group 2 having offsets \\(a\\) and \\(b\\) from the first group’s \\(\\alpha\\) and \\(\\beta\\). The priors for \\(a\\) and \\(b\\) can be full bivariate Normal or can use constraints such as \\(b = 1\\), or \\(a &gt; 0\\) or \\(a &lt; 0\\).\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\nDeriving the prior\nThe priors of \\(\\alpha\\) and \\(ln(\\beta)\\), can be specified directly or derived in one of four ways. When entered explicitly, the user specifies the parameters of the prior bivariate-normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\): the means, standard deviations and the correlation term \\(\\rho\\).\nAlternatively, the user may click the ‘derive prior’ button and select from:\n\nQuantiles at the lowest and highest dose: (based on the “uninformative prior” given in the paper (Neuenschwander, Branson, and Gsponer 2008), for details see this section) - the user specifies the probability of an unacceptable toxicity at the lowest dose, and the probability of under-dosing at the highest dose (0.1 for both is the default, and 0.05 for both is the value used in the paper). Optionally the probability that toxicity is less than the mid-point of the target toxicity band at the median dose can be specified. (Prior to FACTS 6.5 this third data point was not optional and constrained to be at the reference dose, but this had problems if the reference dose was not the media dose – it might also be the lowest dose for example).\nNote this method does not work so well if the reference dose is outside the dose range.\n\n\n\n\n\n\nFigure 42\n\n\n\nScenarios: the model is fitted to each of the toxicity response scenarios (MLE), the parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\n\n\n\n\n\nFigure 43\n\n\n\nSpecific quantiles: The user selects which doses and toxicity rates to provide an expectation – a prior probability that the toxicity rate on the dose will be the specified rate or less. At least 3 such expectations using at least 2 different doses strengths must be supplied. If a large number of specific quantiles are specified (e.g. reproducing the all quantiles method) the large number of different beta distributions sampled from, with the monotonicity constraint applied, results in losing too much variability. So this should only be used quantiles specified at 2-4 doses.\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\nAll quantiles: the user specifies the prior expected toxicity rate at the 2.5%, 50% and 97% quantiles for each dose. (Only available when using explicitly defined doses, not a continuous dose range). Note that using Create Prior with this option will require the facts file to be saved and for there to be at least one virtual subject response profile.\n\n\n\n\n\n\n\nFigure 45\n\n\n\nIn all cases once prior values have been derived they are displayed along with a graph of 100 sampled curves from the prior. The user can accept the values, change derivation method, or cancel the derivation.\nThe plot of the samples can either be viewed as Pr(Tox) or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\nDerivation of the Prior from Quantiles\nDerivation of the parameters of the bivariate Normal prior for \\(\\alpha\\) and \\(ln(\\beta)\\)) in the Quantiles at lowest and highest dose, Specific quantiles and All quantiles cases:\n\nMinimally informative unimodal Beta distributions are fitted for each of the doses where a prior expectation of a toxicity has been specified. For doses where no prior expectation has been specified, the median expected toxicity rate are derived by assuming that the median expected toxicity is linear in log dose on the logit scale, and again a minimally informative unimodal Beta distribution is fitted with the same median.\nPreviously and following (Neuenschwander, Branson, and Gsponer 2008), the parameters of the bivariate Normal distribution were found using a stochastic fit to the prior expectations of toxicity, minimizing the error in the prior toxicity rates at the 2.5%, 50% and 97.5% quantiles. This is still used in the All quantiles and Legacy prior cases. However experience with this method with the standard priors (previously called “uninformative”) showed that it yielded priors with too little uncertainty in the \\(ln(\\beta)\\)) and too high a value for the correlation parameter for many cases and certainly for the prior to be called “uninformative”.\nConsequently, in the Quantiles at lowest and highest dose and Specific quantiles cases, the prior is now derived by sampling from the minimally informative unimodal Beta distributions, and fitting the model to each set of sampled toxicity rates. The parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\nIf a control arm has been included, it may be included in the model, or modelled separately using a beta-binomial model, the user specifies the prior values for the Beta distribution.\n\n\nDerivation of the prior from the scenarios\nIn this screenshot, priors have been derived from the scenarios:\n\n\n\n\n\n\nFigure 46: Design - Toxicity Response sub-tab (after prior derivation)\n\n\n\nClearly in this example the scenarios have a very high correlation between the toxicity at the reference dose (\\(\\alpha\\)) and the log gradient (\\(ln(\\beta)\\)) giving a high value for the correlation in the prior (\\(\\rho\\)). As there are very few scenarios, and they didn’t include extreme cases, the SDs of the priors of parameters will be underestimated.\nSo In this instance we might round the prior means to 2 decimal places, double the SD of \\(\\alpha\\), slightly increase the SD of \\(ln(\\beta)\\) and reduce the correlation to 0.5.\nHowever, it is much better to use Drive from Scenarios after entering a large number of varied and credible scenarios. Indeed is such a collection of scenarios exists, deriving the prior from the scenarios is the simplest approach and often very effective. Only if the performance in the simulations in some scenarios does the prior need re-visiting (usually to slightly increase the \\(ln(\\beta)\\) SD and/or reduce the correlation).\nWe strongly recommend checking the performance of the prior across a wide range of scenarios, and of entering the reported derived values of the fitted prior as an explicit prior and then manually modifying them in the light of the model’s performance on the various scenarios."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-legacy-prior-methods",
    "href": "documentation/v72/userguides/crm.html#sec-legacy-prior-methods",
    "title": "CRM",
    "section": "Legacy prior methods",
    "text": "Legacy prior methods\nIn old versions N-CRM (pre-FACTS 4.0) , the design could be left at the stage where the method of deriving the prior had been specified but the derivation postponed to the simulation stage. We now require that the derivation be performed first, this\n\nEnables the actual prior that results from the derivation to be inspected\nMakes simulation and recommendation faster, as the derivation is not repeated every time the design engine starts.\n\nWhen opening a FACTS N-CRM file created using a pre-FACTS 4.0 version of FACTS, to continue to use the original prior, select “Derive prior”, and the derive prior window will display the old prior and allow an explicit prior to be derived from it using the now deprecated methods in the older versions of FACTS N-CRM.\n\n\n\n\n\n\nFigure 47: Derivation from a legacy prior"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-efficacy-response",
    "href": "documentation/v72/userguides/crm.html#sec-efficacy-response",
    "title": "CRM",
    "section": "Efficacy Response",
    "text": "Efficacy Response\nThe Efficacy Response tab is displayed if an efficacy endpoint is included in the trial.\nThe Efficacy Response is specified separately from the Toxicity Response. The Toxicity and Efficacy models are completely separate except for the x-hat values for the transformed dose strengths, where a common set of values is used for both models. Note that the use of a common set of x-hats for both endpoints is a difference from FACTS bCRM, that means it may not be possible to exactly replicate a bCRM design in FACTS N-CRM, however we think that the additional features and options in FACTS N-CRM will make it possible to create an overall superior design in FACTS N-CRM.\nThe features available for specifying the Efficacy Response model are the same as the Toxicity Response model (see above) – with the exception that the Toxicity Response includes an option for modeling ordinal toxicity, there is no corresponding ordinal efficacy option.\n\n\n\n\n\n\nFigure 48\n\n\n\nThe same options for deriving the prior are available as for the Toxicity Response Model."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-prior-pseudo-subjects",
    "href": "documentation/v72/userguides/crm.html#sec-prior-pseudo-subjects",
    "title": "CRM",
    "section": "Prior Pseudo-subjects",
    "text": "Prior Pseudo-subjects\nThis option allows “prior data” or “pseudo data” to be specified that will be included in every analysis. This is equivalent to the model being fitted with the parameter prior to this pseudo data and the resulting posterior being the new prior, but it is easier and quicker to include the pseudo subject with the real data and do one analysis.\nThe user selects which dose levels at which to include the data and specifies the number of pseudo/prior subjects/observations and the number of toxicities. These are allowed to be fractional, and observations can be at dose levels not being tested in the trial. In each analysis the observed data is augmented with this specified data and the parameters of the toxicity response estimated.\nIf there is an efficacy endpoint as well as toxicity endpoint, pseudo subject data is specified separately for the two endpoints (not surprisingly!). If the data is to be analyzed as “2 groups” pseudo subject data is also specified separately for the two groups.\nThe effect of this data on the prior can be visualized by the “Update Plot” function that estimates the parameters of the toxicity response and plots the curves of 100 samples drawn from the posterior estimates of the parameters of the model.\nThe plot of the samples can either be viewed as Pr(Tox) vs Dose Strength or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\n\n\n\n\nFigure 49: Specifying prior pseudo subjects\n\n\n\n\n\n\n\n\n\n\nExample of effect of pseudo subjects on prior\n\n\n\n\n\nPrior Only\n\n\n\nPrior plus 0.5/0 subjects toxicities on dose 1 and 1/0.5 on dose 8.\n\n\n\nPrior plus 3/0 subjects / toxicities on dose 1 and 3/1.5 on dose 8."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#allocation-rules",
    "href": "documentation/v72/userguides/crm.html#allocation-rules",
    "title": "CRM",
    "section": "Allocation Rules",
    "text": "Allocation Rules\nThe Allocation Rule sub-tab is depicted below (Figure 50); it allows the user to select and set the parameters for the allocation rules.\nThere is an option to mimic the original Neuenschwander paper and rely on just the toxicity model and the overdose control to guide allocation – i.e. allocate to as close to the estimate of the MTD as we can limited only by the overdose control limits. This is “Use only overdose control”. Normally however we think the clinical will want to impose additional allocation rules.\nThe next dose allocated is a combination of the current target dose, and 2 possible maximum allocatable doses:\n\nThe target dose is the dose with the highest probability of being the target: the dose with the greatest estimated probability that its toxicity rate lies in the target toxicity band, or the dose nearest or highest below the MTD depending on the options selected on the Study &gt; Toxicity tab.\nThe highest dose that meets the overdose criteria (if any): the highest dose that does not have a posterior probability that its toxicity rate lies in the excess & unacceptable toxicity bands or unacceptable toxicity bands above the threshold specified on the Study &gt; Toxicity tab.\nThe current cleared dose and how far above that dose can be allocated to as defined by the specified allocation rules (if any).\n\nThe next dose to be allocated to is the lowest of these 3 doses.\nIt is possible to specify a “run-in” phase before this dose escalation phase applies. A run-in phase has a fixed sequence of doses and cohort sizes (typically smaller than the cohort size used in the escalation phase) and lasts up to the first observed toxicity or the end of the sequence of doses.\n\n\n\n\n\n\nFigure 50\n\n\n\nIf the user selects to use an “Initial run-in” then there are 3 run-in types that can be selected from:\n\n“Simple run” (the only type available before FACTS 6.3): cohorts are of a single size and the dose sequence up to the “Run-in cannot go beyond” dose (if specifed) is either:\n\nat every dose starting at the specified “initial dose” if explicit doses have been defined on the the Study &gt; Treatment Arms tab,\nat the dose increment intervals defined in the allocation rules if finely spaced doses have been defined on the Study &gt; Treatment Arm tab.\n\n“Custom run-in” where the user specifies which doses are selected (leaving the “number of subjects” at a dose at 0 mans it is not selected) and at each dose how many subjects are allocated in the cohort at that dose.\nSmall cohort pre-escalation, cohorts are of a single size and the dose sequence follows the dose increment intervals defined in the allocation rules up to the “Run-in cannot go beyond” dose (if specifed).\n\n“Simple run-in” and “Small cohort pre-escalation” the user specifies the “small cohort size” and there is an option to specify a top dose that the “Run-in cannot go beyond”.\nFor all run-in schemes there are options:\n\nEnd run-in on 1st category 2 toxicity: The default for the simulation is to stop the run-in when the first full toxicity is observed, there is an option to instead stop when a lower grade toxicity is observed – a category 2 toxicity. If this option is selected, the simluation of virtual subjects is extended to include the simulation of category 2 tocxicity as well as category 3.\nInclude run-in subjects in overall maximum\nWhen run-in ends expand last cohort to full size\n\nIf this is not set then N-CRM model is applied and the next cohort is allocated as close to the target as possibly, restricted by the overdose restrictions and not allocating any higher than the dose reached in the run-in.\nIf this option is set then last allocated small cohort is treated as the start of a full cohort and the remaining subjects are allocated at the same dose. The N-CRM model is then applied and the next cohort allocated according to the overdose restrictions and allocation rules. [Note it is possible that after observing no toxicities in the run-in or in the expanded cohort, that the overdose control will force a de-escalation in dose, depending on the priors for the model parameters and the overdose control limits]\n\n\nIf simulating 2 groups, and a run-in is specified, group 2 will only use a run-in if on the Study tab the option “Recruit Group 1 and Group 2 together” has been selected. Otherwise only group 1 will use the specified run-in, once group 2 starts it starts with the full cohort (or if open enrollment is being used, the full number of subjects to clear the dose). If using a run-in and recruiting groups 1 and 2 together then both use the same run-in rules, and both will stop on the first toxicity regardless of which group the toxicity occurs in.\nIf used, the Allocation rules have the effect of setting a Highest Allocatable Dose that specifies the highest dose level that can be allocated to by the allocation rules.\n\nAt the start the Highest Allocatable Dose is the user specified Starting dose level.\nA dose is not ‘cleared’ until ‘The minimum cohorts on/near a dose before cleared’ have been allocated to it. If this is set to greater than one, then the specified number of cohorts must have been allocated to the current dose before it is ‘cleared’. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nIf using Open enrolment, this parameter is ’The minimum subjects on a dose before cleared’ and refers to the number of complete subjects.\nThe user can specify that Dose not cleared if proportion toxic on dose &gt; and a maximum level of toxicity that can be tolerated for the current dose to be ‘cleatred’. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe user specifies whether the dose increment rules are to be specified in terms of the number of dose levels that can be incremented or in terms of the ratio by which the dose strength can be increased. If using “dose strength” it is up to the user to ensure that the specified ratio is sufficient to allow all doses to be reached. For example if in the dose range there is a dose of strength 100 and the next dose is of strength 200, then if any dose increment rule is specified that doesn’t allow the dose strength to at least double (using “ratio of dose strength” of less than 2) then the rule will prevent escalation from the 100 dose to the 200 dose.\nThe maximum amount the dose can be incremented can be specified in 3 ways:\n\nSingle value: For all doses, once a dose has been cleared the next ‘highest allocatable dose’ is the dose above the cleared dose by the specified increment – either a number of dose levels or by a proportion of the dose strength.\nBy total number of toxicities: With this rule the maximum dose increase allowed depends on the total number of toxicities that have been observed in the trial so far. The user specifies the maximum increment (in terms of the number of dose levels or the maximum proportional increase in dose strength) when no toxicities, one toxicity, or more than one toxicity has been observed.\n\n\nFigure 9-5 Maximum dose increment varying by number of toxicities observed\n\n\n\n\n\n\nFigure 51: Maximum dose increment varying by number of toxicities observed\n\n\n\n\nBy dose levels: the maximal permitted increment is defined in terms of the number of dose levels or the proportional increase in dose strength that the dose can be increased, in up to 3 bands of dose strength. The user defines the upper and lower doses of the middle increment range, the lower range is then from the lowest up to this band and the upper band is from the top of the middle band to the highest dose. The user then specifies the maximum number of dose levels or the maximum proportional dose strength that can be incremented if the current dose is within each of these bands.\n\n\n\n\n\n\n\nFigure 52: Maximum dose increment varying by dose level\n\n\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently allocated dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\nThe “Fastest Possible Dose Escalation” graph shows the dose allocation permitted by the rules if no toxicities are observed. Or the graph can be changed to show the allocation if a toxicity occurs at a specific dose during the run-in.\nIf there are 2 groups and recruitment to the second group is delayed (either until the first group is complete, has reached a specific dose, or tested a specified number of subjects). Then starting dose for the second dose can be specified as\n\n\n\n\n\n\nFigure 53: Design - Allocation Rule Initial Dose Second Group\n\n\n\n\nAllocation rules when using a fine grained dose range\nWhen a fine grained dose range is being used, the specification of the allocation rules change to accommodate the fact that the trial is no longer stepping up a few pre-defined dose levels. As with the explicit doses, the allocation rules work with the notion of a Currently Permitted Maximum Dose (CPMD).\n\nAt the start the highest allocatable dose is the user specified Starting dose strength.\nA Run-in phase can be specified, this will always begin at the start dose and allocate ‘small cohorts’ following the maximum increment rules (this is different from when there are explicit doses – with explicit doses, the simple run-in simply allocates successive small cohorts to successive doses, ignoring the maximum permitted increment) for a simple run-in or small cohort pre-escalation, or the specified allocation pattern for a custom run-in, until the first toxicity is observed.\nThe degree of increment and specification of what counts as a ‘close’ dose, can be done either in Dose Strength or Ratio of dose strength.\nA dose is not cleared until ’The minimum cohorts on a dose before incrementing’ have been allocated to it. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nThe user can specify that Dose not cleared if ppn toxic on/near dose &gt; and a maximum level of toxicity that can be tolerated before the dose is cleared. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe Max ratio of dose strengths considered as near or Delta in dose strength considered as near defines a margin such that when evaluating whether a dose that has been allocated to counts as “cleared”, then cohorts allocated within that margin all count as being “on” the dose to allow incrementing.\nThe Maximum increment selected by option allows the user to specify varying maximum increments either dependent on the current cleared dose (Dose strength), or on the Total number of toxicities that have been observed, as follows:\n\nSingle value: the amount by which the highest allocatable dose can be above the current cleared doses is constant throughout the trial. (This simple rule can work well when combined with overdose control. The two more complex rules are essentially trying to achieve the same thing as overdose control but are more simplistic and it may be confusing as to whether the allocation rule or overdose control is preventing escalation at any given moment).\nTotal number of toxicities: the amount by which the highest allocatable dose is above the current cleared dose is specified separately for whether zero, one or two or more toxicities have been observed in the entire study.\nDose strength: The user specifies:\n\nthe increment at low doses;\nthe increment at medium doses, along with the upper and lower dose strengths that define the bounds of what constitutes a ‘medium dose’; and\nthe increment at high doses.\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently cleared dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\n\n\n\n\n\n\nFigure 54: Allocation rules with a finely spaced doses"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-backfill-allocation",
    "href": "documentation/v72/userguides/crm.html#sec-backfill-allocation",
    "title": "CRM",
    "section": "Backfill Allocation",
    "text": "Backfill Allocation\nIf the trial is using open enrolment then “Backfill” allocation is an option. Backfilling is enabled on the Study &gt; Study Info tab, where the maximum number of subjects that can be allocated when backfilling is specified.\nBackfilling is the allocation of subjects to a dose below the current dose when the maximum number of subjects on the current dose without final results has been reached.\nThe Backfill Allocation tab allows detailed control of when backfilling can be used. The user specifies:\n\nThe maximum overall number of subjects that can be on a dose for backfilling to be allowed to that dose.\nThe maximum number of subjects that can be allocated to a dose by backfill.\nThe maximum number of dose levels below the current dose that can be used for backfilling (the highest that can be backfilled to will be used)\nThe lowest dose strength that can be backfilled to.\nWhether or not the current escalation dose is a candidate for backfilling as long as the maximum number of subjects in their DLT period (set in the Study &gt; Study Info tab) is not exceeded (also known as “frontfilling”).\nIf frontfilling is enabled, whether these subjects should count towards the backfill allocation cap or the regular study allocation cap (specified in the “Study/Study Info” tab).\n\n\n\n\n\n\n\nFigure 55: Backfill Allocation"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-stopping-criteria",
    "href": "documentation/v72/userguides/crm.html#sec-stopping-criteria",
    "title": "CRM",
    "section": "Stopping Criteria",
    "text": "Stopping Criteria\nThe simulation will always stop when the maximum number of cohorts or subjects has been allocated.\nThe user may also specify that the study may stop early in terms of the amount of information that has been gathered about the target dose. In the N-CRM, the target dose can either be the Maximum Tolerated Dose or the “the allowed dose that has the highest posterior probability of having a toxicity rate in the Target Toxicity Band”; we use the label MTD (Maximum Target toxicity Dose) for this target dose too, for brevity and familiarity from previous CRM methods.\nNote: if overdose limits have been set (see this section), then doses with posterior probabilities of having a toxicity rate in the Excess Toxicity or Unacceptable Toxicity bands that are greater than the specified thresholds, are disallowed for both dose allocation and selection as MTD.\nTo enable early stopping, the user must select “Rules for stopping trial early” there are then 2 rules which if selected are always “AND”ed together, and a further block of rules, the result of which (if specified) are “AND”ed with the first two rules. If more than one rule is selected within the block these may be either logically “AND”ed together or “OR”ed together to give the result of the rules in the block.\nThe two first standalone rules are:\n\nIf a “Required number of cohorts/subjects near MTD” rule is set then the trial will only stop early if at least this number of cohorts or subject has been allocated to the MTD dose or nearby doses. In the box above the “Count as MYD doses differing from MTD by less than or equal” allows how far away a dose can be and for cohorts/subjects on those doses to count towards this total. This can be set to 0 to count only cohorts/subjects on the MTD. It is provided for when fine grain dosing or a large number of explicit doses have been specified. If specified this rule this must always be met along with any other rules set for the trial to stop early.\nIf a “Minimum number of cohorts/subjects accrued” has been set then this specifies a lower limit on the sample size before early stopping is allowed. It is provided to allow a higher number of subjects on the MTD if one of the first doses appears to be the MTD. If specified this rule this must always be met along with any other rules set for the trial to stop early.\n\n\n\n\n\n\n\nFigure 56: Design - Stopping Criteria sub tab\n\n\n\nThe available stopping rules are:\n\nRequired number of cohorts/subjects on/near MTD: Once the specified number of cohorts has been allocated to the dose currently determined to be the MTD, the trial may stop.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\nThe required number of cohorts/subjects can be “near” (rather than “on”) the MTD if a “Count as MTD” interval is defined. This interval is either defined by an interval of “dose strength” using the dose strengths defined on the Study &gt; Treatment Arms tab, or by an interval of “dose strength ratios”. Which is used will correspond to the Selection for “Dose allocation rules apply to” on the Allocation Rules tab. A difference of 0, or factor of 1 can be used to only count cohorts/subjects actually on the selected MTD dose.\nIf open enrolment is being used, there is an option to only pause accrual not stop it when the stopping rules have been met, in case the final results of any subjects that were not complete at the time the stopping rules were met cause the rules to be no longer met (for example by having results that change the dose estimated to be the MTD). If this option is selected then the stopping rules are re-assessed when all the current subjects are complete and the trial resumed if the rules are no longer met. Note this option is not required for cohort enrolment as stopping rules are only evaluated between cohorts.\n\nMinimum cohorts/subjects accrued: this rule ensures that a minimum overall number of cohorts have been tested before the trial is allowed to stop. It makes no sense to use this rule on its own (it would effectively just lower the overall study cap) it should only be used in conjunction with other stopping rules.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\n\nThe Additional Stopping Rules:\n\nRange/Ratio of dose strengths within the credible interval is less than or equal to: This is only enabled if on the Study &gt; Study Info tab, the Type of Target has been set to “Target a single dose”. In this case the credible interval of the dose range that might contain the MTD is calculated, and this option allows the user to specify the width of the credible interval in “dose strength” to consider. How this works is a little counter intuitive: in each MCMC sample the engine determines which dose is the MTD and each doses’ Pr(MTD) is the proportion of the samples it was the MTD. The engine then calculates the minimum range of dose strengths required so that the sum of Pr(MTD) of the doses in the range exceeds 1 – Alpha. (So if a single dose has Pr(MTD) &gt; 1-Alpha, the CI has zero width). If the xhats are log(dose strength) then the minimum range of log(dose strength) is found and exp() of this range returned.\n\nThe stopping rule is met if the width of the CI returned is less than that specified by the user.\nIf fine grain dosing is being used then the width of the target credible interval is defined as a dose strength ratio rather than a number of doses.\n\nStop if adding another DLT free cohort does not alter the MTD: this rule is evaluated by analyzing the existing data supplemented by an additional cohort of a specified size where all the responses are no-toxicity; if this results in no change in selection of MTD then this stopping rule is met. If the study is using Open Enrolment, the user additionally specifies the size of the ‘virtual’ cohort to use.\nProbability of dose being in the target band greater than: in order to stop, the MTD’s posterior probability that its toxicity rate lies in the Target Toxicity band must be at or above the required threshold.\nMaximum Cohorts/Subjects near MTD: this option is supplied for use if the additional stopping rules are being OR’d together. (When they are AND’d together it simply functions the same way as the “Required Cohorts on MTD and stopping will not occur until the higher of the two targets is met).\nJoin condition: if more than one additional stopping rule is selected, whether only any one of them needs to be met for the trial to be able to stop (Join condition = “OR”), of if all of the selected rules need to be met for the trial to be able to stop (Join condition = “AND”).\n\n\nThe study will also be stopped if there are no allowed doses by the overdose rules. However this can occur early in the study if a toxicity is observed in the first or second cohort. It is likely that in practice the clinical team would override the design stopping the study. Whilst it is difficult to fully represent the team’s decision making, a simple rule is included that is intended to approximate it:\n\nMinimum toxicities required before stopping: This allows a requirement to be specified to observe a ‘minimum number of toxicities’ before the trial stops. If no doses are allowed by the overdosing rules, cohorts are assigned to the lowest dose until the minimum number of toxicities are observed, the stopping rules are met, or doses become allowable again after the model is updated after seeing no toxicities.\n\n\n\n\n\n\n\nFigure 57: Stopping Criteria tab with open enrolment and finely spaced doses\n\n\n\nIf an additional efficacy endpoint is used, the MTD stopping criteria refer to when the trial jumps from assigning subjects with the aim of finding the MTD (“MTD phase”) to assigning subjects with the aim of finding the MED (“MED phase”). In the MED phase, it is possible that because of new data being observed, the MTD stopping rules are no longer met and the trial switches back to the MTD phase. A new stopping rule for the MTD phase is added, “Maximum subjects used to determine the MTD”. After this number of subjects has been enrolled, the trial switches from MTD to MED phase and there is no going back.\nIn the MED phase, there are several rules for stopping early (i.e. before the maximum sample size of the trial):\n\nMaximum cohorts/subjects on MED. This behaves analogous to Required number of cohorts/subjects on/near MTD in the MTD stopping rules, with the exception of not using a concept of “near” doses.\nNumber of doses within the credible interval is less than or equal to with the sub-option Alpha for width of credible interval. This behaves analogous to Range/Ratio of dose strengths within the credible interval is less than or equal to in the MTD stopping rules.\nProbability of dose being MED greater than. This behaves analogous to Probability of dose being in the target band greater than in the MTD stopping rules.\n\nA special case arises when the MED estimate is larger than the MTD estimate. If that is the case, subjects are allocated to MTD or the highest cleared dose (whatever is smaller) even in the MED phase. The option “If MED &gt; MTD: Continue until subjects near MTD reach” specifies how many subjects should be assigned to MTD in the MED phase before stopping the trial (and therefore giving up hope that the MED is a safe dose).\n\n\n\n\n\n\nFigure 58: Stopping Criteria tab with open enrolment and both a toxicity and efficacy endpoint"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-simulation-with-variants",
    "href": "documentation/v72/userguides/crm.html#sec-simulation-with-variants",
    "title": "CRM",
    "section": "Simulation with Variants",
    "text": "Simulation with Variants\nThe only difference that specifying design variants (see this section) introduces is to create additional scenarios – one for each Virtual Subject Response (VSR) profile for each variant. For example if there were 5 VSR profiles and then 4 variants (different numbers of maximum cohorts) specified then would now be 20 scenarios in total. The scenario names have “_Var1”, “_Var2”, … appended to them. Once simulations have been run and the .facts file has been saved and re-opened the scenarios are listed in alphabetical order.\n\n\n\n\n\n\nFigure 60: Simulation tab showing variants"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-to-run-simulations",
    "href": "documentation/v72/userguides/crm.html#sec-to-run-simulations",
    "title": "CRM",
    "section": "To run simulations",
    "text": "To run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user may not modify any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-how-many-simulations-to-run",
    "href": "documentation/v72/userguides/crm.html#sec-how-many-simulations-to-run",
    "title": "CRM",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%). This degree of accuracy usually unnecessary for dose escalation designs.\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-simulation-results",
    "href": "documentation/v72/userguides/crm.html#sec-simulation-results",
    "title": "CRM",
    "section": "Simulation results",
    "text": "Simulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nOpen Results Folder: Opens the windows “File Explorer” tool in the results folder of the currently selected scenario. This makes it very easy to locate and open results files.\nCopy to Clipboard: will copy the values displayed in the summary to the clipboard as “CSV” data, enabling it to be pasted straight into a spreadsheet.\nAll: A window containing all the summary results columns\nHighlights: a separate window with the results shown on the main tab\nAllocation, Observed: summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity: summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc: summary results of the posterior probabilities of the properties of interest\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\n\nExplore Results: offers three options:\n\n“Show Per Scenario Highlighted Scenario Graphs” that shows the graph of the simulation results for a specific scenario (see Section 12 below for a description of these graphs)\n“Show Across Scenario Graphs” that displays a trellis plot of summary graphs for each variant and each scenario (see Section 13 below for a description of these graphs).\n“Compare Scenarios in AIRSHIP” to open the simulation results in R with the AIRSHIP R-shiny graphing app. See the AIRSHIP User Guide for details.\n\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nDesign Report: it uses an R script and R libraries to generate a MS Word document describing the design. See the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.\n\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options corresponding to the above options that can be reached from the buttons, in what is sometimes a more ergonomic manner.\n\nMCMC Settings\n\n\n\n\n\n\nFigure 61\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nThe Number of samples per imputation value only applies to analyses using imputed data from a longitudinal model and is irrelevant for N-CRM, hence it is greyed out.\nIf the Number of MCMC files to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output, allowing the user to check convergence.\nThe MCMC output thinning parameter can be used to reduce the amount of data output to the MCMC file. It does not reduce the amount of MCMC samples used within the model fitting.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-facts-grid-simulation-settings",
    "href": "documentation/v72/userguides/crm.html#sec-facts-grid-simulation-settings",
    "title": "CRM",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-detailed-simulation-results",
    "href": "documentation/v72/userguides/crm.html#sec-detailed-simulation-results",
    "title": "CRM",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 62) will reveal more detailed results about the simulation.\n\n\n\n\n\n\nFigure 62: Detailed Simulation Results for a particular scenario\n\n\n\nRight-clicking on a row displays a context menu from which the user can view other columns (the default are the “highlihgts” columns) and also view the corresponding cohort results:\n\n\n\n\n\n\nFigure 63: Cohort Results for a particular simulation\n\n\n\nRight clicking on the cohort results, displays a context menu from which the user can view other columns (the default are the “highlights” columns)."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-sim-aggregation",
    "href": "documentation/v72/userguides/crm.html#sec-sim-aggregation",
    "title": "CRM",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 64\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose, plus two extra rows for ‘below lowest’ and ‘above highest’.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nIf there is no value for the below lowest or above highest rows, they will be left blank.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. cohortsNNN.csv files are aggregated into a single agg_[pivot_]cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-design-report",
    "href": "documentation/v72/userguides/crm.html#sec-design-report",
    "title": "CRM",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "href": "documentation/v72/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "title": "CRM",
    "section": "Results when 2 groups have been simulated",
    "text": "Results when 2 groups have been simulated\nWhen 2 groups have been simulated, the results displayed on the Simulation tab and all the directly viewable summary tables under the “Show other columns” button are from Group 1.\nTo see the results from Group 2 you need to first display the Group 2 highlights – either by selecting “Group 2” on the “Show other columns” menu or after right clocking on a row of results.\nOnce the Group 2 “highlights” results are being displayed, the other sets of summary results can be viewed by right clicking on a row of results in the Group 2 “hightlights” window."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-highlights",
    "href": "documentation/v72/userguides/crm.html#sec-highlights",
    "title": "CRM",
    "section": "Highlights",
    "text": "Highlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nSettings\n1\nDisplays an icon showing the status of the simulation results.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the “MTD” at the end of the study. In CRM this is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on “excess” or “unacceptable” toxicity have been specified, then doses with posterior probabilities above these limits are excluded from being chosen as MTD. (Note this is different from the calculation of MTD used in Neuenschwander, Branson, and Gsponer (2008) where doses with posterior probabilities above these limits were not excluded from being selected as MTD.)\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nIncluded if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the “MED” at the end of the study. This is the dose with the highest posterior probability of being the dose that is the ‘highest dose below’ / ‘nearest’ / ‘lowest dose above’ (as specified on the Study &gt; Effiacy tab) to the target efficacy rate.\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration in weeks of the simulated trials.\n\n\nSD Duration\n1\nThe standard deviation over the simulations of the duration in weeks of the simulated trials.\n\n\nMean Lost\n1\nIf the trial uses open enrollment, this is the number of subjects that could not be allocated a dose because the number of subjects treated but not yet complete had reached the specified “Maximum subjects without final result” limit. Otherwise the value is 0.\n\n\nSD Lost\n1\nThe standard deviation over the simulations of the number of subjects lost.\n\n\nPpn(All Tox)\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nPpn(early Success)\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nPpn(Cap)\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nPpn MTD Under\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Underdosing” toxicity range.\n\n\nPpn MTD Target\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Target” toxicity range.\n\n\nPpn MTD Excess\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Excess” toxicity range.\n\n\nPpn MTD Unacc\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Unacceptable” toxicity range.\n\n\nPpn Correct Under\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Underdosing” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Target\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Target” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Excess+Unacc\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Excess” or “Unacceptable” ranges. This will be 0 if none of the doses in that scenario had a true toxicity that fell in those ranges.\n\n\nPpn MED Under\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or less. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over))\n\n\nPpn MED Over\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or more. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over)"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-allocations-observed",
    "href": "documentation/v72/userguides/crm.html#sec-allocations-observed",
    "title": "CRM",
    "section": "Allocations, Observed",
    "text": "Allocations, Observed\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nNum Phase 1\n1\nIncluded if efficacy is being simulated. This is the mean (over the simulations) of the number of subjects recruited in the first phase (up to the MTD stopping criteria being met) in this scenario.\n\n\nSubj. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subjects: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nCat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nCat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\n80% Num. Subj.\n1\nThe 80th percentile of the distribution of the number of subjects recruited in the simulations"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-fitted-toxicity",
    "href": "documentation/v72/userguides/crm.html#sec-fitted-toxicity",
    "title": "CRM",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nSD Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nMean Alpha Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nSD Mean Alpha Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nAlpha[234] Tox\n2-3\nIf Ordinal toxicity is being simulated, instead of Mean Alpha and SD Mean Alpha columns, there are pairs of columns Mean Alpha2, SD Mean Alpha2, … for Alpha2 & Alpha3 if a 3 point ordinal scale is being used and Alpha2, Alpha3 & Alpha4 if a 4 point ordinal scale is being used. These are the means (over the simulations) of the mean and standard deviation of the various Alpha coefficients in the BLRM model when fitting to the ordinal outcome.\n\n\nSD Alpha[234] Tox\n2-3\nsee row above\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Toxicity: &lt;Dose&gt;\nOne per dose\nThis is the true simulated toxicity rate for each dose in the scenario\n\n\nMean Fit Tox Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose.\n\n\nMean Fit Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-prmtd-etc",
    "href": "documentation/v72/userguides/crm.html#sec-prmtd-etc",
    "title": "CRM",
    "section": "Pr(MTD) Etc.",
    "text": "Pr(MTD) Etc.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study. The dose chosen as MTD is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on excess or unacceptable toxicity have been specified, then doses with posterior probabilities above the specified limit, of having a toxicity rate in those bands are excluded from being chosen as MTD.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum [Tox] Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MTD was rule was met at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 1” (see below).\n\n\nNum [Tox] Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MTD is less than the specified number. [This stopping rule only evaluated if targeting an MTD rather than a toxicity band] If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 2” (see below).\n\n\nNum [Tox] Stop Rule 3\n1\nNumber of times the Pr(MTD) – that the probability that the dose was MTD or dose’s toxicity rate lies in the target toxicity rate band - met the stopping rule threshold at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 3” (see below).\n\n\nNum Stop Rule 4\n1\nNumber of times that observing another cohort with no toxicities would not change the selected MTD stopping rule was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nNum Stop Rule 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose&gt;\nOne per dose\nAs MTD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nOSD+ Selection: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. As OSD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nOSD+ Selection: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose above the tested range of doses was selected as the optimum selected dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(Under): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPost CE MTD+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the maximum tolerated dose.\n\n\nPost CE MTD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was the maximum tolerated dose allowing for a probability that the MTD is at a dose below or above the range of tested doses.\n\n\nPost CE MTD+: plus\n1\nThe posterior probability, after the results of the Cohort Expansion, that a dose above the tested range of doses was the maximum tolerated dose.\n\n\nPost CE OSD+: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose was selected as the optimum selected dose allowing for a probability that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose above the tested range of doses was selected as the optimum selected dose."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-fitted-efficacy",
    "href": "documentation/v72/userguides/crm.html#sec-fitted-efficacy",
    "title": "CRM",
    "section": "Fitted Efficacy",
    "text": "Fitted Efficacy\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nSD Beta Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nAlpha Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nSD Alpha Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy response model for each dose.\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates of the efficacy rate across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Efficacy: &lt;Dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nMean Fit Eff Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.\n\n\nMean Fit Eff Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sec-prmed-etc",
    "href": "documentation/v72/userguides/crm.html#sec-prmed-etc",
    "title": "CRM",
    "section": "Pr(MED) Etc.",
    "text": "Pr(MED) Etc.\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study. The dose chosen as MED is the dose with the highest posterior probability of having a efficacy rate nearest the target rate / is the highest dose with a rate below the target rate / is the lowest dose with a rate above the target rate – as specified on the Study &gt; Efficacy tab.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum Eff Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Eff CI\n1\nThe mean (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nSd Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nPr(MED) &lt;dose&gt;\n\nThe mean (over the simulations) of the posterior probability that each dose is the MED.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose&gt;\nOne per dose\nAs MED Selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(MED+): minus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose below the tested range of doses was the MED.\n\n\nPr(MED+): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities that each dose was the MED. As Pr(MED) but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nPr(MED+): plus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose above the tested dose range is the MED.\n\n\nPost CE MED+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was selected as the minimum efficacious dose allowing for the possibility that the MED is at a dose below or above the range of tested doses.\n\n\nPost CE MED+: plus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after the results of the Cohort Expansion, that each dose was selected as the optimum selected dose allowing for the possibility that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the optimum selected dose."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#allocation-box-and-whisker-plot",
    "href": "documentation/v72/userguides/crm.html#allocation-box-and-whisker-plot",
    "title": "CRM",
    "section": "Allocation Box and Whisker Plot",
    "text": "Allocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 68: Allocation box and whisker plot\n\n\n\nThis graph displays a box and whisker plot of the number of subjects allocated to each dose over the simulations. These plots show:\n\nThe mean allocation over all simulations at each dose plotted as a solid line.\n\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\n\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#response-and-subject-allocation",
    "href": "documentation/v72/userguides/crm.html#response-and-subject-allocation",
    "title": "CRM",
    "section": "Response and Subject Allocation",
    "text": "Response and Subject Allocation\n\n\n\n\n\n\nFigure 69: Response and subject allocation graph\n\n\n\nThis plot shows the mean subject allocation to each dose as a blue bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation and the mean fitted toxicity separately.\nIf efficacy is being simulated, then lines for the mean fitted efficacy and true efficacy are also shown."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#response-and-mtd-distribution",
    "href": "documentation/v72/userguides/crm.html#response-and-mtd-distribution",
    "title": "CRM",
    "section": "Response and MTD Distribution",
    "text": "Response and MTD Distribution\n\n\n\n\n\n\nFigure 70: MTD distribution and response graph\n\n\n\nThis plot shows the proportion of times each dose has been selected as the MTD as a brown bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nThere is an options to display the “MTD+” distribution. This is the proportion of times each dose has been selected as the MTD when a dose below the lowest dose and a dose above the highest dose is included.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately.\nIf efficacy is being simulated then there are versions of the graph showing the histograms showing the distribution of selection of MTD, MED, OSD, and TE targets. The plot of the MTD shows the true and mean fitted toxicity, the plot of the MED shows the true and mean fitted efficacy and the plots of the OSD and TE show both the true and mean fitted toxicity and efficacy.\n\n\n\n\n\n\nFigure 71: Response and TE Distribution for a group"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#toxicity-interval-probabilities",
    "href": "documentation/v72/userguides/crm.html#toxicity-interval-probabilities",
    "title": "CRM",
    "section": "Toxicity Interval Probabilities",
    "text": "Toxicity Interval Probabilities\n\n\n\n\n\n\nFigure 72: Toxicity interval probabilities graph\n\n\n\nThis plot shows the posterior probability for each dose that it’s toxicity rate lies in each of the four toxicity intervals as a stacked bar chart."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#observed-toxicity-and-allocation",
    "href": "documentation/v72/userguides/crm.html#observed-toxicity-and-allocation",
    "title": "CRM",
    "section": "Observed Toxicity and Allocation",
    "text": "Observed Toxicity and Allocation\n\n\n\n\n\n\nFigure 73: Observed Toxicities and Allocation Histogram\n\n\n\nThis graph shows the mean allocation to each dose and the mean number of toxicities observed at each dose. The total height of the bar shows the total allocation, and the red section of the bar shows the proportion that experienced toxicity."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sample-size-mtd-histogram",
    "href": "documentation/v72/userguides/crm.html#sample-size-mtd-histogram",
    "title": "CRM",
    "section": "Sample Size MTD Histogram",
    "text": "Sample Size MTD Histogram\n\n\n\n\n\n\nFigure 74: Sample Size MTD Histogram\n\n\n\nThis graph shows the number of times different sample sizes (number of subjects tested) were observed across the simulations. Each bar is shown as a stacked plot with each color indicating the proportion of times a particular dose was selected as the MTD in the simulations that ended with a particular sample size."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#alloc-history-summary",
    "href": "documentation/v72/userguides/crm.html#alloc-history-summary",
    "title": "CRM",
    "section": "Alloc History Summary",
    "text": "Alloc History Summary\n\n\n\n\n\n\nFigure 75: Allocation History Summary Graph\n\n\n\nThis graph overlays all the allocation histories of those simulations for which “cohorts” files have been output. The lines show the “route” the dose escalation followed and the circles show the dose selected as MTD at the end of the simulation. The lines are heavier and the circles darker the more simulations followed the same route or made the same selection.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nThis graph allows a quick appraisal of how well the design and priors allow the dose escalation to reach and then stop in the target band."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#per-sim-alloc-and-tox-history",
    "href": "documentation/v72/userguides/crm.html#per-sim-alloc-and-tox-history",
    "title": "CRM",
    "section": "Per Sim Alloc and Tox History",
    "text": "Per Sim Alloc and Tox History\n\n\n\n\n\n\nFigure 76: Allocation and toxicity history plot\n\n\n\nThis graph shows the allocation and toxicity history for an individual simulation. Each dot represents a subject, the dot is colored red if toxicity was observed, grey otherwise. The subjects’ outcomes are displayed left to right in the order in which they were dosed, and at a height corresponding to the dose they were given.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed. A history can only be displayed for trials for which a ‘cohorts’ file has been written out.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nOn the left hand side, a black triangle next to the y-axis indicates the highest cleared dose. Green/red triangles show the Model MTD/MED+ and finally the selected MTD/MED.\nIf the trial uses open enrolment this graph is slightly changed.\n\n\n\n\n\n\nFigure 77: Open Enrolment Allocation and Toxicity History graph\n\n\n\nIf the trial uses open enrolment then this graph has an “Interim” picker that shows the data available at a specific interim. Subjects whose outcome has not been observed at this interim are shown as grey squares. Subjects who were not included in the trial because there were already the maximum number of subjects treated but who had not attained their final toxicity / non-toxicity status are shown as yellow crosses at the level below the lowest dose. As the interim displayed is increased subjects symbol will change as their endpoint data becomes available."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#cohort-responses",
    "href": "documentation/v72/userguides/crm.html#cohort-responses",
    "title": "CRM",
    "section": "Cohort Responses",
    "text": "Cohort Responses\n\n\n\n\n\n\nFigure 78: Cohort response plot\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the fitted dose-toxicity model for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIt the trial uses open enrolment then subjects whose outcome has not been observed at the time of the interim being displayed are shown as a light grey part of the “allocated subjects” bar.\nIf trial simulates 2 groups then there are two graphs one for each group.\nIf the trial simulates efficacy, then the bar shows both toxicities and efficacies using half width bars of different colors.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED.\n\n\n\n\n\n\nFigure 79: Cohort Responses for one of two groups showing efficacy and toxicity"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#cohort-band-probabilities",
    "href": "documentation/v72/userguides/crm.html#cohort-band-probabilities",
    "title": "CRM",
    "section": "Cohort Band Probabilities",
    "text": "Cohort Band Probabilities\n\n\n\n\n\n\nFigure 80: Cohort band probabilities graph\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the posterior probabilities that the toxicity rate lies in each of the toxicity bands for each dose, for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#mtd-change-on-expansion",
    "href": "documentation/v72/userguides/crm.html#mtd-change-on-expansion",
    "title": "CRM",
    "section": "MTD Change on Expansion",
    "text": "MTD Change on Expansion\n\n\n\n\n\n\nFigure 81: MTD change on expansion\n\n\n\nThis graph plots the dose selected as MTD before the results of the expansion cohort are available, against the dose selected as MTD after the expansion cohort results are available. Each simulation result is plotted as a ‘dot’, and the graph is divided up into a grid and dots located in the correct cell in the grid and then arranged within the cell depending on how many results fall into the cell. This allows the relative number of results in each cell to be appraised quickly by eye.\nThis graph allows the trial designer to see the degree of risk that the determination of the MTD will change as a result of the expansion cohort and whether the risk is predominantly that the determination will increase or decrease."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#selected-mtd-graph",
    "href": "documentation/v72/userguides/crm.html#selected-mtd-graph",
    "title": "CRM",
    "section": "Selected MTD graph",
    "text": "Selected MTD graph\nThe Selected MTD “Across Scenarios” graph shows for each scenario and each variant a histogram of the proportion of times each dose was selected as the MTD at the end of the simulations in that scenario. The bars are colored to reflect the toxicity band that the “true” toxicity rate of the dose falls into in that scenario.\n\n\n\n\n\n\nFigure 83"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#toxicity-interval-probabilities-graph",
    "href": "documentation/v72/userguides/crm.html#toxicity-interval-probabilities-graph",
    "title": "CRM",
    "section": "Toxicity Interval Probabilities graph",
    "text": "Toxicity Interval Probabilities graph\nThe Toxicity Interval Probabilities “Across Scenarios” graph shows for each scenario and each variant a stacked bar chart of the posterior probability that the toxicity rate at each dose falls into one of the user defined 4 toxicity bands.\n\n\n\n\n\n\nFigure 84"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#fitted-toxicity-1",
    "href": "documentation/v72/userguides/crm.html#fitted-toxicity-1",
    "title": "CRM",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\nThe Fitted Toxicity “Across Scenarios” graph shows for each scenario and each variant, the mean fitted toxicity and the 95-percentile spread of the fitted toxicities across the simulations.\n\n\n\n\n\n\nFigure 85"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#allocation",
    "href": "documentation/v72/userguides/crm.html#allocation",
    "title": "CRM",
    "section": "Allocation",
    "text": "Allocation\nThe Allocation “Across Scenarios” graph shows for each scenario and each variant, a box plot of the spread of the number of subjects allocated to each dose across the simulations. As N-CRM may only be allocating a small number of cohorts the number of subjects allocated to each dose is often not a smooth distribution, but somewhat discontinuous.\n\n\n\n\n\n\nFigure 86"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#sample-size",
    "href": "documentation/v72/userguides/crm.html#sample-size",
    "title": "CRM",
    "section": "Sample Size",
    "text": "Sample Size\nThe Sample Size “Across Scenarios” graph is the only “Across Scenario” graph that is not a trellis plot. It is a single graph with a line plotted per scenario of the mean sample size at each maximum sample size.\n\n\n\n\n\n\nFigure 87"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#entering-the-data-directly",
    "href": "documentation/v72/userguides/crm.html#entering-the-data-directly",
    "title": "CRM",
    "section": "Entering the data directly",
    "text": "Entering the data directly\nThe data can be entered directly in the data grid – each row containing the date for a single subject.\nThe format of the grid is the same as though viewing the “subject.csv” file in a spreadsheet.\nFor historical reasons the data file format contains 5 columns that are now unused but have been retained. Thus the columns are:\n\nFirst column is the subject ID. This column can be left blank, FACTS does not used the value, it is there to allow the data to be cross-referenced to an external data source. If not required there is no harm simply entering ‘1’ on each row.\nThe next 5 columns are unused and can be left blank. Do not enter text containing comma’s in these fields, these will be read as column separators if the data is saved and read back in.\nCohort number, this should be an integer indicating which cohort the subject belonged to (and hence the order in which they entered the trial). This data is sometimes used when determining what doses the allocation rules permit to be used. The FACTS GUI now checks to ensure that this value has been entered.\nDose Strength, if explicit doses are being used this value must match the dose strength of one of the doses defined on the ‘Treatment Arms’ tab – as the dose escalation rules are defined in terms of “number of doses”. If doses have been defined using ‘finely spaced doses’ then this column can contain any value as dose escalation rules defined using “dose strength”. The value entered will be used as the strength of the dose the subject was administered.\nApart from the requirement that the dose strength corresponds to one of the planned doses if the design uses explicit doses, there is no need for the data entered to represent a dose escalation permitted by the design. The team can have been more or less cautious, and there can be more or less data, than originally planned.\nToxicity – if a binary endpoint is being used this must be 0 (not-toxicity) or 1 (toxicity), if Ordinal toxicity is being used then this must be 1, 2, 3 or 4; where 1 is now no toxicity, 2 mild toxicity, 3 toxicity, and 4 severe toxicity.\nEfficacy – this must be 0 (no efficacy) or 1 (efficacy), even if efficacy is not being modelled in the design. If it is not being modelled then whether the value is 0 or 1 is immaterial.\n\n\n\n\n\n\n\nFigure 91\n\n\n\nIf the data entered and then ‘Run Analysis’ clicked, the data is saved to a file called ‘subject.csv’ and the analysis results saved to a folder called ‘Analysis’ within the “_results” folder of the design.\n\n\n\n\n\n\nFigure 92\n\n\n\nThe ‘Save As’ button can be used to save the file with a different name, but it will still be saved within the _Results folder.\nA specific results folder is also created, called ‘Analysis_’.\n\n\n\n\n\n\nFigure 93\n\n\n\n\n\n\n\n\n\nFigure 94"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#loading-data-from-a-file",
    "href": "documentation/v72/userguides/crm.html#loading-data-from-a-file",
    "title": "CRM",
    "section": "Loading Data From a File",
    "text": "Loading Data From a File\nAs well as entering the data via FACTS its possible to load the data from a ‘subject.csv’ file. These can be cerated within FACTS or outside of FACTS and once created can be edited FACTS or outside of FACTS.\nWe have tried to make it particularly easy to enter, modify and analyze data in N-CRM because this is a useful way to explore the properties of the design in addition to simulation.\n\nThe subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nFor historic reasons the CRM subject.csv file format includes fields for patient identification data, however the FACTS design engine does not use this data, but does require that the columns contain data of the expected format. This is the first 6 columns: patient ID, Patient Initials, Year, Month, Day, Time & Cohort. The simplest thing to do is enter simple default values as shown below.\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Toxicity, Efficacy\n1, , , , , , 1, 12.5, 0, 0\nIn FACTS only the last four columns are ever significant: Cohort, Dose, Toxicity and Efficacy. “Dose” needs to contain the dose level (not the dose index)) of the dose given to the subject, For example if the dose levels were specified as 12.5, 25, 50, 100, 150, 200 and 250 so this column should contain one of these values.\nIf the toxicity endpoint is dichotomous then “Toxicity” needs to contain either a ‘1’ (to indicate toxicity observed) or a ‘0’ (for no toxicity). If its ordinal, then it needs to contain a ‘1’, ‘2’, ‘3’ or ‘4’ corresponding to the observed level of toxicity for that subject, where ‘1’ is now “no toxicity’ and ‘3’ is the ‘target toxicity’, ‘2’ is ‘mild toxicity’ and ‘4’ is ‘severe toxicity’.\nA value in the final “Efficacy” column is also required whether or not efficacy is being modelled in the design. The column should contain either a ‘1’ (to indicate efficacy observed) or a ‘0’ (for no efficacy).\nThus if the first cohort had been allocated the lowest dose ’12.5’ and no subjects experienced toxicity (‘0’), the subjects.csv file looks like this:\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Tox, Efficacy\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "href": "documentation/v72/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "title": "CRM",
    "section": "Data file management on the Analysis tab",
    "text": "Data file management on the Analysis tab\nFrom FACTS 6.2 onwards FACTS now supports multiple subject data files and analysis folders.\nButtons that allow the subject data file to be changed:\n\n’Select File to Create New Analysis: launches a file browser that allows the user to select a new “.csv” file from any location. The selected file is copied to the “_Results” folder (retaining its current name) and made the current subject data file.\n‘Rename Current Analysis’ allows the name of the current subject data file to be changed.\n‘Select Difference Analysis’ allows a different subject data file that is in the “_Results” folder to be made the current subject data file.\n‘Delete Analysis’ allows any of the subject data files that are in the “_Results” folder to be deleted.\n\nThe name of the current subject data file and the name of the corresponding analysis folder are shown below the subject data file buttons.\nThere are five buttons that allow the currently loaded subject data file to be modified:\n\n‘Delete Row’ deletes the currently selected row in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Delete All’ clears all the data in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Reload data’ replaces the data in the data grid with the data that is still in the current subject data file.\n‘Save As’ saves the current data in the data grid to a new subject data file in the “_Results” folder, and makes that the current subject data file.\n“Save” saves the current data in the data grid to the current subject data file.\n\nRunning an analysis performs a ‘Save’ before running the analysis."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#running-an-analysis",
    "href": "documentation/v72/userguides/crm.html#running-an-analysis",
    "title": "CRM",
    "section": "Running an Analysis",
    "text": "Running an Analysis\nOnce data has been loaded or entered, the user can click the ‘Run Analysis’ button.\nOnce the analysis has run, FACTS displays the recommendation, and a graph showing the data the fitted toxicity.\n\n\n\n\n\n\nFigure 95: Analysis tab - analysis results\n\n\n\nThe available analysis parameters are\n\nMCMC Burn-in: how many of the initial MCMC samples are discarded before accumulating samples to estimate the posterior distributions of the values of interest.\nNumber of samples: the number of MCMC samples to take in-order to estimate the posterior distributions of the values of interest.\nRandom Seed: the seed to be used initialize the random number sequence, with the same design data and random seed FACTS will return the same results.\nEdit command parameters. This allows the command line string to the design engine to modified, this is an advanced option. The command line options are described in the “FACTS DE User Guide for Trial Execution”."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#analysis-graphs",
    "href": "documentation/v72/userguides/crm.html#analysis-graphs",
    "title": "CRM",
    "section": "Analysis Graphs",
    "text": "Analysis Graphs\nThree graphs are available, the first shows the subject allocation, observed toxicities and resulting fitted curve:\n\n\n\n\n\n\nFigure 96\n\n\n\nThe second shows the posterior probabilities for each dose that its toxicity rate falls in each of the 4 toxicity bands:\n\n\n\n\n\n\nFigure 97\n\n\n\nThe third simply shows the observed data:\n\n\n\n\n\n\nFigure 98\n\n\n\nA fourth graph is available if “Generate MCMC file” is checked before running the analysis, we can now view MCMC trace plots of the fitted parameters such as Alpha and Beta:\n\n\n\n\n\n\nFigure 99"
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#contents-of-summary",
    "href": "documentation/v72/userguides/crm.html#contents-of-summary",
    "title": "CRM",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe columns in summary.csv are common across all the FACST Dose Finding design engines.\nSome columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate summary files. The first summary file “summary.csv” contains the results for the first group, the second summary file “summary2.csv” contains the result for the second group.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber of Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nMean num subjects\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario (including any single patient run-in, but excluding any expansion cohort..\n\n\nPpn Tox\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn Eff\n1 – Efficacy only\nThis is the average proportion of the subjects recruited that experienced a efficacy in this scenario.\n\n\nSD Ppn Tox\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1 – Efficacy only\nThis is the standard deviation of the proportion of efficacy across the simulations.\n\n\nTrue Mean Tox\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile that we are simulating from.\n\n\nMean Beta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\ns.d.Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\nMean Alpha 3 Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\ns.d.Alpha 3 Tox3\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\nMean Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\ns.d.Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\nMean Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\ns.d.Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\nMTD Selection &lt;dose index&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study, as defined by the Selected MTD simulation results column. Index starts at 0 if a control arm is included.\n\n\nMED Selection &lt;dose index&gt;\nOne per dose – Efficacy only\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study, as defined by the Selected MED simulation results column. Index starts at 0 if a control arm is included.\n\n\nOSD Selection &lt;dose index&gt;\nOne per dose\nUnused [it contains values that are copies of the MTD selection. OSD = Optimum Safe Dose, it differs from the MTD only if there is an efficacy endpoint to take into account too]\n\n\nMean num Ph1\n1 – Efficacy only\nThis is the mean (over the simulations) of the number of subjects dosed during the first phase of the trial that targets the MTD.\n\n\nMean Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Fitted Efficacy &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Efficacy &lt;dose index&gt;\nOne per dose -\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Subj per dose&lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subj per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nMean Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nMean Eff per dose &lt;dose index&gt;\nOne per dose -Efficacy only\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\nMean Known per dose &lt;dose index&gt;\nOne per dose\nThe number of known patient outcomes for a dose. In N-CRM there is no simulation of drop-outs so this will always be the same as “Mean subj per dose”\n\n\nSD Known per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of known patient outcomes for each dose across the simulations. In N-CRM there is no simulation of drop-outs so this will always be the same as “SD subj per dose”\n\n\nNum subj 80%ile\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nTox Stopping 1\n1\nThe number of times that toxicity stopping rule 1 was true at the end of the simulation. This is the rule that the trial should stop when the specified minimum number of cohorts has been allocated to the dose currently selected as the MTD.\n\n\nTox Stopping 2\n1\nThe number of times that toxicity stopping rule 2 was true at the end of the simulation. This is the rule that the trial should stop when no more than the specified number of doses within the credible interval for the target toxicity.\n\n\nTox Stopping 3\n1\nThe number of times that toxicity stopping rule 3 was true at the end of the simulation. This is the rule that the trial should stop when a dose achieves the minimum posterior probability of having a toxicity rate within the target band/of being MTD.\n\n\nTox Stopping 4\n1\nThe number of times that toxicity stopping rule 4 was true at the end of the simulation. This is the rule that the trial should stop if testing another cohort and observing no toxicities does not change the dose selected as the MTD.\n\n\nTox stopping 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation.\n\n\nEff Stopping 1\n1 – Efficacy only\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nEff Stopping 2\n1 – Efficacy only\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nEff Stopping 3\n1 – Efficacy only\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Tox CI\n1 – MTD target only\nThe mean number of doses in the MTD CI\n\n\nSD Tox CI\n1 – MTD target only\nThe SD of the number of doses in the MTD CI\n\n\nMean Eff CI\n1 – Efficacy only\nThe mean number of doses in the MED CI\n\n\nSD Eff CI\n1 – Efficacy only\nThe SD of the number of doses in the MED CI\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe mean probability of the dose being MTD\n\n\nPr(MED)\nOne per dose – Efficacy only\nThe mean probability of the dose being the MED\n\n\nMTD+ &lt;1\n1\nThe number of times the MTD was deemed to be less than the lowest dose\n\n\nMTD+ &lt;dose index&gt;\nOne per dose\nThe number of times each dose was selected as the MTD\n\n\nMTD+ &gt;&lt;D&gt;\n1\nThe number of times the MTD was deemed to be above the highest dose\n\n\nMED+ &lt;1\n1 – Efficacy only\nThe number of times the MED was deemed to be less than the lowest dose\n\n\nMED+ &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of times each dose was selected as the MED\n\n\nMED+ &gt;&lt;D&gt;\n1 – Efficacy only\nThe number of times the MED was deemed to be above the highest dose\n\n\nOSD+ &lt;1\n1\nThe same as MTD+ &lt;1, can be ignored\n\n\nOSD+ &lt;dose index&gt;\nOne per dose\nThe same as MTD+ &lt;dose index&gt;, can be ignored\n\n\nOSD+ &gt;&lt;D&gt;\n1\nThe same as MTD+ &gt; &lt;D&gt;, can be ignored\n\n\nDE Version\n1\nThis is the version of the N-CRM design engine that simulated these trial results.\n\n\nGUI Version\n1\nThis is the version of the FACTS GUI that was used to specify the parameters for the trials to be simulated.\n\n\nProject\n1\nThe name of the project or design\n\n\nScenario\n1\nThe name of the scenario within the project or design\n\n\nDate/Time\n1\nThe date and time the simulations were started\n\n\nBest &lt;dose index&gt;\nOne per dose – Efficacy only\nThe probability that the dose is ‘the best’ – that is it has the highest probability of efficacy without toxicity.\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe mean probability that the MTD is below the lowest dose\n\n\nPr(MTD+) &lt;D+1&gt;\n1 – MTD target only\nThe mean probability that the MTD is above the highest dose\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe mean probability that the MED is below the lowest dose\n\n\nPr(MED+) &lt;D+1&gt;\n1 –Efficacy only\nThe mean probability that the MED is above the highest dose\n\n\nPr(Under) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPrior Mean ln(Beta) Tox\n1\nThe value used for the mean value for the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior s.d.(Beta) Tox\n1\nThe value used for the standard deviation of the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior Mean Alpha Tox\n1\nThe value used for mean value for the prior distribution of Alpha in the toxicity model for all the simulations\n\n\nPrior s.d.Alpha Tox\n1\nThe value used for the standard deviation of the prior distribution of Alpha in the toxicity model for all simulations\n\n\nPrior Mean Rho Tox\n1\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the toxicity model for all simulations\n\n\nPrior Mean ln(Beta) Eff\n1 – Efficacy only\nThe value used for the mean value for the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior s.d.(Beta) Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior Mean Alpha Eff\n1 – Efficacy only\nThe value used for mean value for the prior distribution of Alpha in the efficacy model for all the simulations\n\n\nPrior s.d.Alpha Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of Alpha in the efficacy model for all simulations\n\n\nPrior Mean Rho Eff\n1 – Efficacy only\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the efficacy model for all simulations\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration of the trial.\n\n\ns.d.Duration\n1\nThe SD (over the simulations) of the duration of the trial.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe actual toxicity rate being simulated for each dose\n\n\nMean Lost\n1 – Open Enrolment only\nThe mean (over the simulations) of the number of subjects who were available for treatment but could not be included in the trial because the number of treated subjects for whom the final result is not available equals the maximum queue length. This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\ns.d.Lost\n1 – Open Enrolment only\nThe SD (over the simulations) of the number of subjects ‘lost’ (see above). This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\nPostCE MTD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nAll Tox Stop\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nEarly Success\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nCap Stop\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nMean Fitted Tox Lower &lt;D&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the estimates of the toxicity rate across the simulations, at each dose.\n\n\nMean Fitted Tox Upper &lt;D&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the estimates of the toxicity rate across the simulations at each dose."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "href": "documentation/v72/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "title": "CRM",
    "section": "Contents of simulations.csv and cohortsNNN.csv",
    "text": "Contents of simulations.csv and cohortsNNN.csv\nMost of the columns are common to the two file types, but the first few are different.\nAs with the summary.csv file, some columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate simulation.csv and cohorts.csv files. The first simulations file “simulations.csv” contains the results for the first group, the second file “simulations2.csv” contains the result for the second group.\nFor the cohorts files if two groups are being simulated, the files names ‘cohortsNNN.csv’ contain the results for the first group, and the files ‘named cohorts2_NNN.csv’ contain the results for the second group\n\nsimulations.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber\n1\nThe number of the simulation.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nNo.Subjects\n1\nThe number of subjects recruited in this trial in the small cohort run-in and in the cohorts used to locate the MTD, but not those in the expansion cohort.\n\n\nPpn Tox\n1\nThis is the proportion of the subjects recruited that experienced a toxicity in this trial\n\n\nPpn Eff\n1 – Efficacy only\nThis is the proportion of the subjects recruited that had an efficacious outcome in this trial\n\n\nTrue Mean Tox\n1\nThis is the average probability of toxicity for the subjects in the trial, given the doses they were treated with and the toxicity rate being simulated for each of those doses.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average probability of efficacy for the subjects in the trial, given the doses they were treated with and the efficacy rate being simulated for each of those doses.\n\n\nSeeds\n2\nThe two 32 bit numbers that make up the random number seed at the end of the simulation. To exactly re-simulate a specific simulation (e.g. in order to generate an mcmc file or cohorts file for that simulation) enter these values from the line above the simulation to be re-simulated.\n\n\n\n\n\ncohortsNNN.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe number of the cohort in the simulation.\n\n\nAlloc Dose\n1\nThe index of the dose assigned to that cohort. If using open enrolment, -2 means the subject was ‘lost’, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nNumToxic\n1\nThe number of subjects in that cohort that experienced a toxicity\n\n\n\n\n\nCommon simulations.csv and cohortsNNN.csv columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nmean Beta Tox\n1\nThe mean fitted value for the toxicity model Beta parameter\n\n\ns.d.Beta Tox\n1\nThe standard deviation of the toxicity model fitted Beta parameter\n\n\nMean Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\ns.d.Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\nMean Alpha 3 Tox\n1\nThe mean fitted value for the toxicity model Alpha parameter for category 3 (or the only category) toxicity or higher\n\n\ns.d.Alpha 3 Tox\n1\nThe standard deviation of the toxicity model Alpha parameter for category 3 toxicity (or the only category) or higher\n\n\nMean Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 4 toxicity\n\n\ns.d.Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 4 toxicity\n\n\nMean Beta Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Beta parameter\n\n\ns.d.Beta Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model fitted Beta parameter\n\n\nMean Alpha Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Alpha parameter\n\n\ns.d.Alpha Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model Alpha parameter\n\n\nModel MTD\n1\nThe dose index of the model MTD\n\n\nModel MED\n1 – Efficacy only\nThe dose index of the model MED\n\n\nModel OSD\n1 [simulations only]\nThe index of the dose selected as the Optimal Safe Dose (OSD), in N-CRM this will always be the same as the MTD as there is no efficacy to take into consideration.\n\n\nHighest Cleared Dose\n1\nThe highest cleared dose\n\n\nSelected MTD\n1\nThe dose index of the selected MTD (minimum of the Highest Cleared Dose and the model MTD+)\n\n\nSelected MED\n1\nThe dose index of the selected MED (minimum of the Highest Cleared Dose and the model MED+)\n\n\nSelected OSD\n1 [simulations only]\nThe dose index of the selected OSD (minimum of the Highest Cleared Dose and the model OSD+)\n\n\nNo. Ph1\n1 – Efficacy only\nThe number of subjects enrolled during the first, MTD locating, phase, before switching to the MED locating phase\n\n\nToxicity &lt;dose index&gt;\nOne per dose\nThe mean of the final posterior estimate of the toxicity rate at each dose.\n\n\nEfficacy\nOne per dose – Efficacy only\nThe mean of the final posterior estimate of the efficacy rate at each dose.\n\n\nNo. Subj &lt;dose index&gt;\nOne per dose\nThe number of subjects that have been allocated to each dose.\n\n\nTox per dose &lt;dose index&gt;\nOne per dose\nThe number of toxicities that have observed at each dose\n\n\nEff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of efficacies that have observed at each dose\n\n\nKnown per dose &lt;dose index&gt;\nOne per dose\nThe number of subjects for whom final results are available at each dose.\n\n\nTox CI\n1 – MTD target only\nOnly used if targeting a single dose. It’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nEff CI\n1 – Efficacy only\nIt’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nFlags\n1\nA flag value comprising a number of (possible) flag values ’OR’d together to show the current allocation or stopping decisions. See the Flag Values table below for details\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe posterior probability for each dose that that dose is the MTD.\n\n\nPr(MED) &lt;dose index&gt;\nOne per dose – Efficacy only\nThe posterior probability for each dose that that dose is the MED.\n\n\n1st full size\n1 [simulations only]\nThe number of the first full sized cohort, this will be ‘1’ unless the small cohort run-in is enabled. If the small cohort run-in is enabled, then this is the index of the first full size cohort.\n\n\nCohort size\n1 [cohorts only]\nThe number of patients in the cohort.\n\n\nMTD+\n1\nThe dose index of the model MTD+, this is the model MTD using the dose range extended by one dose either end. Dose 0 will be selected if all doses are too toxic and dose D+1 will be selected if no doses are toxic enough.\n\n\nMED+\n1 – Efficacy only\nThe dose index of the model MED+, this is the model MED using the dose range extended by one dose either end. Dose 0 will be selected if all doses are efficacious enough and dose D+1 will be selected if no doses are efficacious enough.\n\n\nOSD+\n1 [simulations only]\nThe dose index of the model OSD+, will be the same as the model MED+ unless this is above the model MTD+, in which case it is same as the model MTD+. If there is no efficacy to take into consideration this is the same as the model MTD+..\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe probability that the model MTD+ is at a dose below the lowest tested doses.\n\n\nPr(MTD+) D+1\n1 – MTD target only\nThe probability that the model MTD+ is at a dose above the highest tested doses.\n\n\nPr(Good) &lt;dose index&gt;\n1 – Efficacy only\nThe posterior probability for each dose of observing efficacy without observing toxicity.\n\n\nBest\n1 – Efficacy only\nThe index of the dose with the highest Pr(Good)\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe probability that the model MED+ is at a dose below the lowest tested doses.\n\n\nPr(MED+) D+1\n1 – Efficacy only\nThe probability that the model MED+ is at a dose above the highest tested doses.\n\n\nPr(Under) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Under-dosing’ toxicity band.\n\n\nPr(Target) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Acceptable’ toxicity band.\n\n\nPr(Excess) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Excessive’ toxicity band.\n\n\nPr(Unacc) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Unacceptable’ toxicity band.\n\n\nDuration\n1 (simulations only)\nDuration of the trial in weeks.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe toxicity rate being simulated for that dose in the scenario.\n\n\nTrue Efficacy &lt;Dose&gt;\nOne per dose – Efficacy only\nThe efficacy rate being simulated for that dose in the scenario.\n\n\nRec Time\n1 - Cohorts only\nIf the trial is using open enrolment this column record the time the subject was available to be dosed.\n\n\nNum Lost\n1 – Open enrolment only (simulations only)\nIf the trial is using open enrolment this is the total number of subjects ‘lost’ during the simulation, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nPostCE MTD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MTD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED+\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPr(Tox) Lower &lt;Dose&gt;\nOne per dose\nThe lower bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Tox) Upper &lt;Dose&gt;\nOne per dose\nThe upper bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Eff) Lower &lt;Dose&gt;\nOne per dose – Efficacy only\nThe lower bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\nPr(Eff) Upper &lt;Dose&gt;\nOne per dose – Efficacy only\nThe upper bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\n\nThe ‘Flags’ column is a number comprised of a number of binary flags that are used to indicate the ‘state’ of the simulator at different analyses. These flags are:\n\n\n\n\n\n\n\n\nFlag Values\n\n\n\n\n\n\nBits used\nValue\nMeaning\n\n\n0x001F\n0x0001\nAllocating to MTD\n\n\n0x001F\n0x0002\nAllocating to MED\n\n\n0x001F\n0x0003\nAllocating to initial dose\n\n\n0x001F\n0x0004\nAllocating new single patient cohort\n\n\n0x001F\n0x0005\nAllocating 1st dose of 2nd sample to dose below MTD from 1st sample\n\n\n0x001F\n0x0006\nAllocating 1st dose of 2nd sample to MTD from 1st sample\n\n\n0x001F\n0x0007\nAllocating expansion cohort\n\n\n0x001F\n0x0008\nAllocating to MTD because can’t allocate to MED because its above MTD\n\n\n0x001F\n0x0009\nExpanding at the current dose\n\n\n0x001F\n0x000A\nExpanding at the dose below\n\n\n0x001F\n0x000D\nAllocating as a backfill\n\n\n0x001F\n0x000E\nAllocating as a frontfill\n\n\n0x001F\n0x000F\nAllocating to max/min for fixed probability\n\n\n0x001F\n0x0011\nStopping for early futility\n\n\n0x001F\n0x0012\nStopping because MED is found\n\n\n0x001F\n0x0013\nStopping because all doses are toxic\n\n\n0x001F\n0x0014\nStopping because MTD is found\n\n\n0x001F\n0x0015\nStopping because MTD is found but there is no MED\n\n\n0x0020\n0x0020\nReached max subjects on MTD\n\n\n0x0030\n0x0030\nReached min required subjects on MTD\n\n\n0x0040\n0x0040\nToxicity confidence interval test passed\n\n\n0x0080\n0x0080\nPr(MTD) test passed\n\n\n0x0100\n0x0100\nReached max subjects on MED\n\n\n0x0200\n0x0200\nEfficacy confidence interval test passed\n\n\n0x0400\n0x0400\nPr(MED) test passed\n\n\n0x1000\n0x1000\nNo MED so using maximum instead\n\n\n0x2000\n0x2000\nUsing maximum permitted dose in place of MTD\n\n\n0x4000\n0x4000\nUnable to escalate due to part filled dose\n\n\n0x8000\n0x8000\nUnable to allocate during open enrolment because maximum permitted subjects without final result reached\n\n\n0x10000\n0x10000\nReached max subjects on MTD\n\n\n0x20000\n0x20000\nUnable to allocate during open enrolment because reached max subjects on MTD, but not all subjects on MTD have final results, so trial is ‘paused’ – resuming if the final results move the MTD so the trial continues, or resuming to allocate an expansion cohort.\n\n\n0x40000\n0x40000\nMax subjects reached and no other stopping rules met\n\n\n0x80000\n0x80000\nMaximum cohorts used to determine MTD has been met when there is also an efficacy endpoint, switching to searching for the MED."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#contents-of-mcmcnnnnn",
    "href": "documentation/v72/userguides/crm.html#contents-of-mcmcnnnnn",
    "title": "CRM",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file, if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (excluding the burnin) and the samples from all the analyses (i.e from every cohort – or if using open enrollment, every subject) in the simulation are included. The first two columns are the cohort’s index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta\n1\nThe estimate of the slope of the logistic regression\n\n\nAlpha &lt;O&gt;\nO\nThe estimate of intercept of the logistic regression – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the second group.\n\n\na\n1\nIf a second group is included this is the estimate of the offset for the intercept of the cat-3 toxicity model.\n\n\n\n\nMCMC File if Efficacy is included\nIf an efficacy endpoint is included in the design, then all the model parameters columns described above are included suffixed with “Tox” and then duplicate, suffixed with “Eff”.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta Tox\n1\nThe estimate of the slope of the logistic regression of the toxicity model\n\n\nAlpha &lt;O&gt; Tox\nO\nThe estimate of intercept of the logistic regression of the toxicity model – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb Tox\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the toxicity model for the second group.\n\n\na Tox\n1\nIf a second group is included this is the estimate of the offset of the intercept for the cat-3 toxicity model.\n\n\nBeta Eff\n1\nThe estimate of the slope of the logistic regression of the efficacy model\n\n\nAlpha Eff\n1\nThe estimate of intercept of the logistic regression of the efficacy model.\n\n\nB Eff\n1\nIf a second group is included this is the estimate of ‘B’ the slope offset for the efficacy model for the second group.\n\n\nA Eff\n1\nIf a second group is included this is the estimate of the offset of the intercept for the efficacy model."
  },
  {
    "objectID": "documentation/v72/userguides/crm.html#exporting-the-results",
    "href": "documentation/v72/userguides/crm.html#exporting-the-results",
    "title": "CRM",
    "section": "Exporting the Results",
    "text": "Exporting the Results\nUsing the menu item File -&gt; Export Project, the .facts file and all the results files can be saved as a single zip file."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html",
    "href": "documentation/v72/userguides/platform.html",
    "title": "Platform Trials",
    "section": "",
    "text": "FACTS Platform Trials is useful for simulating trials in which there are a number of treatments being tested against a common control arm. Unlike in the FACTS Core engine, each of these treatments is meant to stand alone, and decisions are made on individual arms rather than at the trial level. As a result, there is not expected to be a dose-response relationship between the treatments. Two common goals of platform trials are to find many effective treatments among a set of treatments or to find a single effective treatment arm as quickly as possible. Treatments in a platform trial can leave the trial when a decision has been made for that arm or they have reached their sample size cap, and new treatment arms can enter when they are available to begin randomizing.\nFACTS V7.0 was the first version of FACTS to feature a Platform Trial simulator.\nThis first version of the Platform Trial simulator FACTS provides fairly limited options for the statistical analysis:"
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#trial-info",
    "href": "documentation/v72/userguides/platform.html#trial-info",
    "title": "Platform Trials",
    "section": "Trial Info",
    "text": "Trial Info\nThe Trial Info sub-tab provides parameters for specifying characteristics of the platform trial, including whether the design is adaptive, the sample size, the number of arms allowed to enroll simultaneously, and the time to observe the endpoint.\n\nDesign Options:\n\nEnable adaptive features\nSpecify whether the design is adaptive. If adaptive features are not enabled, some adaptive-specific parameters and tabs are hidden, such as the tabs for defining trial updates, early stopping criteria, and adaptive allocation options on the allocation tab.\n\n\nUse longitudinal modeling\nCurrently, longitudinal modelling is not implemented for the platform trials engine.\n\n\nEndpoint Specific Inputs\n\nContinuousDichotomous\n\n\n\nInclude simulation of baseline\nWhether to simulate participant’s baseline score or simply change from baseline. If simulating baseline, whether the analysis is based on change from baseline or final endpoint value.\n\n\n\n\nEnable special longitudinal options\nSpecial longitudinal options are not enabled currently in FACTS platform trials.\n\n\n\n\n\n\n\nTrial Information\nThe platform trial will automatically terminate when all of the defined treatments have had a chance to enter the trial and complete their enrollment and follow-up. Arms that attempt to join the platform, but are not allowed to, are considered complete.\n\nMax enrollment time (wks)\nYou may limit the total enrollment time of the platform as a whole. If the trial reaches this time limit it stops and a final analysis is performed.\n\n\nMax number of participants\nYou may limit the total number of subjects that can be enrolled to the platform. If the trial reaches this number of subjects it stops accruing, follows up subjects, and performs a final analysis.\n\n\nMax successful treatments\nYou may stop the platform when a specified number of successful treatments have been found. This is useful for platfroms only interested in finding a specific number of successful treatments.\n\n\nMax participants per treatment\nEach arm is only allowed a specified maximum number of subjects to be enrolled to them. When an arm reaches this cap, it stopps accruing new subjects, but continues collecting follow-up on those that have been accrued. At full follow-up the arm specific final analysis is performed.\n\n\nMax concurrent treatments\nThis option allows you to specify the maximum number of non-control treatments that can be enrolling during the platform. If there are fewer than this number of active arms enrolling, then new arms are allowed to enter the platform. If there are already arms randomizing equal to the value provided here, then no new treatment arms are allowed to join the platform. Treatment arms that are ready to begin enrolling, but are not allowed to start, begin a waiting period.\n\n\nResponse\nSpecify whether a higher response indicates the subject improving or worsening (and thus whether a higher or lower mean response is a good thing).\n\n\n\nSchedule of Post Baseline Visits\nEnter the time, in weeks, it takes to observe the final response for a subject after enrollment. No visit schedule is currently available, since longitudinal modeling has not been included in FACTS Platform Trials."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#trial-arms",
    "href": "documentation/v72/userguides/platform.html#trial-arms",
    "title": "Platform Trials",
    "section": "Trial Arms",
    "text": "Trial Arms\n\nDefinition\nThis sub-tab is used to specify the set of arms that will be considered for participation throughout the course of the platform trial. Not all of these arms must accrue subject in each simulated trial, and which arms do accrue subjects depends on the state of each individual trial.\n\n\n\n\n\n\nFigure 5: The Trial &gt; Trial Arms &gt; Definition tab.\n\n\n\nIn the current implementation, a control arm is required.\nClicking the “Add” button creates a new active arm. Selecting a row, and clicking “Delete Arm” will remove the selected active arm. Clicking “Clear all arms” will delete all active arms from the table. The arms names can be changed in the table.\nArms are considered independent in the platform trial engine, so Index (d) ordering is irrelevant.\nThe number of arms defined here determines the maximum number of treatments that can enter the simulated trials. If the simulation does not stop through meeting one of the optional maximums that can be specified on the Trial &gt; Trial Info tab, then it will stop when the last treatment from this list to complete enrollment completes the follow-up of the last patient allocated to it.\n\n\nArrivals\nThe arrivals sub-tab is used to specify the timing for treatment arms becoming available in the trial simulation. In the current implementation, the control arm is always available starting at time 0. Multiple arrival schedules may be set up by adding multiple profiles.\n\n\n\n\n\n\nFigure 6: The Trial &gt; Trial Arms &gt; Arrivals tab.\n\n\n\nFor each arm defined on the Trial Arm &gt; Definition tab, the Arrivals tab requires 3 inputs.\n\nEarliest/Latest Possible Arrival (wks)\nThese column specify the range of times (in weeks) that an arm becomes available to enter the trial. For a given simulation, the actual arrival time is simulated uniformly between the earliest and latest possible times. To simulate a trial with an arm that always enters at the exact same time provide the same value for the earliest and latest possible arrivals for the arm.\nOnce an arm has reached its time of arrival, it is allowed to enter the trial if:\n\nThere are less than ‘max concurrent treatments’ currently enrolling in the trial, and\nThe trial is performing:\n\nupdates at milestones,\nupdates on a regular schedule and it is executing an update,\nupdates on a regular schedule and arms can enter between updates.\n\n\n\n\nWithdrawn If Not Used Within\nThis column specifies how long an arm will wait to begin enrolling in the trial before it “gives up” and withdraws from the trial. That is, if the arm becomes available while there are already more active treatment arms than the max concurrent treatments (as specified on the Trial &gt; Trial Info tab), then the arm must wait before being allowed to enter the trial. In that case, if no arm leaves the trial before the end of the waiting period, the waiting arm will withdraw and never enter the trial.\nIn the case of a tie in arrival times, the lower-indexed arm is entered into the trial first.\nTo create a trial in which arms never “time out” and withdraw from the study, enter a very large value in the Withdrawn if not used within column."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#variants",
    "href": "documentation/v72/userguides/platform.html#variants",
    "title": "Platform Trials",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only two design features that can be changed are\n\nthe maximum sample size per treatment.\nThe maximum number of concurrent treatments at any time.\n\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Maximum Participants per Treatment” and “Max Concurrent Treatments” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\n\n\n\n\n\n\nFigure 7: The Variants tab for platform trials.\n\n\n\nIn the above screenshot, 6 variants have been created testing 3 x 2 combinations of Max participants per treatment of 80, 90 and 100 and Max concurrent treatments of 3 and 4. These will override the values for these parameters that have been specified on the Trial Info tab. It there are for example 2 response profiles to simulate, this will give 6 x 2 scenarios to simulate:\n\n\n\n\n\n\nFigure 8: The simulation tab showing 6 different variants creating extra scenarios."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#treatment-classification",
    "href": "documentation/v72/userguides/platform.html#treatment-classification",
    "title": "Platform Trials",
    "section": "Treatment Classification",
    "text": "Treatment Classification\nMany outputs and graphs depend on a classification of the treatment effect as “good,” “mediocre,” or “unacceptable.” The thresholds provided on this tab are used in this classification. These treatment effect thresholds are applied to the true effect size for the simulation. For fixed effects, a treatment arm will be categorized the same in all simulations, but if sampled from a distribution, its categorization may vary from simulation to simulation. This categorization is used for some operating characteristic output such as “The number of good arms that were successful” and plots.\n\n\n\n\n\n\nFigure 9: The Virtual Response &gt; Treatment Classification tab.\n\n\n\nAn example of how this can be important is considering a quantity like power: the probability of success given the treatment has some assumed level of effect. When the treatment effect is assumed to have some fixed value this calculation is easy. When the treatment effect is drawn from a distribution, sometimes the effect is strong, but sometimes it’s weak or even null/negative. Creating a classification system allows for the replacement of a traditional “power” with the probability of success given that the arm’s efficacy falls into the “good” class.\nIt’s still typical to consider an arm’s Type I error to be the probability of success given the arm has a fixed efficacy exactly equal to the control arm."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#response-specification",
    "href": "documentation/v72/userguides/platform.html#response-specification",
    "title": "Platform Trials",
    "section": "Response Specification",
    "text": "Response Specification\nThis tab allows for the specification of the efficacy assumptions for the control and treatment arms in the platform. It is expected that multiple VSR scenarios will be created, so that the design can be simulated across various treatment effect assumptions.\nFor a continuous endpoint, the control arm can have a known mean response or a distribution of potential control rates. The standard deviation of the final response is always assumed as a fixed value. The treatment effects for a continuous trial can be specified as known means, known changes from the control arm, or simulated from a distribution of effects.\nFor a dichotomous endpoint, the control arm can have a fixed assumed response rate or a response rate drawn from a distribution. The treatment arms can be specified has response rates, change from control on the probability scale, change from control on the log-odds scale, or have a change from baseline drawn from a distribution.\n\nArm Response\nThis sub-tab allows the user to set up several scenarios for how the true response to be simulated for each treatment arm.\n\nContinuous EndpointDichotomous Endpoint\n\n\nTwo types of inputs are needed for a given scenario:\n\nThe mean response for each treatment arm. There are three ways that the mean response can be specified.\n\nFixed mean response. This specifies the actual mean value for the arm in the Value column. This option is somewhat redundant with the fixed mean effect option, except in the case when the control arm is sampled from a distribution.\nFixed mean effect. This specifies the difference between the treatment arm and the control arm in the Value column. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative effect value.\nThis option is not available for the control arm.\nA distribution. Rather than specifying a specific value for the mean value, a distribution of values can be chosen. Each simulated trial will then have a (possibly) different value for the mean response for the arm. Distributions will be available for selection in the drop-down menu for the treatment arms, with one option for each distribution profile set up on the Virtual Response &gt; Explicitly Defined &gt; Treatment Distribution sub-tab. If a distribution is chosen, the value in the Value column is ignored.\nIf this selection is made for the control arm, the selection is called “Sampled”, and the distribution is specified on this tab, in the “Control Sampled Mean Response” box.\n\n\n\n\n\n\n\n\nContinuous Response Simulation\n\n\n\nIf a baseline value is not being simulated, the final response simulated is equivalent to ‘change from baseline,’ where baseline is always assumed to be 0. If baseline is being simulated, the user can select whether the response to be analyzed is the change from baseline or absolute response (the option is selected on the Study tab). Depending on that selection, the response specified on this tab is either change from baseline or absolute response.\n\n\n\nThe standard deviation of the response – either through a common SD of response for all treatment arms, or by specifying the standard deviation for the response on each treatment arm separately. [Note that the total variance in the observed final responses can be greater than this if a baseline adjustment for subject response is specified]\n\n\n\n\n\n\n\nFigure 10: The Virtual Response &gt; Response Specification &gt; Arm Response tab for a continuous endpoint.\n\n\n\n\n\nThis sub-tab differs for a dichotomous endpoint in that there are options for how to specify the treatment effect, and no specification of a standard deviation or baseline value is required. The response rate must be specified for each arm. There are four ways that the mean response can be specified.\n\nFixed response rate. This specifies the actual response rate for the arm in the Value column. This option is somewhat redundant with the fixed effect option, except in the case when the control arm is sampled from a distribution.\nFixed effect (rate). This specifies the response rate difference between the treatment arm and the control arm in the Value column. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative effect value. If an effect size would cause a value to be outside of the range [0, 1], then it is set at the appropriate boundary.\nThis option is not available for the control arm.\nFixed effect (log-odds). This specifies the Value column as the difference between the logit of the response rate for the treatment arm and logit of the response rate for the control arm.\nThis option is not available for the control arm.\nA distribution. Rather than specifying a specific value for the response rate, a distribution of values can be chosen. Each simulated trial will then have a (possibly) different value for the response rate for the arm. Distributions will be available for selection in the drop-down menu for the treatment arms, with one option for each distribution profile set up on the Virtual Response &gt; Explicitly Defined &gt; Treatment Distribution sub-tab. If a distribution is chosen, the value in the Value column is ignored.\nIf this selection is made for the control arm, the selection is called “Sampled”, and the distribution is specified on this tab, in the “Control Sampled Response Rate” box.\n\n\n\n\n\n\n\nFigure 11: The Virtual Response &gt; Response Specification &gt; Arm Response tab for a dichotomous endpoint.\n\n\n\n\n\n\n\n\nTreatment Distribution\nThis tab is used to set up distribution profiles, so that the arm response is drawn randomly for each simulated trial. Any distribution profile that is added on this sub-tab will appear as an option in the drop-down menu for the Mean Fixed/Sampled column in the Arm Response sub-tab. There is an auto created treatment distribution created by default on this tab, so “Sampled” from a distribution can be selected on the Arm Response tab before visiting the Treatment Distribution tab.\n\nContinuousDichotomous\n\n\nFor each distribution profile, three components must be established by the user:\n\nResponse vs Effect Size. This selection determines whether the value being simulated is the actual mean response or a difference from the control arm response. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative value if Effect Size is the selection.\nProb same as control / Prob effect size is 0: The distribution that is simulated will be a mixture distribution with a point mass on the value of the control arm, and the remaining probability assigned to a continuous distribution. This value specifies the probability assigned to the point mass. Values of 0 are allowed, meaning that only the continuous distribution would be utilized.\nThe distribution for the continuous portion of the mixture. Currently, three distributions types are available.\n\nThe normal distribution, which requires the specification of the mean and standard deviation of the distribution.\nThe truncated normal distribution, which requires the specification of the mean and standard deviation of the (non-truncated) distribution, along with the minimum and maximum values that are to be allowed. Note: if the maximum (minimum) value is left blank in the gui, the value is interpreted as infinity (negative infinity), allowing for one-sided truncation.\nThe generalized beta distribution, which requires the specification of the minimum, maximum, mean, and standard deviation. Note that not all combinations of values lead to a valid specification of the beta distribution. The mean must be between the minimum and maximum, and the standard deviation must then be no larger than:\n\\[\\sqrt{(Maximum - Mean) \\bullet (Mean - Minimum)}\\]\n\n\n\n\n\n\n\n\nFigure 12: The Virtual Response &gt; Response Specification &gt; Treatment Distribution tab for continuous endpoints showing specification of a distributional treatment effect.\n\n\n\n\n\nThis tab is used to set up distribution profiles, so that the arm response rate is drawn randomly for each simulated trial. Any distribution profile that is added on this sub-tab will appear as an option in the drop-down menu for the Response Rate Fixed/Sampled column in the Arm Response sub-tab. The options here are somewhat expanded relative to the continuous endpoint version. The user must specify three components:\n\nResponse Rate, Effect (Response Rate), Log-Odds, Effect (Log-Odds). This selection determines both whether the value being simulated is: a) the rate or log-odds (logit of rate) and b) the actual value or an effect - difference from the control arm response. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative value if Effect Size is the selection.\nProb same as control / Prob effect size is 0: The distribution that is simulated will be a mixture distribution with a point mass on the value of the control arm, and the remaining probability assigned to a continuous distribution. This value specifies the probability assigned to the point mass. Values of 0 are allowed, meaning that only the continuous distribution would be utilized.\nThe distribution for the continuous portion of the mixture. Currently, three distributions types are available.\n\nThe normal distribution, which requires the specification of the mean and standard deviation of the distribution – available only for if log-odds or effect (log-odds) is selected in step 1.\nThe truncated normal distribution, which requires the specification of the mean and standard deviation of the (non-truncated) distribution, along with the minimum and maximum values that are to be allowed. Note: if the maximum (minimum) value is left blank in the gui, the value is interpreted as infinity (negative infinity), allowing for one-sided truncation. Allowed for all selections in step 1.\nThe generalized beta distribution, which requires the specification of the minimum, maximum, mean, and standard deviation. Note that not all combination of values lead to a valid specification of the beta distribution. The mean must be between the minimum and maximum, and the standard deviation must then be no larger than:\n\\[\\sqrt{(Maximum - Mean) \\bullet (Mean - Minimum)}\n\\]This option is allowed for all selections in step 1.\n\n\nNote that if the selection in step 1 is Effect (Response Rate), it is possible to specify a distribution that has support beyond the range of [0, 1] for the response rate (particularly if the control rate is sampled, since the control rate for any simulation may draw a rate near the boundary). In this case, the distribution in truncated to ensure a rate in the range [0, 1].\n\n\n\n\n\n\nFigure 13: The Virtual Response &gt; Response Specification &gt; Treatment Distribution tab for dichotomous endpoints showing specification of a distributional treatment effect on the probability scale.\n\n\n\n\n\n\n\n\nBaseline (Continuous Only)\n\nResponse is Change from Baseline\nIf the endpoint is continuous, then there will be a Baseline sub-tab within the Response Specification tab.\nIf simulation of baseline has been included on the Study &gt; study Info tab, if response is change from baseline then the explicitly defined dose response is still in terms of change from baseline, and a new virtual subject response tab is available for specifying the baseline score.\n\n\n\n\n\n\nFigure 14: The Virtual Response &gt; Response Specification &gt; Baseline tab showing the specification of a baseline VSR with a baseline adjustment to the response.\n\n\n\nThe simulation of distribution of baseline scores is specified using a normal distribution –with user specified mean and standard deviation and optionally applied upper and lower bounds to reflect limitations on the score range or screening criteria. Note that if the observed baseline score is truncated, then the true mean and SD of the baseline are likely to be different from these values of the mean and SD which are before truncation.\nEither baseline is simulated separately from the final response (for use with BOCF), or the simulation of the final response can include a baseline dependent element.\nIf adjusting the final response based on baseline, then the user selects “Use Baseline Adjustment for Subject Response” and supplies 3 parameters:\n\n\\(\\beta\\)\n\na coefficient that reflects the degree of influence of baseline on final score and the degree of variability in the final score due to baseline.\n\nc\n\na centering offset, typically the expected mean of the observed baseline scores\n\ns\n\na scaling element, typically set to the expected SD of the baseline.\n\n\nExample – in the above screenshot a baseline of mean 25 and SD 10 has been specified – so a centering of 25 and scaling of 10 is used. Wishing to simulate an overall SD of 5 in the final change from baseline and apportion two-thirds the variance to baseline, Beta has been set as follows:\n\nThe desired final variance is 25 (52), divided between 1/3rd dose response and 2/3rd baseline effects.\nThe SD of the simulated response is set to 2.89 \\(\\sqrt{\\left( 25*\\frac{1}{3} \\right)}\\)\nThe SD of the scaled baseline score is 1, so to contribute half the final variance of 25, Beta is set to 4.08 \\(\\sqrt{\\left( 25*\\frac{2}{3} \\right)}\\)\nNote that when simulating a baseline effect in this way, limiting the range of baseline by specifying upper and lower cut-offs – which might be natural limits of the endpoint, or due to inclusion / exclusion criteria in the protocol – can significantly reduce the variance in the final endpoint due to the baseline effect.\n\n\n\nResponse is Final endpoint\nIf the response is specified to be final endpoint score, then baseline is specified as above, but the explicitly defined dose response is now defined in terms of final endpoint."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#dropout-rate",
    "href": "documentation/v72/userguides/platform.html#dropout-rate",
    "title": "Platform Trials",
    "section": "Dropout Rate",
    "text": "Dropout Rate\nThe probability of a subject dropping out before observing their final endpoint value is provided by arm. There is no longitudinal model, so Dropouts Per Arm Per Visit cannot be selected.\n\n\n\n\n\n\nFigure 15: The Virtual Response &gt; Dropout Rate tab, specifying the dropout rate per arm."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#control-response",
    "href": "documentation/v72/userguides/platform.html#control-response",
    "title": "Platform Trials",
    "section": "Control Response",
    "text": "Control Response\nThis sub-tab is used to set up the normal prior distribution for the control arm. There are two separate priors to create - one for the all participants model, and one for the concurrent controls only model.\nIn the all participants model, only the prior mean and standard deviation for the shared control arm must be entered. The treatment effect priors are specified on the Treatment Response tab. All arms are modeled independently in the platform trials engine.\nAdditionally, the Concurrent Control box allows for the specification of the fixed normal prior for the control arm estimate for all of the individual concurrent controls models. The concurrent control arm esimate for all arms has the same prior. There is also a box to specify a window before the arms begin randomizing in which the randomized control participants should be considered concurrent with an active arm. So, if it’s specified that FACTS should include participants up to “8” weeks from treatment entering trial, then all control subjects randomized between 8 weeks before an arm started randomizing to when an arm stopped randomizing (or the current time if still randomizing) are considered concurrent controls."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#treatment-response",
    "href": "documentation/v72/userguides/platform.html#treatment-response",
    "title": "Platform Trials",
    "section": "Treatment Response",
    "text": "Treatment Response\nThis sub-tab is used to set up the prior distribution for the treatment arms and for the common standard deviation of the response. All treatment means are modeled independently. The user can specify a single normal prior that applies to each treatment arm or specify normal priors individually for each treatment arm.\nThis tab also contains the prior distribution for the standard deviation of the responses. The variance is modeled as an inverse-gamma distribution.\nThis tab also contains the method for handling missing data due to dropouts. Since longitudinal modeling is not yet implemented, the only method that currently applies is BOCF, if the endpoint is continuous and baseline data is simulated. Without longitudinal modeling to incorporate post-baseline visit data, the Bayesian multiple imputation from post-baseline method will ignore dropouts."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#allocation",
    "href": "documentation/v72/userguides/platform.html#allocation",
    "title": "Platform Trials",
    "section": "Allocation",
    "text": "Allocation\nAs long as “Enable Adaptive Features” is selected on the Trial &gt; Trial Info tab, a radio button at the top of the Design &gt; Allocation sub-tab allows choice between the two options for specifying arm allocation in the Platform Trial simulator.\n\nFixed Allocation\nIf Fixed Allocation is selected, then the blocking for randomization can either be done by creating a control:treatment block for each active arm, or by specifying a specific block size based on the number of arms currently randomizing. A radio button allows a choice between the two options.\n\n\n\n\n\n\nFigure 19: The fixed allocation tab for the platform trials engine.\n\n\n\n\nConstant proportion allocated to control\nFor this option, the proportion of participants that are allocated to the control arm remains constant, regardless of the number of treatment arms that are active in the trial. The user specifies the number of control participants and treatment participants per sub-block. A full block consists of a number of sub-blocks corresponding to the number of active treatment arms being allocated. Within each sub-block, the number of control participants is fixed, while the participants for any particular active treatment arm may be spread across the sub-blocks.\nAs an example, suppose the inputs for constant proportion allocated to control are\n\nAllocation to control per sub-block = 2\nAllocation to treatments per sub-block = 6\n\nThen, “Sub-block size per treatment” is calculated to be 8. Suppose that there are currently 3 active arms accruing and 1 control arm.\nWhen randomizing participants, the blocks that subjects allocations are drawn from are then a total size of \\(8*4=32\\). It is guaranteed that each sub-block of 8 within the total 32 will have exactly 2 control. The other 24 slots in the block are completely suffled, so there is no guarantee that any number of a non-control arm will be contained within the sub-blocks.\n\n\nAllocation dependent on the number of treatments\nFor this option, the proportion allocated to the control arm is allowed to vary depending on the number of active treatment arms in the trial. For each potential number of active treatment arms, the user specifies the allocation to each treatment and control by specifying Y and X for that allocation Y:Y:…:Y:X. That is, each treatment arm gets an equal number of participants per block, while the control has a (possibly) different allocation. The entire block of size X + Y*(# treatments) is then shuffled.\n\n\n\nAdaptive Allocation\nIf Adaptive Allocation is selected, then the blocking for randomization can either be done by creating a block with a fixed proportion of controls and the same number of adaptively randomized slots per block, or by specifying a specific block size based on the number of arms currently randomizing. A radio button allows a choice between the two options.\nA radio button allows a choice between two options for adaptive allocation.\n\nConstant proportion allocated to control\nThis option works in the same way as the fixed allocation constant proportion allocated to control, except that the non-control parts of the block are randomly assigned to active treatment arms based on the response-adaptive randomization probabilities.\n\n\nAllocation dependent on the number of treatments\nFor this option, the proportion allocated to the control arm is allowed to vary depending on the number of active treatment arms in the trial. For each potential number of active treatment arms, the user specifies the allocation to treatments and control for each block by specifying Y and X for that allocation Y:X of treatment to control. In this case (differing from the fixed allocation version), Y is the total allocation to treatment arms that are being adaptively allocated to. Notice that this Y:X differs from the Y:Y:…:Y:X pattern used for fixed allocation.\n\n\nRAR Probabilities\nThe response-adaptive randomization (RAR) follows the same scheme as FACTS Core with respect to specifying the quantity(ies) of interest to use in determining allocation by arm. However, one important difference is the definition of the burn-in period.\nUnlike FACTS Core, the burn-in period applies to each treatment arm separately rather than to the beginning of the trial.\nThe fixed allocation period for each treatment arm is specified at the bottom of the Allocation sub-tab. A treatment arm is not part of the RAR scheme until this number of participants have been enrolled and an analysis has been run to update the relevant QOIs. This latter aspect is distinct from FACTS Core, where an analysis is always run as soon as the allocation burn-in period has completed. The user may need to consider this aspect when determining how interim timing is to be set up. While in the burn-in phase, a treatment arm is allocated a fixed 1/T ratio of the adaptive allocation slots, where T is the number of active treatment arms.\n\n\n\n\n\n\nFigure 20: The Design &gt; Allocation tab with an Adaptive Allocation target specified."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#interims",
    "href": "documentation/v72/userguides/platform.html#interims",
    "title": "Platform Trials",
    "section": "Interims",
    "text": "Interims\nThe Interims sub-tab is used to specify the timing of trial update analyses and to specify which set of stopping criteria (if any) are to be applied at the update.\n\n\n\n\n\n\nFigure 21: The Interims tab with information specified as opportunity to complete, updates every 4 weeks, follow-up before a final analysis if the decision for the arm is success, and decision milestones at 30, 55, and 80 subjects with the opportunity to complete.\n\n\n\n\nUpdate Frequency\nTrial updates can be specified by time, information, a mixture of time and information, or by milestones reached.\nInformation is defined in the same way as FACTS Core, either by number of participants enrolled, number of participants with complete data (at a particular visit when longitudinal modeling is used), or number of participants who have had the opportunity to provide complete data (at a particular visit when longitudinal modeling is used). This definition of information is used for milestones as well as (optionally) update timing.\nThere are two distinct modes for trial update timing. A radio button toggles between “Updates occur whenever a treatment milestone is met” and “Updates occur on a regular schedule”. The former option is only available if Fixed Allocation is chosen on the Design &gt; Allocation sub-tab.\n\nUpdates occur whenever a treatment milestone is met\nIn this mode, the only time an analysis is performed is when a treatment arm reaches a milestone – i.e. when a particular level of information has been reached, as specified in the Treatment Milestones portion of the sub-tab. The analysis will include all patients enrolled up to that point in time and provide model output for all arms, but decisions will be made only for the treatment arm whose milestone triggered the analysis. An analysis will also occur if a treatment arm has reached full follow-up on its maximum number of enrolled participants.\n\n\nUpdates occur on a regular schedule\nIn this mode, analyses are performed on a regular schedule based on time or information at the trial level (based on all participants). The timing of the first update can be specified as either time from the start of the trial or a fixed level of information. If adaptive allocation is being utilized, it may begin after this first analysis, provided that at least one individual treatment arm burn-in has been reached. After the first update, updates can be specified to occur at either fixed time intervals or after a fixed incremental level of information has accrued.\nNote that in this mode, it is possible for treatment milestones to be skipped. E.g. if milestones were set up to occur at enrollment of 30, 55, and 80 participants, it is possible that a treatment arm could have 20 participants enrolled at one trial update and 60 participants at the next. If this were the case, any decisions associated with milestone 1 would be skipped and milestone 2 decisions would be evaluated instead.\nAn exception to the regular schedule is that an analysis is always performed whenever a treatment arm has reached its time for a final analysis. When this happens, an “off-schedule” trial update is performed, but no actions are taken except for the declaration of success or futility for the completed arm.\n\n\n\nSubject Follow-up Options\nThe follow-up options mimic those of FACTS Core. However, in the Platform Trial simulator, the follow-up decision applies only to those participants that were allocated to the arm that is being stopped. Control participants and participants on other treatment arms continue to be followed.\n\n\nTreatment Milestones\nTreatment milestones are benchmarks that a treatment can reach that determine which success/futility fules should be used on the arm at that point in time. Before an arm has reached a milestone, there are no success or futility rules for that arm to evaluate. Once it reaches the first milestone, any time its status is evaluated it will use the set of success/futility rules that correspond to the most recent milestone that has been reached.\nIf updates occur every time a treatment milestone is met, then there is an update performed immediately every time an arm crosses a milestone threshold. This update is specific to the arm reaching the threshold, and while all arm data is used in the analysis models, only the arm that reached the threshold can make a decision at this milestone based update.\nWhen updates are regularly scheduled, an update triggers based on the specified rules, and all arms have an opportunity to make a decision as long as they have reached at least the first milestone. Each arm keeps track of its own milestone benchmark and uses its own rules based on its progress through the platform. When regularly scheduled updates are selected, further input from the user is required in the Treatment Milestones box to dictate if milestone specific criteria can be assessed many times, or only once.\n\nEvaluate milestone criteria at each update\n\nWith this selection, decisions associated with a particular milestone may be evaluated more than once. E.g., suppose milestones occurred at enrollment of 30 and 55 participants, and at a trial update with 32 participants enrolled, an early futility decision was evaluated. At the next trial update, if only 50 participants are enrolled, the early futility decision associated with milestone 1 would be re-evaluated with the updated model results.\n\nEvaluate milestone criteria when milestone first reached\n\nWith this selection, a given milestone can be evaluated at most once – the first time it is reached (if a higher milestone hasn’t already been reached). In the example above, there would be no futility evaluation at 50 participants because the milestone had already been evaluated to 32 participants."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#successfutility-criteria",
    "href": "documentation/v72/userguides/platform.html#successfutility-criteria",
    "title": "Platform Trials",
    "section": "Success/Futility Criteria",
    "text": "Success/Futility Criteria\nThe success/futility criteria sub-tab is used to specify the rules for early stopping and final evaluation. The basic structure of defining criterion with a QOI, a direction, and a threshold follows the conventions used in FACTS Core. However, the Platform Trial Simulator has some differences and additions, as outlined below.\n\n\n\n\n\n\nFigure 22: The Success/Futility Criteria tab, specifying the milestone 1 criteria for all subjects.\n\n\n\nEarly stopping criteria can be set up for each milestone defined on the Design &gt; Interims sub-tab. Use the Create button to add a sub-tab for early stopping for a chosen milestone. There are check-boxes to indicate if early futility or early success should be evaluated at the milestone. If the same decision criteria are to be used for multiple consecutive milestones, then only the first in the series needs to be created, and subsequent milestones will use the criteria until a new milestone decision rule applies. E.g., if the first and second milestones use early futility with the same stopping criteria, while early success is not allowed until the third milestone, the user need only set up the Milestone 1 and Milestone 3 criteria. For convenience, the “Copy from” button can be used to copy all criteria from a different milestone. The Final Evaluation criteria are applied for a treatment arm only if:\n\nthe treatment arm has enrolled participants and followed up its maximum number of participants, or\nthe treatment arm had previously hit an early stopping criteria and has completed all expected follow-up on its participants.\n\nOne fundamental difference from FACTS Core in setting up a decision criterion is that all QOIs are available, not the scalar “Decision QOIs” that FACTS Core uses. The value utilized is always the one corresponding to the treatment arm whose milestone is being evaluated.\nThe Platform Trial simulator also allows for differential specification of evaluation criteria by treatment arm. Any criteria that are specified on the “All Treatments” sub-tab apply to each treatment arm, and only on this sub-tab can the checkboxes for early stopping be checked. The combination of “All Treatments” criteria are combined with the criteria for an individual treatment in the following way.\n\nThe criteria on the “All Treatments” tab are combined with the AND/OR as specified on that tab to determine an “All Treatments” TRUE or FALSE.\nThe criteria on the individual treatment arm’s tab are combined with the AND/OR as specified on that tab to determine an individual treatment arm TRUE or FALSE.\nThe “All Treatments” and individual treatment results are then combined differently for early success or futility:\n\nFor success, the two are combined with an AND – both must be met (as typically a specific treatment arm would be allowed to have a stricter rule for success but not a laxer rule).\nFor futility, the two are combined with an OR – either can be met (as typically a specific treatment arm might be allowed more leeway to withdraw from a trial)."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#simulation-options",
    "href": "documentation/v72/userguides/platform.html#simulation-options",
    "title": "Platform Trials",
    "section": "Simulation Options",
    "text": "Simulation Options\n\nNumber of simulations\nThis box allows the user to enter the number of simulations that they would like FACTS to run for each scenario listed in the table at the bottom of the simulation tab. There is no set number of simulations that is always appropriate.\n\n10 simulations\n\nYou might want to run 10 simulations if you just want to look at a few simulated trials and assess how the decision rules work and if FACTS is simulating what you expected based on what you input on the previous tabs. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\n\n100 simulations\n\nYou might want to run 100 simulations if you want to look at many individual trials to make sure that what you want to happen is nearly always happening. You can also start to get a very loose idea about operating characteristics like power based on 100 sims. 100 simulations is also usually sufficient to spot big problems with the data analysis such as poor model fits or significant bias in the posterior estimates.\n\n1,000 simulations\n\nYou might want to run 1,000 simulations if you want estimates of operating characteristics like power, sample size, and Type I error for internal use or while iterating the design. This generally isn’t considered enough simulations for something like a regulatory submission. With 1,000 simulations the standard error for a typical type I error calculation is on the order of \\(0.005\\).\n\n10,000 simulations\n\nYou might want to run 10,000 simulations per scenario if you are finalizing a design and are preparing a report. This is generally enough simulations for a regulatory submission, especially in non-null simulation scenarios. The standard error for a typical type I error calculation using 10,000 simulations is on the order of \\(0.0015\\).\n\n&gt; 10,000\n\nYou might want to run more than 10,000 simulations if you want to be very certain of an operating characteristic’s value - like Type I error. And plan to use the measurement of the quantity for something important like a regulatory submission. The standard error of a Type I error calculation with 100,000 simulations, e.g., is on the order of \\(0.0005\\).\n\n&gt; 100,000\n\nYou probably don’t want to run more than 100,000 simulations per scenario. Maybe your finger slipped an hit an extra 0, or you thought there were 5 zeroes in that number when there were actually 6. If the simulated trial is adaptive, this is going to take a while.\n\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. Not all scenarios must be run with the same number of simulations. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run.\n\n\nStart at Simulation\nThe “Start at simulation” option allows for the simulation of a particular trial seen in a previous set of simulations without having to simulate all of the previous trials in that previous set to get to it.\nThe initial random seed for FACTS simulations is set in the simulation tab. The first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. Thus, it is possible to re-run a specific simulation out of a large set without re-running all of them. For example, say the 999th simulation out of a set displayed some unusual behavior, in order to understand why, one might want to see the individual interim analyses for that simulation (the “weeks” file), the sampled subject results for that simulation (the “Subjects” files) and possibly even the MCMC samples from the analyses in that simulation. You can save the .facts file with a slightly different name (to preserve the existing simulation results), then run 1 simulation of the specific scenario, specifying that the simulations start at simulation 999 and that at least 1 weeks file, 1 subjects file and the MCMC samples file (see the “MCMC settings” dialog) are output.\n\nThe parallelization packet size, this allows simulation jobs to be split into runs of no-more than the specified number of trials to simulate. If more simulations of a scenario are requested than can be done in one packet, the simulations are started as the requisite number of packets and the results combined and summarized when they are all complete – so the final results files look just as though all the simulations were run as one job or packet. FACTS tries to set a sensible default size given the overall number of simulations to be run.\nWhen running simulations on the local machine FACTS enterprise version will process as many packets in parallel as there are execution threads on the local machine. The overhead of packetization is quite low so a packet size of 10 to 100 can help speed up the overall simulation process – threads used to simulate scenarios that finish quicker can pick up packets for scenarios that take longer, if the number of scenarios is not directly divisible by the number of threads packetization uses all threads until the last few packets have to be run and finally the “Simulations complete” figure can be updated at the end of each packet, so the smaller the packet the better FACTS can report the overall progress.\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run.\n\n\nParallelization Packet Size\nThe parallelization packet size option allows simulation jobs to be split into runs of no-more than the specified number of trials that are run in parallel. If more simulations of a scenario are requested than can be done in one packet, the simulations are broken into the requisite number of packets, run, and combined and summarized when they are all complete. The final results files will look just as though all the simulations were run as one job or packet.\nThe packet size must be a perfect divisor of the number of simulations. This is usually easy since common numbers of simulations are multiples of 100, but don’t try to use a prime number for Number of Simulations or you’re stuck with only 2 packet size options.\nBy default (if the check box with Choose Parallel Packet Size is not checked) the number of simulations per packet depends on the number of simulations per scenario. If the number of simulations is less than 1000, then each scenario is packaged as a single packed and simulated. If the number of simulations per scenario is greater than or equal to 1000, the default packet size is 10 and all simulations are decomposed into packets of size 10.\n\n\n\n\n\n\nPacketization with VSRs using .mvsr files\n\n\n\nIf an external file is used to create explicit VSRs (a .mvsr file), then the packet size should be a multiple of the number of rows in that MVSR file. Each packet will get passed the entire .mvsr file to run. If there are multiple .mvsr files with differing numbers of lines then only the VSR scenarios that have a .mvsr file that has a number of rows that is a divisor of the packet size will be run. The rest will error. The packet size can then be modified to get each of the .mvsr specified VSR files to be run.\nCare should be taken when packetizing a scenario that includes an external data file to supply the virtual subject responses; in this situation, a of copy of the external file is included in each packet which can cause the packetisation process to run out of memory as the packets are being created. In this case, use a smaller number of larger packets, such as packets that are 1/10th of the total number of simulations.\n\n\nWhen running simulations, FACTS will create and run as many packets in parallel as there are execution threads on the local machine. In general, the overhead of packetization is quite low, so a packet size of 10 to 100 can help speed up the overall simulation process. Threads used to simulate scenarios that finish quickly can pick up packets for scenarios that take longer. The progress bar updates as simulation packets complete, so the smaller the packet size, the more accurately FACTS can report the overall progress of the simulation execution.\n\n\nRandom Seed\nRandom number generation plays a huge role in FACTS’s virtual patient generation and statistical analyses. In order to exactly reproduce a statistical set of results, it is necessary to start the random number generation process from an identical “Random Seed”. Using the same random seed in the same version of FACTS guarantees that simulated trials will always be reproducible. Changing the design parameters or the version of FACTS may or may not remove this reproducibility depending on the change.\nEven a small change in the random seed will produce very different simulation results.\nIn addition to setting the seed, the user can choose whether they want the “Same seed for all scenarios” or “Different seed” for different scenarios. If “Same seed for all scenarios” is selected, the subjects generated for each simulated trial will match for the different scenarios. This induces a correlation among the simulation output for different scenarios. This can be good if you’re trying to compare operating characteristics for different scenarios, but it can also be misleading. To disable this option select the “Different Seed” option. If “Different seed” is selected, then each scenario has its own seed that samples a different set of subjects than any other scenario. This uncorrelates the simulation output across scenarios, which can be advantageous if the absolute value of the operating characteristics are more valuable to you than the comparison of operating characteristics across scenarios.\n\n\nMCMC Settings\nTo set advanced settings for simulation, the user may click the “MCMC Settings” button, which will display a number of additional specifiable parameters for simulation in a separate window.\n\n\n\n\n\n\nFigure 24: Advanced MCMC settings.\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the MCMC chain to reach its equilibrium distribution.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest.\n\nThe third parameter controls the number of MCMC samples taken after each imputation of missing data using the longitudinal model. Since there is no longitudinal modeling in the Platform Trials engine currently, this parameter does nothing. \nThe next parameter concerns the output of the MCMC samples to a file. It is possible to have the design engine output the sampled values from the MCMC in all of the interims of the first N simulated trials of each scenario by specifying the “Number of MCMC files to output” to be greater than 0. The resulting files, ‘mcmcNNNN.csv’, will be in the results directory with all the other results files for that scenario. These files include the burn-in samples from the MCMC chains.\nThe final parameter in MCMC Settings is the thinning parameter. This parameter will only keep every \\(N^{th}\\) sample taken during MCMC where \\(N\\) is the thinning parameter. Thinning MCMC samples can reduce the autocorrelation of consecutive MCMC iterations, which increases the effective samples per retained sample, but also results in needing many more MCMC iterations to reach the same number of retained samples. Generally, we do not recommend thinning for standard simulation runs.\n\n\n\n\n\n\nWarning about thinning\n\n\n\nUnlike other software that performs MCMC, when you choose to thin by a value, FACTS does not increase the number of MCMC iterations it performs in order to retain the value specified in “Number of Samples”. So if you leave “Number of Samples” at its default value, \\(2500\\), and thin by \\(10\\), you will be left with \\(250\\) retained samples. You should adjust for this by increasing the “Number of Samples” if you choose to thin."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#results-output",
    "href": "documentation/v72/userguides/platform.html#results-output",
    "title": "Platform Trials",
    "section": "Results Output",
    "text": "Results Output\nThe results output section of the Simulation tab allows for the specification of how many output files should be generated for files that are individually created for each simulation. Summary files (summary.csv) that have 1 line per scenario are always created. Simulations files (simulations.csv) that have 1 line per simulation are always created. Weeks files (weeksXXXXX.csv), patients files (patientsXXXXX.csv), and frequentist weeks files (weeks_freq_{missingness}_XXXXX.csv) are not created for every single simulation. Instead, the number of simulation specific output files can be set per type. This limits the amount of output files that FACTS will save.\nSome plots in FACTS that are created based on weeks files, and if very few weeks files are saved, the plots will not be as accurate or descriptive."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#run-simulations",
    "href": "documentation/v72/userguides/platform.html#run-simulations",
    "title": "Platform Trials",
    "section": "Run simulations",
    "text": "Run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user is prevented from modifying any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario.\n\nFACTS Grid Simulation Settings\nA user with access to a computational grid, may choose to run simulations on the grid instead of running them locally. This frees the user’s computer from the computationally intensive task of simulating so that they can continue other work or even shutdown their PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#simulation-results",
    "href": "documentation/v72/userguides/platform.html#simulation-results",
    "title": "Platform Trials",
    "section": "Simulation Results",
    "text": "Simulation Results\nIn the center of the simulation tab, the summary simulation results are displayed. Once simulations have been run the table will be populated with results. The results that are shown by default are highlights of the operating charactersics for each scenario run. FACTS outputs many columns of results, and they are organized into related groups of sub-windows, which can be displayed by clicking on the “Show More Columns” button or right clicking on a row of the table.\n\n\n\n\n\n\nFigure 25: The “Show More Columns” menu on the Simulation tab.\n\n\n\nThese options will open windows that show:\n\n\n\nName\nColumns that will be shown\n\n\n\n\nAll\nShow all available summary columns.\n\n\nHighlights\nShow only the columns shown on the main tab. See below.\n\n\nTiming\nThe columns indicating timing information for treatment arms entry and exit.\n\n\nAllocation\nThe columns that report on participant recruitment and allocation.\n\n\nResponse\nThe columns that report the Bayesian model based estimates of the response for each treatment.\n\n\nObserved\nThe raw endpoint output and the dropout rates by arm and visit.\n\n\nProbabilities\nThe final estimates for the QOIs that were computed for the trial.\n\n\nModel Parameters\nThe columns that report the estimates of the values of the model parameters.\n\n\nSimulation Results\nOpens a window that displays the individual simulation results for that scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results (see below) the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of the Simulation Results highlights window.\n\n\nSimulation Duration\nA window that displays simulation run time information.\n\n\n\n\nRight Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 26: The Right Click menu for the simulations table.\n\n\n\nThese provide access to all the actions available from the “Results Options” – except aggregation. These will respectively:\n\nOpen windows showing the various “Other Columns” options described above\nShow Per Scenario Graphs: Opens the FACTS graph control displaying the graphs for that scenario. See section 14 for details.\nShow Across Scenario Graphs: Opens the FACTS graph control displaying the graphs comparing all scenarios.\nCompare Scenarios in Airship: Opens the R AIRSHIP package for comparing results across scenarios, see the AIRSHIP User Guide for details.\nOpen Results Folder: Open a new Windows directory browser window showing the contents of the simulation results for that scenario.\nCopy to Clipboard: Copies all the rows of the summary to the clipboard, as tab-delimited text.\nOpen in R: Opens R, loading in the result files for the currently selected scenario as separate data-frames (doesn’t load the aggregated files if they exist, for that use the “Open in R” button).\nDesign Report: Create a design report (a word document describing the design), see the Design Report User Guide for details.\n\n\n\nDouble Click\nDouble clicking on a row of simulation results opens a window listing the results of each individual simulation for that scenario. See section 15 below for a description."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#results-options",
    "href": "documentation/v72/userguides/platform.html#results-options",
    "title": "Platform Trials",
    "section": "Results Options",
    "text": "Results Options\n\nExplore Results\nThis button displays the following options:\n\n\n\n\n\n\nFigure 27: The Explore Results button menu.\n\n\n\n\nShow Per Scenario Graphs: Opens the FACTS graph control displaying the graphs for that scenario. See section 14 for details.\nShow Across Scenario Graphs: Opens the FACTS graph control displaying the graphs comparing all scenarios.\nCompare Scenarios in Airship: Opens the R AIRSHIP package for comparing results across scenarios, see the AIRSHIP User Guide for details.\nOpen Results Folder: Open a new Windows directory browser window showing the contents of the simulation results for that scenario.\nCopy to Clipboard: Copies all the rows of the summary to the clipboard, as tab-delimited text.\n\n\n\nOpen in R\nThe “Open in R” button allows for the creation of an R script that has pre-populated code for loading in output files created by the FACTS simulations.\nBy default, any/all of the simulation output files can be included in the created script. If “Aggregation” (see below) has been performed, then only the aggregated files will be available for being loaded in R.\nWhen the button is clicked, FACTS will create an R script with the correct file paths to load in the data, as well as creating a function that will read the files in correctly. The file is then opened in the default R editor for the user. If there is no default program for opening a .R file, your operating system should ask how you want to open the file.\n\n\nAggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 28: Window that appears when aggregating simulation results.\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nValues in columns that are independent of dose will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. WeeksNNNNN.csv files are aggregated into a single agg_[pivot_]weeks.csv file. PatientsNNNNN.csv files are aggregated into a single agg_patients.csv file, but they are never pivoted because each row already refers to a single dose.\nRegionIndex.csv is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nRecruitment Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g. External Subjects Profile if there are no external scenarios)\n\n\nDropouts Profile\n\n\n\nVirtual Response Profile\n\n\n\nTreatment Arm Arrival Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in weeks and patients files.\n\n\nTreatment\nTreatment Arm. Only present if pivoted.\n\n\n\n\n\nDesign Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#simulation-output-table",
    "href": "documentation/v72/userguides/platform.html#simulation-output-table",
    "title": "Platform Trials",
    "section": "Simulation Output Table",
    "text": "Simulation Output Table\n\nHighlights\nThese are the care completed. They can also be displayed in the separate “Highlights” columns displayed on the simulations tab after simulations results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Tr eatments Started\n1\nThe average number (over the simulations) of non-control treatments that entered the trial and were eligible to enrollment participants.\n\n\nMean Tr eatments Analyzed\n1\nThe average number (over the simulations) of non-control treatments that reached a final analysis within the trial – either because they reached an early stopping decision with no follow-up or because they completed follow-up on all participants.\n\n\nMean Good Tr eatments Analyzed\n1\nThe average number (over the simulations) of “good” treatment arms that reached a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable Tr eatments Analyzed\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that reached a final analysis, “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable Tr eatments Analyzed\n1\nThe average number (over the simulations) of “mediocre” treatment arms that reached a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean S uccesses\n1\nThe average number (over the simulations) of treatment arms that were declared successful at a final analysis.\n\n\nMean Good T reatment S uccesses\n1\nThe average number (over the simulations) of “good” treatment arms that were declared successful at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable T reatment S uccesses\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared successful at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre T reatment S uccesses\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared successful at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Fu tilities\n1\nThe average number (over the simulations) of treatment arms that were declared futile at a final analysis.\n\n\nMean Good T reatment Fu tilities\n1\nThe average number (over the simulations) of “good” treatment arms that were declared futile at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable T reatment Fu tilities\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared futile at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre T reatment Fu tilities\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared futile at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Incon clusives\n1\nThe average number (over the simulations) of treatment arms that were declared inconclusive at a final analysis.\n\n\nMean Good T reatment Incon clusives\n1\nThe average number (over the simulations) of “good” treatment arms that were declared inconclusive at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable T reatment Incon clusives\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared inconclusive at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre T reatment Incon clusives\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared inconclusive at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn S uccesses | T reatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Fu tilities | T reatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Incon clusives | T reatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn S uccesses | T reatment Unac ceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Fu tilities | T reatment Unac ceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Incon clusives | T reatment Unac ceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn S uccesses | T reatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Fu tilities | T reatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Incon clusives | T reatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Tr eatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable Tr eatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Tr eatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Tr eatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable Tr eatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Tr eatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Tr eatments | Inco nclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable Tr eatments | Inco nclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Tr eatments | Inco nclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn 1+ S uccesses\n1\nThe proportion of simulations that had at least one treatment arm declared successful.\n\n\nPpn Good Tr eatments | 1+ S uccesses\n1\nThe proportion of simulations that had at least one “good” treatment arm declared successful, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Su ccesses: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared successful.\n\n\nPpn Fut ilities: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared futile.\n\n\nPpn Inconc lusives: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared inconclusive.\n\n\nPpn Good T reatment | E nrolled: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable T reatment | E nrolled: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre T reatment | E nrolled: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good T reatment | A nalyzed: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable T reatment | A nalyzed: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre T reatment A nalyzed: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Duration\n1\nThe average (in weeks) over the simulations of the duration of the trial from the start to completion of the trial.\n\n\nMean First Success Time\n1\nAmongst simulations that had at least one success, the average time (in weeks) at which the first success was declared.\n\n\nMean Part icipants\n1\nThe average number of participants enrolled across all simulations.\n\n\nMean A vailable Time: &lt;Arm&gt;\n1\nThe average time across simulations that the treatment arm became available to enter the trial (regardless of whether the trial ended prior to it becoming available).\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nTiming\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Duration\n1\nThe average (in weeks) over the simulations of the duration of the trial from the start to completion of the trial.\n\n\nMean First Success Time\n1\nAmongst simulations that had at least one success, the average time (in weeks) at which the first success was declared.\n\n\nMean A vailable Time: &lt;Arm&gt;\n1 per arm\nThe average time (in weeks) across all simulations at which the treatment because available for entry into the trial. Note: the available time is reported whether or not it became available after the end of the trial.\n\n\nMean Start Time: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became available to enroll participants, the average time (in weeks) that the arm became eligible for enrollment.\n\n\nMean End Time: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became available to enroll participants, the average time (in weeks) that the arm became stopped enrolling.\n\n\nMean Final Analysis Time: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis, the average time (in weeks) that the arm’s final analysis occurred.\n\n\n\n\n\nAllocation\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Part icipants\n1\nThe average number (over the simulations) of participants enrolled in the trial.\n\n\nSD Mean Part icipants\n1\nThe standard deviation of the number (over the simulations) of participants enrolled in the trial.\n\n\nMean Alloc.: &lt;Arm&gt;\n1 per arm\nThe average (over the simulations) of the number of participants enrolled onto the arm.\n\n\nSD Alloc.: &lt;Arm&gt;\n1 per arm\nThe standard deviation (over the simulations) of the number of participants enrolled onto the arm.\n\n\n\n\n\nResponse\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Resp.: &lt;Arm&gt;\n1 per arm\nAverage (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nSD Resp.: &lt;Arm&gt;\n1 per arm\nStandard deviation (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nMean Sigma\n1\nAverage (over the simulations) of the estimate of the SD of the dose response across all the treatment arms (‘sigma’)\n\n\nSD Mean Sigma\n1\nStandard deviation (over the simulations) of the estimate of the SD of the dose response across all the treatment arms.\n\n\nTrue Mean Resp: &lt;Arm&gt;\n1 per arm\nTrue mean response from which the simulated participant data was sampled for the arm.\n\n\nSD True Resp.: &lt;Arm&gt;\n1 per arm\nTrue standard deviation of the simulated participant data for the arm.\n\n\nMean Baseline Beta\n\nThis is the mean (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nSD Baseline Beta\n\nThis is the SD (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nMean Baseline\n1\nAverage (over the simulations) of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\nStandard deviation (over the simulations) of the estimate of the mean baseline score.\n\n\nTrue Mean Baseline\n1\nTrue mean from which baseline scores where simulated (including accounting for possible truncation of the baseline scores)\n\n\nTrue SD Baseline\n1\nTrue standard deviation of the distribution from which baseline scores were simulated (including accounting for possible truncation of the baseline scores)\n\n\n\n\n\nObserved\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Arm&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Arm&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean D ropouts: &lt;Arm&gt;,\n&lt;Visit&gt;\n1 per arm per visit\nAverage (across the simulations) of the number of dropouts for the arm by visit.\n\n\n\n\n\nProbabilities\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt;: &lt;Arm&gt;\n1 per arm per QOI\nFor each Posterior Probability, Predictive Probability, p-value, or Target QOI, this is the mean over the simulations of the estimate of the probability of the QOI for each dose.\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition. The probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial).\n\n\n\n\n\nModel Parameters\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Sigma\n1\nAverage (over the simulations) of the posterior estimate of sigma, the SD in the participant’s final responses.\n\n\nSD Mean Sigma\n1\nStandard deviation (over the simulations) of the posterior estimate of sigma, the SD in the participant’s final responses.\n\n\nMean Baseline Beta\n1\nAverage (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nSD Baseline Beta\n1\nStandard deviation (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\n\n\n\nDetailed Per Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulations output in the table by double-clicking on the appropriate row. A separate window pops up and displays the results for each individual simulation. This is a “prettier” view of the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 29: The simulation results pop out viewer showing simulation highlights."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#graphs-of-simulation-results",
    "href": "documentation/v72/userguides/platform.html#graphs-of-simulation-results",
    "title": "Platform Trials",
    "section": "Graphs of Simulation Results",
    "text": "Graphs of Simulation Results\nTo enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described in the Output Files section below.\n\n\n\n\n\n\nBox and whisker plot conventions\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\nPer Scenario Graphs\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select that row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks. (Not displayed if the ‘y’ value must lie in the interval 0-1.\nShow Truth – this toggles whether the true response by arm is plotted.\n\n\nSimulation Summary Graphs\nThese graphs provide output averaged across all trials simulated for the scenario.\n\nOutcome by Treatment\n\n\n\n\n\n\nFigure 30: A stacked bar plot of outcomes for each arm in the trial.\n\n\n\nThis graph summarizes the final status of each treatment arm in the trial, including availability, completeness, and conclusion. The “Trt. Classification” drop-down menu in the Controls box allows the user to specify whether the plot should be restricted to treatment effects with a particular classification.\n\n\nSimulated Treatment Classification\n\n\n\n\n\n\nFigure 31: A stacked bar plot of the treatment classification (Good, Mediocre, or Unacceptable) for each arm in the trial. Arms with fixed treatment effects will always be 1 color.\n\n\n\nThis graph summarizes the treatment effect classification of each treatment arm in the trial. If no sampling is used for treatment arms, these bars will all be one solid color, but may show variation if either the control arm or the arm itself is sampled. The classification into “good,” “unacceptable,” and “mediocre” uses the definitions provided on the Virtual Response &gt; Treatment Classification tab. The “General Outcome” drop-down menu in the Controls box allows the user to restrict the plot to treatment arms that reached a particular outcome. For example, choosing “Success” will show the distribution of treatment classifications for the arms only amongst arms that achieved a “Early Success,” “Late Success,” or “Futility to Success Flip” outcome.\n\n\nAllocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 32: A box plot of the sample size per arm.\n\n\n\nThis graph displays a box and whisker plot of the number of subjects enrolled into each arm. This provides the distribution, over all simulations, of the number of subjects allocated to each arm. If a treatment arm is not able to enter the trial in a simulation it gets a 0 allocation for that simulation. In very simple platform trials, the number may be the same in every simulation, and the box and whiskers collapse to a single line.\n\n\nResponse and Participant Allocation\n\n\n\n\n\n\nFigure 33: A two-axis plot showing the mean allocation per arm and estimated response rate with the credible interval.\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black dot-and-whisker shows the distribution of the true mean response across the simulations, as 2.5%, 50%, and 97.5% quantiles. This will only be a black dot if the response is fixed.\nThe green dot-and-whisker shows the distribution of the estimated response across the simulations, as 2.5%, 50%, and 97.5% quantiles.\n\n\n\nPer Treatment: QOIs\n\n\n\n\n\n\nFigure 34: Boxplots across all simulations of a QOI value, separated by treatment.\n\n\n\nThese plots show box and whisker plots to display the distribution (across the simulations) of the final values for the QOIs. The drop-down menu in the Controls panel allows the user to change which QOI is displayed.\n\n\nCumulative Operating Characteristics Plot\nThere are two graphs, one that shows the cumulative proportion of simulations that have a duration less than the x-axis provided value across all simulations, and another that shows the cumulative proportion of simulations that had fewer than the x-axis provided number of subjects. A radio button allows for selection between the two graphs.\n\nCumulative Proportion of DurationCumulative Proportion of Subjects\n\n\n\n\n\n\n\n\nFigure 35: The proportion of trials that have a duration lower than the x-axis value.\n\n\n\n\n\n\n\n\n\n\n\nFigure 36: The proportion of trials that have a total sample size lower than the x-axis value.\n\n\n\n\n\n\n\n\n\nPer Sim Graphs\nThe Per Sim graphs allow the user to select a particular Simulation to examine in detail. The simulation number is selected in the Controls box.\n\n\n\n\n\n\nFigure 37: The control for selecting which simulation to display in the Per Sim graph.\n\n\n\n\nPer Sim: Arm and Participant Arrivals\n\n\n\n\n\n\nFigure 38: A plot summarizing the platform trial. It shows arm entry times, exit times, decisions made, and the classification of the arm’s true effect.\n\n\n\nThis graph depicts an overview of a single simulated trial, providing timing information (in weeks). The graph contains several components:\n\nA dashed line from the time that the arm became available for entry into the trial until the time time that it either entered the trial (began enrolling) or stopped waiting to enter the trial and withdrew. Note: if the arm began enrolling as soon as it became available, the dashed line will not appear.\nA thick solid line from the beginning to end of the enrollment period.\nA dot-dashed line from the end of enrollment to the time of final analysis. If there is no follow-up after an early stopping decision is made, then this line will not appear.\nA symbol at the end of each treatment arm’s line indicating the final status of the treatment arm.\nA symbol above the final status symbol indicating the treatment effect classification.\nOptionally, checking the “Show Participant Arrivals” checkbox in the Controls box will add additional lines to the plot: number of participants accrued versus time.\n\n\n\nPer Sim: Response and Participant Alloc.\n\n\n\n\n\n\nFigure 39: For a single simulated trial, this graph provides the number of subjects accrued on each arm as well as the estimated response rate and credible intervals.\n\n\n\nThis graph mimics the other Response and Participant Allocation graph but is for a single simulation, as selected in the Controls box. In this graph, the ochre dot-and-whiskers represent a 95% credible interval for the response, based on the Bayesian model fit, while the green dot-and-whiskers represents a frequentist 95% confidence interval for the response.\n\n\nPer Sim: Posterior Quantities\n\n\n\n\n\n\nFigure 40: For a single simulated trial, this graph provides the value of a selected QOI for each arm as well as the estimated response rate and credible intervals.\n\n\n\nThis graph mimics the Per Sim Response and Participant graph but replaces the enrollment bar chart with a bar chart for a QOI value at the trial’s final analysis. The QOI Value to display is chosen via a drop-down menu in the Controls box.\n\n\n\nPer Update Graphs\nThese graphs mimic previous graphs but allow the user to look at quantities as of a specific trial update within a simulation. The user can select the simulation and update in the Controls box. The updates are labeled with both the number of the trial update and the time (week) in which the update took place. Note: final analyses for arms at the end of follow-up are considered updates for the purpose of this numbering, though they may have no adaptive decisions associated.\nThe per update graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\n\n\n\n\n\n\nFigure 41: The controls for selecting which simulation and update to display in the Per Update graph.\n\n\n\n\nPer Update: Response and Participant Alloc.\nThis graph mimics the Per Update: Response and Participant Alloc. graph, but showing the outcomes at a single update, rather than the final analysis.\n\n\nPer Update: Posterior Quantities\nThis graph mimics the Per Update: Posterior Quantities graph, but showing the outcomes at a single update, rather than the final analysis.\n\n\nPer Update: Allocation Probability\nThe only new graph in the per update set is the allocation probability graph. This graph is intended primarily to examine the behavior of adaptive allocation but can also be used to see which arms are enrolling and how allocation changes with the number of available arms even in the non-adaptive allocation case. This graph is identical to the Per Update: Response and Participant Allocation graph, except that in place of a bar chart for past enrollment, it gives the current probability of allocation for the arms.\n\n\n\n\n\n\nFigure 42: This graph shows the estimated response and credible interval for each arm, as well as the allocation rate resulting from these estimated values. If using RAR, this provides the RAR probabilities, otherwise it is fixed based on the number of accruing arms.\n\n\n\n\n\n\nExplore graphs\nTwo graphs, one for futility and one for success, are available to help calibrate a design. For a particular treatment arm and a particular QOI, the line graph displays the proportion of simulated trials for which the QOI exceeds (or falls below) a threshold, indicating a potential way to set up early stopping rules to achieve desired operating characteristics.\nThese graphs require the use of weeks files to get detailed information updates, so it may be critical to increase the number of weeks files saved (as set on the Simulation tab, defaulting to 100) to provide sufficient information in these graphs.\nSince the graphs rely on the existing simulations, then any early stopping that is applied will limit the data available for later interims, and make the data conditional on not stopping early, which may make interpretation difficult. Thus, it may better to run simulations with no early stopping when utilizing these graphs. However, unlike the Core engine where removing early stopping lead to identical results up to the stopping point of interest, removing early stopping from all arms may affect the trajectory of the platform trial, leading to fewer arms enrolling, a difference in proportion of controls, and other changes in behavior of the platform trial beyond the early stopping of arms. The graphs may provide rough estimates of where thresholds should be set, but ultimately, these will need to be calibrated in the context of early stopping for all arms.\n\n\n\n\n\n\n\n\n\n\n\n(a) The Cumulative Proportion of Simulations Satisfying the Futility Criteria for a specific treatment.\n\n\n\n\n\n\n\n\n\n\n\n(b) The Cumulative Proportion of Simulations Satisfying the Success Criteria for a specific treatment.\n\n\n\n\n\n\n\nFigure 43\n\n\n\n\n\n\nAcross Scenario Graphs\n\nSelect Scenarios and Variants to Display\nThere is only 1 Across Scenario Graph available in the platform trials engine currently. The column facets separate the simulation scenarios, and the row facets separate the design variants.\nThe Across Scenario Graphs have a control labelled, “Select Scenarios/Variants” that allows for the filtering to only certain scenarios or certain variants. This is especially helpful if there are many simulation scenarios.\nThe control simply presents a list of the scenarios and a list of the variants, with a checkbox alongside each one allowing it to be de-selected:\n\n\n\n\n\n\nFigure 44: The Select Scenarios/Variants pop-up allowing for filtering of results shown in the Across Scenarios graphs.\n\n\n\n\nQOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\n\n\n\n\n\nFigure 45: For each scenario and each design variant, this graph provides the distribution of the selected QOI for each arm in the trial."
  },
  {
    "objectID": "documentation/v72/userguides/platform.html#output-files",
    "href": "documentation/v72/userguides/platform.html#output-files",
    "title": "Platform Trials",
    "section": "Output Files",
    "text": "Output Files\nFACTS stores the results of simulations as ‘.csv’ files in a results folder. For each row in the simulations table, there is a folder named by the profiles that make up the scenario, which contains the corresponding ‘.csv’ files.\n\n\n\n\n\n\nFigure 46: Output files in the Windows file viewer.\n\n\n\nThese files can be opened using Microsoft Excel, but versions of Excel before 2007 are restricted to 256 columns, which is too few to view some files in their entirety. The ‘Calc’ application in ‘OpenOffice’ will show all the columns (and will open two files that have the same name at the same time!). Because Excel takes out a file lock on any file it has open, while a file is open in Excel it cannot be deleted or modified by another application. The most common cause for an error to be reported when simulating trials in FACTS is because the user has one of the previous results files is still open in Excel.\nIn the scenario directory there are the following types of results file:\nSummary.csv Contains a single row of data that summarizes the simulation results. This is the source of the shown on the simulations tab.\nSimulations.csv Contains one row per simulation describing the final state of each simulation for every trial simulated.\nPatientsNNNNN.csv Contains one row per patient in a simulation, where NNNNN is the number of the simulation. By default this file is written out only for the first simulation, but this can be changed on the simulations tab.\nWeeksNNNNN.csv Contains one row for each trial update during a simulation where NNNNN is the number of the simulation. By default this file is written only for the first 100 simulations, but this can be changed via the simulation tab. The values in the last row of the cohorts file will be the same as the final values for that simulation in the simulations.\n\nContents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the column headings.\n\nContents of the summary.csv file.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNsim\n1\nThe number of simulations contributing to the summary file.\n\n\nMean Treatments Started\n1\nThe average number (over the simulations) of non-control treatments that entered the trial and were eligible to enroll participants.\n\n\nMean Treatments Analyzed\n1\nThe average number (over the simulations) of non-control treatments that reached a final analysis within the trial – either because they reached an early stopping decision with no follow-up or because they completed follow-up on all participants.\n\n\nMean Good Treatments Analyzed\n1\nThe average number (over the simulations) of “good” treatment arms that reached a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatments Analyzed\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that reached a final analysis, “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatments Analyzed\n1\nThe average number (over the simulations) of “mediocre” treatment arms that reached a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Successes\n1\nThe average number (over the simulations) of treatment arms that were declared successful at a final analysis.\n\n\nMean Good Treatment Successes\n1\nThe average number (over the simulations) of “good” treatment arms that were declared successful at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatment Successes\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared successful at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre Treatment Successes\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared successful at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Futilities\n1\nThe average number (over the simulations) of treatment arms that were declared futile at a final analysis.\n\n\nMean Good Treatment Futilities\n1\nThe average number (over the simulations) of “good” treatment arms that were declared futile at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatment Futilities\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared futile at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre Treatment Futilities\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared futile at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean In conclusives\n1\nThe average number (over the simulations) of treatment arms that were declared inconclusive at a final analysis.\n\n\nMean Good Treatment In conclusives\n1\nThe average number (over the simulations) of “good” treatment arms that were declared inconclusive at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatment In conclusives\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared inconclusive at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre Treatment In conclusives\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared inconclusive at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Successes | Treatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Futilities | Treatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn In conclusives | Treatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Successes | Treatment U nacceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Futilities | Treatment U nacceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn In conclusives | Treatment U nacceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Successes | Treatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Futilities | Treatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn In conclusives | Treatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatments | I nconclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatments | I nconclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatments | I nconclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn 1+ Successes\n1\nThe proportion of simulations that had at least one treatment arm declared successful.\n\n\nPpn Good Treatments | 1+ Successes\n1\nThe proportion of simulations that had at least one “good” treatment arm declared successful, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Success &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared successful.\n\n\nPpn Futility &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared futile.\n\n\nPpn In conclusives &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared inconclusive.\n\n\nPpn Good Treatment | Started &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatment | Started &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatment | Started &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatment | Analyzed &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatment | Analyzed &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatment Analyzed &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Duration\n1\nThe average (in weeks) over the simulations of the duration of the trial from the start to completion of the trial.\n\n\nMean First Success Time\n1\nAmongst simulations that had at least one success, the average time (in weeks) at which the first success was declared.\n\n\nMean Available Time &lt;Arm&gt;\n1 per arm\nThe average time (in weeks) across all simulations at which the treatment because available for entry into the trial. Note: the available time is reported whether or not it became available after the end of the trial.\n\n\nMean Start Time &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became open to enroll participants, the average time (in weeks) that the arm began enrolling.\n\n\nMean End Time &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became open to enroll participants, the average time (in weeks) that the arm stopped enrolling.\n\n\nMean Final Analysis Time &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis, the average time (in weeks) that the arm’s final analysis occurred.\n\n\nMean No. P articipants\n1\nThe average number of participants enrolled across all simulations.\n\n\nSE P articipants\n1\nThe standard error of the number of participants enrolled across all simulations.\n\n\nMean P articipants\n1\nThe average number (over the simulations) of participants enrolled in the trial.\n\n\nSE Mean P articipants\n1\nThe standard error (over the simulations) of the number of participants enrolled in the trial.\n\n\nMean Alloc &lt;Arm&gt;\n1 per arm\nThe average (over the simulations) of the number of participants enrolled onto the arm.\n\n\nSE Alloc &lt;Arm&gt;\n1 per arm\nThe standard error (over the simulations) of the number of participants enrolled onto the arm.\n\n\nMean resp &lt;Arm&gt;\n1 per arm\nAverage (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nSE resp &lt;Arm&gt;\n1 per arm\nStandard error (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nMean Sigma\n1\nAverage (over the simulations) of the estimate of the SD of the dose response across all the treatment arms (‘sigma’)\n\n\nSE Mean Sigma\n1\nStandard error (over the simulations) of the estimate of the SD of the dose response across all the treatment arms.\n\n\nTrue Mean resp &lt;Arm&gt;\n1 per arm\nTrue mean response from which the simulated participant data was sampled for the arm.\n\n\nTrue SD resp &lt;Arm&gt;\n1 per arm\nTrue standard deviation of the simulated participant data for the arm.\n\n\nN o.Dropouts: &lt;Arm&gt;, &lt;Visit&gt;\n1 per arm per visit\nAverage (across the simulations) of the number of dropouts for the arm by visit.\n\n\nMean Baseline Beta\n\nThis is the mean (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model. [Continuous endpoint only]\n\n\nSD Baseline Beta\n\nThis is the SD (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model. [Continuous endpoint only]\n\n\nQOIs (named according to QOI naming convention, as described in the QOIMappi ngFile.csv)\n1 per arm per QOI\nFor each Posterior Probability, Predictive Probability, p-value, or Target QOI, this is the mean over the simulations of the estimate of the probability of the QOI for each dose.\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition. The probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial).\n\n\n\n\n\nContents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the last analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each update in the trial and a row for each final analysis that occurs off-cycle from regular updates.\nThe first line is a header line, starting with a ‘#’, and containing\n\nThe FACTS GUI version number\nThe name of the FACTS file\nThe name of the scenario\nThe time stamp of the start of the simulation\n\nThe second and third lines are header lines. Most header names are identical in the second and third lines, but differ for some quantities, particularly QOIs. The alternate names for QOIs are summarized in the QOIMappingFile.csv that is produced in the main results directory.\nMost of the columns are common to the simulations and weeks file types, except as noted below.\n\nContents of the simulations/weeks files.\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nUpdate Number\n1\n\n✔\nThe index of the interim analysis being performed.\n\n\nStage\n1\n✔\n✔\n0 = Accruing (standard trial update)1 = Final (final analysis for a particular arm)2 = Accrued (standard trial update after full enrollment)4 = Paused (accrual is paused – currently not allowed)\n\n\n# Participants\n1\n✔\n✔\nNumber of participants enrolled in the trial\n\n\nAlloc &lt;Arm&gt;\n1 per arm\n\n\nNumber of participants enrolled on the arm\n\n\nStatus &lt;Arm&gt;\n1 per arm\n✔\n✔\nStatus of the treatment arm:-99 = Turned Away (could not enter trial)-98 = Waiting In Treatment Queue-97 = Never Arrived, (trial ended before arm arrived)-1 = Not Started (weeks file only)0 = Enrolling1 = In Followup (done enrolling)2 = Paused (not currently available)99 = Complete\n\n\nAvailable Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) at which the treatment becomes available for entry into the trial. Note: the available time is always reported, even before the trial has reached that point.\n\n\nStart Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) that the arm became eligible to enroll participants. If not reached, -9999 is used.\n\n\nEnd Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) that the arm stopped enrolling participants. If not reached, -9999 is used.\n\n\nFinal Analysis Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) that the arm’s final analysis occurred. If not reached, -9999 is used.\n\n\nMilestone &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe milestone being evaluated for the treatment arm at the given trial update. -1 indicates no milestone is being evaluated. 9999 indicates a final analysis.\n\n\nOutcome &lt;Arm&gt;\n1 per arm\n✔\n✔\nA flag categorizing final study outcome:-9999 = Not applicable (for control arm)-1 = Not available (arm enrolling but no outcome yet)0 = Not started (arm still hasn’t entered the trial)1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive8 = Success to inconclusive flip-flop9 = Futility to inconclusive flip-flop10 = Incomplete (trial ended without reaching final analysis)\n\n\nDesired Outcome &lt;Arm&gt;\n1 per arm\n✔\n✔\nTreatment effect classification for the treatment arm for this simulation.0 = “Unacceptable”1 = “Mediocre”2 = “Good”\n\n\n#Enrolled Treatments\n1\n✔\n✔\nThe number of arms that have been open to enrollment in the trial.\n\n\n#Analyzed Treatments\n1\n✔\n✔\nThe number of arms that have reached a final analysis in the trial.\n\n\nFirst Success Time\n1\n✔\n✔\nIf any treatment has been declared successful, the time of the first successful final analysis. Otherwise, -9999.\n\n\n#Successes\n1\n✔\n✔\nThe number of arms that have been declared successful.\n\n\n#Futilities\n1\n✔\n✔\nThe number of arms that have been declared futile.\n\n\n#Inconclusive\n1\n✔\n✔\nThe number of arms that have been declared inconclusive.\n\n\n#GoodTrt\n1\n✔\n✔\nThe number of arms with treatment effects considered “good.”\n\n\n#GoodSucc\n1\n✔\n✔\nThe number of “good” arms that have been declared successful.\n\n\n#GoodFut\n1\n✔\n✔\nThe number of “good” arms that have been declared futile.\n\n\n#GoodInconc\n1\n✔\n✔\nThe number of “good” arms that have been declared inconclusive.\n\n\n#UnaccTrt\n1\n✔\n✔\nThe number of arms with treatment effects considered “unacceptable.”\n\n\n#UnaccSucc\n1\n✔\n✔\nThe number of “unacceptable” arms that have been declared successful.\n\n\n#UnaccFut\n1\n✔\n✔\nThe number of “unacceptable” arms that have been declared futile.\n\n\n#UnaccInconc\n1\n✔\n✔\nThe number of “unacceptable” arms that have been declared inconclusive.\n\n\n#MedTrt\n1\n✔\n✔\nThe number of arms with treatment effects considered “mediocre.”\n\n\n#MedSucc\n1\n✔\n✔\nThe number of “mediocre” arms that have been declared successful.\n\n\n#MedFut\n1\n✔\n✔\nThe number of “mediocre” arms that have been declared futile.\n\n\n#MedInconc\n1\n✔\n✔\nThe number of “mediocre” arms that have been declared inconclusive.\n\n\nPr(Alloc) &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe probability of allocation to the different arms following the update.\n\n\nMean resp &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe estimated response (or response rate for Dichotomous endpoints) of each treatment arm.\n\n\nSD resp &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nMean resp (lower CI) &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe lower bound of the 95% credible interval for response rate. (Dichotomous only)\n\n\nMean resp (upper CI)\n1 per arm\n✔\n✔\nThe upper bound of the 95% credible interval for response rate. (Dichotomous only)\n\n\nTrue Mean &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe true response for the arm for this simulation.\n\n\nTrue SD resp\n1 per arm\n✔\n✔\nThe true standard deviation of the observed response for the arm. (Continuous only)\n\n\nSigma\n1\n✔\n✔\nThe (posterior mean) estimate of sigma, the pooled standard deviation for observations. (Continuous only)\n\n\nSD_Sigma\n1\n✔\n✔\nThe posterior standard deviation of the sigma parameter. (Continuous only)\n\n\nBeta\n1\n✔\n✔\nThe estimate of ‘Beta’ the baseline adjustment coefficient, if baseline adjustment is being used. (Continuous only)\n\n\nSD_Beta\n1\n✔\n✔\nThe SD of the estimate of ‘Beta’ the baseline adjustment coefficient. (Continuous only)\n\n\nMean Baseline\n1\n✔\n✔\nThe estimate of the mean baseline score. (Continuous only)\n\n\nSE Mean Baseline\n1\n✔\n✔\nThe standard error of the estimate of the mean baseline score. (Continuous only)\n\n\nSD Baseline\n1\n✔\n✔\nThe estimate of the SD of the baseline score. (Continuous only)\n\n\nTrue Mean Baseline\n1\n✔\n✔\nThe true mean of the simulated baseline score. (Continuous only)\n\n\nTrue SD Baseline\n1\n✔\n✔\nThe true SD of the simulated baseline score. (Continuous only)\n\n\nMean Raw Response &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe observed mean response on each treatment arm (unadjusted by any modeling).\n\n\nSE Mean Raw Response &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe standard error of the estimate of the mean raw response on each dose.\n\n\nComplete &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis.\n\n\nComplete Information &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe number of subject who count as complete for the purposes of update timing, as defined on the Design &gt; Trial Updates tab: enrolled, complete, or opportunity to complete.\n\n\n#Dropouts &lt;Arm&gt; &lt;Visit&gt;\n1 per arm per visit\n✔\n✔\nThe number of dropouts on the arm, broken out into the visit at which they dropped. (The current implementation has only a single visit.)\n\n\nQOI Columns\n1 per arm per QOI*\n✔\n✔\nAll QOI values are reported for each arm. The row 1 and row 2 names for the columns are given in the QOIMappingFile.csv that is output in the top-level “_results” folder. There is an additional column for each Target QOI that specifies the arm index of the arm with the highest posterior probability of being the target arm.\n\n\n\n\n\nContents of PatientsNNNNN.csv\n\nContents of the patients files.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Participant\n1\nThe participant id number, starting at 1.\n\n\nArm\n1\nThe index of the arm on which the subject was enrolled.\n\n\nRegion\n1\nThe index of the region the subject was recruited in, based on the regions defined on the Accrual tab.\n\n\nDateInWeeks\n1\nThe time (in weeks) from the start of the trial, of the subject enrollment (and if relevant, baseline visit).\n\n\nLastVisit#\n1\nThe index of the last visit for which the participant’s data was collected. (The current implementation only allows for a single visit.)\n\n\nDropout\n1\n1 = dropout, 0 = no dropout.\n\n\nBaseline\n1\nParticipant baseline if simulated.\n\n\nVisit &lt;visit&gt;\nV\nParticipant response at each visit. (Currently only one visit allowed.)\n\n\n\n\n\nContents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContinuousDichotomous\n\n\n\nContents of the MCMC output files for Continuous Platform Trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis in the simulation. This corresponds to the rows in the weeksNNNNN.csv file.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nTheta &lt;Arm&gt;\n1 per arm\nThe estimate of the mean response for each arm.\n\n\nSigma\n1\nThe estimate of the standard deviation of the endpoint.\n\n\n\n\n\n\nContents of the MCMC output files for Continuous Platform Trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis in the simulation. This corresponds to the rows in the weeksNNNNN.csv file.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Arm&gt;\n1 per arm\nThe estimate of the mean response rate for each arm.\n\n\n\n\n\n\n\n\nMapping Files\nWhen any simulations are run, two files are generated, which are placed in the top-level “_results” folder. These files apply to any results produced for individual scenarios and are intended to help with tracking the QOIs in use for the simulation.\n\nQOIMappingFile.csv\nThis file provides a list of all QOIs that are computed for the simulations, along with details about the QOI definition. This file utilizes the same structure as for the Core engine, though it is largely redundant for the Platform engine, since there is a row for each treatment for each individual QOI.\n\nContents of the QOIMappingFile\n\n\n\n\n\n\nColumn Title\nDescription\n\n\n\n\nFACTS filename\nThe .facts file from which the QOI mapping was constructed.\n\n\nQOI Category Index\n1 = Posterior Probability, 2 = Predictive Probability, 3 = p-value, 4 = Target Probability\n\n\nQOI Category (text)\nText version of the QOI category index.\n\n\nQOI Alternative Name\nThe base text name for the QOI, which will be written in the second row of header columns for the simulations.csv and weeksNNNNN.csv files.\n\n\nCondition\nFor posterior probabilities, the direction of the comparison of the inference parameter.\n\n\nResponse Relation\nFor posterior probabilities, whether the parameter is being compared relative to another arm’s parameter or to an absolute reference value.\n\n\nRelative To (Treatment Index)\nFor posterior probabilities, index of the treatment arm being compared to (if any).\n\n\nRelative To (Treatment Name)\nFor posterior probabilities, text name of the treatment arm being compared to (if any).\n\n\nDelta\nFor posterior probabilities, the absolute reference value being compared to, or for relative comparisons, the additional delta value to add to the relative treatment arm parameter.\n\n\nPhase\nFor predictive probabilities, trial phase being predicted. (Currently only future trial is available.)\n\n\nTest Type\nFor predictive probabilities or p-values, the type of frequentist test being computed/predicted.\n\n\nSample Size\n(Currently unused for platform trials.)\n\n\nAlpha\nFor predictive probabilities or p-values, one-sided alpha level for the frequentist test.\n\n\nSubjects Per Arm\nFor predictive probabilities of future trials, the number of participants enrolled on each arm in the trial being predicted.\n\n\nMargin\nFor predictive probabilities, the margin of superiority being tested.\n\n\nTarget Type\nFor target probabilities, the type of target – currently only the Max is available.\n\n\nTarget Dose\nCurrently unused.\n\n\nTarget Dose Treatment Index\nCurrently unused.\n\n\nTarget Dose QOI Alternative Name\nCurrently unused.\n\n\nQOI Treatment Index\nIndex of treatment arm QOI is being computed for.\n\n\nQOI Treatment Name\nText name of treatment arm QOI is being computed for.\n\n\nQOI Variable\nFull label name of the QOI, which is used in the first header row of the simulations.csv and weeksNNNNN.csv files.\n\n\nQOI Label\nFull alternate label name of the QOI, which is used in the second header row of the simulations.csv and weeksNNNNN.csv files.\n\n\nQOI Category Subindex\nIndexed value of QOI within the QOI category\n\n\n\n\n\nDecisionMappingFile.csv\nThis file provides a comprehensive list of all QOIs that are used in futility and success decisions throughout the trial.\n\nContents of the DecisionMappingFile.\n\n\n\n\n\n\nColumn Title\nDescription\n\n\n\n\nFACTS filename\nThe .facts file from which the QOI mapping was constructed.\n\n\nDecision Category Index\nIndex value of the type of decision for which the QOI is being used. 1 = Futility, 2 = Success.\n\n\nDecision Category (text)\nText version of the Decision Category Index\n\n\nQOI Category Index\nIndex of the QOI category for the QOI being used in the decision\n1 = Posterior Probability, 2 = Predictive Probability, 3 = p-value, 4 = Target Probability\n\n\nQOI Category (text)\nText version of the QOI category index.\n\n\nQOI Alternate Name\nText name of the QOI, as defined by QOI Alternate Name in QOIMappingFile.csv.\n\n\nCondition\nThe direction of comparison (&gt; or &lt;).\n\n\nThreshold\nFor threshold the QOI is being comparing to.\n\n\nDecision Variable\nFor posterior probabilities, the absolute reference value being compared to, or for relative comparisons, the additional delta value to add to the relative treatment arm parameter.\n\n\nDecision Label\nA full text description of the decision and the comparison being computed.\n\n\nStart At Milestone Index\nThe first milestone at which this decision is evaluated.\n\n\nEnd Before Milestone Index\nThe first milestone at which this decision is no longer evaluated.\n\n\nTreatment\nThe treatment arm to which this decision applies, either given by the treatment name or “All Treatments” if it applies to all arms."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html",
    "title": "Ordinal Endpoint QOIs",
    "section": "",
    "text": "The FACTS Ordinal Engine features several special quantities of interest."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#cumulative-logistic-modeling",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#cumulative-logistic-modeling",
    "title": "Ordinal Endpoint QOIs",
    "section": "Cumulative logistic modeling",
    "text": "Cumulative logistic modeling\nWhen the cumulative logistic family is in use, each arm other than control has an odds ratio parameter relative to control. Posterior probability QOIs are defined based on these odds ratios. By default, the following posterior probability QOIs are available, (here \\(OR_d\\) is \\(\\exp(\\theta_d)\\) where the \\(\\theta_d\\)’s are the parameters defined in the dose-response model):\n\nThe probability of being better than control, which is Pr(\\(OR_d &gt; 1\\)) when large values of the ordinal index are good, or Pr(\\(OR_d &lt; 1\\)) when small values are good.\nThe probability of being better than control by a clinically significant difference, which is Pr(\\(OR_d - 1 &gt; CSD\\)) when large values are good, or Pr(\\(OR_d - 1 &lt; -CSD\\)) when small values are good. The CSD is entered by the user in the Standard Evaluation Variables section of the Quantities of Interest tab."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#dirichlet-modeling",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#dirichlet-modeling",
    "title": "Ordinal Endpoint QOIs",
    "section": "Dirichlet modeling",
    "text": "Dirichlet modeling\nWhen the user has selected independent Dirichlet modeling for the ordinal endpoint, the posterior probability QOIs are based on expected utility. The default posterior probability QOIs are:\n\nThe probability of being better than control with respect to expected utility, which is Pr(\\(EU_d &gt; EU_{Control}\\)). Here \\(EU_d = \\sum_{k=1}^K p^d_k U_k\\), where \\(p^d_k\\) is the probability of ordinal outcome \\(k\\) for arm \\(d\\) and where \\(U_k\\) is the utility for outcome \\(k\\).\nThe probability of being better than control by a clinical significant difference, which is Pr(\\(EU_d &gt; EU_{Control} + CSD\\)) regardless of whether large or small values of the ordinal endpoint are good (large values of utility are always good). The CSD is entered by the user in the Standard Evaluation Variables section of the Quantities of Interest tab."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#utility-t-test",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#utility-t-test",
    "title": "Ordinal Endpoint QOIs",
    "section": "Utility \\(t\\)-test",
    "text": "Utility \\(t\\)-test\nOne option is to perform a \\(t\\)-test on the utility values observed, comparing each experimental arm to control, in the same way as with a continuous endpoint."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#wilcoxon-mann-whitney-test",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#wilcoxon-mann-whitney-test",
    "title": "Ordinal Endpoint QOIs",
    "section": "Wilcoxon-Mann-Whitney test",
    "text": "Wilcoxon-Mann-Whitney test\nFACTS will also perform a Wilcoxon-Mann-Whitney (rank sum) test comparing each experimental arm to control. FACTS adjusts for ties in computing the \\(p\\)-value."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#proportional-odds-likelihood-ratio-test",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#proportional-odds-likelihood-ratio-test",
    "title": "Ordinal Endpoint QOIs",
    "section": "Proportional odds likelihood ratio test",
    "text": "Proportional odds likelihood ratio test\nFACTS will also perform a likelihood ratio test based on the proportional odds model, comparing each experimental arm to control. It fits a multinomial model to both arms together, and also fits a model where the experimental arm has a single proportional odds model deviation from control, and compares twice the logged ratio of maximized likelihoods to the chi-squared distribution with one degree of freedom. In practice, this test will often give similar results to the Wilcoxon test. (See, for example, Harrell.)"
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#dichotomized-ordinal-test",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#dichotomized-ordinal-test",
    "title": "Ordinal Endpoint QOIs",
    "section": "Dichotomized ordinal test",
    "text": "Dichotomized ordinal test\nFACTS will also test whether the probability of a “good” outcome is larger for an experimental arm than it is for control. The user defines a “good” outcome using the “Definition of Success” selector in the “Standard Evaluation Variables” section. The test is the same as for a dichotomous endpoint.\nDichotomizing an ordinal outcome is rarely a good choice because much of the information in the ordinal outcome is wasted. See, for example, Podcast by Scott Berry. FACTS simulations can be useful in demonstrating the reduction in power when an ordinal endpoint is dichotomized."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#current-trial-bayesian-predictive-probabilities",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#current-trial-bayesian-predictive-probabilities",
    "title": "Ordinal Endpoint QOIs",
    "section": "Current Trial Bayesian Predictive Probabilities",
    "text": "Current Trial Bayesian Predictive Probabilities\nFor general discussion of current trial Bayesian predictive probabilities, see the core QOIs page. In the present version of the FACTS Ordinal Engine, current trial predictive probabilities are only available for the full sample size intended at the end of the trial."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#future-trial-bayesian-predictive-probabilities",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#future-trial-bayesian-predictive-probabilities",
    "title": "Ordinal Endpoint QOIs",
    "section": "Future Trial Bayesian Predictive Probabilities",
    "text": "Future Trial Bayesian Predictive Probabilities\nFor general discussion of future trial Bayesian predictive probabilities, see the core QOIs page."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#conditional-power",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#conditional-power",
    "title": "Ordinal Endpoint QOIs",
    "section": "Conditional Power",
    "text": "Conditional Power\nConditional power in trials with an ordinal endpoint will be added in a future release of FACTS Ordinal."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#definition-of-success",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#definition-of-success",
    "title": "Ordinal Endpoint QOIs",
    "section": "Definition of Success",
    "text": "Definition of Success\nIn some circumstances, one may wish to dichotomize an ordinal endpoint, and define an ordinal value such that achieving that value or better counts as a success, while reaching a worse value is a failure. This value can be chosen here, and it will be used in p-values comparing treatments to control with respect to the probability of a success. “Better” than a specified value here is taken to mean a more favorable ordinal index (a higher index if high values are good, or a lower index if low values are good), regardless of which utility values have been entered."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#clinically-significant-difference",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#clinically-significant-difference",
    "title": "Ordinal Endpoint QOIs",
    "section": "Clinically significant difference",
    "text": "Clinically significant difference\nHere the user may define a clinically significant difference (CSD) to use in posterior probabilities of being better than control by a CSD.\n\nIf the user has selected independent Dirichlet modeling, the CSD is on the scale of utility. For example if the CSD is 0.5, FACTS by default computes the probability that each treatment has an expected utility that is more than 0.5 points higher than the expected utility for the control arm. Regardless of whether the high values of the ordinal index are good, large utilities are good in FACTS Ordinal.\nIf the user has selected cumulative logistic modeling, the CSD is on the scale of an odds ratio. For example if the CSD is 0.3 and large values are good, the default posterior probability of an odds ratio of at least 1 + 0.3 = 1.3 is calculated. If the CSD is 0.2 and small values are good, the posterior probability of an odds ratio less than 1 - 0.2 = 0.8 is calculated."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/ordinalendpt.html#super-superiority-and-non-inferiority",
    "href": "documentation/v72/userguides/core/qois/ordinalendpt.html#super-superiority-and-non-inferiority",
    "title": "Ordinal Endpoint QOIs",
    "section": "Super-superiority and non-inferiority",
    "text": "Super-superiority and non-inferiority\nSuper-superiority and non-inferiority are complicated concepts in ordinal endpoints and they have different meanings, or no established meaning, depending on how the ordinal endpoint is analyzed. These will be enabled in a future version of FACTS Ordinal."
  },
  {
    "objectID": "documentation/v72/userguides/core/analysis.html",
    "href": "documentation/v72/userguides/core/analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "The analysis tab allows the user to supply a specific data set for analysis by the design specified in the Design tab of the “.facts” file.\nClicking on the “Use Design to Analyze Data” button, will create an empty “subject.csv” file in the main simulation results directory and an ‘Analysis’ sub-directory there for running the analysis and saving the outputs.\nAlternatively, clicking on the “Import Data to Analyze” launches a file browser, allowing the user to select a ‘.csv’ file to load as the data to analyze. This is a shortcut for first clicking on the “Use Design to Analyze Data” button, and then clicking on the “Select File to Create New Analysis” button on the subject data tab.\nAfter enabling data analysis, the analysis screen is shown with no data loaded. By clicking on the “Subject Data” tab the user is now able to enter data values directly, or to load a ‘.csv’ file already containing data:"
  },
  {
    "objectID": "documentation/v72/userguides/core/analysis.html#the-subject.csv-file-format",
    "href": "documentation/v72/userguides/core/analysis.html#the-subject.csv-file-format",
    "title": "Analysis",
    "section": "The subject.csv file format",
    "text": "The subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nThe format of the file is the same as the ‘patientsNNNN.csv’ output file (continuous/dichotomous, time-to-event), or multiple endpoint and the column values described above."
  },
  {
    "objectID": "documentation/v72/userguides/core/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "href": "documentation/v72/userguides/core/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "title": "Analysis",
    "section": "Converting arrival date value from days to weeks",
    "text": "Converting arrival date value from days to weeks\nFrom FACTS 7.0 the value in the Date field is interpreted as being in weeks (rather than days as in previous versions). If you have existing data, a simple conversion tool is provided “Convert Date from Days to Weeks” that simply divides all of thete values by 7. Having run the conversion, you then need to save the modified data. The values of the Date field will only make a difference to TTE analyses.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: The data as provided to the analysis tab with dates in days.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: The same dates, but converted to weeks."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html",
    "title": "Ordinal Output",
    "section": "",
    "text": "Ordinal-specific graphs in FACTS are under development. For your convenience, we have reproduced the continuous and dichotomous graphs materials here."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#per-scenario-graphs",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#per-scenario-graphs",
    "title": "Ordinal Output",
    "section": "Per Scenario Graphs",
    "text": "Per Scenario Graphs\n\nAllocation Box and Whisker Plot\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\n\n\nResponse and Subject Allocation\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 6: Response and Subject Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Response and Subject Allocation.\n\n\n\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black line shows the true mean response being simulated.\nThe green dashed line shows the mean of the estimated response across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated mean response across the simulations (if less than 20 simulations have been run it simply shows the full spread).\n\n\n\nResponse and Target Selection graphs\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 8: Response and QOI Selection.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Response and QOI Selection.\n\n\n\n\n\n\nThese plots show the true simulated mean response and the mean and 95% spread of the mean fitted mean responses along with bars showing the proportion of simulations the different treatment arms are the selected target arm for each of the “Probability of being target” QOIs, such as Pr(Max), Pr(EDq relative to ..: Quantile=q) and Pr(MED relative to…: Delta=\\(\\delta\\)).\n\n\nPer Dose QOIs (Box and Whisker Plots)\nThis set of graphs show the distribution over the simulations of the posterior probability estimates of quantities of interest for each dose. All the Posterior Probability, Predictive Probability, P-value and Probability of being Target QOIs that have been defined are available for plotting, via a drop down list.\nNote that in any one analysis the Probability of being Target will sum to 1 across the doses (and can sum to less than one in the case of the MED target if none of the doses look likely to be better than the CSD), whereas all the other QOIs are assessed individually for each dose.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(\\theta_d\\) - \\(\\theta_{Control}\\) &gt; 0.1) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.1) Across Simulations.\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(P_d\\) - \\(P_{Control}\\) &gt; 0.15) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.2) Across Simulations.\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\nTarget Response by Sample Size Scatter Plot\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 12: Posterior Mean Response at P(MED relative to Control: Delta}=0.1) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior Mean Response at P(MED relative to Control: Delta}=0.2) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\nThis graph shows a scatter plot of trial outcomes with the selected Target QOI as the y-axis and total number of subjects recruited as the x-axis. The trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as it is possible?\nWhen and with what mean response trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\n\n\nCumulative Operating Characteristics Plot\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\nTime course for stopping\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial. The user can select whether time or subject enrollment is used as the x-axis.\n\n\nArm Dropping Graphs\nIf the design has used arm dropping, the following graphs are available.\n\nResponse and Ppn Arms Dropped\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 18: Response and Proportion Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Response and Proportion Arm Dropped\n\n\n\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\nTime Course for Arm Dropping\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 20: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\nTime Course for Arm retention\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 22: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\nArm Retention Proportion\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 24: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\nFrequentist\nThese graphs are available if frequentist analysis is enabled.\n\nFrequentist P(significance)\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 26: Distribution of P(LOCF. Bonferroni Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Distribution of P(LOCF. Unadjusted Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\nFrequentist: Response and Significance\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from Unadjusted, Bonferroni or Dunnett’s and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 28: Response and Ppn LOCF. Bonferroni Significance\n\n\n\n\n\n\n\n\n\n\n\nFigure 29: Response and Ppn Fail. Dunnett Significance\n\n\n\n\n\n\n\n\n\nPer Sim: Response and Subject Alloc\nThis graph shows the final analysis at the end of individual simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 30: Response and Subject Allocation\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Response and Subject Allocation\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBlue bars showing the number of subjects allocated on each treatment arm.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 32: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\nControl to select the individual simulation results to be displayed\n\n\nPer Sim: Posterior Quantities\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 34: Response and Pr(MED relative to Control: Delta = 0.1) Probability\n\n\n\n\n\n\n\n\n\n\n\nFigure 35: Response and Pr(EDq relative to Control: Quantile = 0.9) Probability\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot.\n\n\nPer Interim Response Graphs\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\nNote the ‘Interim’ control in both of the below figures.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 36: Response and Pr(MED relative to Control: Delta = 0.1) Probability with side panel allowing for selection of simulation, interim, and quantity of interest.\n\n\n\n\n\n\n\n\n\n\n\nFigure 37: Response and Subject Allocation with side panel allowing for selection of simulation and interim number."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#post-simulation-boundary-finding-graphs",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#post-simulation-boundary-finding-graphs",
    "title": "Ordinal Output",
    "section": "Post Simulation Boundary Finding Graphs",
    "text": "Post Simulation Boundary Finding Graphs\n\nExplore Success/Futility Criteria\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 39\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: e.g. Pr(Max), and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis). The graphs show for all the decision QOIs defined for the selected Target Dose the proportion of simulations that would have met the threshold values.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\nExplore Early Success/Futility Criteria\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (unlike the examples above where the shape of the “existing stopping rules” line indicates that early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which decision QOI is evaluated and from which interim onwards stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\nExplore Arm Dropping Criteria\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 42: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\nFigure 43: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\nSuccess/Futility Stopping Contours\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 45\n\n\n\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the decision QOI to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are successful/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#mcmc-trace-plots",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#mcmc-trace-plots",
    "title": "Ordinal Output",
    "section": "MCMC Trace plots",
    "text": "MCMC Trace plots\n\nDichotomous\n\n\n\n\n\n\n\n\nFigure 46: Trace plot of an MCMC parameter.\n\n\n\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC.\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#across-scenario-graphs",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#across-scenario-graphs",
    "title": "Ordinal Output",
    "section": "Across Scenario Graphs",
    "text": "Across Scenario Graphs\n\nSelected Arms\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 47: Across scenario graph for selected arms with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 48: Across scenario graph for selected arms with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\nQOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 49: Across scenario graph for showing QOIs per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50: Across scenario graph for showing QOIs per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\nPpn Success\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 51: Across scenario graph for showing the proportion of trials that result in success with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 52: Across scenario graph for showing the proportion of trials that result in success with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\nResponse\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 53: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 54: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\nAllocation\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 55: Across scenario graph for showing the distribution of allocation per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 56: Across scenario graph for showing the distribution of allocation per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\nSample Size\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 57: Across scenario graph for showing the mean sample size of the study with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 58: Across scenario graph for showing the mean sample size of the study with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\nInterim vs Final Scatter Plot\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 59: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\n\n\nFigure 60: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\nReceiver Operating Characteristics\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 61: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.\n\n\n\n\n\n\n\n\n\n\n\nFigure 62: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#summary-per-scenario",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#summary-per-scenario",
    "title": "Ordinal Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after ordinal simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\nThis is the same name as used for the results directory\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn\nSuc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn\nFut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Incon- clusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn\nIncorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation columns provided in FACTS output for ordinal trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj. 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nPPn Arms Drop: &lt; Dose&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\n\n\n\nResponse\n\nOrdinal\n\n\nIn ordinal trials, some elements of the Response results depend on whether the ordinal endpoint has been modeled using the cumulative logistic framework or using independent Dirichlet models.\n\nResponse columns available in FACTS output for ordinal simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Resp: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of a quantity summarizing this arm.\nFor cumulative logistic modeling, this is the log odds ratio (zero for control).\nFor Dirichlet modeling, this is the expected utility.\n\n\nSD Resp: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this arm, either the log odds ratio or the expected utility as above.\n\n\nTrue Mean Resp: &lt;Dose&gt;\nOne per arm\nThis is the true response rate from which the simulation data was sampled.\nThis is the expected utility regardless of how the ordinal endpoint is being modeled. (The “true log odds ratio” is not necessarily a well-defined quantity.)\n\n\n\n\n\n\n\n\nObserved\n\nObserved columns available in FACTS output for ordinal simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the mean (over the simulations) of dropouts per arm per visit in this scenario.\n\n\n\n\n\nProbabilities\n\nProbabilities columns available in FACTS output for ordinal simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\nTiming\n\nTiming columns available in FACTS output for ordinal simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\n\n\n\nModel Parameters\n\nOrdinal\n\n\nNOTE: we do not yet have a model parameters for ordinal available.\n\nModel Parameters columns available in FACTS output for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory"
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#detailed-per-simulation-results",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#detailed-per-simulation-results",
    "title": "Ordinal Output",
    "section": "Detailed Per Simulation Results",
    "text": "Detailed Per Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulations output in the table by double-clicking on the appropriate row. A separate window pops up and displays the results for each individual simulation. This is a “prettier” view of the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 63: The simulation results pop out viewer showing simulation highlights."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-summary.csv",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-summary.csv",
    "title": "Ordinal Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;Dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;Dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nMean Sigma\n1\nThe mean of the estimate of sigma – the average standard deviation of the dose response\n\n\nSE Mean Sigma\n1\nThe standard error of the estimate of sigma – the average standard deviation of the dose response\n\n\nTrue Mean resp &lt;Dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nTrue SD resp &lt;Dose&gt;\nD\nThe true SD of the response for each treatment arm of the simulated subject responses\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nMean Beta\n1\nThe mean of the estimates of the coefficient of baseline adjustment\n\n\nSE beta\n1\nThe standard error of the estimates of the coefficient of baseline adjustment\n\n\nMean Baseline\n1\nThe mean of the estimate of the mean of the baseline score\n\n\nSE Mean Baseline\n1\nThe standard error of the estimate of the mean of the baseline score\n\n\nSD Baseline\n1\nThe mean of the estimate of the SD of the baseline score\n\n\nSE SD Baseline\n1\nThe standard error of the estimate of the SD of the baseline score\n\n\nTrue Mean Baseline\n1\nThe true mean of the baseline score (accounting for possible truncation)\n\n\nTrue SD Baseline\n1\nThe true SD of the baseline score (accounting for possible truncation)\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\nContents of the summary.csv file for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;Dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;Dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-summary_freq_missingnesstype.csv",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-summary_freq_missingnesstype.csv",
    "title": "Ordinal Output",
    "section": "Contents of Summary_freq_{missingnessType}.csv",
    "text": "Contents of Summary_freq_{missingnessType}.csv\nThere is a frequentist summary file for each type of treatment of missing values.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\nSE Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\nMean Beta (b aseline)\n1\nThe mean of the estimate of the baseline coefficient.\n\n\nSE Mean Beta (b aseline)\n1\nThe standard error of the estimate of the baseline coefficient\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.\n\n\n\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for dichotomous simulations.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\n\nSE Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of | simulations where the unadjusted p-value is | less than the user specified one-sided alpha. | | The marginal probability of significance per | dose (using unadjusted p-values).\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Ordinal Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nRandom Number Seed\n1\n✔\n✔\nBase random number seed.\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nMean resp &lt;Dose&gt;\nD\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation.\n\n\nTrue SD resp &lt;Dose&gt;\nD\n✔\n✔\nThe true SD of the response of each treatment arm for this simulation\n\n\nSigma\n1\n✔\n✔\nThe pooled estimate of the SD of the response across all the treatment arms\n\n\nSD_Sigma\n1\n✔\n✔\nThe SD of the pooled estimate of the SD of the response across all the treatment arms\n\n\nBeta\n1\n✔\n✔\nThe estimate of ‘Beta’ the baseline adjustment coefficient, if baseline adjustment is being used.\n\n\nSD_Beta\n1\n✔\n✔\nThe SD of the estimate of ‘Beta’ the baseline adjustment coefficient.\n\n\nMean Baseline\n1\n✔\n✔\nThe estimate of the mean baseline score.\n\n\nSE Mean Baseline\n1\n✔\n✔\nThe standard error of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\n✔\n✔\nThe estimate of the SD of the baseline score.\n\n\nTrue Mean Baseline\n1\n✔\n✔\nThe true mean of the simulated baseline score\n\n\nTrue SD Baseline\n1\n✔\n✔\nThe true SD of the simulated baseline score\n\n\nMean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe observed mean response on each treatment arm (unadjusted by any modeling).\n\n\nSE Mean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe SE of the estimate of the mean raw response on each dose.\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis.\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subject who count as complete for the purposes of timing interims – whether complete or opportunity to complete and at which visit is as defined on the Interims tab. If interims are by enrolment then CompleteInformation is the number of subjects that have completed – final endpoint data is available.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit.\n\n\nDR Param &lt;Param&gt;\n10\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 10 parameters, most less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the index “&lt;Param&gt;”.\n\n\nSd DR Param &lt;Param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod_ &lt;Model&gt; &lt;Parameter&gt; &lt;Visit&gt;\nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. For linear regression the parameters reported are: - Alpha – per visit – the mean estimate of the constant offset in the change in response from this visit to the final visit - Beta – per visit – the mean estimate of the coefficient of change in response from this visit to the final visit - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. For time course hierarchical the parameters reported are: - Alpha – per visit – the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit.. - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. - Tau – the mean estimate of the SD of the per subject random effect For ITP the parameters are: - K – per model – the mean estimate of the ITP shape parameter - Tau – per model - the mean estimate of the SD of the per subject random effect - Lambda – per model – the mean estimate of the Sd of the residual error. - Omega – per treatment arm – the mean estimate of the mean treatment arm effect. For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used. This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses.\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim\n\n\nMean resp &lt;Dose&gt;\nD\n✔\n✔\nThe estimated response of each treatment arm\n\n\nSD resp &lt;Dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm\n\n\nMean resp (lower CI) &lt;Dose&gt;\nD\n✔\n✔\nThe lower bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nMean resp (upper CI) &lt;Dose&gt;\nD\n✔\n✔\nThe upper bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nTrue Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation\n\n\nMean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe observed rate of response on each treatment arm (unadjusted by any modeling)\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects that have achieved the information criteria as specified on the Interims tab. May be the number enrolled, complete, or with the opportunity to complete. If complete or opportunity to complete, then the visit that should be complete is also specified.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit\n\n\nDR Param &lt;Param&gt;\n4\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the column their value appears in here\n\n\nSd DR Param &lt;Param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter\n\n\nLongmod_ &lt;Model&gt; &lt;Parameter&gt; &lt;Visit&gt;\nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. Each value is reported per visit unless otherwise stated.For Beta-Binomial the parameters reported are:• Alpha0 & Beta 0 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 0• Prob01 – the mean probability of the Beta(Alpha0, Beta0) – the probability of 1 being the final result if the result at the visit is 0• Alpha1 & Beta 1 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 1• Prob11 – the mean probability of the Beta(Alpha1, Beta1) – the probability of 1 being the final result if the result at the visit is 1For logistic regression the parameters reported are: • Prob11 – the probability of 1 being the final result if the result at the visit is 1and • Prob01 – the probability of 1 being the final result if the result at the visit is 0For restricted Markov the parameters reported are:• Alpha0 – Alpha for state 0• AlphaS – Alpha for stable state• Alpha1 – Alpha for state 1• Prob0 – Transition probability to state 0• ProbS – Probability of remaining stable• Prob1 – Transition probability to state 1(values are for the transition to the next visit, so thre are no values for the final visit)If using a dichotomized continuous response, these columns will be for the selected continuous longitudinal model:For linear regression the parameters reported are:• Alpha – the mean estimate of the constant offset in the change in response from this visit to the final visit• Beta – the mean estimate of the coefficient of change in response from this visit to the final visit• Lambda – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.For ITP the parameters are:• K – single value per model – the mean estimate of the ITP shape parameter• Tau – single value per model - the mean estimate of the SD of the per subject random effect• Lambda – single value per model – the mean estimate of the Sd of the residual error.• Omega – per treatment arm not visit – the mean estimate of the mean treatment arm effect.For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThese are only calculated and written out if the ITP longitudinal model is being used for a dichotomized continuous endpoint.This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns are grouped in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "title": "Ordinal Output",
    "section": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv",
    "text": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv\nThese file contain the frequentist results after an interim analysis (weeks file) or final analysis (simulations file), apart for the first column, the columns are the same in the two types of file, and the same across the different methods for treating missing data.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation (simulations_freq_…csv only)\n\n\n# Weeks\n1\nThe week of the interim (weeks_freq_…csv only)\n\n\nTheta\nD\nUnadjusted estimate of response per treatment arm\n\n\nTheta SD\nD\nThe SD of the estimate of response per treatment arm\n\n\nTrt Effect\nD\nThe relative size of the treatment effect compared to control per arm (treatment effect for control is always 0). The direction of the comparison takes into account whether a response represents a subject’s condition improving or worsening, so that a positive treatment effect shows the response on the dose is better than on control.\n\n\nt-Stat\nD\nThe t-test statistic per treatment arm (for control this is always 0)\n\n\np-Value\nD\nThe unadjusted t-test p-value per arm (for control this is always 0)\n\n\nlower CI\nD\nThe unadjusted lower bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nupper CI\nD\nThe unadjusted upper bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nMin p-Value\n1\nThe smallest of the t-test p-values\n\n\np-Value Dunnett\nD\nThe Dunnett adjusted t-test p-value per arm (for control this is always 0).\n\n\nlower CI Dunnett\nD\nThe Dunnett adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper CI Dunnett\nD\nThe Dunnett adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Dunnett\n1\nThe smallest of the Dunnett adjusted p-values.\n\n\np-Value Bonf\nD\nThe Bonferroni adjusted t-test p-value per arm.\n\n\nlower-CI Bonf\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper-CI Bonf\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Bonf\n1\nThe smallest of the Bonferroni adjusted t-test value per arm.\n\n\nt-Stat trend\n1\nThe t statistic from the trend test.\n\n\np-Value trend\n1\nThe p-value from the trend test."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-patientsnnnnn.csv",
    "title": "Ordinal Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nBaseline\n1\nSubject baseline response, if simulated. If not simulated, then fixed at -9999.\n\n\nVisit &lt;Visit&gt;\nV\nSubject’s response at each visit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nBaseline\n1\nAlways -9999 since there’s no baseline in dichotomous response trials.\n\n\nVisit &lt;Visit&gt;\nV\nSubject’s response at each visit."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/ordinal.html#contents-of-mcmcnnnnn.csv",
    "title": "Ordinal Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nTheta &lt;Dose&gt;\nD\nThe estimate of the mean response for each dose, based on the dose response model fitted.\n\n\nSigma\n1\nThe estimate of the SD of the endpoint.\n\n\nBeta\n1\nIf baseline is included in the simulated data and using the “Baseline Adjusted Model”, this is the value of “Beta”, the baseline coefficient.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\n\n\nIf using a dichotomized continuous variable, then the parameters for the LM are the same as for the continuous model MCMC output listed in the Continuous tab.\nIf using a dichotomous longitudinal model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\nIf using a restricted markov model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nCurrently the values of Alpha0, Alpha1, AlphaS (the parameters to the Dirichlet distribution) are (unnecessarily – these values are not sampled) output, along with the values for P0, P1 and PS the transition probabilities of each model at each visit."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described below.\nTo view the graphs, select a row of scenario results by clicking on it and then click on the ‘View Graph’ button.\nBelow we have often shown full screen shots of the graphs in the graph manager, but the graph display supports copying just the graph to the clipboard to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘png’ format to a file.\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select that row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nTo view multiple graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot. You can select the graph type, filter the design variants and filter which scenarios displayed:\n\n\n\n\n\n\nFigure 1: Pop up to select scenarios and variants to display.\n\n\n\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks (Not displayed if the ‘y’ value must lie in the interval 0-1.)\nSpace doses evenly – if the x-value is ‘dose’, then data for each dose can either be spaced equally or proportionate to the doses’ effective dose strength.\nIf using a 2D treatment arm model then it is possible to display graphs where the different doses (or “arms”) form the x-axis, then there is an option to show the row factors as separate series – otherwise the different combinations are displayed in effective strength order\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Arms shown in effective strength order\n\n\n\n\n\n\n\n\n\n\n\n(b) Arms shown with row factors as a separate series\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Arms shown in effective strength order\n\n\n\n\n\n\n\n\n\n\n\n(b) Arms shown with row factors as a separate series\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 6: Response and Subject Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Response and Subject Allocation.\n\n\n\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black line shows the true mean response being simulated.\nThe green dashed line shows the mean of the estimated response across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated mean response across the simulations (if less than 20 simulations have been run it simply shows the full spread).\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 8: Response and QOI Selection.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Response and QOI Selection.\n\n\n\n\n\n\nThese plots show the true simulated mean response and the mean and 95% spread of the mean fitted mean responses along with bars showing the proportion of simulations the different treatment arms are the selected target arm for each of the “Probability of being target” QOIs, such as Pr(Max), Pr(EDq relative to ..: Quantile=q) and Pr(MED relative to…: Delta=\\(\\delta\\)).\n\n\n\nThis set of graphs show the distribution over the simulations of the posterior probability estimates of quantities of interest for each dose. All the Posterior Probability, Predictive Probability, P-value and Probability of being Target QOIs that have been defined are available for plotting, via a drop down list.\nNote that in any one analysis the Probability of being Target will sum to 1 across the doses (and can sum to less than one in the case of the MED target if none of the doses look likely to be better than the CSD), whereas all the other QOIs are assessed individually for each dose.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(\\theta_d\\) - \\(\\theta_{Control}\\) &gt; 0.1) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.1) Across Simulations.\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(P_d\\) - \\(P_{Control}\\) &gt; 0.15) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.2) Across Simulations.\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 12: Posterior Mean Response at P(MED relative to Control: Delta}=0.1) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior Mean Response at P(MED relative to Control: Delta}=0.2) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\nThis graph shows a scatter plot of trial outcomes with the selected Target QOI as the y-axis and total number of subjects recruited as the x-axis. The trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as it is possible?\nWhen and with what mean response trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\n\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial. The user can select whether time or subject enrollment is used as the x-axis.\n\n\n\nIf the design has used arm dropping, the following graphs are available.\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 18: Response and Proportion Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Response and Proportion Arm Dropped\n\n\n\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 20: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 22: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 24: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\nThese graphs are available if frequentist analysis is enabled.\n\n\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 26: Distribution of P(LOCF. Bonferroni Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Distribution of P(LOCF. Unadjusted Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from Unadjusted, Bonferroni or Dunnett’s and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 28: Response and Ppn LOCF. Bonferroni Significance\n\n\n\n\n\n\n\n\n\n\n\nFigure 29: Response and Ppn Fail. Dunnett Significance\n\n\n\n\n\n\n\n\n\n\nThis graph shows the final analysis at the end of individual simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 30: Response and Subject Allocation\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Response and Subject Allocation\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBlue bars showing the number of subjects allocated on each treatment arm.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 32: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\nControl to select the individual simulation results to be displayed\n\n\n\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 34: Response and Pr(MED relative to Control: Delta = 0.1) Probability\n\n\n\n\n\n\n\n\n\n\n\nFigure 35: Response and Pr(EDq relative to Control: Quantile = 0.9) Probability\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot.\n\n\n\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\nNote the ‘Interim’ control in both of the below figures.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 36: Response and Pr(MED relative to Control: Delta = 0.1) Probability with side panel allowing for selection of simulation, interim, and quantity of interest.\n\n\n\n\n\n\n\n\n\n\n\nFigure 37: Response and Subject Allocation with side panel allowing for selection of simulation and interim number.\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 39\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: e.g. Pr(Max), and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis). The graphs show for all the decision QOIs defined for the selected Target Dose the proportion of simulations that would have met the threshold values.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (unlike the examples above where the shape of the “existing stopping rules” line indicates that early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which decision QOI is evaluated and from which interim onwards stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 42: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\nFigure 43: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 45\n\n\n\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the decision QOI to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are successful/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates.\n\n\n\n\n\nDichotomous\n\n\n\n\n\n\n\n\nFigure 46: Trace plot of an MCMC parameter.\n\n\n\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC.\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot.\n\n\n\n\n\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 47: Across scenario graph for selected arms with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 48: Across scenario graph for selected arms with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 49: Across scenario graph for showing QOIs per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50: Across scenario graph for showing QOIs per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 51: Across scenario graph for showing the proportion of trials that result in success with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 52: Across scenario graph for showing the proportion of trials that result in success with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 53: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 54: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 55: Across scenario graph for showing the distribution of allocation per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 56: Across scenario graph for showing the distribution of allocation per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 57: Across scenario graph for showing the mean sample size of the study with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 58: Across scenario graph for showing the mean sample size of the study with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 59: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\n\n\nFigure 60: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 61: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.\n\n\n\n\n\n\n\n\n\n\n\nFigure 62: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#per-scenario-graphs",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#per-scenario-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "ContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 6: Response and Subject Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Response and Subject Allocation.\n\n\n\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black line shows the true mean response being simulated.\nThe green dashed line shows the mean of the estimated response across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated mean response across the simulations (if less than 20 simulations have been run it simply shows the full spread).\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 8: Response and QOI Selection.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Response and QOI Selection.\n\n\n\n\n\n\nThese plots show the true simulated mean response and the mean and 95% spread of the mean fitted mean responses along with bars showing the proportion of simulations the different treatment arms are the selected target arm for each of the “Probability of being target” QOIs, such as Pr(Max), Pr(EDq relative to ..: Quantile=q) and Pr(MED relative to…: Delta=\\(\\delta\\)).\n\n\n\nThis set of graphs show the distribution over the simulations of the posterior probability estimates of quantities of interest for each dose. All the Posterior Probability, Predictive Probability, P-value and Probability of being Target QOIs that have been defined are available for plotting, via a drop down list.\nNote that in any one analysis the Probability of being Target will sum to 1 across the doses (and can sum to less than one in the case of the MED target if none of the doses look likely to be better than the CSD), whereas all the other QOIs are assessed individually for each dose.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(\\theta_d\\) - \\(\\theta_{Control}\\) &gt; 0.1) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.1) Across Simulations.\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(P_d\\) - \\(P_{Control}\\) &gt; 0.15) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.2) Across Simulations.\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 12: Posterior Mean Response at P(MED relative to Control: Delta}=0.1) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior Mean Response at P(MED relative to Control: Delta}=0.2) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\nThis graph shows a scatter plot of trial outcomes with the selected Target QOI as the y-axis and total number of subjects recruited as the x-axis. The trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as it is possible?\nWhen and with what mean response trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\n\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial. The user can select whether time or subject enrollment is used as the x-axis.\n\n\n\nIf the design has used arm dropping, the following graphs are available.\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 18: Response and Proportion Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Response and Proportion Arm Dropped\n\n\n\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 20: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 22: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 24: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\nThese graphs are available if frequentist analysis is enabled.\n\n\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 26: Distribution of P(LOCF. Bonferroni Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Distribution of P(LOCF. Unadjusted Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from Unadjusted, Bonferroni or Dunnett’s and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 28: Response and Ppn LOCF. Bonferroni Significance\n\n\n\n\n\n\n\n\n\n\n\nFigure 29: Response and Ppn Fail. Dunnett Significance\n\n\n\n\n\n\n\n\n\n\nThis graph shows the final analysis at the end of individual simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 30: Response and Subject Allocation\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Response and Subject Allocation\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBlue bars showing the number of subjects allocated on each treatment arm.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 32: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\nControl to select the individual simulation results to be displayed\n\n\n\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 34: Response and Pr(MED relative to Control: Delta = 0.1) Probability\n\n\n\n\n\n\n\n\n\n\n\nFigure 35: Response and Pr(EDq relative to Control: Quantile = 0.9) Probability\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot.\n\n\n\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\nNote the ‘Interim’ control in both of the below figures.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 36: Response and Pr(MED relative to Control: Delta = 0.1) Probability with side panel allowing for selection of simulation, interim, and quantity of interest.\n\n\n\n\n\n\n\n\n\n\n\nFigure 37: Response and Subject Allocation with side panel allowing for selection of simulation and interim number."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#post-simulation-boundary-finding-graphs",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#post-simulation-boundary-finding-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "ContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 39\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: e.g. Pr(Max), and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis). The graphs show for all the decision QOIs defined for the selected Target Dose the proportion of simulations that would have met the threshold values.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (unlike the examples above where the shape of the “existing stopping rules” line indicates that early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which decision QOI is evaluated and from which interim onwards stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 42: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\nFigure 43: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 45\n\n\n\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the decision QOI to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are successful/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#mcmc-trace-plots",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#mcmc-trace-plots",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "Dichotomous\n\n\n\n\n\n\n\n\nFigure 46: Trace plot of an MCMC parameter.\n\n\n\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC.\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#across-scenario-graphs",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#across-scenario-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "This graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 47: Across scenario graph for selected arms with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 48: Across scenario graph for selected arms with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 49: Across scenario graph for showing QOIs per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50: Across scenario graph for showing QOIs per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 51: Across scenario graph for showing the proportion of trials that result in success with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 52: Across scenario graph for showing the proportion of trials that result in success with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 53: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 54: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 55: Across scenario graph for showing the distribution of allocation per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 56: Across scenario graph for showing the distribution of allocation per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 57: Across scenario graph for showing the mean sample size of the study with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 58: Across scenario graph for showing the mean sample size of the study with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 59: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\n\n\nFigure 60: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 61: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.\n\n\n\n\n\n\n\n\n\n\n\nFigure 62: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#summary-per-scenario",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#summary-per-scenario",
    "title": "Continuous and Dichotomous Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after continuous or dichotomous simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn S uc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn F ut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inco nclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt; Dose&gt;&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn I ncorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation columns provided in FACTS output for continuous or dichotomous trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nResponse\n\nContinuousDichotomous\n\n\n\nResponse columns available in FACTS output for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the mean response on this dose.\n\n\nSD Trt.: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this dose.\n\n\nMean Sigma\n1\nThe mean (over the simulations) of the estimate of the SD of the dose response across all the treatment arms (‘sigma’)\n\n\nSD Mean Sigma\n1\nThe SD (over the simulations) of the estimate of the SD of the dose response across all the treatment arms.\n\n\nTrue Mean Resp: &lt;Dose&gt;\nOne per arm\nThis is the true mean response from which the simulation data was sampled for each treatment arm.\n\n\nSD True Resp.: &lt;Dose&gt;\nOne per arm\nThis is the true SD of the dose response for each treatment arm\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit, based on the independent estimate of the dose response (omega_d), scaled by the longitudinal model's estimate of the proportion of the final effect observed at the visit.\n\n\nSD Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the SD ( over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Baseline Beta\n\nThis is the mean (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nSD Baseline Beta\n\nThis is the SD (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nMean Baseline\n1\nThis is the mean (over the simulations) of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\nThis is the SD (over the simulations) of the estimate of the mean baseline score.\n\n\nTrue Mean Baseline\n1\nThis is the true mean from which baseline scores where simulated (including accounting for possible truncation of the baseline scores)\n\n\nTrue SD Baseline\n1\nThis is the true SD of the distribution from which baseline scores were simulated (including accounting for possible truncation of the baseline scores)\n\n\n\n\n\n\nResponse columns available in FACTS output for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the response on this dose.\n\n\nSD Trt.: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this dose.\n\n\nTrue Mean Resp: &lt;Dose&gt;\nOne per arm\nThis is the true response rate from which the simulation data was sampled.\n\n\n\n\n\n\n\n\nObserved\n\nObserved columns available in FACTS output for continuous or dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the mean (over the simulations) of dropouts per arm per visit in this scenario.\n\n\n\n\n\nProbabilities\n\nProbabilities columns available in FACTS output for continuous or dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\nModel Parameters\n\nContinuousDichotomous\n\n\n\nModel Parameters columns available in FACTS output for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the posterior estimate of sigma, the SD in the subject’s final responses.\n\n\nSD Mean Sigma\n1\nThis is the SD (over the simulations) of the estimate of sigma.\n\n\nMean Baseline Beta\n1\nThis is the mean (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nSD Baseline Beta\n1\nThis is the SD (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nBAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\n\n\n\n\nModel Parameters columns available in FACTS output for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nD*V\nThese are only calculated and written out if the ITP longitudinal model is being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nBAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#detailed-per-simulation-results",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#detailed-per-simulation-results",
    "title": "Continuous and Dichotomous Output",
    "section": "Detailed Per Simulation Results",
    "text": "Detailed Per Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulations output in the table by double-clicking on the appropriate row. A separate window pops up and displays the results for each individual simulation. This is a “prettier” view of the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 63: The simulation results pop out viewer showing simulation highlights."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-summary.csv",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-summary.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;Dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;Dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nMean Sigma\n1\nThe mean of the estimate of sigma – the average standard deviation of the dose response\n\n\nSE Mean Sigma\n1\nThe standard error of the estimate of sigma – the average standard deviation of the dose response\n\n\nTrue Mean resp &lt;Dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nTrue SD resp &lt;Dose&gt;\nD\nThe true SD of the response for each treatment arm of the simulated subject responses\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nMean Beta\n1\nThe mean of the estimates of the coefficient of baseline adjustment\n\n\nSE beta\n1\nThe standard error of the estimates of the coefficient of baseline adjustment\n\n\nMean Baseline\n1\nThe mean of the estimate of the mean of the baseline score\n\n\nSE Mean Baseline\n1\nThe standard error of the estimate of the mean of the baseline score\n\n\nSD Baseline\n1\nThe mean of the estimate of the SD of the baseline score\n\n\nSE SD Baseline\n1\nThe standard error of the estimate of the SD of the baseline score\n\n\nTrue Mean Baseline\n1\nThe true mean of the baseline score (accounting for possible truncation)\n\n\nTrue SD Baseline\n1\nThe true SD of the baseline score (accounting for possible truncation)\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\nContents of the summary.csv file for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;Dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;Dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-summary_freq_missingnesstype.csv",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-summary_freq_missingnesstype.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of Summary_freq_{missingnessType}.csv",
    "text": "Contents of Summary_freq_{missingnessType}.csv\nThere is a frequentist summary file for each type of treatment of missing values.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\nSE Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\nMean Beta (b aseline)\n1\nThe mean of the estimate of the baseline coefficient.\n\n\nSE Mean Beta (b aseline)\n1\nThe standard error of the estimate of the baseline coefficient\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.\n\n\n\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for dichotomous simulations.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\n\nSE Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of | simulations where the unadjusted p-value is | less than the user specified one-sided alpha. | | The marginal probability of significance per | dose (using unadjusted p-values).\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nRandom Number Seed\n1\n✔\n✔\nBase random number seed.\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nMean resp &lt;Dose&gt;\nD\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation.\n\n\nTrue SD resp &lt;Dose&gt;\nD\n✔\n✔\nThe true SD of the response of each treatment arm for this simulation\n\n\nSigma\n1\n✔\n✔\nThe pooled estimate of the SD of the response across all the treatment arms\n\n\nSD_Sigma\n1\n✔\n✔\nThe SD of the pooled estimate of the SD of the response across all the treatment arms\n\n\nBeta\n1\n✔\n✔\nThe estimate of ‘Beta’ the baseline adjustment coefficient, if baseline adjustment is being used.\n\n\nSD_Beta\n1\n✔\n✔\nThe SD of the estimate of ‘Beta’ the baseline adjustment coefficient.\n\n\nMean Baseline\n1\n✔\n✔\nThe estimate of the mean baseline score.\n\n\nSE Mean Baseline\n1\n✔\n✔\nThe standard error of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\n✔\n✔\nThe estimate of the SD of the baseline score.\n\n\nTrue Mean Baseline\n1\n✔\n✔\nThe true mean of the simulated baseline score\n\n\nTrue SD Baseline\n1\n✔\n✔\nThe true SD of the simulated baseline score\n\n\nMean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe observed mean response on each treatment arm (unadjusted by any modeling).\n\n\nSE Mean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe SE of the estimate of the mean raw response on each dose.\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis.\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subject who count as complete for the purposes of timing interims – whether complete or opportunity to complete and at which visit is as defined on the Interims tab. If interims are by enrolment then CompleteInformation is the number of subjects that have completed – final endpoint data is available.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit.\n\n\nDR Param &lt;Param&gt;\n10\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 10 parameters, most less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the index “&lt;Param&gt;”.\n\n\nSd DR Param &lt;Param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod_ &lt;Model&gt; &lt;Parameter&gt; &lt;Visit&gt;\nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. For linear regression the parameters reported are: - Alpha – per visit – the mean estimate of the constant offset in the change in response from this visit to the final visit - Beta – per visit – the mean estimate of the coefficient of change in response from this visit to the final visit - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. For time course hierarchical the parameters reported are: - Alpha – per visit – the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit.. - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. - Tau – the mean estimate of the SD of the per subject random effect For ITP the parameters are: - K – per model – the mean estimate of the ITP shape parameter - Tau – per model - the mean estimate of the SD of the per subject random effect - Lambda – per model – the mean estimate of the Sd of the residual error. - Omega – per treatment arm – the mean estimate of the mean treatment arm effect. For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used. This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses.\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim\n\n\nMean resp &lt;Dose&gt;\nD\n✔\n✔\nThe estimated response of each treatment arm\n\n\nSD resp &lt;Dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm\n\n\nMean resp (lower CI) &lt;Dose&gt;\nD\n✔\n✔\nThe lower bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nMean resp (upper CI) &lt;Dose&gt;\nD\n✔\n✔\nThe upper bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nTrue Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation\n\n\nMean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe observed rate of response on each treatment arm (unadjusted by any modeling)\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects that have achieved the information criteria as specified on the Interims tab. May be the number enrolled, complete, or with the opportunity to complete. If complete or opportunity to complete, then the visit that should be complete is also specified.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit\n\n\nDR Param &lt;Param&gt;\n4\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the column their value appears in here\n\n\nSd DR Param &lt;Param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter\n\n\nLongmod_ &lt;Model&gt; &lt;Parameter&gt; &lt;Visit&gt;\nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. Each value is reported per visit unless otherwise stated.For Beta-Binomial the parameters reported are:• Alpha0 & Beta 0 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 0• Prob01 – the mean probability of the Beta(Alpha0, Beta0) – the probability of 1 being the final result if the result at the visit is 0• Alpha1 & Beta 1 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 1• Prob11 – the mean probability of the Beta(Alpha1, Beta1) – the probability of 1 being the final result if the result at the visit is 1For logistic regression the parameters reported are: • Prob11 – the probability of 1 being the final result if the result at the visit is 1and • Prob01 – the probability of 1 being the final result if the result at the visit is 0For restricted Markov the parameters reported are:• Alpha0 – Alpha for state 0• AlphaS – Alpha for stable state• Alpha1 – Alpha for state 1• Prob0 – Transition probability to state 0• ProbS – Probability of remaining stable• Prob1 – Transition probability to state 1(values are for the transition to the next visit, so thre are no values for the final visit)If using a dichotomized continuous response, these columns will be for the selected continuous longitudinal model:For linear regression the parameters reported are:• Alpha – the mean estimate of the constant offset in the change in response from this visit to the final visit• Beta – the mean estimate of the coefficient of change in response from this visit to the final visit• Lambda – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.For ITP the parameters are:• K – single value per model – the mean estimate of the ITP shape parameter• Tau – single value per model - the mean estimate of the SD of the per subject random effect• Lambda – single value per model – the mean estimate of the Sd of the residual error.• Omega – per treatment arm not visit – the mean estimate of the mean treatment arm effect.For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThese are only calculated and written out if the ITP longitudinal model is being used for a dichotomized continuous endpoint.This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns are grouped in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv",
    "text": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv\nThese file contain the frequentist results after an interim analysis (weeks file) or final analysis (simulations file), apart for the first column, the columns are the same in the two types of file, and the same across the different methods for treating missing data.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation (simulations_freq_…csv only)\n\n\n# Weeks\n1\nThe week of the interim (weeks_freq_…csv only)\n\n\nTheta\nD\nUnadjusted estimate of response per treatment arm\n\n\nTheta SD\nD\nThe SD of the estimate of response per treatment arm\n\n\nTrt Effect\nD\nThe relative size of the treatment effect compared to control per arm (treatment effect for control is always 0). The direction of the comparison takes into account whether a response represents a subject’s condition improving or worsening, so that a positive treatment effect shows the response on the dose is better than on control.\n\n\nt-Stat\nD\nThe t-test statistic per treatment arm (for control this is always 0)\n\n\np-Value\nD\nThe unadjusted t-test p-value per arm (for control this is always 0)\n\n\nlower CI\nD\nThe unadjusted lower bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nupper CI\nD\nThe unadjusted upper bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nMin p-Value\n1\nThe smallest of the t-test p-values\n\n\np-Value Dunnett\nD\nThe Dunnett adjusted t-test p-value per arm (for control this is always 0).\n\n\nlower CI Dunnett\nD\nThe Dunnett adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper CI Dunnett\nD\nThe Dunnett adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Dunnett\n1\nThe smallest of the Dunnett adjusted p-values.\n\n\np-Value Bonf\nD\nThe Bonferroni adjusted t-test p-value per arm.\n\n\nlower-CI Bonf\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper-CI Bonf\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Bonf\n1\nThe smallest of the Bonferroni adjusted t-test value per arm.\n\n\nt-Stat trend\n1\nThe t statistic from the trend test.\n\n\np-Value trend\n1\nThe p-value from the trend test."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-patientsnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nBaseline\n1\nSubject baseline response, if simulated. If not simulated, then fixed at -9999.\n\n\nVisit &lt;Visit&gt;\nV\nSubject’s response at each visit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nBaseline\n1\nAlways -9999 since there’s no baseline in dichotomous response trials.\n\n\nVisit &lt;Visit&gt;\nV\nSubject’s response at each visit."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/contanddichot.html#contents-of-mcmcnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nTheta &lt;Dose&gt;\nD\nThe estimate of the mean response for each dose, based on the dose response model fitted.\n\n\nSigma\n1\nThe estimate of the SD of the endpoint.\n\n\nBeta\n1\nIf baseline is included in the simulated data and using the “Baseline Adjusted Model”, this is the value of “Beta”, the baseline coefficient.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\n\n\nIf using a dichotomized continuous variable, then the parameters for the LM are the same as for the continuous model MCMC output listed in the Continuous tab.\nIf using a dichotomous longitudinal model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\nIf using a restricted markov model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nCurrently the values of Alpha0, Alpha1, AlphaS (the parameters to the Dirichlet distribution) are (unnecessarily – these values are not sampled) output, along with the values for P0, P1 and PS the transition probabilities of each model at each visit."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html",
    "href": "documentation/v72/userguides/core/simulation/tte.html",
    "title": "Time to Event Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described in Section 16, below.\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select a row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nTo view graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot.\n\n\n\n\n\n\nFigure 1: Select the scenarios to include in Across Scenario graphs.\n\n\n\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks. (Not displayed if the ‘y’ value must lie in the interval 0-1.\nSpace doses evenly – if the x-value is ‘dose’, then data for each dose can either be spaced equally or proportionate to the doses’ effective dose strength.\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#allocation-box-and-whisker-plot",
    "href": "documentation/v72/userguides/core/simulation/tte.html#allocation-box-and-whisker-plot",
    "title": "Time to Event Output",
    "section": "Allocation Box and Whisker Plot",
    "text": "Allocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 2: Distribution of Subject Allocation Across Simulations.\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\n\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\nThe mean number of events observed in each arm across the simulations shown as a red triangle."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#events-boxplot",
    "href": "documentation/v72/userguides/core/simulation/tte.html#events-boxplot",
    "title": "Time to Event Output",
    "section": "Events Boxplot",
    "text": "Events Boxplot\n\n\n\n\n\n\nFigure 3: Distribution of Number of Events Across Simulations\n\n\n\nThis graph displays a box and whisker plot of the number of events observed in each arm. These plots show:\n\nThe distribution over all simulations of the number of events observed in each arm shown as a box and whisker plot."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#hazard-ratio-and-subject-allocation",
    "href": "documentation/v72/userguides/core/simulation/tte.html#hazard-ratio-and-subject-allocation",
    "title": "Time to Event Output",
    "section": "Hazard Ratio and Subject Allocation",
    "text": "Hazard Ratio and Subject Allocation\n\n\n\n\n\n\nFigure 4: Hazard Ratio and Subject Allocation\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean number of events observed and the mean estimated hazard ratio. Specifically:\n\nThe blue bars show the mean number of subjects without events and the brown bars mean number of subjects who had events.\nThe black line shows the true Hazard Ratio being simulated (but without allowing for any effect of the predictor)\nThe green dashed line (drawn if a response model is fitted), shows the mean of the estimated hazard ratios across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated hazard ratios (if less than 20 simulations have been run it simply shows the full spread)."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#hazard-ratio-and-target-selection-graphs",
    "href": "documentation/v72/userguides/core/simulation/tte.html#hazard-ratio-and-target-selection-graphs",
    "title": "Time to Event Output",
    "section": "Hazard Ratio and Target Selection graphs",
    "text": "Hazard Ratio and Target Selection graphs\n\n\n\n\n\n\nFigure 5: Hazard Ratio and \\(Pr\\)(MED relative to Control: Delta=-0.2) Selection\n\n\n\nThese plots show the true simulated hazard ratio (without allowance for the effect of the predictor) and the mean and 95% spread of the mean fitted hazard ratios along with bars showing the proportion of simulations the different treatment arms are the finally selected target. The target displayed is selected by a control on the graph, from any of the defined Target QOIs.:"
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#predictor-response-and-allocation",
    "href": "documentation/v72/userguides/core/simulation/tte.html#predictor-response-and-allocation",
    "title": "Time to Event Output",
    "section": "Predictor: Response and Allocation",
    "text": "Predictor: Response and Allocation\n\n\n\n\n\n\nFigure 6: Predictor Hazard Ratio and Subject Allocation\n\n\n\nThese plots show the true simulated predictor response and the mean and 95% spread of the mean fitted predictor response model, with the blue bars showing the mean number of subjects allocated to each arm across the simulations."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#per-dose-qois",
    "href": "documentation/v72/userguides/core/simulation/tte.html#per-dose-qois",
    "title": "Time to Event Output",
    "section": "Per Dose: QOIs",
    "text": "Per Dose: QOIs\n\n\n\n\n\n\nFigure 7: Distribution of P(Succ. Future Trial: N=250; Sup. \\(\\alpha=0.025\\), \\(\\delta=0\\) Adjusted Significance) Across Simulations\n\n\n\nThese plots show a box plot showing the distribution of the values for any of the defined QOIs for each dose.\nWhich QOI is displayed can be selected by the user from a drop down list on the graph. Any of the Posterior Probability, Predictive Probability, P-value and Target QOI’s can be selected."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#target-hazard-ratio-scatter-plot",
    "href": "documentation/v72/userguides/core/simulation/tte.html#target-hazard-ratio-scatter-plot",
    "title": "Time to Event Output",
    "section": "Target Hazard Ratio Scatter Plot",
    "text": "Target Hazard Ratio Scatter Plot\n\n\n\n\n\n\nFigure 8: Posterior Mean Hazard Ratio at Pr(Max) vs Total Sample Size per Simulation Run\n\n\n\nThis graph shows a scatter plot of trial outcomes with the estimate hazard ratio as the y-axis and total number of subjects recruited as the x-axis. A drop down list on the graph allows the user to select which target QOI is used to supply the hazard ratio:\nThe trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as its possible?\nWhen and with what response rate trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\nThere are two further variants of this graph, these have alternative y-axes: the hazard ratio at the EDx or at the MED."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#cumulative-operating-characteristics-plot",
    "href": "documentation/v72/userguides/core/simulation/tte.html#cumulative-operating-characteristics-plot",
    "title": "Time to Event Output",
    "section": "Cumulative Operating Characteristics Plot",
    "text": "Cumulative Operating Characteristics Plot\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 9\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#time-course-for-stopping",
    "href": "documentation/v72/userguides/core/simulation/tte.html#time-course-for-stopping",
    "title": "Time to Event Output",
    "section": "Time course for stopping",
    "text": "Time course for stopping\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 10\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial.\nThe x-axis is configurable, the user can select cumulative stopping to be plotted relative to time, sample size or number of events observed."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#arm-dropping-graphs",
    "href": "documentation/v72/userguides/core/simulation/tte.html#arm-dropping-graphs",
    "title": "Time to Event Output",
    "section": "Arm Dropping Graphs",
    "text": "Arm Dropping Graphs\nIf the design has used arm dropping, the following graphs are available.\n\nHazard ratio and Ppn Arms Dropped\n\n\n\n\n\n\nFigure 11: Hazard Ratio and Proportion Arm Dropped\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\nTime Course for Arm Dropping\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\n\n\n\n\n\nFigure 12: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\nTime Course for Arm retention\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\n\n\n\n\n\nFigure 13: Cumulative Proportion of Trials Where Arm Retained\n\n\n\n\n\nArm Retention Proportion\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\n\n\n\n\n\nFigure 14: Arm Retention Proportion"
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#frequentist",
    "href": "documentation/v72/userguides/core/simulation/tte.html#frequentist",
    "title": "Time to Event Output",
    "section": "Frequentist",
    "text": "Frequentist\nThese graphs are available if frequentist analysis is enabled.\n\nFrequentist P(significance)\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\n\n\n\n\n\nFigure 15: Distribution of \\(P\\)(Log Rand Adjusted Significance) Across Simulations\n\n\n\n\n\nFrequentist: Response and Significance\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from the Unadjusted, Bonferroni or Log Rank, Log Rank Bonferroni, Wilcox and Wilcox Bonferroni p-values.\n\n\n\n\n\n\nFigure 16: Hazard Ratio and Ppn Log Rank Significance"
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#per-sim-graphs",
    "href": "documentation/v72/userguides/core/simulation/tte.html#per-sim-graphs",
    "title": "Time to Event Output",
    "section": "Per Sim Graphs",
    "text": "Per Sim Graphs\nThis set of graphs includes a control that allows the user to select which simulation to graph the results from. Each graph shows the output from only 1 simulated trial.\n\n\n\n\n\n\nFigure 17: The plot window when selecting a Per Sim graph. Note the Simulation control in the bottom left allowing for the selection of which simulated trial the graph should be made for.\n\n\n\n\nHazard Ratio and Subject Allocation\nThis graph shows the final analysis at the end of individual simulations.\n\n\n\n\n\n\nFigure 18: Hazard Ratio and Subject Allocation\n\n\n\nOn each graph\n\nThe black line shows the ‘True’ Hazard Ratio being simulated, (not adjusted to include any predictor effects)\nThe blue bars show the mean number of subjects without events and the brown bars mean number of subjects who had events.\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\n\n\n\nPosterior Quantities\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\n\n\n\n\n\nFigure 19: Hazard Ratio and \\(Pr\\)(\\(HR_d\\) - 1 &lt; -0.2) Probability\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from and which QOI to plot. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot, including QOI’s based on the predictor.\n\n\nPredictor Response and Allocation\n\n\n\n\n\n\nFigure 20: Predictor Hazard Ratio and Subject Allocation (Week: 80)\n\n\n\nIf the simulations include a predictor this graph is available.\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ Predictor response being simulated\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\nBlue Bars showing number of subjects allocated to each treatment arm."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#per-interim-graphs",
    "href": "documentation/v72/userguides/core/simulation/tte.html#per-interim-graphs",
    "title": "Time to Event Output",
    "section": "Per Interim Graphs",
    "text": "Per Interim Graphs\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100). Note the control in the bottom left that allows for specification of Simulation number as well as Interim number.\n\n\n\n\n\n\nFigure 21: The plot window when selecting a Per Interim graph. Note the Simulation control in the bottom left allowing for the selection of which simulated trial and which interim analysis the graph should be made for."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#post-simulation-boundary-finding-graphs",
    "href": "documentation/v72/userguides/core/simulation/tte.html#post-simulation-boundary-finding-graphs",
    "title": "Time to Event Output",
    "section": "Post Simulation Boundary Finding Graphs",
    "text": "Post Simulation Boundary Finding Graphs\n\nExplore Success/Futility Eval Criteria\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: Max, EDx or MED, and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis).\nFor the given target the proportion of trials that would meet each of the criteria over the range of threshold values is plotted. As in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\nExplore Early Success/Futility Eval Criteria\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 22\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (as in the examples above where the shape of the “existing stopping rules” line indicates that no early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which stopping criteria is evaluated and from which interim stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\nExplore Arm Dropping Criteria\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\nSuccess/Futility Stopping Contours\n\n\n\n\n\n\n\n\n\n\nFigure 24\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the criterion to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are a success/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#selected-arms",
    "href": "documentation/v72/userguides/core/simulation/tte.html#selected-arms",
    "title": "Time to Event Output",
    "section": "Selected Arms",
    "text": "Selected Arms\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\n\n\n\n\n\nFigure 25: Across scenario graph for selected arms with 4 scenarios and 3 design variants."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#qoi-box-plots",
    "href": "documentation/v72/userguides/core/simulation/tte.html#qoi-box-plots",
    "title": "Time to Event Output",
    "section": "QOI Box Plots",
    "text": "QOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\n\n\n\n\n\nFigure 26: Across scenario graph showing QOIs per arm with 4 scenarios and 3 design variants."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#ppn-success",
    "href": "documentation/v72/userguides/core/simulation/tte.html#ppn-success",
    "title": "Time to Event Output",
    "section": "Ppn Success",
    "text": "Ppn Success\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\n\n\n\n\n\nFigure 27: Across scenario graph for showing the proportion of trials that result in success with 4 scenarios and 3 design variants."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#response",
    "href": "documentation/v72/userguides/core/simulation/tte.html#response",
    "title": "Time to Event Output",
    "section": "Response",
    "text": "Response\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\n\n\n\n\n\nFigure 28: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 4 scenarios and 3 design variants."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#allocation",
    "href": "documentation/v72/userguides/core/simulation/tte.html#allocation",
    "title": "Time to Event Output",
    "section": "Allocation",
    "text": "Allocation\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\n\n\n\n\n\nFigure 29: Across scenario graph for showing the distribution of allocation per arm with 4 scenarios and 3 design variants."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#sample-size",
    "href": "documentation/v72/userguides/core/simulation/tte.html#sample-size",
    "title": "Time to Event Output",
    "section": "Sample Size",
    "text": "Sample Size\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\n\n\n\n\n\nFigure 30: Across scenario graph for showing the mean sample size of the study with 4 scenarios and 3 design variants."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#interim-vs-final-scatter-plot",
    "href": "documentation/v72/userguides/core/simulation/tte.html#interim-vs-final-scatter-plot",
    "title": "Time to Event Output",
    "section": "Interim vs Final Scatter Plot",
    "text": "Interim vs Final Scatter Plot\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\n\n\n\n\n\nFigure 31: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#receiver-operating-characteristics",
    "href": "documentation/v72/userguides/core/simulation/tte.html#receiver-operating-characteristics",
    "title": "Time to Event Output",
    "section": "Receiver Operating Characteristics",
    "text": "Receiver Operating Characteristics\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\n\n\n\n\n\nFigure 32: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#summary-per-scenario",
    "href": "documentation/v72/userguides/core/simulation/tte.html#summary-per-scenario",
    "title": "Time to Event Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after time-to-event simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn S uc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn F ut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inco nclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt; Dose&gt;&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn I ncorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation tabs provided in FACTS output for time-to-event trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nResponse\nThe following columns provide summaries of the estimated hazard ratios based on the bayesian model incorporating the dose response model and the predictor model, if one is specified.\n\nResponse columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nSD Trt .: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the treatment hazard ratio for each arm. The SD of the treatment response for the control arm is always 0.\n\n\nTrue Resp: &lt;Dose&gt;\nOne per arm\nThis is the true Hazard Ratio from which the simulation data was sampled. When using VSR with Event Rate | Predictor, these rates will not be the same as the Dose Response HRs entered on the VSR &gt; Explicitly Defined (ER | P) &gt; Dose Response tab.\n\n\n\n\n\nObserved\nBy right clicking and selecting the Observed columns, a pop-out will appear that provides the following columns. These columns relate to the raw data observed in the trial.\n\nObserved columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Total Events\n1\nThe mean (over the simulations) of the number of events observed in each trial.\n\n\nMean Events &lt;Dose&gt;\nOne per arm\nThe mean (over the simulations) of the number of events observer in each arm in each trial.\n\n\nMean Events &lt;Dose&gt; &lt;S egment&gt;\nOne per arm per segment\nThe mean (over the simulations) of the number of events observer in each arm and segment in each trial.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Predictor Data, Opportunity to Complete Predictor, Events, Predictor Events) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\nOne per arm\nIf dropouts are simulated this is the mean (over the simulations) of the number of subjects in each arm that dropout before an event is observed.\n\n\nMean Exposure &lt;Dose&gt; &lt;S egment&gt;\nOne per arm\nThe mean (over the simulations), of the total exposure of the subjects observed on each arm and segment.\n\n\n\n\n\nProbabilities\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns. These columns provide summaries of the Quantities of Interest values.\n\nProbabilities columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\nHierarchical Prior Parameters\nBy right clicking and selecting the Hierarchical Prior Parameters columns, a pop-out will appear that provides the following columns.\n\nHierarchical Prior Parameters columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean BAC Mu\n1\nThe average (over the simulation) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nSD BAC Mu\n1\nThe average (over the simulations) of the standard deviation of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nMean BAC Tau\n1\nThe average (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nSD BAC Tau\n1\nThe standard deviation (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Control (hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nMean BAAC Mu\n1\nThe average (over the simulation) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nSD BAAC Mu\n1\nThe average (over the simulations) of the standard deviation of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nMean BAAC Tau\n1\nThe average (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nSD BAAC Tau\n1\nThe standard deviation (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Active Comparator (hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator, otherwise the column contains -9999.\n\n\n\n\n\nPredictor\nBy right clicking and selecting the Predictor columns, a pop-out will appear that provides the following columns. These colums provide summaries of the bayesian model estimates relating to the predictor endpoint.\n\nPredictor columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Pred. Resp.: &lt;Dose&gt;\nOne per arm\nThe mean (over the simulations) of the mean estimate of the predictor response for each arm. The response reported is:\nContinuous predictor: The mean change from baseline\nDichotomous predictor: The response rate\nTTE predictor: The hazard ratio\nNo predictor: 0\n\n\nSD Pred. Resp.: &lt;Dose&gt;\nOne per arm\nThe SD (over the simulations) of the mean estimate of the predictor response for each arm.\n\n\nMean Pred. Sigma\n1\nThis is the average (over the simulations) of the estimate of sigma (SD of response) of a continuous predictor, if one was being used. Otherwise the column contains -9999.\n\n\nSD Pred Sigma\n1\nThis is the standard deviation (over the simulations) of the estimate of the sigma (SD of the response) of a continuous predictor, if one was being used. Otherwise the column contains -9999.\n\n\nTrue P redictor Resp: &lt;Dose&gt;\nOne per arm\nThis is the true predictor response from which the simulated subject predictor responses were sampled. For a continuous predictor this is the mean of the response, for a dichotomous predictor it is the predictor response rate and for a time-to-event predictor it is the hazard ratio of the predictor, otherwise the column contain -9999.\n\n\nTrue P redictor Sigma: &lt;Dose&gt;\nOne per arm\nThis is the true standard deviation of the predictor response from which the simulated subject predictor responses were sampled when the predictor is a continuous measure, otherwise the column contains -9999\n\n\nMean Pred. Lambda: &lt;Dose&gt;\nOne per arm\nThis is the average (over the simulations) of the estimate of Lambda for each dose in the endpoint predictor model – the estimated mean time to the final event for each dose either where the predictor is zero for a continuous or dichotomous predictor, or for the time after observing the event of time-to-event predictor. Otherwise the column contains 0.\n\n\nSD Pred. Lambda: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of Lambda for each dose (see above). Otherwise the column contains 0.\n\n\nMean Pred. Beta\n1\nThis is the average (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor. Otherwise the column contains -9999.\n\n\nSD Pred. Beta\n1\nThis is the standard deviations (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor. Otherwise the column contains -9999.\n\n\n\n\n\nModel Parameters\nBy right clicking and selecting the Model Parameters columns, a pop-out will appear that provides the following columns.\n\nModel Parameters columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nTrue P redictor Baseline Hazard Lambda: &lt;s egment&gt;\nOne per VSR pr edictor hazard rate segment\nThe true hazard rate for the predictor (if the predictor is itself an event), in each of the predictor VSR time segments.\n\n\nLambda: &lt;s egment&gt;\nOne per Design, Hazard Model segment\nThe mean (over the simulations) of the mean estimate of lambda – the event rate (events per week) in each time segment of the hazard model.\n\n\nSE Lambda: &lt;s egment&gt;\nOne per Design, Hazard Model segment\nThe standard error (over the simulations) of the mean estimate of lambda for each time segment of the hazard model.\n\n\nTrue Lambda: &lt;s egment&gt;\nOne per VSR control hazard segment\nThe true hazard rate for each segment of the virtual subject response hazard rate profile."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#detailed-simulation-results",
    "href": "documentation/v72/userguides/core/simulation/tte.html#detailed-simulation-results",
    "title": "Time to Event Output",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 15‑1) displays the individual results for each simulation. This is the contents of the “simulations.csv” file, which is described below.\n\n\n\n\n\n\nFigure 33: The simulation results pop out viewer showing simulation highlights."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#contents-of-summary.csv",
    "href": "documentation/v72/userguides/core/simulation/tte.html#contents-of-summary.csv",
    "title": "Time to Event Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContents of the summary.csv file for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nT imestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTC that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.subj 80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nP(ES)\n\n1\n\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;dose&gt;\nD\nThe mean of the estimates of hazard ratio of each treatment arm. (This is always 1 for the control arm)\n\n\nSE Resp &lt;dose&gt;\nD\nThe standard error, over the simulations, of the estimate of hazard ratio of each treatment arm. (This is always 0 for the control arm).\n\n\nTrue Mean resp &lt;dose&gt;\nD\nThe true hazard ratio for each treatment arm used to derive the simulated subject event times (the HR for the control arm is always 1).\n\n\nMean p redictor resp &lt;dose&gt;\nD\nThe mean (over the simulations) of the estimated predictor response per arm – this is the estimated mean response for a continuous predictor, the estimated response rate for a dichotomous predictor and the estimated hazard ratio for a time to event predictor.\n\n\nSE p redictor resp &lt;dose&gt;\nD\nThe standard error (over the simulations) of the estimated predictor response per arm.\n\n\nMean P redictor Sigma\n1\nThe mean (over the simulation) of the estimated standard deviation in the predictor response when modeling a continuous predictor.\n\n\nSE P redictor Sigma\n1\nThe standard error (over the simulations) of the estimated standard deviation in the predictor response when modeling a continuous predictor.\n\n\nTrue Mean p redictor resp &lt;dose&gt;\nD\nThe true predictor response rate used to drive the simulation of the subjects’ predictor outcomes – this is the mean of the response for a continuous predictor, the response rate for a dichotomous predictor and the hazard ratio for a time to event predictor.\n\n\nTrue Sigma p redictor &lt;dose&gt;\nD\nThe true sigma used to drive the simulation of the subjects’ predictor outcomes when the predictor is a continuous measure.\n\n\nMean P redictor Lambda &lt;dose&gt;\nD\nThe mean (over the simulations) of the estimate of lambda – the estimated mean time to the final event in the endpoint predictor model – the estimated mean time to the final event for each dose when the predictor is zero (continuous or dichotomous) or for the time after observing the predictor event.\n\n\nSE P redictor Lambda &lt;dose.\nD\nThis is the standard error (over the simulations) of the estimate of lambda for each dose.\n\n\nMean P redictor Beta\n1\nThis is the mean (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor.\n\n\nSE p redictor Beta\n1\nThis is the standard error (over the simulations) of the estimate of Beta.\n\n\nMean TTE P redictor Baseline Hazard Lambda &lt;s egment&gt;\nS\nThis is the mean (over the simulations) of the estimate of the event rate on the control arm in each hazard model time segment.\n\n\nSE TTE P redictor Baseline Hazard Lambda &lt;s egment&gt;\nS\nThis is the standard error (over the simulations) of the estimate of the event rate on the control arm in each hazard model time segment.\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nEst. Lambda &lt;s egment&gt;\nS\nThe mean, over the simulations, of the estimate of the event rate on the control arm in each time segment of the hazard model.\n\n\nSE lambda &lt;s egment&gt;\nS\nThe mean, over the simulations, of the SD of the estimate of the event rate on the control arm in each time segment of the hazard model.\n\n\nMean Total Events\n1\nThe mean, over the simulations of the total number of events observed in the trials.\n\n\nSE Total Events\n\nThe standard error, over the simulations, of the total number of events observed in the trials.\n\n\nNo. Events &lt;dose&gt;\nD\nThe mean, over the simulations, of the number of events observed on each arm.\n\n\nSE No. Events &lt;dose&gt;\n\nThe standard error, over the simulations, of the number of events observed on each arm.\n\n\nMean Exposure &lt;dose&gt;\n\nThe mean, over the simulations, of the total exposure of the subjects observed on each arm.\n\n\nSE Exposure &lt;dose&gt;\n\nThe standard error, over the simulations, of the total exposure of the subjects observed on each arm.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nQOI Columns\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#contents-of-summary_freq.csv",
    "href": "documentation/v72/userguides/core/simulation/tte.html#contents-of-summary_freq.csv",
    "title": "Time to Event Output",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContents of the summary_freq.csv file for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn LR Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Log Rank test) is less than the user specified one-sided alpha.\n\n\nPpn LR Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Log Rank test) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn LR Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Log Rank test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn LR Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Log Rank test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn Wilcox Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Wilcoxon test) is less than the user specified one-sided alpha.\n\n\nPpn Wilcox Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Wilcoxon test) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn Wilcox Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Wilcoxon test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn Wilcox Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Wilcoxon test) is less than the Bonferroni correct user specified one-sided alpha.\n\n\nMean HR &lt;dose&gt;\nD\nThe mean Hazard Ratio per dose.\n\n\nSE HR &lt;dose&gt;\nD\nThe standard error of the Hazard Ratio per dose\n\n\nPpn HR Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Cox model) is less than the user specified one-sided alpha.\n\n\nPpn HR Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Cox model) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn HR Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Cox model) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn HR Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Cox model) is less than the Bonferroni correct user specified one-sided alpha.\n\n\nBias &lt;dose&gt;\nD\nThe difference between the mean response and the true (simulated) response per dose\n\n\nCoverage &lt;dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true response rate used to simulate subject responses.\n\n\nMean KM med &lt;dose&gt;\nD\nThe mean Kaplan-Meier estimate of the median survival time per dose\n\n\nSE KM med &lt;dose&gt;\nD\nThe standard error of the Kaplan-Meier estimate of the median survival time per dose\n\n\nPredictor Cols\n\nThe appropriate output columns for the chosen type of predictor. See next tables for specifics of columns provided for each predictor type.\n\n\n\n\nContinuous Predictor:\n\nExtra columns on the summary_freq.csv file when using a continuous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox Mean HR &lt;dose&gt;\nD\nThe mean value of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Cox SE HR &lt;dose&gt;\nD\nThe standard error of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Avg. Min &lt;dose&gt;\nD\nThe average of the minimum value for the predictor per dose.\n\n\nPredictor Avg. 10-percentile &lt;dose&gt;\nD\nThe average of the 10th percentile values for the predictor per dose.\n\n\nPredictor Avg. 25-percentile &lt;dose&gt;\nD\nThe average of the 25th percentile values for the predictor per dose.\n\n\nPredictor Avg. Median &lt;dose&gt;\nD\nThe average of the median values for the predictor per dose.\n\n\nPredictor Avg. 75-percentile &lt;dose&gt;\nD\nThe average of the 75th percentile values for the predictor per dose.\n\n\nPredictor Avg. 90-percentile &lt;dose&gt;\nD\nThe average of the 90th percentile values for the predictor per dose.\n\n\nPredictor Avg. Max &lt;dose&gt;\nD\nThe average of the maximum value for the predictor per dose.\n\n\nPredictor Mean &lt;dose&gt;\nD\nThe mean of the mean value of the predictor per dose.\n\n\nPredictor SE &lt;dose&gt;\nD\nThe standard error of the mean value of the predictor per dose.\n\n\n\n\n\nDichotomous Predictor:\n\nExtra columns on the summary_freq.csv file when using a dichotomous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox Mean HR &lt;dose&gt;\nD\nThe mean value of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Cox SE HR &lt;dose&gt;\nD\nThe standard error of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Mean Response Rate &lt;dose&gt;\nD\nThe mean of the predictor response rate per dose.\n\n\nPredictor SE Response Rate &lt;dose&gt;\nD\nThe standard error of the predictor response rate per dose.\n\n\n\n\n\nTime-to-Event Predictor\n\nExtra columns on the summary_freq.csv file when using a time-to-event predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Mean KM Median &lt;dose&gt;\nD\nThe mean of the Kaplan-Meier estimate of the median time to the predictor event per dose.\n\n\nPredictor SE KM Median &lt;dose&gt;\nD\nThe standard error of the Kaplan-Meier estimate of the median time to the predictor event per dose."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nSimulations and weeks file column names for a time-to-event endpoint.\n\n\n\n\n\n\n\n\n\nColumn Title\nNu mber of col umns\nIn s imula tions file\nIn w ee ks fi le\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nLastInt erimNumber\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:\n\n= Early success\n= Late success\n= Late futility\n= Early futility\n= Success to futility flip-flop\n= Futility to success flip-flop\n= Inconclusive\n\n\n\nEarly Success\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nMean resp &lt;dose&gt;\nD\n✔\n✔\nThe estimated hazard ratio of each treatment arm.\n\n\nSD resp &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of the hazard ratio of each treatment arm.\n\n\nTrue Mean resp &lt;dose&gt;\nD\n✔\n✔\nThe true mean response (hazard ratio) of each treatment arm for this simulation.\n\n\nMean Predictor Resp &lt;dose&gt;\nD\n✔\n✔\nThe estimated response of the predictor at each treatment arm.\nCts: mean change from baseline\nDich: response rate\nTTE: hazard ratio with control\n\n\nSD Predictor Resp &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of the predictor response.\n\n\nMean Predictor Sigma\n1\n✔\n✔\nThe mean of the estimate of the sigma of the predictor response (the sd of the response of a continuous predictor)\n\n\nSD Predictor Sigma\n1\n✔\n✔\nThe SD of the estimate of the sigma of the predictor response (the sd of the response of a continuous predictor)\n\n\nTrue Predictor Mean resp &lt;dose&gt;\nD\n✔\n✔\nThe true mean predictor response being simulated for each treatment arm (mean of a continuous predictor, response rate of a dichotomous predictor and the hazard ratio of a time-to-event predictor)\n\n\nTrue Predictor Sigma &lt;dose&gt;\nD\n✔\n✔\nThe true sigma of the predictor response of each treatment arm, if the predictor is a continuous endpoint\n\n\nMean Predictor Lambda &lt;dose&gt;\nD\n✔\n✔\nThe mean of the estimate of Lambda for each dose in the endpoint predictor model – the estimated mean time to the final event for each dose where the predictor is zero for a continuous or dichotomous predictor or the time after observing the event of a time-to-event predictor.\n\n\nSD Predictor Lambda &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of Lambda for each dose in the endpoint predictor model (see above).\n\n\nMean Predictor Beta\n1\n✔\n✔\nThe mean of the estimate of the Beta coefficient in the endpoint predictor model.\n\n\nSD Predictor Beta\n1\n✔\n✔\nThe standard deviation of the estimate of the Beta coefficient in the endpoint predictor model.\n\n\nMean TTE Predictor Baseline Hazard Lambda &lt;seg&gt;\nS\n✔\n✔\nThe mean estimate of the hazard rate of the predictor event on the control arm, when using a time-to-event predictor, for each time segment of the predictor control hazard model.\n\n\nMean TTE Predictor Baseline Hazard Lambda &lt;seg&gt;\nS\n✔\n✔\nThe standard deviation of the estimate of the hazard rate of the predictor event on the control arm, when using a time-to-event predictor, for each time segment of the predictor control hazard model.\n\n\nNum Events &lt;dose&gt;\n&lt;segment&gt;\nS * D\n✔\n✔\nThe number of events observed on each arm in each control hazard model time segment.\n\n\nTotal Exposure &lt;dose&gt;\n&lt;segment&gt;\nS * D\n✔\n✔\nThe total exposure (in weeks) of the subjects on each arm in each control hazard model time segment.\n\n\nSeed1, Seed2\n2\n✔\n✔\nThe random number seeds at the start of the interim.\nDue to a change in the random number generator to one that uses seeds far larger than two 32-bit integers these values are not currently being written out.\n\n\nDR Param &lt;param&gt;\n4\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled α1, α2, .. the subscripts corresponding to the column their value appears in here.\n\n\nSd DR Param &lt;param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nMean Lambda &lt;seg&gt;\nS\n✔\n✔\nThe mean estimate of the hazard rate on the control arm, for each time segment of the analysis model.\n\n\nSd Lambda &lt;seg&gt;\nS\n✔\n✔\nThe standard deviation of the estimate of the hazard rate on the control arm, for each time segment of the analysis model.\n\n\nTrue Lambda\nS\n✔\n✔\n\n\n\nNum Events &lt;dose&gt;\nD\n✔\n✔\nThe number of events observed on each treatment arm.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns are grouped in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#contents-of-simulations_freq.csv",
    "href": "documentation/v72/userguides/core/simulation/tte.html#contents-of-simulations_freq.csv",
    "title": "Time to Event Output",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\nContents of the simulations_freq.csv file for a time-to-event simulation.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nLR Stat &lt;dose&gt;\nD\nUnadjusted log-rank test statistic per arm\n\n\nLR pval &lt;dose&gt;\nD\nThe unadjusted log-rank p-value per arm\n\n\nLR adj_pval &lt;dose&gt;\nD\nThe Bonferroni adjusted log-rank p-value per arm\n\n\nWilcoxon stat &lt;dose&gt;\nD\nThe Wilcoxon test statistic per arm\n\n\nWilcoxon pval &lt;dose&gt;\nD\nThe unadjusted Wilcoxon p-value per arm\n\n\nWilcoxon adj_pval &lt;dose&gt;\nD\nThe Bonferroni adjusted Wilcoxon p-value per arm\n\n\nHR &lt;dose&gt;\nD\nThe Cox model hazard ratio per arm\n\n\nHR pval &lt;dose&gt;\nD\nThe unadjusted Cox model p-value per arm\n\n\nHR lower CI &lt;dose&gt;\nD\nThe unadjusted lower bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nHR upper CI &lt;dose&gt;\nD\nThe unadjusted upper bound of the alpha confideneeeddce interval of the hazard ratio estimate.\n\n\nHR adj pval &lt;dose&gt;\nD\nThe Bonferroni adjusted Cox model p-value per arm\n\n\nHR adj lower CI &lt;dose&gt;\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nHR adj upper CI &lt;dose&gt;\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nKM med &lt;dose&gt;\nD\nThe Kaplan Meier estimates of the median survival time per arm.\n\n\nPredictor Cols\nD\nThe appropriate output columns for the chosen type of predictor. See next tables for specifics of columns provided for each predictor type.\n\n\n\n\nContinuous predictor\n\nExtra columns on the simulations_freq.csv file when using a continuous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox HR &lt;dose&gt;\nD\nThe Cox model Hazard Ratio per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pred\n1\nThe Cox model predictor coefficient\n\n\nPredictor Cox HR pval &lt;dose&gt;\nD\nThe Cox model p-value per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pval pred\n1\nThe Cox model predictor p-value\n\n\nPredictor Cox HR lower CI &lt;dose&gt;\nD\nThe Cox model lower bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR lower CI pred\n1\nThe Cox model lower bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Cox HR upper CI &lt;dose&gt;\nD\nThe Cox model upper bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR upper CI pred\n1\nThe Cox model upper bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Min &lt;dose&gt;\nD\nThe minimum value of the predictor value for each treatment arm.\n\n\nPredictor 10-percentile &lt;dose&gt;\nD\nThe 10th percentile value of the predictor value for each treatment arm.\n\n\nPredictor 25-percentile &lt;dose&gt;\nD\nThe 25th percentile value of the predictor value for each treatment arm.\n\n\nPredictor Median &lt;dose&gt;\nD\nThe mediam value of the predictor value for each treatment arm.\n\n\nPredictor 75-percentile &lt;dose&gt;\nD\nThe 75th percentile value of the predictor value for each treatment arm.\n\n\nPredictor 90-percentile &lt;dose&gt;\nD\nThe 90th percentile value of the predictor value for each treatment arm.\n\n\nPredictor Max &lt;dose&gt;\nD\nThe maximum value of the predictor value for each treatment arm.\n\n\nPredictor Mean &lt;dose&gt;\nD\nThe mean of the predictor value for each treatment arm\n\n\nPredictor SD &lt;dose&gt;\nD\nThe SD of the mean of the predictor value for each arm\n\n\n\n\n\nDichotomous predictor\n\nExtra columns on the simulations_freq.csv file when using a dichotomous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox HR &lt;dose&gt;\nD\nThe Cox model Hazard Ratio per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pred\n1\nThe Cox model predictor coefficient\n\n\nPredictor Cox HR pval &lt;dose&gt;\nD\nThe Cox model p-value per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pval pred\n1\nThe Cox model predictor p-value\n\n\nPredictor Cox HR lower CI &lt;dose&gt;\nD\nThe Cox model lower bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR lower CI pred\n1\nThe Cox model lower bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Cox HR upper CI &lt;dose&gt;\nD\nThe Cox model upper bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR upper CI pred\n1\nThe Cox model upper bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Response Rate &lt;dose&gt;\nD\nThe predictor response rate per arm.\n\n\n\n\n\nTime-to-event Predictor\n\nExtra columns on the simulations_freq.csv file when using a time-to-event predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor KM Median &lt;dose&gt;\nD\nThe Kaplan Meier estimate of the median time to the predictor event per arm"
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\nThe patients file output when simulating a time-to-event tria..\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nDuration\n1\nThe time of observation of the subject (in weeks)\n\n\nOutcome\n1\nWhether an event was observed (1) or not (0)\n\n\nPredictor\n1\nThe value of the observed predictor\n\n\nPred Outcome\n1\nA flag indicating whether the predictor was observed or not\n\n\nDropout\n1\nA flag indicating whether the subject dropped out (1) or not (0)."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nMCMC file format for a time-to-event simulation.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nHR &lt;dose&gt;\nD\nThe estimate of the hazard ratio for each dose, based on the dose response model fitted.\n\n\nLambda &lt;seg&gt;\nS\nThe estimate of the control hazard rate in each segment of the control hazard rate model\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nPredictor …\n\nThe parameters of the predictor endpoint, these vary by the type of endpoint that the predictor has – they will be consistent with the parameters that would be output if the predictor was the only endpoint in a single endpoint design."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/ordinal.html",
    "href": "documentation/v72/userguides/core/vsr/ordinal.html",
    "title": "Virtual Subject Response - Ordinal Endpoint",
    "section": "",
    "text": "In FACTS Core with an ordinal endpoint there are 2 different ways to specify the virtual subject response:"
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/ordinal.html#dose-response",
    "href": "documentation/v72/userguides/core/vsr/ordinal.html#dose-response",
    "title": "Virtual Subject Response - Ordinal Endpoint",
    "section": "Dose Response",
    "text": "Dose Response\n\nDose response profiles can be added and deleted. For each profile the user may use either of these specifications:\n\nThe outcome probabilities for each treatment arm (if “Use proportional odds” is unchecked). FACTS allows the user to supply unnormalized probabilities (for example, counts) and will convert the input into a vector of probabilities that sum to one.\nThe outcome probabilities for the control arm (which can be entered unnormalized), and an odds ratio for each other arm. If the normalized probabilities for the control arm are denoted by \\((p^0_1,p^0_2,\\ldots,p^0_K)\\) and the odds ratio for the \\(d^{th}\\) arm is \\(\\exp(\\theta_d)\\), then the probabilities for the \\(d^{th}\\) arm are given by \\[\\mbox{logit}\\left(\\sum_{j=1}^k p^d_j\\right)=\\mbox{logit}\\left(\\sum_{j=1}^k p^0_j\\right) - \\theta_d\\] for \\(k=1,2,\\ldots,K-1\\).\n\nIn addition, FACTS provides a check box that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/index.html",
    "href": "documentation/v72/userguides/core/vsr/index.html",
    "title": "Virtual Subject Response",
    "section": "",
    "text": "The Virtual Subject Response (VSR) tab allows the user to explicitly define virtual subject response profiles that dictate the distribution that patients’ endpoints should be simulated from. VSRs can be specified explicitly by specifying the control distribution and treatment effects, or by importing externally simulated patient responses.\nIf specifying, the VSR in the “Explicitly Defined” tab you will always have the dose response VSR to specify, and if subjects have multiple visits, then a Longitudinal VSR must be specified as well. If using a continuous endpoint and simulating a baseline, then a baseline simulation VSR must be specified as well.\nA dose response VSR specifies how all arms in the study should have their final endpoint value simulated. For continuous endpoints this is the mean and standard deviation of the normal distribution. For dichotomous endpoints it is the probability of response, and for time-to-event endpoints it is the hazard rate for each arm.\nThe longitudinal VSR dictates how subject visits are correlated with the final endpoint value. Each endpoint type has different methods of simulating longitudinal correlation, which are explained in detail in the following sections.\nIt is common, and advisable, to create a variety of VSR scenarios. Each scenario is a combination of a dose response VSR, a longitudinal VSR (if it exists), and a baseline VSR (if it exists). Generally, at least one null VSR, and a set of alternative scenarios are created. In a null scenario the treatment arms have the  same efficacy For a superiority study. In a non-inferiority study the treatment arms will have an efficacy profile that lies on the control minus the non-inferiority margin.  profile as the control arm.  Alternative This term (and the term ‘null scenario’) is borrowed from hypothesis testing. It indicates that the assumed scenario belongs to the alternative space of a traditional hypothesis test.  VSRs have treatments with a variety of treatment effects, usually with treatment arms simulated to be better than control.\nIf an external file is used to specify the subject responses to be simulated, it replaces the dose response, longitudinal, and baseline profiles specified in an explicitly defined VSR tab. An entire sequence of visit responses for a subjects is drawn from the uploaded patient responses file. This is elaborated on more in each endpoint’s VSR description.\nEach endpoint type (Continuous, Dichotomous, Time-to-event, and Ordinal) has its own method for specifying the dose response VSR and the visit to visit correlation."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/tte.html",
    "href": "documentation/v72/userguides/core/vsr/tte.html",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual externally simulated patient responses. When simulations are executed, they will be executed for a specific scenario – where a scenario is a combination of one each of predictor VSR, control hazard rate, dose response VSR, accrual rate, and dropout rate. If an external file is used to specify the subject responses to be simulated, this effectively replaces the predictor, control hazard and dose response profiles in a scenario.\nUnlike other endpoints where just a response (mean change from base line or rate) is specified, specification of subject responses for a time-to-event endpoint is done by first specifying a piecewise exponential event rate for the control population and then hazard ratios for the treatment arms. For simplicity, this means of specifying the simulated event rates is also used when no control arm is present and the comparison is with historic control rates.\nIn FACTS Core TTE, there is also the ability to include the simulation of a ‘predictor’ endpoint. Predictor endpoints can be a continuous measure, dichotomous outcome, or a precursor event. The interface for specifying how the predictor endpoint data is to be simulated is different in each case."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/tte.html#explicitly-defined",
    "href": "documentation/v72/userguides/core/vsr/tte.html#explicitly-defined",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nWith no predictor, there are two screens for defining virtual subject responses; the first is used to define the control hazard rate and the second to define the hazard ratio of the events on each arm to control.\n\nControl Hazard Rates\nThe user may create a number of different control hazard rate profiles to simulate from. The different profiles will be used to define different simulation scenarios in combination with other profiles defining the other properties that have to be simulated.\nThe hazard rate to simulate in a profile is specified as a piecewise exponential. The follow-up time can be divided into different time segments and a different event rate simulated in each segment.\nSegment boundaries and hazard rates are always entered using “weeks” as the time unit. While this might not always be the most convenient, it allows FACTS to use the same time unit everywhere.\nDifferent segments in the follow-up period are specified by adding segment ends. Adding a ‘segment end’ adds a segment interval to the list to allow the event rate for that interval to be specified. To delete an interval, select the interval starting with the breakpoint to be deleted and click the “Delete” button.\nThe graph can show the hazard rate, the cumulative probability of not having an event or the probability of not having seen an event over time, using the event rates specified. This is useful for checking that the segments and event rates have been entered correctly.\n\n\n\n\n\n\nFigure 1: The tab for specifying the control arm hazard rate for an explicitly defined VSR.\n\n\n\n\n\nDose Response\nThe user may create a number of different dose response profiles to simulate from. The different profiles will be used to define different simulation scenarios in combination with other profiles defining the other properties that have to be simulated.\nWithin each profile the user specifies:\n\nThe hazard ratio compared to control for each treatment arm (except control itself, where of course the ratio is 1).\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\n\nOn the graph the different hazard ratios for the profile are plotted, along with the ‘target’ – this is the default CSHRD or NIHRD offset from the QOI tab, the direction of the offset is dependent on whether events are ‘good’ or ‘bad’ and whether the aim of the trial is to show superiority on non-inferiority.\n\n\n\n\n\n\nFigure 2: Specify the hazard ratio per dose\n\n\n\n\n\nLoading Scenario Control Hazard Rates and Scenario Hazard Ratios from file\nIf the “Load scenario hazard rates from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses across the simulations.\nEach individual simulation uses one set of responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\nFor a TTE endpoint the user must supply 2 files – one for the Control Hazard Rates and one for the Hazard Ratios in each group. MVSR hazard rates are only combined with MVSR control hazard rates. The lines from each file are paired up for each simulation, so the first control hazard rate is used with the first dose response hazard ratio, the second control hazard rate is used with the second dose response hazard ratio and so on. There must be the same number of lines in each file.\n\n\n\n\n\n\nFigure 3: Load a scenario’s control hazard rates from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the different control hazard rates over time.\n\n\n\n\n\n\nFigure 4: Load a scenario’s treatment hazard ratios from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the different individual hazard ratios for each dose.\nThe VSR parameters are provided in two separate files, (the number of lines in the files should be the same for the two files). The formats are:\n\nControl Hazard Rate File: Each line should contain columns [L1, L2, … , LS] giving the true control hazard rates (\\(\\lambda\\)) for each of the S segments. (Note: FACTS will treat the hazard rate as per week).:\nHazard Ratio File: Each line should contain columns [HR1, HR2, … , HRD] giving the true Mean Hazard Ratios for each of the D dose arms. (Note: HR1 = 1 by definition, but the column of 1’s to be included here for completeness.)\n\nThe use of MVSR files has not been extended to the case where a predictor is being used."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/tte.html#external-files",
    "href": "documentation/v72/userguides/core/vsr/tte.html#external-files",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External Files",
    "text": "External Files\nAs well as simulating subject responses within FACTS they can be simulated externally and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data can be done from the External Files sub-tab depicted in Figure 5 below.\nTo import an external file, the user must first add a profile to the table. After adding the profile, the user must click “Browse” to locate the file of externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 5: The external files sub-tab for uploading patient data to be sampled from.\n\n\n\n\nRequired Format of Externally Simulated Data\nThe supplied data should have the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nUncensored time to event in weeks\nThis column is a placeholder for predictor data. It may be filled out or empty when there is no predictor. It will be ignored.\n\nThe GUI requires that the file name has a “.dat” suffix. The file need not have column headers, but if it does the first column name must start with a pound sign (#) which tells FACTS to ignore that row.\nThe following shows values from an example file with a dichotomous predictor. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n\n\n\n\n\n\n\n\n\n#Patient ID\nDose Index\nUncensored Time to Event (weeks)\nOptional Predictor value\n\n\n\n\n1\n1\n8.87\n0\n\n\n2\n1\n9.34\n0\n\n\n3\n2\n6.78\n-9999\n\n\n4\n2\n10.23\n0\n\n\n5\n2\n9.96\n0\n\n\n6\n2\n5.6\n1\n\n\n7\n1\n37.01\n20.36\n\n\n8\n1\n28.67\n0\n\n\n9\n1\n39.70\n0\n\n\n\nIn the above, column headers have been included to make it clearer to read but they are not required.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/tte.html#explicitly-defined-event-rate-predictor",
    "href": "documentation/v72/userguides/core/vsr/tte.html#explicitly-defined-event-rate-predictor",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined (Event Rate | Predictor)",
    "text": "Explicitly Defined (Event Rate | Predictor)\nThe first method (described in this section) may in some circumstances be the more natural way to think of the data (we might want to simulate for instance that subjects with a reduction in tumor size have a longer survival time), however it gives rise to data which does not have an exponential distribution. To retain an exponential distribution in the simulated event rate it is necessary to use the second method where the control rate and treatment arm hazard ratios are specified first and then the probability of the observed predictor derived from that (described in the next section).\n\nContinuous Predictor\nIf a predictor with a continuous endpoint is being used then there is a new tab in the VSR section to allow the specification of profiles that define how the predictor endpoint is to be simulated.\n\nPredictor\nFirstly, the predictor endpoint to be simulated is specified in the same way as a FACTS Core continuous endpoint. A number of profiles can be specified, in each one the mean change from baseline of the predictor for each treatment arm is specified, along with the variability in the change. The variability is specified as the SD of a Normal error in the observed change, a single value can be specified for all arms, or separate values for each arm.\nThe mean hazard rate for the simulated subjects now depends on the value of the predictor \\(Z\\), the baseline hazard rate for the time segment \\(\\mu_{s}\\), the log hazard ratio for the dose \\(\\theta_{d}\\), and with parameters \\(b_{Zd}\\), center and scale for the predictor:\n\\[h_{sdZ} = \\mu_{s}exp\\left( \\theta_{d} \\right)\\exp\\left( b_{Zd}\\frac{Z - center}{scale} \\right)\\]\nNote that in this form of the predictor, the observed event rate is affected by the predictor, it will not be the same as specified in the dose-response profile, also the observed times to event will not be exponentially distributed. The impact of this is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a control hazard rate of 0.01, and dose response hazard ratio of 1, when a continuous predictor is added with SD 2, and mean response of 1 on control and 3 on the treatment arm, with center specified at 2, scale at 3 and \\(b_{z}\\) at 0.2, simulations of 1000 subjects’ times-to-events on each of control and treatment arm give (ignoring censoring) a HR of between 1.05 and 1.21.\nNote that if events are bad and higher predictor scores indicate subject improvement (lower hazard rate) then the \\(b_{z}\\) coefficients need to be negative. The same is true if events are good and lower predictor scores indicate subject improvement.\n\n\n\n\n\n\nFigure 6: Specifying the continuous predictor’s VSR and how it effects the hazard rate for an arm.\n\n\n\nOn the graph showing the predictor response to be simulated, the ‘target’ line shows the offset of the predictor CSD from the predictor response on the Control arm.\n\n\nControl Hazard Rates\nWith a continuous predictor, the control hazard rate tab does not change. It’s the same as the non-predictor tab.\n\n\nDose Response\nThe manner of specifying the dose response does not change – multiple profiles may be specified and in each profile the hazard ratio for each treatment arm is specified. However, the overall hazard ratio simulated will depend on the combination of the control hazard rate, the dose response hazard ratio and the effect of the predictor on the final event rate.\nThis leads to two different ways for the predictor to be brought into the simulation. First, the hazard ratios \\(\\theta_{d}\\) may all be 1, while the mean of the predictor may change across doses. This would indicate that dose makes no difference given a fixed value of the predictor, but that the different doses achieve their effect by changing the distribution of the predictor values themselves. If the \\(\\theta_{d}\\) values differ from 1, then this indicates that there is a dose effect even conditional on a fixed value of the predictor (thus, for example, a control subject with Z=1 has a different hazard than a treatment subject with Z=1).\nThe resulting hazard ratio is plotted in the graph at the bottom of the tab. The predictor profile and control hazard rate profile to use in generating the graph can be selected in the controls to the right of the graph.\n\n\n\n\n\n\nFigure 7: Specify the hazard ratio per arm. The true simulated hazard ratio will be a combination of this value and the values provided in the predictor tab.\n\n\n\n\n\n\nDichotomous Predictor\nWhen simulating the event rate conditional on the predictor, the predictor endpoint to be simulated is specified in the same way as a for a simple dichotomous endpoint, the control hazard rate is specified separately for each dichotomous predictor value, and the Dose Response for the time-to-event endpoint has a hazard ratio that depends on the dichotomous predictor’s value.\nSo, the effect of the predictor on the background event rate is seen on the control hazard rate tab, and on the treatment arm hazard ratios on the dose response tab.\nOn those tabs the user specifies\n\nseparate control hazard rates for subjects who have a predictor response and those who do not\nseparate hazard ratios for each dose for subjects who have a predictor response and those who do not.\n\nThe hazard rates and ratios apply from the moment a subject is recruited (they do not change after the dichotomous predictor is assessed) and do not depend on the predictor being observed (which could be prevented if the event happens first).\n\n\n\n\n\n\nFigure 8: Specifying the dichotomous predictor’s probability of response for each arm.\n\n\n\nThe graph shows the specified response rate for each treatment arm and the target rate on control plus CSD.\n\nControl Hazard Rate with Dichotomous Predictor\n\n\n\n\n\n\nFigure 9: Specify the control arm hazard rate for each potential value of the dichotomous predictor.\n\n\n\nAs usual, multiple control hazard rate profiles can be created and the hazard rate on the control arm specified over different time segments. What differs from the case where there is no predictor is that, if a dichotomous predictor (with event rate simulated dependent on the predictor) is being used, separate hazard rates are specified for subjects depending on whether or not they will have the dichotomous predictor response or not.\n\n\nDose Response with a Dichotomous Predictor\n\n\n\n\n\n\nFigure 10: Specify the hazard ratio for active arms given the value of the dichotomous predictor.\n\n\n\nAs usual, multiple dose response profiles can be created, and in each profile the hazard ratio to simulate for each treatment arm compared to the control arm is specified. What differs from the case where there is no predictor, is that, if a dichotomous predictor is being used, separate hazard ratios are specified for subjects depending on whether or not they will have the dichotomous predictor response or not.\nAs with the continuous predictor, the observed event rate is affected by the predictor (it will not be the same as specified in the dose-response profile). Similarly, the observed times to event will not be exponentially distributed. The graph shows the effective combined hazard ratio for a given combination of control hazard rate, predictor rates and dose response. The controls for selecting which control hazard rate and which predictor rates to use are to the right of the graph.\nThe impact is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a predictor response rate on control of 0.1 and control hazard rates of 0.01 (with a predictor response) and 0.02 (with no predictor response), and treatment arm with a predictor response rate of 0.2 and hazard ratios of 0.9 (with a predictor response) and 0.8 (with no predictor response), simulations of 1000 subjects times to events on each of control and treatment arm give (ignoring censoring) a HR of between 0.75 and 0.90.\n\n\n\nTime-to-Event Predictor\n\nPredictor\nThe predictor endpoint to be simulated is specified in a similar manner to specifying the simulation of a time to event endpoint. A number of profiles can be specified; in each one the hazard rate on the control arm is specified over one, or more, time segments, and the overall hazard ratio of the time to the predictor event of each treatment arm to the control arm is specified.\nThe simulation of the time to the final event is in terms of the event rate (over one, or more, time segments) on the control arm after the predictor event, and then the hazard ratio of the time to the final event after the predictor event of each treatment arm to the control arm.\nOn those tabs the user specifies\n\nseparate control hazard rates for subjects post predictor event\nhazard ratios for each treatment arm the time to final event, after the predictor event has occurred.\n\n\n\n\n\n\n\nFigure 11: Specifying the dichotomous predictor’s control hazard rate and active arm hazard ratios for each arm.\n\n\n\nMultiple predictor profiles can be created.\nFor each profile the control hazard rate can be specified over an arbitrary set of time segments (i.e the time segments can vary from profile to profile, can be different from the observation times (if any) and different from the time segments used in the analysis model.\nThe hazard ratio of the control arm to itself has to be 1 and cannot be modified. For the other treatment arms, the time to predictor event is specified by specifying the hazard ratio on that treatment arm, to the control arm.\nThe graph can be used to show the hazard rate, probability of event or probability of not having the event on each arm.\n\n\nControl Hazard Rates\nSpecifying the simulation of the control hazard rate with a TTE predictor is the same as specifying the control hazard rate without a predictor. The difference is that with a TTE predictor, the hazard rate specified here is only simulated after the predictor has been seen.\n\n\n\n\n\n\nFigure 12: Specify the control arm’s event rate after the predictor endpoint event has occured.\n\n\n\n\n\nDose Response\n\n\n\n\n\n\nFigure 13: Specify the hazard ratio for the active arms on the hazard rate after the predictor event is observed. The primary endpoint event time can also be set to equal the predictor event time with some probability.\n\n\n\nAs usual, multiple hazard ratio profiles can be created, and in each profile the hazard ratio to simulate each treatment arm compared to the control arm specified. What differs from the case where there is no predictor is that if a TTE predictor is being used,\n\nthe hazard ratios are specified for the occurrence of the final event having observed the predictor event\na probability can be specified that the post predictor event to endpoint event time is 0.\n\nAs with the continuous predictor, the observed event rate is affected by the predictor, it will not be the same as the hazard ratio for endpoint post event predictor. The observed times to event will not be exponentially distributed. The impact is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a predictor hazard rate on control of 0.1 and post predictor hazard rates of 0.01 and probability that the post predictor event time is 0 of 0.1, and treatment arm with a predictor hazard ratio of 0.9 and post predictor hazard ratio of 0.8 and probability that the post predictor event time is 0 of 0.1, simulations of 1000 subjects times to events on each of control and treatment arm give (ignoring censoring) a HR of between 0.74 and 0.88."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/tte.html#explicitly-defined-predictor-event-rate",
    "href": "documentation/v72/userguides/core/vsr/tte.html#explicitly-defined-predictor-event-rate",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined (Predictor | Event Rate)",
    "text": "Explicitly Defined (Predictor | Event Rate)\nTo simulate event times and an associated predictor with a possible correlation between the two in a way that preserves the exponential distribution of the event times, use this tab to specify the simulation of subjects’ time to event first and then the probability of the observed predictor derived from that. This is only supported for Dichotomous and TTE predictors.\nThe “Explicitly Defined (Predictor | Event Rate)” tab allows the specification of profiles that define how the predictor endpoint is to be simulated in relation to the control hazard rate and hazard ratios for the final events. This allows the simulated final observed final events to still have an exponential distribution.\nOnce the time-to-event for a subjects has been simulated, a simple user specified transformation of the time-to-final-event provides the expected value of the predictor’s distributions.\n\nDichotomous\nWhen using a dichotomous predictor, control hazard rates and dose response hazard ratios are specified as when there is no predictor.\n\nPredictor\n\n\n\n\n\n\nFigure 14: Specify the distribution of the predictor per arm given the uncensored endpoint value for a patient.\n\n\n\nAs usual, multiple profiles can be defined. To simulate the dichotomous endpoint given the event rate, the predictor values are simulated by drawing from the Bernoulli distribution with probability given by the inverse logit(\\(\\alpha + \\beta Y\\)), where \\(\\alpha\\) and \\(\\beta\\) are specified here and \\(Y\\) is the subject’s final time to event (in weeks). A single set of values for \\(\\alpha\\) and \\(\\beta\\) can be specified, or separate values per treatment arm can be specified. The expected response rate is shown in the plot at the bottom of the predictor tab.\n\n\n\nTime-to-Event\nWhen simulating a time-to-event predictor, control hazard rates and dose response hazard ratios are specified as when there is no predictor.\n\nPredictor\n\n\n\n\n\n\nFigure 15: Specify the distribution of the predictor event per arm given the uncensored endpoint value for a patient.\n\n\n\nMultiple profiles can be defined. To simulate predictor event endpoint given the event rate of the primary event, the predictor values are simulated by drawing from the Exponential distribution with rate given by (\\(\\lambda_{z}\\exp(\\ \\beta Y)\\)), where \\(\\lambda_z\\) and \\(\\beta\\) are specified here and \\(Y\\) is the subject’s time to event (in weeks). A single set of values for \\(\\lambda_z\\) and \\(\\beta\\) can be specified, or separate values per treatment arm can be specified.\nThe arm specific hazard ratios of the predictor given the final endpoint event rate of a specified scenario is shown in the plot at the bottom of the predictor tab. The final endpoint scenario can be changed using the dropdown boxes the right of the figure."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/tte.html#external",
    "href": "documentation/v72/userguides/core/vsr/tte.html#external",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subject responses with predictors within FACTS they can be simulated externally and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below (Figure 6‑13).\nTo import an external file, the user must first add a profile to the table. After adding the profile, the user must click “Browse” to locate the file of externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 16: The external files sub-tab for uploading patient data to be sampled from.\n\n\n\n\nRequired Format of Externally Simulated Detail\nThe supplied data should have the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nUncensored time to event in weeks\nUncensored Predictor value, one of the following depending on the predictor type:\n\nContinuous: “NN.NN” change from baseline\nDichotomous: 0 for no response, 1 for response\nTTE: uncensored time to event in weeks\n\n\nThe GUI requires that the file name has a “.dat” suffix. The file need not have column headers, but if it does the first column name must start with a pound sign (#).\nThe following shows values from an example file with a dichotomous predictor. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n\n\n\n\n\n\n\n\n\n#Patient ID\nDose Index\nUncensored Time to Event (weeks)\nUncensored Predictor value\n\n\n\n\n1\n1\n8.87\n0\n\n\n2\n1\n9.34\n0\n\n\n3\n2\n6.78\n1\n\n\n4\n2\n10.23\n0\n\n\n5\n2\n9.96\n0\n\n\n6\n2\n5.6\n1\n\n\n7\n1\n37.01\n0\n\n\n8\n1\n28.67\n0\n\n\n9\n1\n39.70\n0\n\n\n\nIn the above, column headers have been included to make it clearer to read but they are not required.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/dichotomous.html",
    "href": "documentation/v72/userguides/core/longitudinalmodels/dichotomous.html",
    "title": "Longitudinal Models for Dichotomous Endpoints",
    "section": "",
    "text": "LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {\\(y_{it}\\)} is the set of observed responses from early visits, and \\(y_{i t_m}\\) is the last observed value of \\(y_{it}\\), then the LOCF model for the final endpoint \\(Y_i\\) is\n\\[Y_i \\mid \\{y_{it}\\} = y_{i t_m}\\]\n\n\nBeta Binomial\nThe Beta Binomial longitudinal model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(\\pi_{t y_{it}})\\]\nwhere \\(\\pi_{t y_{it}}\\) is the probability that a patient is a response at the final endpoint given its early observed endpoint at time \\(t\\) is \\(y_{it}\\),\n\\[\\pi_{t y_{it}} = \\Pr(Y_i = 1 \\mid y_{it}) \\sim \\text{Beta}(\\alpha_{t {y_it}}, \\beta_{t y_{it}})\\]\nWe use the set cardinality operator \\(\\mid \\ldots \\mid\\) to obtain the posterior distributions of \\(\\alpha_t\\) and \\(\\beta_t\\) as:\n\\[\\alpha_{t0} = \\alpha_{\\mu 0} + \\left| Y_i = 1, y_{it} = 0 \\right| \\] \\[\\alpha_{t1} = \\alpha_{\\mu 1} + \\left| Y_i = 0, y_{it} = 0 \\right| \\] \\[\\beta_{t0} = \\beta_{\\mu 0} + \\left| Y_i = 1, y_{it} = 1 \\right| \\] \\[\\beta_{t1} = \\beta_{\\mu 1} + \\left| Y_i = 0, y_{it} = 1 \\right| \\]\ni.e. a prior value \\((\\alpha_{\\mu 0}, \\alpha_{\\mu 1}, \\beta_{\\mu 0}, \\beta_{\\mu 1})\\) plus the number of subjects for which the final response is known to be 1 for \\(\\alpha_{tx}\\) (or 0 for \\(\\beta_{tx}\\)) and the response at time \\(t\\) is \\(x\\).\nThe \\(\\alpha_{tx}\\) and \\(\\beta_{tx}\\) parameters are independently estimated using only patients in their model instance, and may or not have identical priors \\(\\alpha_{\\mu *}\\) and \\(\\beta_{\\mu *}\\) depending on the Model Priors selection in FACTS. A common non-informative prior for the \\(\\pi_{t0}\\) and \\(\\pi_{t1}\\) parameters is \\(\\text{Beta}(1,1)\\).\n\n\nLogistic regression\nThe Logistic regression longitudinal model works similarly to the Beta Binomial imputation model. The difference is in the method of modeling the probability of a transition from an interim visit to the final visit \\(\\Pr(Y_i = 1 \\mid y_{it})\\). Like the Beta Binomial model, the logistic regression model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(\\pi_{t y_{it}})\\]\nwhere \\(\\pi_{t y_{it}}\\) is the probability of a response at the final endpoint time given that its early observed endpoint at time \\(t\\)$ is \\(y_{it}\\). Then, we define the parameter\n\\[\\theta_{ty_{it}} = \\text{logit}\\left( \\pi_{ty_{it}} \\right) = \\log\\left( \\frac{\\pi_{ty_{it}}}{1 - \\pi_{ty_{it}}} \\right)\\].\nThe priors on \\(\\theta_{t0}\\) and \\(\\theta{t1}\\) are:\n\\[\\theta_{t0} \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\] \\[\\theta_{t1} \\sim \\text{N}(\\mu_1, \\sigma_1^2)\\]\nThe model computes the posterior distribution of \\(\\theta_{t0}\\) and \\(\\theta_{t1}\\) using all patients on arms belonging to the model instance that have observed endpoint values at time \\(t\\) and the final endpoint time \\(T\\).\nThe priors on \\(\\theta_{t0}\\) and \\(\\theta_{t1}\\) may be shared across model instances and/or visits depending on the selection made in the Model Priors section of the FACTS Logistic regression Longitudinal model page.\nA possible starting place for non-informative priors in this model would be: \\(\\mu=0\\), \\(\\sigma=2\\). A weakly informative set of priors that an early response makes a final response more likely could be \\[\\theta_{t0} \\sim \\text{N}(-.75, 1.25^2)\\] and \\[\\theta_{t1} \\sim \\text{N}(0.75, 1.25^2)\\]\n\n\nRestricted Markov Model (Absorbing Markov Chain)\n\n\n\n\n\n\nUsing the Restricted Markov Model\n\n\n\nThe restricted markov model is special in the sense that it can be used if and only if the “Use longitudinal modeling” check box is checked, the “Enable Special Longitudinal Options” check box is checked, and “Use restriced Markov model” is selected. When these conditions are met the Virtual Subject Response tab changes and the Design &gt; Longitudinal tab only has the Restricted Markov option.\n\n\nThe Absorbing Markov Chain model assumes that at each visit a subject is in one of three states – responder (1), stable (S), or failure (0). The responder and failure states are absorbing, meaning that once a patient has entered one of those states they must remain in that state for the remainder of the visits. Patients in the stable state may move to a responder or a failure in subsequent visits.\nUnlike most other longitudinal models in FACTS, the Restricted Markov Model estimates the probability of a result at a visit based on the visit right before it, rather than predicting directly to the final endpoint from the early visit.\n\\[\\Pr(y_{it} = n \\mid y_{i, t-1} = S) \\sim \\text{Dirichlet}(\\{\\alpha_{0,t}, \\alpha_{1,t}\\, \\alpha_{S,t}\\}) \\text{ for } t\\ge 2\\]\nWhere n can be 0, 1, or S, denoting the probability of going to the Fail state, the Responder state, or the Stable state at visit \\(t\\) from the Stable state at visit \\(t-1\\). \\(t\\) must be greater than or equal to \\(2\\), because we do not impute the first visit – a subject missing visit 1 and all visits after does not contribute to the longitudinal model or dose response model.\nThe priors for the \\(\\alpha\\) parameters are specified in terms of the prior number of transitions from Stable at \\(t-1\\) to each different state at time \\(t\\). For example, if the prior value for the parameter \\(\\alpha_{1,3}\\) is \\(2\\), we are putting apriori information into the Dirichlet distribution suggesting that \\(2\\) patients transitioned from Stable at visit 2 to Responders at visit 3. Specified priors can either be common or different across model instances based on user specification.\nThe parameters defining the posterior distribution of the state probabilities are available in closed form as:\n\\[\\alpha_{0,t} = \\gamma_{0,t} + \\left|y_{it}=0, y_{i, t-1} = S\\right|\\] \\[\\alpha_{S,t} = \\gamma_{S,t} + \\left|y_{it}=S, y_{i, t-1} = S\\right|\\] \\[\\alpha_{1,t} = \\gamma_{1,t} + \\left|y_{it}=1, y_{i, t-1} = S\\right|\\]\nTo create a dichotomous endpoint, the user specifies in the Study &gt; Study Info &gt; Design Options section whether patients remaining in a stable state at the final visit should be dichotomized as responders or failures.\n\n\nDichotomous Endpoint: Dichotomized Continuous Longitudinal Model\nThe user may select (on the Study tab) to assume that the dichotomous final endpoint is generated by observing continuous longitudinal data and then dichotomizing the final endpoint based on whether it is greater than or less than a provided threshold. If the user selects this option, then they may select any of the continuous longitudinal models specified in the Continuous Longitudinal Models section. The engine will impute incomplete subjects according to the continuous model, resulting in a continuous imputed final endpoint. The imputed dichotomous final endpoint is then simply the dichotomized version of the continuous imputation.\nAll priors and methods are identical to the continuous longitudinal models mentioned above."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/tte.html",
    "href": "documentation/v72/userguides/core/longitudinalmodels/tte.html",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "",
    "text": "In FACTS Core TTE, ‘predictor endpoints’ can be used in a similar way to early observations of the clinical endpoint measure in FACTS Core Continuous and Dichotomous. That is, they are used to inform the analysis about what the final outcome is likely to be for subjects for whom the final outcome has not  yet been observed If a subject drop’s out, their final endpoint will never be observed, but their predictors are still used to impute the subject’s possibly final time to event if the predictor is observed before the dropout. . The estimates of the predictor response can also be used in QOIs, and thus somewhat like an additional endpoint and used for decision making.\nThe dose-predictor model is used to impute subjects’ predictor responses based on their treatment allocation when their predictor response has not yet been observed. The predictor-endpoint model is used to impute their time to final endpoint when that has not yet been observed.\nThe differences between predictor endpoints in FACTS Core TTE and early endpoints with longitudinal modeling in FACTS Core Continuous and Dichotomous are that:\nFor all predictor responses (\\(Z\\)) for time-to-event endpoints, the engine estimates both a marginal predictor distribution (normal mean and variance for continuous, probability of response for dichotomous, or a piecewise exponential hazard model for time to event predictors), and a working model relating the predictor to the final endpoint. The marginal predictor distribution is used to impute predictors for subjects lacking an observed predictor value, and may also be used for stopping (see section on stopping). The working model is used to impute final event times for subjects lacking a final endpoint. For subject missing both a predictor and final endpoint, the predictor is imputed first from its marginal distribution, and then the final endpoint is imputed conditionally on the predictor \\(Z\\)."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/tte.html#continuous-dichotomous-predictors",
    "href": "documentation/v72/userguides/core/longitudinalmodels/tte.html#continuous-dichotomous-predictors",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Continuous & Dichotomous Predictors",
    "text": "Continuous & Dichotomous Predictors\nFor the predictor model with a continuous or dichotomous predictor, there are two tabs within the Design &gt; Predictor Model tab – the Dose Response and the Relationship to Endpoint tabs.\nThe dose response model for the predictor is the model used to estimate the marginal distribution of the predictor response at each dose as a normal mean and variance for a continuous endpoint or mean and variance of the log-odds for a dichotomous endpoint. The dose response options are as per the continuous and dichotomous endpoint standard dose response options (except that the use of BAC for the predictor is not supported). The predictor dose-response model can be selected and specified completely independently of the dose response model for the primary time-to-event endpoint (the model of the log hazard ratio).\n\n\n\n\n\n\nFigure 1: The dichotomous predictor model showing a Simple NDLM dose response. The NDLM model for the predictor response works exactly like the NDLM dose response model.\n\n\n\n\nContinuous Predictor\nWithin each dose (including control and active comparator), the marginal distribution of the predictor \\(Z\\) is a normal distribution with mean \\(\\theta_{Zd}\\) and standard deviation \\(\\sigma_Z\\). The standard deviation is common across the doses, but the means \\(\\theta_{Zd}\\) are allowed to vary across the same range of doses based on the predictor’s dose response model. The dose response for the predictor does not need to match the dose response for the final endpoint, and the two models are estimated independently of eachother.\n\n\nDichotomous Predictor\nA dichotomous predictor is handled similarly to a continuous predictor, with the predictor’s marginal distribution having its own dose response model. Like in the typical dose response model on a dichotomous endpoint when it is the primary endpoint, the dose response model estimates the log-odds of the response rate for each dose."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor",
    "href": "documentation/v72/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Time-to-Event Predictor",
    "text": "Time-to-Event Predictor\nFor the predictor model with a time-to-event predictor, there are three tabs within the Design &gt; Predictor Model tab – the Hazard Model, Dose Response, and the Relationship to Endpoint tabs.\nAs in the response model for the primary event, the response model for a time to event predictor comprises a hazard model of the predictor for the event rate on the control arm, and a dose response model used to estimate the marginal distribution of the non-control dose predictor response for each dose as a normal log-hazard ratio.\nThe hazard model and dose response options for the predictor endpoint are similar to the hazard model and dose response options for the primary time-to-event endpoint, except that the use of the cox model or BAC for the predictor hazard model is not supported. The predictor dose-response model is selected, specified, and estimated completely independently of the dose response model for the time-to-event endpoint."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/tte.html#continuous-and-dichotomous-predictors",
    "href": "documentation/v72/userguides/core/longitudinalmodels/tte.html#continuous-and-dichotomous-predictors",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Continuous and Dichotomous Predictors",
    "text": "Continuous and Dichotomous Predictors\nThe relationship to endpoint model is:\n\n\\(T_{i} \\sim \\text{Exp}(\\lambda_{d}\\ e^{\\beta Z_{i}})\\) for a continuous predictor where \\(Z_i\\), is the observed value of the predictor for the \\(i\\)th subject\n\\(T_{i} \\sim \\text{Exp}(\\lambda_{d}\\ e^{\\beta Z_{i}})\\) for a dichotomous predictor where \\(Z_i=0\\) or \\(1\\) is the observed dichotomous predictor for the ith subject\n\nThat is, for the ith subject, the time to their event is taken to follow an exponential distribution with mean \\(\\lambda_d\\ e^{\\beta Z_i}\\).\n\n\n\n\n\n\nFigure 2: The Relationship to Endpoint tab for a continuous or dichotomous predictor.\n\n\n\nFor both continuous and dichotomous predictors, the priors on the \\(\\beta\\) and \\(\\lambda_d\\) parameters are:\n\\[ \\lambda_d \\sim \\text{Gamma}(\\alpha_d, \\beta_d) \\]\n\\[  $\\beta \\sim \\text{N}(m, s)\\]\n\\(\\lambda_d\\) is esimated independently for all doses. The coefficient \\(\\beta\\) does not have a subscript, because it is constant across all doses.\nFor the dichotomous predictor, we can simplify the notation of the relationship to endpoint model since \\(Z\\) can only be two different values (0 or 1).\n\\[T\\mid (Z=0) \\sim \\text{Exp}(\\lambda_d)\\] and \\[T \\mid (Z=1) \\sim Exp(\\lambda_d e^{\\beta})\\].\n\n\n\n\n\n\nNotes on priors for continuous/dichotomous relationshipt to endpoint models\n\n\n\nFor a continuous predictor, \\(\\lambda_d\\) is the dose dependent hazard rate when the predictor is 0. For a dichotomous predictor \\(\\lambda_d\\) is the dose dependent hazard rate when the predictor is 0. The \\(\\lambda\\)s have independent gamma priors for each dose, specified by their expected mean and a weight in equivalent number of events seen. Thus, a weight of 1 would be very weakly informative with any normal number of events. A weight of 0.1 would be uninformative with even a small number of events. A way to think about the use of a weight of \\(&gt;1\\) is, if weight of \\(n&gt;1\\) is used, then after \\(n\\) actual events are observed the posterior mean estimate of \\(\\lambda\\) will be the average of the observed mean and the prior mean.\nThe \\(\\beta\\) parameter has a normal prior, specified by its prior mean and standard deviation. For a continuous predictor, this is the log scaling factor of the hazard rate for the correlation of the event rate to the predictor. For a dichotomous predictor, it is the log scaling factor of the hazard rate for subjects who have had response on the predictor. For a continuous predictor with the center and scale values set so that the value of \\(Z\\) will vary approximately between \\(-1\\) and \\(1\\), and a dichotomous predictor where the predictor value is \\(0\\) or \\(1\\), a prior distribution of \\(N(0,5)\\) for \\(\\beta\\) would mean that \\(e^{\\beta Z}\\) will take values between \\(22,000^{-1}\\) to 22,000 and the prior could be deemed to be essentially uninformative. Large values (&gt;&gt;5) for the SD of the prior for \\(\\beta\\) can lead to numeric overflow in the simulator. If the values of Z do not largely fall in the range \\([-1, 1]\\), it is suggested that the prior for \\(\\beta\\) be constructed to limit the coefficient \\(\\exp{(Z\\beta)}\\) to lie within its plausible range. If the prior for \\(\\beta\\) is left uninformative and \\(Z\\) can take values \\(&gt;&gt; 1\\) it can result in inflation in the uncertainty in the estimates of the hazard ratios of subject’s time to final event from a few extreme imputed time-to-event values for subjects whose times-to-event are imputed, particularly if their predictor value is imputed too."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor-1",
    "href": "documentation/v72/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor-1",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Time-to-Event Predictor",
    "text": "Time-to-Event Predictor\nFor the time-to-event predictor, the relationship to endpoint model is simply a per-dose hazard rate for the time from the predictor event to the final event. For each dose, the user specifies the parameters of a gamma prior distribution for this ‘post-predictor event’ hazard rate. This model is a simple exponential, not piecewise.\n\n\n\n\n\n\nFigure 3: The Relationship to Endpoint tab for the time-to-event engine with a time-to-event predictor.\n\n\n\nThe time-to-event predictor is qualitatively different than the continuous or dichotomous predictors. Instead of the predictor adjusting the hazard, the time-to-event predictor is viewed as an offset. The final endpoint is viewed as a sum of a predictor time \\(Z_1\\) and a post-predictor time \\(Z_2\\), where \\(Z_1\\) and \\(Z_2\\) are independent random variables and the final endpoint is thus \\(Z_1 + Z_2\\).\nFor the relationship to endpoint model, \\(Z_1 \\sim PWExp(\\lambda_{1s}*\\theta_{1d})\\) and \\(Z_2 \\sim Exp(\\lambda_{2d})\\), with priors \\(\\theta_{1s} \\sim Gamma(\\alpha_{1s}, \\beta_{1s})\\), \\(\\theta_{2d} \\sim Gamma(\\alpha_{2d}, \\beta_{2d})\\) (with \\(Z_1\\)’s control hazard model potentially being piecewise exponential).\nFor imputation of the primary endpoint event time, a subject missing both the biomarker and final endpoint times has both \\(Z_1\\) and \\(Z_2\\) imputed, with the final endpoint imputed as the sum of the two. For a subject with a predictor time but no final endpoint, \\(Z_2\\) is imputed and added to the observed predictor time to impute the final endpoint."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/ordinal.html",
    "href": "documentation/v72/userguides/core/study/ordinal.html",
    "title": "Study Tab - Ordinal Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include accrual style,  whether interim analyses will be simulated, and more.\n\n\n\nIn the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, and whether they wish to use the cumulative logistic family of dose-response models or independent Dirichlet models for the ordinal endpoint.\n(Longitudinal modelling and include simulation of baseline are available for other FACTS endpoints but not yet for Ordinal.) These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation. \n\n\n\n\nThe trial information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuously or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a higher ordinal index indicates improving or worsening for the participant. The directions of Wilcoxon tests, Dichotomized Ordinal Tests, and Frequentist Proportional Odds models are defined by this selection. If the user has selected Cumulative Logistic modeling, selecting Higher ordinal index is improvement here implies that an odds ratio greater than 1 corresponds to an arm that is superior to control. Correspondingly, if Lower ordinal index is improvement, then an arm with an odds ratio smaller than 1 is superior to control.\nIf the user has selected Dirichlet modeling, then for the purpose of posterior probability quantities and t-tests, an arm that is superior to control is one with a large expected utility, regardless of what is selected here.\n\n\n\n\n\nThis is where the user defines the number of possible values of the ordinal endpoint. They can be set explicitly using the Add button or Auto-Generated. The user may give names to the ordinal values by clicking in the Name column and entering text.\nHere the user also enters Utility values for the possible outcomes, where large values of Utility are meant to represent positive outcomes. It is possible to create good designs in FACTS without taking the Utility values entered here seriously, but we encourage users to think carefully about which outcomes are better than others and by how much. In particular, when Dirichlet outcome modeling has been selected, all posterior probability quantities of interest are based on Expected Utility (i.e. the mathematical expectation of the Utility according to the probabilities of each outcome). When Cumulative Logistic modeling has been selected, Utilities are less critical, because posterior probability quantities of interest are based on Odds Ratios. At a minimum, the utility values entered here should agree with whether Higher or Lower ordinal index is improvement as selected in the Trial Information section."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/ordinal.html#design-options",
    "href": "documentation/v72/userguides/core/study/ordinal.html#design-options",
    "title": "Study Tab - Ordinal Endpoint",
    "section": "",
    "text": "In the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, and whether they wish to use the cumulative logistic family of dose-response models or independent Dirichlet models for the ordinal endpoint.\n(Longitudinal modelling and include simulation of baseline are available for other FACTS endpoints but not yet for Ordinal.) These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/ordinal.html#trial-information",
    "href": "documentation/v72/userguides/core/study/ordinal.html#trial-information",
    "title": "Study Tab - Ordinal Endpoint",
    "section": "",
    "text": "The trial information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuously or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a higher ordinal index indicates improving or worsening for the participant. The directions of Wilcoxon tests, Dichotomized Ordinal Tests, and Frequentist Proportional Odds models are defined by this selection. If the user has selected Cumulative Logistic modeling, selecting Higher ordinal index is improvement here implies that an odds ratio greater than 1 corresponds to an arm that is superior to control. Correspondingly, if Lower ordinal index is improvement, then an arm with an odds ratio smaller than 1 is superior to control.\nIf the user has selected Dirichlet modeling, then for the purpose of posterior probability quantities and t-tests, an arm that is superior to control is one with a large expected utility, regardless of what is selected here."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/ordinal.html#ordinal-values",
    "href": "documentation/v72/userguides/core/study/ordinal.html#ordinal-values",
    "title": "Study Tab - Ordinal Endpoint",
    "section": "",
    "text": "This is where the user defines the number of possible values of the ordinal endpoint. They can be set explicitly using the Add button or Auto-Generated. The user may give names to the ordinal values by clicking in the Name column and entering text.\nHere the user also enters Utility values for the possible outcomes, where large values of Utility are meant to represent positive outcomes. It is possible to create good designs in FACTS without taking the Utility values entered here seriously, but we encourage users to think carefully about which outcomes are better than others and by how much. In particular, when Dirichlet outcome modeling has been selected, all posterior probability quantities of interest are based on Expected Utility (i.e. the mathematical expectation of the Utility according to the probabilities of each outcome). When Cumulative Logistic modeling has been selected, Utilities are less critical, because posterior probability quantities of interest are based on Odds Ratios. At a minimum, the utility values entered here should agree with whether Higher or Lower ordinal index is improvement as selected in the Trial Information section."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/multendpt.html",
    "href": "documentation/v72/userguides/core/study/multendpt.html",
    "title": "Study Tab - Multiple Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include accrual style, visit schedule, whether interim analyses will be simulated, and more.\n\n\n\n\n\n\nFigure 1: The Study &gt; Study Info tab for a multiple endpoint trial\n\n\n\nThe study tab is simpler for multiple endpoint than it is for other FACTS Core design engines (Continuous, Dichotomous, or Time-to-event). Some options that are endpoint specific have been moved to the Endpoints tab, which only exists in the multiple endpoint engine.\n\n\nIn the design options section of the Study tab the user gets a check box for whether they want to enable adaptive features or not. Endpoint specific choices about using longitudinal modelling or special longitudinal options are moved to the Endpoints tab.\n\n\nSpecify whether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\n\nThe study information section allows the user to specify how many subjects to accrue and how subject accrual should be simulated.\n\n\nSpecify the maximum number of subjects that can be enrolled in the trial. Adaptive designs may stop sooner than this value, but no simulation can ever go past it.\n\n\n\nIn multiple endpoint, subject accrual can only be done continuously or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nThe overall visit schedule is specified here. It’s called the “overall” visit schedule in the multiple endpoint engine because the visits entered here make up the set of all visits where any of the endpoints can be observed. When the details of the different endpoints are entered on the Endpoints tab, you will be able to specify which of these visits each endpoint is observed at and which visit will be the final observation for that endpoint.\n\n\n\n\n\n\nFigure 2: The Study Tab in the Multiple Endpoint engine.\n\n\n\nVisits can be specified one at a time by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/multendpt.html#design-options",
    "href": "documentation/v72/userguides/core/study/multendpt.html#design-options",
    "title": "Study Tab - Multiple Endpoint",
    "section": "",
    "text": "In the design options section of the Study tab the user gets a check box for whether they want to enable adaptive features or not. Endpoint specific choices about using longitudinal modelling or special longitudinal options are moved to the Endpoints tab.\n\n\nSpecify whether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/multendpt.html#study-information",
    "href": "documentation/v72/userguides/core/study/multendpt.html#study-information",
    "title": "Study Tab - Multiple Endpoint",
    "section": "",
    "text": "The study information section allows the user to specify how many subjects to accrue and how subject accrual should be simulated.\n\n\nSpecify the maximum number of subjects that can be enrolled in the trial. Adaptive designs may stop sooner than this value, but no simulation can ever go past it.\n\n\n\nIn multiple endpoint, subject accrual can only be done continuously or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nThe overall visit schedule is specified here. It’s called the “overall” visit schedule in the multiple endpoint engine because the visits entered here make up the set of all visits where any of the endpoints can be observed. When the details of the different endpoints are entered on the Endpoints tab, you will be able to specify which of these visits each endpoint is observed at and which visit will be the final observation for that endpoint.\n\n\n\n\n\n\nFigure 2: The Study Tab in the Multiple Endpoint engine.\n\n\n\nVisits can be specified one at a time by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/multendpt.html#d-treatment-arm-model",
    "href": "documentation/v72/userguides/core/study/multendpt.html#d-treatment-arm-model",
    "title": "Study Tab - Multiple Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens.\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/multendpt.html#create-endpoints",
    "href": "documentation/v72/userguides/core/study/multendpt.html#create-endpoints",
    "title": "Study Tab - Multiple Endpoint",
    "section": "Create Endpoints",
    "text": "Create Endpoints\nThe small table on the middle-left side of the screen allows for the creation of up to 4 endpoints. The name of each of the endpoints can be changed in the table by clicking on the entry. The order of the non-first endpoints can be changed by clicking on an endpoint and then clicking on the up or down arrows to the right of the table.\n\n\n\n\n\n\nFigure 6: The Treatments tab with the first endpoint (U1: Pain) selected.\n\n\n\nFor each endpoint, the Endpoint Properties section allows for the endpoint specific characteristics to be supplied."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/multendpt.html#endpoint-properties",
    "href": "documentation/v72/userguides/core/study/multendpt.html#endpoint-properties",
    "title": "Study Tab - Multiple Endpoint",
    "section": "Endpoint Properties",
    "text": "Endpoint Properties\n\nContinuous Endpoints\nFor continuous endpoints, the first value provided is whether a higher response is subject improvement, or if a lower response is subject improvement.\nThen, you select whether to simulate a baseline value for subjects. If yes, then specify whether the VSR (Virtual Subject Response) will be specified as a change from the baseline value or as a standalone final endpoint value.\nNext, specify whether to use longitudinal modeling for this endpoint. If the “Use longitudinal Modeling” option is checked, then select one or more visits that subjects will observe this endpoint at. If not using longitudinal modeling, then select which visit is considered the final observed value for this endpoint. Each endpoint can have its own visit schedule, as long as those visits are included as part of the overall visit schedule on the Study Info tab.\n\n\nDichotomous Endpoints\nFor dichotomous endpoints, the first value provided is whether an observed response is a positive outcome or a negative outcome.\nNext, specify whether to use longitudinal modeling for this endpoint. If the “Use longitudinal Modeling” option is checked, then the user can decide if they would like to use the restricted markov model to simulate longitudinal data or the standard transition matrix method. If using the restricted Markov model, specify whether subjects that reach the end of their follow-up without going to either absorbing states should be considered a success or a failure. Then, select one or more visits that subjects will observe this endpoint at.\nIf not using longitudinal modeling, select which visit is considered the final observed value for this endpoint. Each endpoint can have its own visit schedule, as long as those visits are included as part of the overall visit schedule on the Study Info tab.\n\n\nUtility Function\nThe Multiple Endpoint design engine allows clinical trials to be designed and simulated where the within-trial and end-of-trial decisions can be based on multiple endpoints by using a composite score, or utility, derived by combining the different endpoint estimates. The utility function approach is incredibly open-ended and flexible, able to cope with different types of endpoints, different endpoint scales and different endpoint interrelations.\nThe utility function approach has two stages:\n\nFirst, each endpoint is converted to its own utility score for each dose.\nThen, the endpoint specific utilities are combined into a single overall utility for each dose.\n\nEach endpoint has its own utility function, as described above. Utilities are flexible piecewise functions of the estimated response for the endpoint.\nFirst use the “Add” button to add the knots of the utility function – these are the segment boundaries in the range of the endpoint measure where different functions will be specified in each segment. For each segment created by adding a “knot” a row is created in the table, allowing the user to specify the coefficients of the utility function in that segment specifically.\nThe components of the utility that can be weighted based on their coefficients are: fixed, linear, quadratic, exponential, and log terms.\nThe coefficients that can be specified are:\n\nAlpha: the coefficient of the quadratic term\nBeta: the coefficient of the linear term\nGamma: the coefficient of the fixed term\nDelta: the coefficient of the exponential term\nEpsilon: the coefficient of x in the exponential term\nPhi: the coefficient of the log term\nPsi: the coefficient of x in the log term\n\nFinally, the user specified whether the \\(x\\) in the utility function is relative to control or not. If the “Parameterize response relative to control” is checked, then \\(x=\\theta_d - \\theta_0\\) for a continuous endpoint, and \\(x = P_d - P_0\\) for a dichotomous endpoint. If the “Parameterize response relative to control” is checked, then \\(x=\\theta_d\\) for a continuous endpoint and \\(x=P_d\\) for a dichtomous endpoint.\nFor a continuous endpoint, the utility function is defined on the range of \\(x \\in (-\\infty, \\infty)\\). For dichotomous endpoints, if \\(x\\) is relative to control, then the utility is on the range \\(x \\in (-1, 1)\\), and if \\(x\\) is not relative to control, then the utility is on the range \\(x \\in (0, 1)\\).\n\n\n\n\n\n\nNote on the coefficients of the utility function\n\n\n\nIt is not the intention that all, or even most, of the available coefficients are used in any one segment, typically only one or two are. The different terms are provided so that the required form can be selected for each segment – flat linear, quadratic, exponential or log. The default values of the coefficients are set so that only the linear component of the utility contributes.\n\n\n\n\nEstimation of Utility in FACTS\nAn important aspect of the way FACTS estimates utility is that it estimates a probability density for the utility within the MCMC sampling. That is, the utility is calculated for every parameter sample within the MCMC, and the final distribution of the utility is based on those samples just like the estimates for the model parameters.\nEach dose has a distribution of utility scores - not a single value. The utility is not estimated from the mean estimates of the dose response from the different endpoints.\nThis has some notable effects on the estimates of utility. If, for example, the utility function is a step function – for example if its value is 1 for response rates below a threshold and 0 above – as the estimate of the response rate will have some uncertainty then when the mean response estimates are around the threshold value, the estimate of utility will be between 0 and 1, based on the proportion of MCMC samples the fitted response rate was below the threshold. Indeed, the utility can be interpreted as the ‘probability the response rate is below the threshold’, and thus be useful or even exactly what is required.\nSimilarly, any utility function that has a floor or ceiling will result in a bias in the estimate of the utility to be above the floor or below the ceiling. – because in the distribution of the values for the utility the lowest the value can be is the floor and the final estimate of the utility would only be at the floor if all the values for the utility sampled in the MCMC were at the floor. Another effect of utilities with a floor or ceiling is that the estimate of the mean utility has a smaller standard error the further the value of the estimate of the mean of the underlying response is from an inflexion point (or “knot”).\nThese are not errors, nor does it mean utility functions with steps, ceilings or floors should be avoided, but these artefacts need to be understood – particularly when graphs of the estimated utility and the true utility are compared.\n\n\n\n\n\n\nNote about FACTS’s utilities\n\n\n\nFACTS’ utility is based on the estimates of the response on each endpoint in each treatment arm. It is not the utility of outcome for each individual that would be a composite score and an endpoint in itself. To use that kind of utility, simulate the external subjects and their scores outside of FACTS using a program or script and calculate the composite score for each individual and write the results to a file in FACTS external virtual subject response format, this can then be used to drive simulations in FACTS of trial designs using that composite score. This might be a single endpoint (FACTS Core Continuous) design, or a Multiple Endpoint design, with the Composite score as the primary endpoint and up to 3 of the component scores as auxiliary endpoints.\nIn this latter case, the design would probably be making decisions based solely on the composite score (so the utility based criteria are unused), and FACTS Multiple Endpoint’s result summarization and charting is used to understand how when different responses are simulated on the component scores this translates into the composite score and the likely trial results.\n\n\n\n\nCalculation of Arms’ probability of having the greatest utility\nLike other probabilities in FACTS this is calculated during the MCMC sampling – the probability that an arm has the greatest probability is based on the proportion of MCMC samples when that arm had the greatest utility. It is not estimated from the utility of the mean estimates of the dose response for the different endpoints.\nIn any given sample if several arms have the same maximum toxicity, the arm with the lowest dose strength is selected. This is particularly useful when using dose response models with plateau features (the Plateau and U-shaped models). It means that where the utility is driven by this model, the arm that will be ranked most likely to have the greatest utility will be the one that lies at the start of the flat maximum response section. However it has a less desirable effect when the overall utility is formed by multiplying the utilities of individual endpoints and one of the endpoint utility functions has a segment where the utility is 0."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/multendpt.html#component-utility-combination-method",
    "href": "documentation/v72/userguides/core/study/multendpt.html#component-utility-combination-method",
    "title": "Study Tab - Multiple Endpoint",
    "section": "Component Utility Combination Method",
    "text": "Component Utility Combination Method\nIn order to derive the overall utility score for a dose we first need to decide how the different utilities are to be used how they are to be combined.\nAt the top of the Endpoint tab is a control that is displayed regardless of which endpoint is being specified. It allows the formula to be selected that defines the values of the individual utility scores for each endpoint are to be combined to form the overall utility. The only operators supported are addition and multiplication, but all logically distinct combinations (given that the user can re-order the auxiliary endpoints) are provided:\n\n\n\n\n\n\nFigure 7: All combinations of endpoint specific utilities into the overall utility.\n\n\n\n\n\n\n\n\n\nGuide for Utility Combination\n\n\n\nUtilities can be combined by multiplying them together or adding together, so when there are just two endpoints there are just two methods of combination: U1 + U2 and U1 * U2. With more endpoints there are more possible combinations. These allow utilities to be formed for a number of circumstances:\n\nOne efficacy and one safety/tolerability endpoint, U1 * U2. Here the purpose of the combination is to scale back the efficacy score if there are safety or tolerability issues. At its simplest, the utility function of the safety / tolerability endpoint is defined so that the estimate of the probability of an adverse event or lack of tolerability is transformed to so the utility is 0 where the safety / tolerability is completely unacceptable, and 1 where it is completely acceptable, with possibly a transition region in between. The utility of the efficacy endpoint could be simply the value of the efficacy endpoint.\nThis is not intended to replace SAE monitoring and the withdrawal of treatments arms that are unsafe, the safety monitoring may be for indicators of potential safety problems when the drug is taken for a longer duration than can be studied, or it may be for tolerability issues that would give compliance problems, or acceptability problems given the other treatments available.\nSome possible variants are: where current treatments have a level of unpleasant side effects, the utility of the probability of a side effect for our drug may be &gt;1 for side effect rates below this. The utility of the efficacy outcome may be 0 below a certain floor efficacy and capped at a maximum above a ceiling efficacy, perhaps to stop undue weight being given to an outcome that this thought unfeasible or avoid a level of utility on the efficacy score that would yield an overall utility that would be judged viable at a poor (but not utility 0) level of safety / tolerability.\nTwo efficacy endpoints, U1 + U2. Here the purpose of the combination is to allow success if either the response on either endpoint is very good, or is quite good on both endpoints. Some care will be required to define the utility functions of the endpoints so that different combinations of efficacy correctly yield sufficient utility or insufficient utility.\nAn efficacy endpoint and a ‘necessary but not sufficient’ biomarker, U1 * U2. Here the purpose of the combination is to yield an overall utility of 0 if the biomarker is not observed at the necessary levels, otherwise the utility is driven by the primary endpoint that is observed much later. This allows early stopping for futility, arm dropping, or adaptive allocation away from arms with poor levels of biomarker, but for success to be determined only on the basis of the primary endpoint.\n\nUtilities with more than two endpoints are usually some form of combination of the above, for example a primary secondary and secondary endpoint and a safety/tolerability endpoint: (U1 + U2) * U3.\n\n\nDevising and agreeing upon utility functions with a clinical team is part art and part science (and possibly, part politics). Some have expressed the opinion that these methods could never be used in practice because it would be impossible to get agreement, but experience shows agreement is possible. Generally, the process followed is roughly as follows:\n\nThe team agrees how the endpoints will be combined and specifies some key utility points – at specific combinations of response at the different endpoints – for example:\n\nIf there were no observed side effects, what is the minimum level of efficacy that would be a useful drug?\nIf the maximum expected level of efficacy was observed, what is the maximum level of side effects that could be observed that would leave a useful drug\nSee “Dose-Finding Based On Efficacy-Toxicity Trade-Offs” by Thall and Cook (2004), for a description of such an elicitation process.\n\nThe statistician creates utility functions for the endpoints that yield the desired overall utility value at the specified points.\nUsing FACTS some simple trials are simulated and example simulated datasets and analyses are studied and reviewed with the team. The team is asked the question “Given the data that was simulated, (and the distributions they were simulated from) what do they think of the utility assigned to the treatment arms?”\nThe statistician adjusts the utility functions and iterates the process of simulating and reviewing with the team.\nOnce the team is happy with individual examples of how utility is assigned, a larger number of simulations can be run and the estimates of the operating characteristics considered and other aspects of the trials design considered.\n\n\nEstimation of Utility in FACTS\nAn important aspect of the way FACTS estimates utility is that it estimates a probability density for the utility within the MCMC sampling. That is, the utility is calculated for every parameter sample within the MCMC, and the final distribution of the utility is based on those samples just like the estimates for the model parameters.\nEach dose has a distribution of utility scores - not a single value. The utility is not estimated from the mean estimates of the dose response from the different endpoints.\nThis has some notable effects on the estimates of utility. If, for example, the utility function is a step function – for example if its value is 1 for response rates below a threshold and 0 above – as the estimate of the response rate will have some uncertainty then when the mean response estimates are around the threshold value, the estimate of utility will be between 0 and 1, based on the proportion of MCMC samples the fitted response rate was below the threshold. Indeed, the utility can be interpreted as the ‘probability the response rate is below the threshold’, and thus be useful or even exactly what is required.\nSimilarly, any utility function that has a floor or ceiling will result in a bias in the estimate of the utility to be above the floor or below the ceiling. – because in the distribution of the values for the utility the lowest the value can be is the floor and the final estimate of the utility would only be at the floor if all the values for the utility sampled in the MCMC were at the floor. Another effect of utilities with a floor or ceiling is that the estimate of the mean utility has a smaller standard error the further the value of the estimate of the mean of the underlying response is from an inflexion point (or “knot”).\nThese are not errors, nor does it mean utility functions with steps, ceilings or floors should be avoided, but these artefacts need to be understood – particularly when graphs of the estimated utility and the true utility are compared.\n\n\nCalculation of Arms’ probability of having the greatest utility\nLike other probabilities in FACTS this is calculated during the MCMC sampling – the probability that an arm has the greatest probability is based on the proportion of MCMC samples when that arm had the greatest utility. It is not estimated from the utility of the mean estimates of the dose response for the different endpoints.\nIn any given sample if several arms have the same maximum toxicity, the arm with the lowest dose strength is selected. This is particularly useful when using dose response models with plateau features (the Plateau and U-shaped models). It means that where the utility is driven by this model, the arm that will be ranked most likely to have the greatest utility will be the one that lies at the start of the flat maximum response section. However it has a less desirable effect when the overall utility is formed by multiplying the utilities of individual endpoints and one of the endpoint utility functions has a segment where the utility is 0.\n\n\n\n\n\n\nCaution when using 0 in your utilities\n\n\n\nHaving segments of utility 0 for an endpoint, and calculating the overall utility by multiplying the component utilities will lead to segments of 0 in the overall utility. If in only a proportion of the MCMC samples all arms have a utility of 0, this will result in that proportion of the probability of being the arm with the maximum utility being placed on the arm with the lowest dose strength, which might be odd given the utility in the other samples. It can lead to counter intuitive and sometimes undesired adaptations, such as in the adaptive allocation of subjects between the arms.\nThe solution is to not use 0 for the segments of low utility, but a small value such as 0.01, the multiplication with the utility of the other endpoints will then not flatten them all to exactly 0, but retain the utility profile at attenuated values."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/hazardmodel.html",
    "href": "documentation/v72/userguides/core/design/hazardmodel.html",
    "title": "Hazard Model and Predictor Model for Time to Event Trials",
    "section": "",
    "text": "The two main differences in the design tabs available for a time-to-event endpoint, rather than continuous or dichotomous, are ways that the control arm hazard model is defined and the use of a predictor model."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/hazardmodel.html#fixed-priors",
    "href": "documentation/v72/userguides/core/design/hazardmodel.html#fixed-priors",
    "title": "Hazard Model and Predictor Model for Time to Event Trials",
    "section": "Fixed priors",
    "text": "Fixed priors\nIf “Enable hierarchical data modeling for the control arm” is not selected, then independent gamma distributions with parameters specified in Hazard Rate tab’s table are used. The gamma parameters have been reparameterized so that the mean hazard rate and a weight (in terms of number of events) are provided instead of the traditional \\(\\alpha\\) and \\(\\beta\\) parameters.\n\n\n\n\n\n\nFigure 2: Specifying a fixed prior for a piecewise exponential model with 2 time segments."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/hazardmodel.html#hierarchical-prior",
    "href": "documentation/v72/userguides/core/design/hazardmodel.html#hierarchical-prior",
    "title": "Hazard Model and Predictor Model for Time to Event Trials",
    "section": "Hierarchical Prior",
    "text": "Hierarchical Prior\nIf “Enable hierarchical data modeling for the control arm” is selected, then the independent priors specified per segment are augmented by additional data specified on the “Hierarchical Priors” tab.\n\n\n\n\n\n\nFigure 3: Specifying the sufficient statistics and hierarchical models hyper-parameters for the bayesian augmented control model.\n\n\n\nThe additional data comes in the form of sufficient statistics from an outside data source. The sufficient statistics are, for each segment in each study, the number of events on the control arm and the control arm exposure time in subject weeks. The information from the prior study can be ‘down-weighted’ by reducing, pro-rata, the number of events and exposure time before entering them. By supplying the summary statistics from previous trials and specifying the parameters for prior distributions for the parameters of a hierarchical model, the gamma priors are updated and become the priors applied to the data collected on the virtual subjects.\nThe hyper-parameters are the mean and standard deviation of Normal distribution for the log hazard ratios of the event rates of the historical studies and the current study. The prior distribution for the mean hyper-parameter is a Normal distribution, for which the user specifies the mean and standard deviation. The prior distribution for the standard deviation hyper-parameter is an Inverse-Gamma distribution for which the user specifies either as a mean and a weight, or via Alpha and Beta parameters (depending on the user selection on Settings &gt; Options &gt; Gamma Distribution Parameters).\n\n\n\n\n\n\nSetting the priors for Hierarchical model hyper parameters\n\n\n\nUnless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.\nIn this case the following would be reasonable:\n\nSet the prior mean value for Mu as the mean of the log-hazard ratios of the event rates of the control arm and the historic studies (usually this will be 0)\nSet the prior SD for Mu equal to at least the largest log hazard ratio of the event rates for the historic studies.\nSet the mean for tau to the same value as the prior SD for Mu.\nSet the weight for tau to be &lt; 1.\n\nOne can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small relative to the number of studies, then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.\nTo give some prior preference towards pooling or separate analysis the weight for tau has to be large, relative to the number of historic studies. To have a design that is like pooling the historic studies, the mean for tau needs to be small – say 10% or less of the value suggested above. For there to be no borrowing from the historic studies the value for tau needs to be large – say 10x or more the value suggested above.\nThe best way to understand the impact of the priors is try different values and run simulations."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/dirichletprior.html",
    "href": "documentation/v72/userguides/core/design/dirichletprior.html",
    "title": "Dirichlet Prior(s) and Dose Response for Ordinal Trials",
    "section": "",
    "text": "The ordinal engine’s first design tab allows the user to define prior exponents for Dirichlet distributions. If the user has selected independent Dirichlets on the Study Info tab for ordinal outcome modeling, each experimental arm gets its own set of exponents. If the user has selected cumulative logistic modeling, only the control arm gets a set of exponents, and the prior distributions for other experimental arms are derived from this one set of exponents and the prior distributions for the dose-response model parameters.\nPrior exponents for Dirichlet distributions are often interpreted as counts of subjects observed in an earlier experiment, but positive non-integer values are allowed.\nThe prior mean for the probability of a given ordinal outcome under the Dirichlet distribution is the exponent corresponding to that outcome, divided by the sum of the exponents. For example, if there are four ordinal outcomes and the prior exponents are 10, 20, 30, and 40, the prior probabilities of the four outcomes are 0.1, 0.2, 0.3, and 0.4.\nIf a relatively non-informative prior distribution is desired, one suggestion is to use equal exponents that add up to approximately 0.8, i.e. \\(0.8/K\\) where \\(K\\) is the number of ordinal outcomes (Berger, Bernardo, and Sun, Bayesian Analysis 2015)."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/index.html",
    "href": "documentation/v72/userguides/core/design/index.html",
    "title": "Design Overview",
    "section": "",
    "text": "The FACTS core engine allows for the design and simulation of fixed and adaptive clinical trials, especially focused on, but not limited to, Bayesian designs with multiple active arms.\n\nSub-tabs of the Design Tab\nTrials designed in the core engines are comprised of a number of elements:\n\nThe dose response model: the user must specify how the doses are related to each other in the primary analysis, though there is a simple ‘no model’ option that estimates the mean treatment effect of each arm independently. A fixed trial uses the dose response model for the final Bayesian analysis of the data; an adaptive trial uses the same model both for the final analysis and at the interim updates. For time-to-event endpoints, there is an additional tab for estimating the control arm hazard rate called hazard model. For ordinal endpoints, there is an additional tab for specifying the Dirichlet prior parameters.\nFrequentist analysis: frequentist p-values can be calculated comparing each dose to the control arm (or a fixed value if there is no control). P-values can be used as decision making quantities at interim updates or final analyses. p-value cannot benefit from the dose reponse models or longitudinal models, which are specific to the Bayesian model in FACTS.\nThe longitudinal model or predictor model: whether the trial is adaptive or fixed, the user may select whether to use a longitudinal model for continuous or dichotomous primary endpoints or a predictor model when using a time-to-event primary endpoint. In a fixed trial the longitudinal model can be used to multiply impute final values for subjects that have dropped out. In an adaptive trial it is also used at the interim updates to multiply impute final values for subjects who have been recruited but do not yet have final values. In a fixed trial with no subject dropouts using a longitudinal model would have no effect on the outcome, analysis, or conduct of the trial.\nAllocation rules: in a fixed trial the user just specifies the proportion of subjects to be recruited to each arm, and the same can be done in an adaptive trial (i.e. an adaptive trial does not have to adapt the allocation), but an adaptive trial has a range of options that the user can use to adapt how subjects are allocated to the different treatment arms as the trial progresses.\nInterims: in an adaptive trial the user specifies how interim analyses are triggered, either by time or by accumulating numbers of subjects enrolled, with complete data, or with opportunity to complete. The user also specifies whether subjects continue to be followed up after the trial stops at an interim analysis. In a fixed trial there are no interim analyses and this tab does not apply.\nSuccess/Futility Criteria: in an adaptive trial the user can select the criteria and specify the thresholds at which the trial should be stopped at any interim where the conditions are satisfied. Early stopping is optional, and even in an adaptive design the user can opt to always recruit the maximum permitted number of subjects. In a fixed trial there are no interim analyses and hence no opportunity to stop early. The same Bayesian evaluation criteria are available whether the trial is fixed or adaptive. The user selects which criteria to use and what thresholds will constitute success or failure. The success and failure criteria do not have to be complements of each other, and any analysis that doesn’t completely satisfy either the success or futility criteria is labeled “inconclusive.”\n\n\n\n\n\n\n\nDesign tabs for multiple endpoint designs\n\n\n\nThe tab layout for multiple endpoint designs is slightly different when compared to the single endpoint engines. The multiple endpoint engine must allow for separate Dose Response, Frequentist Analysis, and Longitudinal specifications for each endpoint.\nTo allow for this, there is a “Response Models” tab as a first level sub-tab of the Design tab. There is 1 sub-tab below Response Models for each endpoint. Within the endpoint tab there will be a Dose Response, Frequentist Analysis, and Longitudinal tab (if applicable).\n\n\n\n\nEvaluation of Bayesian Posterior Estimates\nAt every interim and final analysis there is a Bayesian model fit to the data observed up to that point in the trial. The Bayesian model contains a dose response model and, often, a longitudinal model. In the absence of a longitudinal model, the posterior is calculated as:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)}\\]\nwhere \\(\\phi\\) is the set of parameters of the selected response model, \\(p(\\phi)\\) is the prior for those parameters, \\(y_i\\) is the final response for each subject and \\(n\\) is the number of subjects with complete data.\nWith a longitudinal model, this becomes:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)\\prod_{i = 1}^{n}{\\prod_{j = 1}^{L}{p(y_{ij}|\\psi)p(\\psi)}}}\\]\nwhere \\(\\psi\\) is the set of parameters of the selected longitudinal model, \\(p(\\psi)\\) is the prior for those parameters, \\(y_{ij}\\) is the response for each subject \\(i\\) at each visit \\(j\\) and \\(L\\) is the number of visits.\nThe posterior is evaluated using  MCMC Markov Chain Monte Carlo  with individual parameters updated by Metropolis Hastings (or Gibbs sampling where possible), using only the \\(y_i\\) and \\(y_{ij}\\) data available at the time of the update.\nThe likelihood and priors for each of the dose response models are provided in the description of the Dose Response tab, and the likelihood and priors for the multiple imputation models are provided in the description of the Longitudinal Models tab."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/successfutility.html",
    "href": "documentation/v72/userguides/core/design/successfutility.html",
    "title": "Success/Futility Criteria",
    "section": "",
    "text": "The Success/Futility Criteria tab is where users specify the decision rules for determining study success or failure. The Final Analysis criteria always exist, and should, in general, be specified for every simulated trial. If simulating an adaptive trial, then interim analysis decision rules are also specified here.\n\nDecisions in FACTS Core\nThere are \\(7\\) possible decisions that can be made in a FACTS Core design, each with a numeric identifier that FACTS uses in the .csv output to denote decisions. The Outcome column contains the decision made, and the number 1-7 map to decisions as follows:\n\n1. Early Success\n\nEarly Success is achieved if and only if the trial meets the success condition at an interim analysis, and does not meet the futility criteria at the final analysis. The final analysis criteria must not be met whether or not subjects were selected to follow-up after an early success decision.\n\n2. Late Success\n\nLate Success is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then meets the final analysis success criteria.\n\n3. Late Futility\n\nLate Futility is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then meets the final analysis futility criteria. Late futility is not automatically the complement of late success; the futility rule must be specified as the complement of the success rule to make it true.\n\n4. Early Futility\n\nEarly Futility is achieved if and only if the trial meets the futility condition at an interim analysis, and does not meet the success criteria at the final analysis. The final analysis criteria must not be met whether or not subjects were selected to follow-up after an early success decision.\n\n5. Success to Futility Flip-Flop\n\nSuccess to Futility Flip-Flop is achieved if and only if the trial meets the success condition at an interim analysis, but meets the futility condition at the final analysis. Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision.\n\n6. Futility to Success Flip-Flop\n\nFutility to success flip-flop is achieved if and only if the trial meets the futility condition at an interim analysis, but meets the success condition at the final analysis. Futility to Fuccess Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision.\n\n7. Inconclusive\n\nInconclusive is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then does not meet the final success or final futility criteria.\n\n\nEvery simulated trial will result in exactly one of these decisions. Non-adaptive trials will result in either Late Success, Late Futility, or Inconclusive. An adaptive trial that does not stop at an interim analysis will result in Late Success, Late Futility, or Inconclusive. An adaptive trial that stops enrolling for early success at an interim analysis will end up as an Early Success or a Success to Futility flip-flop. An adaptive trial that stops enrolling for early futility will result in Early Futility or Futility to Success flip-flop.\n\n\nFinal Evaluation\nOn the Final Evaluation tab, the user can specify rules for judging the study for final futility or final success at its end.\nThe left column of the Final Evaluation tab contains the specification of the trial final futility rule, and the right column contains the specification of the final success rule.\nTo add a decision rule, click the “Add…” button within the appropriate column, select a decision quantity QOI, a comparison inequality sign, and a threshold. Final success and final futility criteria can each have multiple components to them, and the selection at the bottom of the column called “Combine criteria using:” dictates if success or futility should be declared if every single criteria is met (AND) or if any criteria is met (OR).\nThe success and futility rules need not be complementary - there can be trials that do not meet either criteria at the final analysis. These trials would be considered inconclusive. It is allowable, although generally not recommended, to specify overlapping success and futility rules. If a trial were to satisfy both the success and futility criteria at the final analysis it would be considered a final futility.\nThe Final Evaluation Criteria are always applied at the time of the final analysis, even if the study stops early for success or futility, and whether or not “Continue follow-up if study stopped for success” or “Continue follow-up if study stopped for futility” are selected. As a result, setting final analysis criteria that are easier to satisfy than interim analysis criteria may result in the labelling of trials that meet the interim analysis criteria but do not satisfy the final analysis criteria as flip-flops.\n\n\n\n\n\n\nFigure 1: The final evaluation tab within the success futility tab.\n\n\n\n\n\nInterim Analysis Criteria\nOn the success/futility criteria of a design with “Enable adaptive features” checked on the Study &gt; Study Info page, the user can specify rules for judging the study for futility or success at an interim and at the final evaluation. If the trial has no interims there will be just a tab for the Final Evaluation criteria. If the trial has interim analyses, then there can be tabs that define different early success/futility criteria at the different interims.\nAt the top of the main tab is a control to allow tabs to be created for different interims. In a newly created adaptive design, FACTS will create a tab for interim 1.\nIf early success/futility criteria are specified for an interim, then they will be taken to apply to all subsequent interims until the next one for which criteria are supplied, then those criteria will apply until the next interim for which criteria are applied and so on. There will be no stopping for success or futility until the first interim for which early stopping criteria have been defined.\nIt is possible to specify overlapping early success and early futility criteria at an interim, but it is not possible to stop for both success and futility at the same time. The FACTS design engines will stop the trial but the user should not rely on one or other outcome taking precedence. It is not considered good practice to have success and futility rules that could both be true, so FACTS does not  guarantee a “tie break” rule As of FACTS 7.1 early futility will be the result if both early success and early futility criteria are met. .\nIn the output files there are columns labeled “Success ” and “Futile ” indicating whether any decision criteria became true, and “Success Combined” and “Futile Combined” indicating if all the criteria for a success or futility determination have been met. If both sets of criteria are met simultaneously, FACTS will only flag one of “Success Combined” and “Futile Combined” as being met, corresponding to how the outcome of the trial has been flagged.\n\n\n\n\n\n\nFigure 2: An interim evaluation tab within the success futility tab. The selected interim evaluation tab controls the interim analysis decisions at the 3rd interim analysis and all later interims.\n\n\n\nHaving created a tab to define the early success/futility criteria at an interim, the user has options to delete the tab or to copy criteria that have already been entered on another tab before editing them.\nThe user specifies:\n\nWhether early stopping for futility or success is to be allowed by selecting the “Futility criteria” or “Success criteria”\nThe stopping rules are specified, by selecting the QOI to be tested, the direction of the comparison and the threshold value for the criteria to be met, as in the Final Evaluation tab above.\nThe user can specify whether, if multiple criteria have been specified, they all need to be meet (Criteria combined using “AND”) or only any one of them needs to be met (Criteria combined using “OR”). These are specified independently for stopping for success and for futility.\nIf stopping is allowed, the user specifies what the minimum amount of information (using the information type specified on the Interims tab). This can be specified both in overall terms and information on specific doses. If multiple minimum-information criteria are specified they must all be met for the QOI criteria to be evaluated."
  },
  {
    "objectID": "documentation/v72/userguides/core/execution.html",
    "href": "documentation/v72/userguides/core/execution.html",
    "title": "Execution Tab",
    "section": "",
    "text": "The Accrual sub-tab provides an interface for specifying accrual profiles. Accrual profiles define the mean recruitment rate week by week during the course of the trial. Virtual subjects are simulated from a Poisson process in which the expected number of subjects per week is allowed to change week by week.\nAccrual profiles are shown as a list on the left of the screen, as depicted below. These accrual profiles may be renamed by double-clicking on them and typing a new profile name. After creating a profile, the user must create at least one recruitment region. Early in the trial design process, detailed simulation of the expected accrual pattern is typically not necessary and a single region with a simple mean accrual rate is sufficient.\nTo model the expected accrual rates more precisely over the course of the trial, the user may specify multiple regions for each accrual profile and separately parameterize them. Regions are added via the table in the center of the screen Figure 1. Within this table, the user may modify:\n\nthe peak mean weekly recruitment rate,\nthe start date (in weeks from the start of the trial) for this recruitment region,\nwhether the region will have a ramp up phase and if so when the ramp up will be complete (in weeks from the start of the trial).\nWhether the region will have a ramp down, and if so when the ramp down start and when the ramp down will complete (in weeks from the start of the trial).\n\nRamp up/ramp down define simple linear increase/decreases in mean recruitment rate from the start to the end of the ramp. Note that simulation of accrual is probabilistic, but ramp downs are defined in terms of time, so even if ramp downs are planned so that at the average accrual rate they will occur as the trial reaches cap, there is a risk in simulations when accrual has been slower than average, that ramp downs occur before the full sample size is reached. It is advisable to have at least one region that doesn’t ramp down to prevent simulations being unable to complete.\nA graph of the recruitment rate of the highlighted region is shown as well. As the recruitment parameters are changed, the graph will update to show the time at which full accrual is reached. An accrual profile that does not reach full accrual is invalid and cannot be used to run simulations.\n\n\n\n\n\n\nFigure 1: Execution &gt; Accrual tab.\n\n\n\nIn the screenshot above you can see the two step ramp up in accrual from two regions – each starting at different offsets into the trial.\nNote that the accrual profile graph is only the mean expectation; actual accrual is simulated using exponential distributions for the intervals between subjects, derived from the mean accrual profile specified here. Thus some simulated trials will recruit more quickly than this and some more slowly.\nThere are commands to import and export region details from/to simple external XML files. When importing, the regions defined in the external file are added to the regions already defined, they don’t replace them.\nThis is an example of a very simple region file defining just one region:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;regions&gt;\n&lt;region&gt;\n&lt;name&gt;Region 1&lt;/name&gt;\n&lt;rate&gt;5&lt;/rate&gt;\n&lt;start&gt;0&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;/regions&gt;\n\n\nIf “Deterministic” accrual has been specified on the Study &gt; Study Info tab, then on the Accrual tab, rather that specifying an accrual profile from which subject recruitment times are simulated, the user loads a file of specific accrual dates for every subject.\n\n\n\n\n\n\nFigure 2: Execution &gt; Deterministic Accrual tab.\n\n\n\nThe user specifies a “.dat” file to load that contains the  subject accrual dates in weeks This value is in weeks from FACTS 7.0 onwards, previously it was in days.  from the start of the trial.\nThe required file format is a text file with comma separate values. One row per subject, with 3 fields on each row:\n\nthe subject ID, (an integer)\nthe ID of the region where the subject was recruited (an integer)\nand the subjects randomization date (in weeks from the start of the trial – this is a ‘real’ number allowing fractions of a week to be specified)\n\nThe file must contain sufficient entries to allow the maximum number of subjects specified on the Study &gt; Study Info tab to be recruited.\nAfter successfully loading a file, the FACTS GUI shows a plot of the resulting weekly accrual rate"
  },
  {
    "objectID": "documentation/v72/userguides/core/execution.html#deterministic-accrual",
    "href": "documentation/v72/userguides/core/execution.html#deterministic-accrual",
    "title": "Execution Tab",
    "section": "",
    "text": "If “Deterministic” accrual has been specified on the Study &gt; Study Info tab, then on the Accrual tab, rather that specifying an accrual profile from which subject recruitment times are simulated, the user loads a file of specific accrual dates for every subject.\n\n\n\n\n\n\nFigure 2: Execution &gt; Deterministic Accrual tab.\n\n\n\nThe user specifies a “.dat” file to load that contains the  subject accrual dates in weeks This value is in weeks from FACTS 7.0 onwards, previously it was in days.  from the start of the trial.\nThe required file format is a text file with comma separate values. One row per subject, with 3 fields on each row:\n\nthe subject ID, (an integer)\nthe ID of the region where the subject was recruited (an integer)\nand the subjects randomization date (in weeks from the start of the trial – this is a ‘real’ number allowing fractions of a week to be specified)\n\nThe file must contain sufficient entries to allow the maximum number of subjects specified on the Study &gt; Study Info tab to be recruited.\nAfter successfully loading a file, the FACTS GUI shows a plot of the resulting weekly accrual rate"
  },
  {
    "objectID": "documentation/index.html",
    "href": "documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Welcome to the Documentation section — your go-to repository of in-depth user guides and practical examples that illuminate every aspect of FACTS. Whether you’re just getting started or delving into advanced functionalities, these resources are designed to help you navigate our software with confidence and ease.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/index.html#what-youll-find-here",
    "href": "documentation/index.html#what-youll-find-here",
    "title": "Documentation",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nUser Guides: Access detailed manuals covering the full range of our engines. From foundations to advanced features, learn how to set up your desired trial design, learn about best practices and obtain focused understanding of the subject matter, and troubleshoot common issues.\n\n3+3 and mTPI - Rule Based Dose Escalation\nPhase II & III Designs\nSeamless Designs\nEnrichment Designs\nCRM - Model Based Dose Escalation\n2D CRM - 2D Model Based Dose Escalation\nFACTS/R - How to Call FACTS from R\nFLFLL - FACTS Linux File Loader Light\nPlatform Trials\nFACTS Design Report\n\nExamples: Explore a range of constructed and real-world studies that demonstrate FACTS’s capabilities in action and discover strategies to optimize performance.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/index.html#how-to-get-the-most-out-of-this-section",
    "href": "documentation/index.html#how-to-get-the-most-out-of-this-section",
    "title": "Documentation",
    "section": "How to Get the Most Out of This Section",
    "text": "How to Get the Most Out of This Section\nThis section contains resources for the current and past versions of FACTS, which you can choose in the sidebar. If necessary, begin by reviewing the installation guides. Later, you may consult engine-specific user manuals for deeper insights, and explore our curated examples to bring theory into practice. With these resources at your fingertips, you’ll gain a richer, more informed understanding of FACTS and maximize your potential for success.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/versions/roadmap.html",
    "href": "documentation/versions/roadmap.html",
    "title": "FACTS Roadmap",
    "section": "",
    "text": "We are excited to announce that we are currently working on:\n\nFACTS Core Ordinal Endpoint\nFACTS Dose Escalation: i3+3, mTPI-2 and BOIN\nNew Response Adaptive Allocation options\nGreater flexibility in adaptive decisions (e.g. being able to combine different QOIs in an AND/OR logic, but also many more)\nQuick Start options to enter standard design types (such as widley used group sequential designs)\n\nIf you have any questions, suggestions or requests, please contact us.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS Roadmap"
    ]
  },
  {
    "objectID": "documentation/versions/roadmap.html#section",
    "href": "documentation/versions/roadmap.html#section",
    "title": "FACTS Roadmap",
    "section": "",
    "text": "We are excited to announce that we are currently working on:\n\nFACTS Core Ordinal Endpoint\nFACTS Dose Escalation: i3+3, mTPI-2 and BOIN\nNew Response Adaptive Allocation options\nGreater flexibility in adaptive decisions (e.g. being able to combine different QOIs in an AND/OR logic, but also many more)\nQuick Start options to enter standard design types (such as widley used group sequential designs)\n\nIf you have any questions, suggestions or requests, please contact us.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS Roadmap"
    ]
  },
  {
    "objectID": "documentation/versions/v7/facts710.html",
    "href": "documentation/versions/v7/facts710.html",
    "title": "FACTS 7.1.0 Release Notes",
    "section": "",
    "text": "Introduction\nFACTS 7.1.0 is now available for download via App Center. FACTS users can now:\n\nPerform concurrent control analyses using posterior probabilities, predictive probabilities and p-values in Platform Trial designs.\nMake interim and final decisions based on conditional power as well as Bayesian predictive probabilities.\nExport the data associated with the in-built graphs FACTS provides into a CSV file.\nExplore, via integration with AIRSHIP, simulation results graphically in a much more generic, versatile way; namely, by allowing the user to view the impact of simulation input dimensions through dynamic filtering of the simulation results.\nCreate designs which use an independent Beta Binomial dose response model for analyzing (simulated) data for dichotomous endpoints in Core and Staged designs.\n\nCreate designs where frequentist (p-value) calculations are performed using the Fisher exact test rather than a normal approximation for dichotomous endpoints.\nProvide three separate patient queue lengths for Continual Reassessment Methods (CRM) designs, based on dose clearance and the current model estimate of the MTD. In addition, the ability to backfill to the current escalation dose (“frontfill”) is now available.\nView explanations of some of the most important inputs in Core Continuous/Dichotomous designs through informative tooltips.\nSet different random number seeds when simulating multiple design scenarios.\nCreate designs where decision QOIs can be used in multiple stopping criteria simultaneously.\n\n\n\nFACTS Core and Staged Improvements\n\nIn Core and Staged designs making use of dichotomous endpoints, FACTS users can specify a beta binomial model to independently model the response rate on each arm. With this model, the user specifies a Beta distribution prior on the response rate.\nIn Core and Staged designs which perform frequentist analyses on dichotomous endpoints, users can now specify whether p-value calculations (including Current Trial Predictive Probability QOIs) should be performed using a normal approximation as currently, or using a Fisher exact test. The latter is of particular use when sample sizes are small.\nIn Staged designs, the reported Early Success Time will now correctly only be reported for simulations which have stopped for early success. Previously, simulations which have graduated early to stage 2 in stage 1 were being reported as having an Early Success Time.\nWhen running analyses in FACTS Core and Staged designs, the user can now specify the number of MCMC samples to generate per imputation of missing data modelled by a longitudinal model.\nIn Core and Staged designs with a dichotomous endpoint, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Core and Staged designs, a new Operating Characteristics graph displaying the cumulative proportion of simulations having stopped at a given duration, sample size or number of events (in Time-to-Event) has been added.\nIn Core designs, two new summary columns “Ppn Overall Success” and “Ppn Overall Futility” have been added, which simply sum up the existing columns “Ppn Early Success” and “Ppn Late Success”, and “Ppn Early Futility” and “Ppn Late Futility”, respectively. This allows users to quickly view the estimated type I error and power of the design.\nIn Core and Staged designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Staged designs, the Stage 2 “Explore” Final Success/Futility graphs will now have the option to include/exclude simulations which have stopped in Stage 1.\nIn Core and Staged designs, the criteria for selecting a dose at the end of the trial have moved from the “Variants” tab to the Success/Futility tab. These criteria will be used when reporting the proportion of times the correct/incorrect arm was selected at the end of the trial (as reported in the “Ppn Correct/Incorrect Arm” columns, which are now reported in the summary file).\nIn Core and Staged Multiple Endpoint designs, designs not using a control arm will no longer crash when adding a new endpoint, and no longer crash when adding a new treatment to a design which uses Virtual Subject Response external data files.\nIn Core and Staged designs, the “Legacy Second Order NDLM” dose response model and the “Legacy Adaptation” allocation option have been removed. Older designs making use of these features will be migrated over to the “Second Order NDLM” dose response model and the “Fixed Allocation” allocation option, respectively, when loaded in FACTS 7.1. A warning prompt will appear for such designs.\nUsers can now choose a decision QOI in multiple stopping criteria simultaneously. Previously, after a stopping rule was created based on a certain decision QOI, any additional stopping rule could not use this decision QOI.\nIn Core and Staged Designs, a new class of QOI as a sub-category of Predictive Probabilities was introduced: Conditional Power. Contrary to the existing Bayesian Predictive Probabilities, Conditional Power assumes the observed treatment effect estimates to be the truth and then calculates the probability of being successful either: 1) at a later final analysis, 2) after the currently enrolled subjects are followed up or 3) in a future trial.\nFACTS will now correctly calculate the Bayesian Predictive Probabilities of success at the final analysis when a continuous endpoint and change from baseline as the final endpoint are used.\nIn Core and Staged Time-to-Event designs, minimum information required to trigger an interim will now be available when interims are based on predictor events.\nIn Core and Staged Time-to-Event designs, the complete information columns report at interims with the weeks files will now display the correct information when interims are based on predictor events.\nIn Core and Staged Time-to-Event designs with a Time-to-Event predictor, when interim information events and the event max cap differ (e.g. max cap on final events but interims based on predictor events), simulations will correctly stop at the specified max event cap.\nIn Core and Staged Time-to-Event designs with a Time-to-Event predictor, current trial predictive probability QOIs are now handled correctly when max event caps refer to predictor events.\nIn Staged Time-to-Event designs with a Time-to-Event predictor, current trial predictive probability QOIs are now handled correctly when interim information events and the event max cap differ (e.g. max cap on final events but interims based on predictor events).\nIn Staged Time-to-Event designs, the censoring and event time of subjects in Stage 2 which have not observed their event in Stage 1 will now be handled correctly for both their final and predictor endpoints.\nIn Core and Staged design, (Continuous, Dichotomous, and TTE) when a trial stops for early success or early futility and the option to continue follow-up after that decision is not selected, the final analysis model is no longer run. The data at the final analysis would be identical to the data at the interim analysis that the stopping threshold was hit at, so the model output for the final is now identical to the interim.\nTime Course Hierarchical longitudinal model has improved performance with informative priors on the variance components.\nIn Staged designs, FACTS will now correctly handle the Stage 1, Stage 2 and overall sample size caps (maximum number of subject caps and maximum number of event caps) specified in the Variants tab.\nIn Core and Staged Time-to-Event designs, the Cox Proportional Hazards current trial predictive probability calculation has been corrected.\nIn Core and Staged designs, when using predictive probabilities that predict success at trial maximum, but the trial is stopped at an interim, the predictive probability is now calculated based on the originally specified number of subjects at trial maximum rather that assigning the predictive probability a value of 0 or 1 depending on the p-value of the interim data.\nIn Core and Staged designs, when using predictive probabilities, visit values are now correctly imputed when there is only baseline visit data available.\nIn Core and Staged designs, when using a dichotomous endpoint and predictive probabilities of treatment versus control, FACTS now performs a Farrington and Manning Test when there is a superiority or non-inferiority margin.\nIn Staged design, the arm selection logic has been updated to properly account for arm-dropping in stage 1 when selecting a single arm from each group.\n\n\n\nFACTS Enrichment Design Improvements\n\nIn Enrichment Dichotomous designs, users can now perform frequentist calculations using a normal approximation as currently, or using a Fisher exact test. The latter is of particular use when sample sizes are small.\nIn Enrichment Time-to-Event designs, the GUI will now correctly calculate the mean frequentist estimated treatment effect and its standard error.\nWhen running analyses in Enrichment designs, the user can now specify the number of MCMC samples to generate per imputation of missing data modelled by a longitudinal model.\nIn Enrichment Dichotomous designs, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Enrichment designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration, sample size or number of events (in Time-to-Event) has been added.\nIn Enrichment designs, two new summary columns “Ppn Overall Success” and “Ppn Overall Futility” have been added, which simply sum up the existing columns “Ppn Early Success” and “Ppn Late Success”, and “Ppn Early Futility” and “Ppn Late Futility”, respectively. This allows users to quickly view the estimated type I error and power of the design.\nIn Enrichment designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Enrichment designs, the frequentist test reported in the frequentist simulations file will now make it clear whether the performed test is a one-sided or two-sided test. In the associated frequentist summary file, the proportion of frequentist results for each test type that are significant will now correctly take into account the correct alpha level depending on whether the test is one-sided or two-sided.\n\n\n\nFACTS Platform Trial Improvements\n\nBREAKING CHANGE: in Platform trial designs, the numerical value representing the outcomes for late futility and early futility have been swapped in FACTS 7.1.0. Any older designs with futility criteria will need to be re-simulated. Decision numeric values in FACTS Platform Trials and FACTS Core now match.\nIn Platform Trial designs, users can now specify whether Posterior Probability QOIs, Predictive Probability QOIs or p-value QOIs should be calculated based on the entire control population in the trial (as previously) or based on the given treatment’s concurrent control population. These concurrent control QOIs can be used as treatment stopping criteria like any other QOI.\nA time window allowing a treatment’s concurrent controls to additionally include control patients a certain number of weeks from the treatment entering the trial can also be specified.\nPr(Max) Target Dose QOIs can now be calculated based on all arms in the trial (as previously), only active arms or only randomizing arms.\nIn Platform Trial Dichotomous designs, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Platform Trial designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration or sample size has been added.\nIn Platform trial designs, users can now specify different variants of the design by modifying both the maximum number of participants per treatment as well as the maximum number of concurrent treatments. These variants will display as separate scenarios on the Simulations tab.\nIn Platform trial designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Platform trial designs, the “Per Sim: Arm and Participant Arrival” graph will now correctly display the accrual period for the control arm to end when the last treatment’s accrual period ends.\nUsers can now choose a decision QOI in multiple stopping criteria simultaneously. Previously, after a stopping rule was created based on a certain decision QOI, any additional stopping rule could not use this decision QOI.\nFACTS will now correctly calculate the Bayesian Predictive Probabilities of success at the final analysis when a continuous endpoint and change from baseline as the final endpoint are used.\n\n\n\nFACTS Dose Escalation Improvements\n\nThe Dose Escalation design types CRM(Toxicity), CRM(Ordinal), bCRM and CRM(Efficacy) have all been deprecated. Users can still create new designs or open existing designs with these design types. We recommend using the more general and versatile “Continual Reassessment Methods (CRM)” design type (formerly known as “N-CRM”) for any 1D model based CRM designs.\nIn CRM with open enrollment, users can now specify whether they want to allow backfilling to the highest dose (“frontfilling”) and conditions under which to do so. Users can choose whether to count these patients towards the backfill subject cap or regular allocation cap.\nIn CRM with open enrollment, previously two queues (maximum number of subjects on uncleared doses and maximum number of subjects on cleared doses) determined the allocation behavior. In order to give users more flexibility and control, there are now three queue concepts (maximum number of subjects on uncleared doses, maximum number of subjects on cleared doses at MTD and maximum number of subjects below MTD). These queues are now used in the same way in the MTD and MED phase of the trial. The concept of max cleared dose and the new queues are now harmonized.\nIn CRM, the stopping rule checker design was updated. In case of regular dosing, a concept of near doses like that of fine spaced dosing was introduced and both concepts aligned for both stopping in the MTD and MED phase. The stopping rule checker now also correctly checks the hierarchies and join conditions.\nIn CRM with open enrollment, backfill will now correctly evaluate all cleared doses for eligibility.\nIn CRM with open enrollment and fine spaced dosing, near doses are now correctly evaluated in regular allocation, backfill and for the purposes of stopping.\nIn CRM designs, FACTS will now correctly apply study size constraints and queue size constraints when the underlying toxicity model considers all doses to be toxic. This also includes preventing an expansion cohort from being allocated in this situation.\nIn Dose Escalation designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration or sample size has been added.\nIn 2D-CRM designs, the dose response model’s eta parameter can now be specified in log-normal space.\nIn 2D-CRM designs with custom run-in, FACTS now proceeds to the chosen escalation scheme more promptly, without first allocating more than one full-size cohort unless they are specified in the run-in.\nThe “Per Sim Allocation History Grouped” plot will now be displayed correctly if, previously, the “Per Sim Allocation History Group 1” plot had been set to space interims equally.\nIn CRM with an initial run-in, escalation will now be performed correctly and maximum cleared dose tracked correctly in all circumstances.\nIn CRM with small-cohort pre-escalation, we now only switch to regular escalation based on observed toxicities, not MTD estimate or overdose control rules.\nIn CRM, introduce the notion of “Selected MTD”, “Selected OSD” and “Selected MED” at the end of trial as a function of the respective model estimates and the max cleared dose.\nIn CRM with two groups that enroll consecutively, there are now several options regarding what dose level to start escalation at in group 2.\nIn CRM with fine grained dosing, the concept of near doses is now correctly applied in the escalation phase, when calculating the max cleared dose and adjusted for the new queue concepts.\nIn CRM, cohort expansion will now enroll the correct number of patients even when accrual is very fast.\nIn CRM, when using a cohort expansion or a two group design, the trial state on which both of these concepts depends is now the final state of the previous trial, not the state of the previous trial when its stopping criteria were met.\nIn CRM, when using two groups and expansion cohorts in both groups, the group 2 expansion cohort now correctly uses its own cap regarding maximum number of subjects. In the same setting, a rare circumstance led to the allocation in the group 1 cohort expansion to continue beyond the max cap – this is now resolved.\nIn CRM, stopping based on the max cap of subjects for escalation now does not lead to an overrun in patients in rare circumstances.\nIn CRM with both an MTD and an MED phase (toxicity and efficacy endpoints), the transition between the phases is now handled correctly.\nIn CRM, improved labels and default values in the GUI, such as improved values for the overdose control and open enrollment queue lengths and clearer labels in the Allocation tab.\nSeveral GUI stability improvements in the CRM engine.\nIn CRM, there are several improvements to the “Per Sim Allocation History”, “Alloc and Tox History”, “Cohort Band Probabilities” and “Cohort Response” graphs, including showing the cohort expansions subjects separately, showing the max cleared dose at any point in time and showing the selected MTD/MED at the end of the trial.\n\n\n\nGeneral Improvements\n\nFACTS has now been integrated with AIRSHIP, which allows simulation results to be explored graphically in a much more generic, versatile way. Once simulation have completed, results can be explored with AIRSHIP by clicking on “Explore Results…” &gt; “Compare Scenarios in AIRSHIP”. Note that use of AIRSHIP requires at least two scenarios to have been simulated and their results aggregated.\nThe process of making the FACTS inputs more intuitive has started as of FACTS 7.1.0. In FACTS Core Continuous/Dichotomous designs, tooltips will appear against many of the inputs (when hovering over the relevant input) with explanatory text about their use and impact on the design. Tooltips can be disabled by going to Settings &gt; Options &gt; Tooltip Configuration.\nThe data associated with all graphs displayed in FACTS can now be exported into a CSV format. In addition, hovering over these graphs will display the associated data point value as a tooltip.\nWhen simulating multiple scenarios, each simulated scenario can now be simulated with a different random number seed.\nWhen running simulations for a directory of FACTS files in FACTS Command Line or FLFLL, a different base seed can be set for each design within the directory.\nThe order of scenarios to simulate as displayed on the Simulation tab has been set to be consistently displayed in alphabetical order.\nWhen aggregating simulation results for a design using variants, the relevant variant number will now correctly be displayed in the aggregated files.\nSeveral bug fixes and improvements have been made to all design reports.\nThe associated engine executable now provides a new -o output flag argument specifying the location where to output all files generated by the engine [Enterprise licensees only].\n\nPlease contact us regarding any questions.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.1.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v7/facts720.html",
    "href": "documentation/versions/v7/facts720.html",
    "title": "FACTS 7.2.0 Release Notes",
    "section": "",
    "text": "Introduction\nFACTS 7.2.0 is now available for download via App Center. FACTS users can now:\n\nCreate simple operational QOIs that can be used to make decisions.\nCreate BOIN designs.\n\n\n\nFACTS Core and Staged Improvements\n\nIn Staged designs, the “Mirror Stage 1 Longitudinal Model in Stage 2” on the Design &gt; Longitudinal tab will be fully visible.\nIn Core and Staged designs, designs created with FACTS 6.1 or older and loaded in FACTS 7.2 will now correctly generate frequentist output files for backward compatability.\nDesigns simulated using a self hosted HPC grid will now be handled in a more user-friendly way; by first checking if the HPC grid URL is correct, and by only reporting grid submission failures once all simulations have been processed to prevent job submission processing from being blocked [Enterprise licensees only].\n\n\n\nGeneral Improvements\n\nFACTS will no longer display a warning message for using the “Legacy Adaptation” allocation option when the underlying design does not make use of adaptive features.\nFACTS will no longer mark a simulated scenario as having errored when simulations have completed successfully but the associated temporary “pkt” simulation folders where each simulation packet is run have failed to be deleted. This can typically arise when simulating a FACTS design saced in a shared drive.\n\nPlease contact us regarding any questions.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.2.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts616.html",
    "href": "documentation/versions/v6/facts616.html",
    "title": "FACTS 6.1.6 Release Notes",
    "section": "",
    "text": "FACTS 6.1.6 is a maintenance release for FACTS 6.1.0. The first product release of FACTS 6.1.0 was FACTS 6.1.3. Subsequent releases have introduced the following changes:\n\nFACTS 6.1.4: This was FACTS 6.1.3 with some additional logging when using the grid interface.\nFACTS 6.1.5: This was FACTS 6.1.4 with the Dose Escalation N-CRM re-compiled to allow a higher number (40) of dose strengths to be defined when using “Explicit Doses” rather than finely spaced doses.\nFACTS 6.1.6: This was FACTS 6.1.5 with 2 problems fixed in FACTS Core with a TTE endpoint & FACTS Staged Design with a TTE endpoint. In either engine the calculation of a “Current Trial Predictive Probability of Success at Current Enrollment” had 2 problems:\n\nThere was an error in the way timings of future events were simulated in the calculation of the predictive probability. The result was approximately correct, and erred on the conservative side, the error is more manifest if the trial has long follow-up times.\nThere was an error if the design also includes a predictor endpoint. This effects a much smaller set of designs, but the effect was much more marked and its impact was difficult to characterize in general. Our current advice is to not use predictive probability QOIs in combination with a “Predictor” endpoint using FACTS prior to FACTS 6.1.6.\n\n\nUpgrading FACTS 6.1.6 should introduce no changes to the simulation or analysis results relative to FACTS 6.1.3 except in designs using a time-to-event endpoint and a “Predictive Probability of Success in the Current Trial at Current Enrollment” Quantity of Interest.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.1.6 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts650.html",
    "href": "documentation/versions/v6/facts650.html",
    "title": "FACTS 6.5.0 Release Notes",
    "section": "",
    "text": "1 Introduction\nFACTS 6.5.0 is now available for download via App Center. Please contact us regarding any questions.\nFACTS users can now:\n\nSpecify frequentist margins (“deltas”) in the calculation of p-value and predictive probability QOIs for FACTS Core and Staged designs (except Time-to-Event designs).\nCreate designs with interims triggered based on predictor events for FACTS Core and Staged Time-to-Event designs with a Time-to-Event predictor.\nCreate designs where the final event endpoint analysis can be performed without any imputation based on the predictor endpoint for FACTS Core and Staged Time-to-Event designs with a predictor endpoint.\nObserve significant improvements in the mixing of MCMC chains within the Bayesian Augmented Control and Active Comparator (BAC and BAAC respectively) hierarchical models for FACTS Core and Staged and Enrichment designs.\nGenerate design reports for FACTS Core Multiple Endpoint designs, FACTS Enrichment designs (Continuous, Dichotomous and Time-to-Event) and FACTS N-CRM designs.\n\n\n\n2 FACTS Core and Staged Improvements\n\nIn FACTS Core and Staged designs (except Time-to-Event designs), FACTS users can now globally specify a frequentist super-superiority/non-inferiority margin on the Quantities of Interest tab under the Standard Evaluation Variables area, which will be applied to the calculation of all p-value QOIs and “Current Trial” Predictive Probability QOIs. Note that this globally defined margin does not apply to “Future Trial” Predictive Probability QOIs, which can have their own separate margin defined. In addition, users now have the option on the “Frequentist Analysis” tab to use the frequentist super-superiority/non-inferiority margin in the frequentist analysis.\nIn FACTS Core and Staged Time-to-Event designs with a Time-to-Event predictor, users can now specify designs with interims triggered based on the number of predictor events that have been observed. In addition, and independently of how interims are triggered, users can now specify maximum event caps based on either Final events or Predictor events.\nIn FACTS Core and Staged Time-to-Event designs with a predictor endpoint, users can now specify the final endpoint analysis to not depend on any imputation from the predictor endpoint. This can be achieved by selecting the “No imputation” option within the “Imputation on Predictor” panel on the Design &gt; Predictor Model &gt; Relationship to Endpoint tab.\nIn FACTS Core and Staged designs, the mixing of MCMC chains within Bayesian Augmented Control and Active Comparator (BAC and BAAC respectively) hierarchical models has been improved significantly; in particular, when the prior on tau^2 in the hierarchical model is forced to be small.\nFACTS Core Multiple Endpoint now provides the ability to generate a design report once the design has been simulated. As a result, all FACTS Core design types can now generate design reports.\nFACTS Core and Staged designs now correctly display a trial as having stopped for futility if all arms have been dropped.\nFACTS Core and Staged designs now correctly prevent interims from being performed beyond full enrollment when the “Discontinue interim analysis beyond full enrolment” setting on the Interims tab is selected.\nFACTS Staged Time-to-Event designs now correctly handle interim timings in Stage 2 for the various data inclusion rules as specified on the Data Inclusion tab, and interim information based on “Just Stage 2 data” or “Stage 2 and included Stage 1 data”, as specified on the Stage 2 Interims tab.\nFACTS Staged Time-to-Event designs now correctly handles interim timings based on complete predictor data, when a predictor is included in the design and the “Primary endpoint is censoring for intermediate predictor” setting is selected.\nFACTS Core and Staged Time-to-Event designs now correctly handle predictor based imputation when using a dichotomous predictor endpoint.\nOn the Analysis tab in FACTS Core and Staged Time-to-Event designs, current trial predictive probabilities that estimate an accrual rate no longer require input data to be sorted by accrual time.\nFACTS Core Multiple Endpoint and FACTS Staged Dichotomous designs will now correctly output p-value trend test QOIs as a single output column, rather than one output column per dose, in summary files.\nFACTS Staged Multiple Endpoint designs will now correctly display the endpoint number when outputting p-value trend test QOIs in summary files.\nFACTS (Staged) Multiple Endpoint designs will now correctly display the posterior probability QOI comparison options (“Rates” and “Log-odds”) for dichotomous endpoints. Changing the endpoint from being dichotomous to continuous will delete posterior probability QOIs using the “Log-odds” comparison.\nFACTS Core and Staged Multiple Endpoint analyses will now correctly handle endpoints with visits schedules. Namely, when an endpoint contains only one visit schedule or when an endpoint’s visit schedule involved non-consecutive visits.\nFACTS Core and Staged Multiple Endpoint analyses will now correctly handle endpoints whose visit schedule contains missing data.\n\n\n\n3 FACTS Enrichment Design Improvements\n\nFACTS Enrichment designs (Continuous, Dichotomous and Time-to-Event) now provide the ability to generate design reports once the designs have been simulated. As a result, all FACTS Enrichment design types can now generate design reports.\nThe mixing of MCMC chains within Bayesian Augmented Control (BAC) hierarchical model has been improved significantly; in particular, when the prior on tau^2 in the hierarchical model is forced to be small.\nThe clustering in Enrichment designs has been improved for situations when the prior on tau^2 is chosen to be small (e.g. 0.01 with weight 1).\nThe patients file output from simulations (patients.csv) now correctly populates the dropout state of patients, and can now be used as subject data input on the Analysis tab without requiring modification.\nMCMC Trace plots are now available for all Enrichment design types when viewing simulation results graphs and when performing analyses. To view these graphs, at least one MCMC file needs to be generated. This can be done by going to the Simulations tab &gt; MCMC Settings.\nExternal data file validation has been improved.\n\n\n\n4 FACTS Dose Escalation Improvements\n\nN-CRM now provides the ability to generate design reports once the design has been simulated.\nIn N-CRM designs which include efficacy, the “Maximum cohorts used to determine MTD” setting on the Allocation Rule tab is now observed correctly.\nIn N-CRM designs, when deriving toxicity/efficacy priors from quantiles at the lowest and highest doses, the “middle” probability option now refers to the median dose rather than the reference dose and is now optional. When pre-6.5 FACTS files are loaded in FACTS 6.5, the “middle” probability option will be ignored and replaced by 0.5.\nIn N-CRM designs, the specification of at least two dose levels is now required when deriving toxicity/efficacy priors from specific quantiles. Previously, the specification of at least three dose levels was required.\n\nFACTS Command Line mode and FLFLL (FACTS Linux File Loader Lite) now correctly handle the processing of N-CRM designs whose prior was derived using the “Legacy Prior” option.\nIn N-CRM designs using open enrollment without accrual pausing, FACTS will now output entries in the cohort file for subjects that have been lost following stopping rules being met.\nIn N-CRM designs using open enrollment, FACTS will no longer prematurely apply stopping conditions when there are still subjects whose outcomes have been observed, but not yet processed.\nIn N-CRM designs using open enrollment, FACTS will now report the selected MTD/MED/OSD and associated state to be the dose where the stopping conditions were first met, unless the MTD has subsequently decreased post stopping rules being met.\nIn N-CRM designs using open enrollment, dose escalation rules when using fine-grained dosing will behave correctly.\nIn N-CRM designs using open enrollment and two groups, stopping rules and dose escalations rules will now apply to the correct group.\n\n\n\n5 General Improvements\n\nFACTS now targets .NET Framework 4.8, the latest major version of the .NET Framework.\nA new “Simulation Duration” table can be viewed when right-clicking on a simulation design scenario. The Simulation Duration table gives a granular view of simulation start and end times, as well as its total duration.\nSeveral major improvements to FLFLL (enterprise licensees only): in particular, the ability to process specific scenarios of a design, the ability to process all FACTS files contained within a specified directory, and the reporting of design scenario validation errors. See FLFLL documentation for details.\nIn FACTS Command Line mode and FLFLL, a new flag is available to specify the number of MCMC samples to generate for imputation purposes.\nIn FACTS Command Line mode, the ability to generate a design report has been added. This can be achieved by adding the -report flag and the -rpath flag, where the latter is used to specify the path to the R executable.\nFACTS now provides links to FACTS introductory videos hosted on YouTube via the Help menu.\nSimulation engine errors in FACTS are now displayed in the GUI more informatively.\nThe remaining time left on a FACTS license is now displayed correctly on the FACTS splash screen and Help menu.\nFACTS can now output up to 99,999 patients/weeks/frequentist/MCMC files. Previously, this was capped at 9,999 files.\nAll designs supporting design report generation can have their design report generated without having to perform an additional command execution step in RStudio, by selecting a valid R installation under Settings &gt; Options &gt; R Configuration.\nFACTS will now correctly handle the serialization/deserialization of text inputs involving the following characters: “&gt;”, “&lt;”, “&”, “ ’ ” and “\\”.\nWhen viewing FACTS simulation results through the GUI, estimates of responses, effects and hazard ratios (for Time-to-Event designs) will be display more obviously in the results column headers.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.5.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts640.html",
    "href": "documentation/versions/v6/facts640.html",
    "title": "FACTS 6.4.0 Release Notes",
    "section": "",
    "text": "FACTS 6.4.0 is now available for official release. Please contact us regarding any questions.\nThe key features of this release are:\n\nThree new dose response models in FACTS (Staged) Core designs.\nAlternative parametrizations to Posterior Probability Quantities of Interest (QOIs) in FACTS (Staged) Core Dichotomous and Time-to-Event designs.\nThe ability to run FACTS from R and to run FACTS in command-line mode on Linux (Enterprise licensees only).\n\nIn detail the new features in FACTS 6.4.0 are:\n\nThree new dose response models have been added across all FACTS (Staged) Core designs. These new options will appear in the model selection dropdown on the Dose Response tab. The new models are as follows:\n\nThe Simple Hierarchical model – a model in which the mean responses for each of the arms in the design are drawn from a normal distribution, whose mean and variance are estimated by FACTS. The control arm can be included in the hierarchical model, or modeled separately, in which case it has its own prior mean and variance. The control arm cannot be included in this model for Time-to-Event designs.\nThe Simple Linear model – a linear model which assumes that the mean responses for each of the arms in the design are linear functions of the associated arm strength. In particular, the arm with the largest mean response is guaranteed to be either the largest dose or the smallest arm in this model. Note that the “2-Parameter Logistic” model in FACTS (Staged) Core Dichotomous designs has been replaced by the “Simple Linear model”. FACTS (Staged) Core Dichotomous designs making use of the 2-Parameter Logistic model will be automatically migrated to the Simple Linear model.\nThe Simple Hierarchical Linear model – a model which uses a linear model as a base dose-response structure but allows deviations from linearity in a manner similar to the Hierarchical Logistic dose response model. Given appropriate priors, if the data and prior distributions are consistent with linearity, the hierarchical variance parameter will be estimated to be small and the model fit will be essentially linear, but if the data is non-linear the variance parameter will be large allowing a significantly non-linear model fit.\n\nIn FACTS (Staged) Core Dichotomous and TTE designs, Posterior Probability QOIs with alternative parametrizations can be set when creating a new QOI. This can be achieved by selecting the appropriate option in the “Compare” dropdown of the QOI dialog. The options are as follows:\n\nFor FACTS (Staged) Core Dichotomous designs, Posterior Probability QOIs comparing the log-odds ratio of the response rate for each arm against that of a given arm can now be created. Previously, only the response rates could be compared.\nFor FACTS (Staged) Core TTE designs, Posterior Probability QOIs comparing the hazard rates of the response for each arm against that of a given arm can now be created. Previously, only the hazard ratios (HR) could be compared.\n\nEnterprise FACTS licensees will now be able to access and run FACTS Core and Enrichment Design (ED) analysis models from R via an R wrapper, the output of which is an MCMC file pertaining to the model. This can be used to simulate trials that require posterior quantities that FACTS does not include (e.g., probability that a dose has a treatment effect in a certain range) or simulate trials that make decisions that FACTS does not include (e.g., sample size re-assessment).\nEnterprise FACTS licensees will also now be able to run FACTS in command-line mode on Linux via a separate executable: FACTS Linux File Loader Lite (FLFLL). Mono 6.8.0+ is a pre-requisite for running FLFLL. Executing a valid FACTS design with FLFLL will generate the same results output as its Windows GUI counterpart; in particular, it will generate the simulations, summary, weeks and patients files. FLFLL can be used to automate the simulation of multiple (potentially related) FACTS designs and, more generally, can be used as a key component of a more complex trial design simulation pipeline.\n\nThe following features were also implemented in FACTS 6.4.0:\n\nThe control arm can now be modelled separately in TTE predictor dose response models within FACTS (Staged) Core TTE designs.\nFACTS Core designs will now report the time of the stopping decision of the trial through a new simulations output column named “EarlySuccess Time”.\nFACTS now computes lower and upper frequentist CI bounds, bias and coverage at the simulation level for all design types and summarized them in the associated summary file.\nA command line option for the number of samples per imputation called “samples-per-imputation” has now been added to FACTS when run in command-line mode. This applied to FACTS (Staged) Core and ED designs.\nThe analysis tab now accepts subject files with missing values for intermediate visits (denoted by -9999).\nThe analysis tab in Multiple Endpoint now accepts data files when the design includes visits where none of the endpoints are observed.\nThe “Interim vs Final” Scatter plot graph in the “Across Scenarios” now handles interactive selection of QOI and setting of thresholds, including the use of p-value QOIs.\nThe FACTS installer will now include an option to share basic, anonymous usage and crash data with the FACTS team. This option can also be enabled/disabled by going to Setting &gt; Options &gt; Analytics. Any change in this area will take effect the next time FACTS is loaded. By default, FACTS will NOT collect any usage/crash data. However, we strongly encourage licensees to enable this option to help the FACTS team proactively improve the software in the areas that matter the most. We take our licensee’s data privacy and security very seriously, so do not hesitate to get in touch if you have any questions about this feature.\nFACTS will now, by default, automatically calculate the simulation parallelization packet size based on the number of requested simulations. A manual parallelization packet size can be set instead by setting the “Parallelization packet size” checkbox on the Simulation tab. In FACTS command-line mode, the packet size is automatically set unless the user explicitly specifies the “-p” flag.\nInformation about the FACTS license, namely its expiry date, is now available in Help &gt; About.\n[Enterprise licensees only] FACTS will automatically retry any actions involving communication with the FACTS HPC server if initial communication fails (e.g., due to an intermittent connectivity). The following FACTS infrastructure changes were performed as a part of our roadmap to modernize FACTS to make use of the latest available tech stack. Please communicate the following information to your IT team as needed:\nFACTS 6.4.0 will now target .NET Framework 4.5.2. Previous versions of FACTS target .NET Framework 4. Future versions of FACTS will target the latest available .NET Framework version.\nFACTS 6.4.0 is now a 64-bit application and will be installed on the target machine’s Program Files area. Previous versions of FACTS were 32-bit applications installed on the target machine’s Program Files (x86) area. This release addresses some situations in FACTS 6.3.0 and older versions that could cause different simulation results. If any of your designs replicate these circumstances, you are recommended to upgrade to FACTS 6.4.0:\nThe “Pause accrual and wait for completers if stopping rules are met” option on the Stopping Criteria tab of FACTS Dose Escalation N-CRM designs making use of open enrollment did not have the correct behavior when the option was unchecked. This is fixed in FACTS 6.4.0.\nThe standard deviation (SD) of the number of subjects having observed a Cat 2 Toxicity in FACTS Dose Escalation N-CRM designs was calculated incorrectly. This is fixed in FACTS 6.4.0.\nFACTS Dose Escalation N-CRM designs simulations results differed between Windows and Linux (including Windows VMs running on top of Linux) when the Toxicity response Rho parameter was non-zero. The Linux results are now consistent with the Windows results in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims triggered by complete predictor data were not respecting any specified minimum information required on the number of predictor completers before an interim can be triggered. This is fixed in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims triggered by complete predictor data an addition interim at “full predictor data” was being simulated even if not asked for. This is fixed in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims in stage 2 by time, after full accrual a circumstance can arise when follow up stops prematurely and an “inconclusive” result declared. This is fixed in FACTS 6.4.0.\nFACTS Staged TTE with a predictor endpoint and stage 1 data included in stage 2, any stage 1 subjects who had not had their predictor observed by the end of stage 1 had their predictor outcome censored rather than observed in stage 2. This is fixed in FACTS 6.4.0. Finally, there are two unique situations and areas identified in FACTS 6.4.0 (and prior versions) that will be continued developed and improved in future releases:\nIn FACTS Staged Design TTE, where the data inclusion is: “included where we have neither observed an event or the predictor and they are on an arm that is kept in stage 2” and stage 2 interim timings are based on “complete predictor data” and “stage 2 and included stage 1 data”, then FACTS is failing to include the included stage 1 subjects in calculating the timings of the interims in stage 2.\nIn FACTS Stage Design TTE where events are censoring for predictor outcomes, this censoring is not taken into account in the timing of interims by “Predictor Complete”. Resulting in the interims being too early.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.4.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts624.html",
    "href": "documentation/versions/v6/facts624.html",
    "title": "FACTS 6.2.4 Release Notes",
    "section": "",
    "text": "Berry Consultants would like to announce a new maintenance release, FACTS 6.2.4. FACTS 6.2.4 contains the following fixes to the FACTS 6.2.3 version:\nUpdating to 6.2.4 is recommended for those of you wishing to use predictive probabilities in FACTS Core TTE, in combination with a TTE predictor endpoint:\n\nIn FACTS Core with a Time-to-Event end point and a Time-to-Event predictor, the imputations of final event times for subjects with a predictor event but no final event during the estimation of “predictive probability of success at full enrolment” could produce an error in the prior version. There are two rare situations in FACTS 6.2.3 that uncover a bug in the dose escalation simulator and causes it to produce an error:\nIn FACTS Dose Escalation, in the N-CRM with only 3 doses the simulator could produce an error during some dose escalation decisions.\nIn FACTS Dose Escalation CRM (Toxicity) could produce an error when simulating 2 samples. The remaining, minor fixes in FACTS 6.2.4 are:\nIn FACTS N-CRM, the GUI was improved to handle the “Variant” options making is easier to change them once they were set.\nA fix to FACTS Dose Escalation 3+3 (!) – improved to handle the circumstance when the starting dose is not the lowest dose, and the dose assignment de-escalates to below the starting dose and validates the next lower dose.\n\nPlus we improved the labeling of a class of prior parameters:\n\nIn the FACTS GUI labels of parameters for prior with an Inverse-Gamma distribution the wording has been changed from “mean value” (which is technically incorrect) to “central value”.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.2.4 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts620.html",
    "href": "documentation/versions/v6/facts620.html",
    "title": "FACTS 6.2.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.2.0\nBerry Consultants is delighted to announce that FACTS 6.2.0 is ready for release!\nBuilding on FACTS 6.1.0, FACTS 6.2.0 adds new features to “FACTS N-CRM”, the ability to generate a “Design Report” from FACTS Core designs and extending the ability to compute predictive probabilities to FACTS Core TTE and FACTS Staged TTE.\n\nFACTS N-CRM extensions. FACTS has had versions of the CRM with an efficacy endpoint, ordinal toxicity endpoint and 2 groups since its inception. But these were in separate engines and used the old CRM model for analysis. We have now added all these features as options to the N-CRM so they can be used with the 2 parameter Bayesian Logistic Regression model, targeting toxicity bands and the option to use overdose control. These features cannot only now but used with this better methodology, but can be used in combination with each other, and in combination with the other advanced features that were already included in the FACTS DE N-CRM simulator such as, run ins, stopping rules, escalation rules, fine grain dosing and open enrollment.\n\n\n\n\nNew N-CRM Graph\n\n\n\nFACTS Design Report. In FACTS Core there is now the ability to generate a “Design Report” as a MS Word file that describes the design and simulation results. The file is not intended as the final article but as something where the bulk of the straightforward text (and equations) have been provided and should just require polishing, particularly with the details of the indication and trial setting that FACTS is inevitably unaware of.\n\n\n\n\nNew Design Report\n\n\n\nFACTS 6.2 completes the implementation of predictive probabilities. Predictive probabilities in the current trial with a TTE endpoint are considerably more complex than predictive probabilities in the other endpoints. For the other endpoints the expected about of information after full enrollment and full follow-up is known, for time-to-event it can depend on multiple things such as accrual rate and the expected number of events so a degree of simulation within the simulation is required.\n\nFACTS 6.2.0 is fully backwards compatible with FACTS 6.1.0, 6.0.0 and 5 – it can load and run all your FACTS 6.0.0 and FACTS 5 designs – and then use new FACTS 6.2.0 features with those designs. You can have FACTS 6.2.0 and FACTS 6.1.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Dose Escalation, the N-CRM (also known as Bayesian Logistic Regression) now has options for:\n\nAn ordinal toxicity endpoint\nTo simulate a trial across 2 groups (e.g. Adults and Pediatrics)\nAn additional binary Efficacy endpoint\nThese options can be combined with each other and all the other N-CRM options.\n\nFACTS Core TTE\n\nThe ability to compute the predictive probability of success at the full enrollment of the current trial.\n\nFACTS Staged Design TTE\n\nThe ability to compute the predictive probability of success\n\nin Stage 1 at full enrollment\nof Stage 2 (whilst in Stage 1)\nin Stage 2 at full enrollment (whilst in Stage 2).",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.2.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html",
    "href": "documentation/v71/userguides/de.html",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document describes how to install and use the Rule-Based Dose Escalation (DE) Fixed and Adaptive Clinical Trial Simulator (FACTS) software (from now on referred to as Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software). It is intended for all end users of the system.\n\n\n\nThis document covers the Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software by describing the user interface. It covers the 3+3 and mTPI. It does not cover the CRM design engine which has its own User Guide.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS, 5 or later if changed, installed on Windows 7 and Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nThis document is a guide to the version 7.1 release of Dose Escalation FACTS.\nThere have been no changes to these elements of FACTS since FACTS 6.1.\n\n\n\nPlease cite FACTS wherever applicable using this citation.\n\n\n\nAn overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#purpose-of-this-document",
    "href": "documentation/v71/userguides/de.html#purpose-of-this-document",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document describes how to install and use the Rule-Based Dose Escalation (DE) Fixed and Adaptive Clinical Trial Simulator (FACTS) software (from now on referred to as Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software). It is intended for all end users of the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#scope-of-this-document",
    "href": "documentation/v71/userguides/de.html#scope-of-this-document",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document covers the Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software by describing the user interface. It covers the 3+3 and mTPI. It does not cover the CRM design engine which has its own User Guide.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS, 5 or later if changed, installed on Windows 7 and Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#context-of-this-issue",
    "href": "documentation/v71/userguides/de.html#context-of-this-issue",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "This document is a guide to the version 7.1 release of Dose Escalation FACTS.\nThere have been no changes to these elements of FACTS since FACTS 6.1.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#citing-facts",
    "href": "documentation/v71/userguides/de.html#citing-facts",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#definition-of-terms",
    "href": "documentation/v71/userguides/de.html#definition-of-terms",
    "title": "3+3 / mTPI",
    "section": "",
    "text": "An overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-7.1-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-7.1-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 7.1 Changes to Dose Escalation",
    "text": "FACTS 7.1 Changes to Dose Escalation\nThe CRM (Toxicity), CRM (Ordinal), CRM (Efficacy), bCRM are now considered deprecated. Existing designs will still work as expected, but users starting a new design are encouraged to switch over to the new CRM engine if possible.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-7.0-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-7.0-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 7.0 Changes to Dose Escalation",
    "text": "FACTS 7.0 Changes to Dose Escalation\nIn FACTS 7.0 the only change was on the Analysis tab, when entering subject data manually, the FACTS GUI now ensures that a cohort number is entered for every subject.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.5-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.5-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.5 Changes to Dose Escalation",
    "text": "FACTS 6.5 Changes to Dose Escalation\nIn FACTS 6.5 there were no changes to the Dose Escalation simulators.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.4-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.4-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.4 Changes to Dose Escalation",
    "text": "FACTS 6.4 Changes to Dose Escalation\nIn FACTS 6.4 there were no changes to the Dose Escalation simulators.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.3-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.3-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.3 Changes to Dose Escalation",
    "text": "FACTS 6.3 Changes to Dose Escalation\nIn FACTS 6.3 there were no changes to Dose Escalation except in N-CRM which is described in its own User Guide.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.1-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.1-changes-to-dose-escalation",
    "title": "3+3 / mTPI",
    "section": "FACTS 6.1 Changes to Dose Escalation",
    "text": "FACTS 6.1 Changes to Dose Escalation\nIn FACTS 6.1:\n\nTwo new types of design have been added:\n\nThe mTPI design - described in this document.\nThe 2D-CRM for phase 1 dose escalation trials of combinations of multiple doses of 2 drugs described in the FACTS DE 2D-CRM User Guide.\n\nA ‘design variant’ facility has been added to the N-CRM engine that allows the user to easily simulate and evaluate an N-CRM design at different sample sizes. See the updated FACTS DE N-CRM User Guide for details.\nNote that from FACTS 6.0 onwards the default MCMC sampling length for Dose Escalation designs was increased from 2500 to 25000. This was because it was found that with the shorter sampling length the results were too inconsistent. It has however significantly slowed the speed with which FACTS DE designs are run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#introduction-1",
    "href": "documentation/v71/userguides/de.html#introduction-1",
    "title": "3+3 / mTPI",
    "section": "Introduction",
    "text": "Introduction\nTo open the application, select FACTS Dose Escalation from your Start menu. At different points in the FACTS GUI, the user is required to make decisions about how to model or simulate various aspects of a clinical trial. To simplify data entry, FACTS shows the user only the information that is relevant to the current decisions.\nFACTS has a tabbed design to allow entry of different categories of information about the design.\nThe File menu allows the user to create a new design, open an existing design, save a design, or copy a design to a new name (Save as). The saved design includes all parameters entered as well as simulation results, if they have been produced.\nThe Settings menu provides the ability to configure how simulation jobs should be submitted to the Grid. Selecting whether to execute simulations locally or on the Grid is done from the Simulation tab.\nThe Help menu allows the user to learn more about the FACTS software and verify the application’s version number.\nFinally, note that in all tabs of the application, red exclamation points\n\n\n\n\n\n\nFigure 2\n\n\n\nindicate errors in data entry from the user that must be corrected. Moving the cursor over the exclamation point causes a pop-up help text indicating what the error is, helping the user remedy the error.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#selecting-a-design-model",
    "href": "documentation/v71/userguides/de.html#selecting-a-design-model",
    "title": "3+3 / mTPI",
    "section": "Selecting a Design Model",
    "text": "Selecting a Design Model\nOnce the FACTS application has been opened, the user may select to model a clinical trial using any of the following design engines from the introduction screen or the File &gt; New &gt; Dose Escalation menu:\n\n\n\n\n\n\nFigure 3\n\n\n\nOnce a design has been selected, the associated form of the GUI will be displayed, the contents of the tabs and sub-tabs of FACTS displaying the information relevant to the selected model. Additionally, the model type selected will appear in the title bar of FACTS to verify the user’s choice of design.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#study-info",
    "href": "documentation/v71/userguides/de.html#study-info",
    "title": "3+3 / mTPI",
    "section": "Study Info",
    "text": "Study Info\nThe Study Info sub-tab provides basic study parameters such as Trial Size and Cohort options, as relevant to the type of design. Depending on the choice of design engine, the Study info tab may also allow the user to enter specification for a two-sample population, Toxicity and/or Efficacy targets, response category options, or Joint Efficacy and Toxicity options.\n\nRecruitment, like N-CRM the mTPI design includes the option to use Open Enrollment instead of recruitment by cohorts. A phase 1 trial using open enrolment recruits ‘all comers’ with a cap on the number of subjects who can be treated but have not yet had a final result. Subjects that arrive whilst this cap has been reached are deemed to be ‘lost’ to the study. If selected the user specifies the following additional parameters.\n\nMaximum study size (subjects): the maximum number of subjects who can be recruited into the study.\nMean recruitment ate: for simulation purposes this is the average number of subjects who will be expected to be available for recruitment during each time period. Recruitment is simulated using a standard Poisson process.\nTime until final result: the time for each subject’s final result to become available. This is simulated as a fixed length of time that is the same for all subjects.\nMaximum subjects without a final result: the cap on the number of subjects who can be recruited but not yet completed. This is also known as the ‘maximum queue length’. Subjects arriving and available for recruitment while the number of subjects treated but not yet completed is a the cap are dropped and assumed not available for recruitment once the current subjects complete but the study has to await further new subjects to become available.\n\nStudy Size – (unless an mTPI design using open enrollment) for all designs this is specified in terms of the maximum number of cohorts.\n\nIf the design is CRM(Toxicity) or CRM(Efficacy) and the option to include a “Two sample population” analysis is included then the Study Size applies to the first sample and with an additional parameter the user specifies the maximum size of the second sample, also in cohorts.\n\nCohort - (unless an mTPI design using open enrollment and if not a “3+3” design where cohorts are always of size 3 and must always complete before the next cohort is recruited) then the user can specify:\n\nCohort size: The number of subjects in each cohort. The maximum sample size for the trial is simply: Max Trial Size (cohorts) * Cohort Size.\nCohort information delay: Normally left at zero, this allows for the trial to be simulated with cohorts recruited more quickly than if waiting for the last cohort to complete before treating the next cohort. This allows the trial to complete in shorter time, but is likely to lead to a larger incidence of toxicities. If the delay is set to 1, the second cohort will be recruited before the results for the first cohort are available, the third cohort will be recruited after the results of the first cohort are available but before the results for the second cohort are available, and so on.\n\nJoint Efficacy and Toxicity – (bCRM only) – there is a flag that the user can set to indicate that a subject cannot experience both toxicity and efficacy (toxicity censors or prevents efficacy).\n\n\n\nSingle subject run-in – (CRM(Toxixity), bCRM, CRM(Ordinal) only) – this allows the user to specify that the trial is to start with a single subject run-in. With a single subject run-in, subjects are allocated in cohorts of one, incrementing by one dose strength each cohort until a toxicity is observed. The user has the option to specify whether, on observing the first toxicity, the first full cohort is then allocated at that dose, or at the dose below.\nTarget – (mTPI only) – somewhat similar to the N-CRM, in mTPI the target is defined as toxicity bands rather than single toxicity rate. The user specifies the lower and upper bounds of the target toxicity band as well as a target toxicity rate within that band.\nThe parameters to specify the Toxicity target are displayed for CRM(Toxicity), bCRM and CRM(Ordinal) designs. These allow the user to specify the maximum tolerated toxicity target and choose which dose should be selected based one of the following criteria:\n\nnearest dose to target\nnearest dose above the target (except for ordinal CRM)\nnearest dose below the target\nIf a control arm is included with the Treatment arms then there is the option to specify that the target toxicity rate is relative to control, not absolute.\n\nSimilarly the parameters to specify the Efficacy target are displayed for CRM(Efficacy) and bCRM. These allow the user to specify the maximum tolerated toxicity target and choose which dose should be selected based one of the following criteria:\n\nnearest dose to target\nnearest dose above the target (except for ordinal CRM)\nnearest dose below the target\nIf a control arm is included with the Treatment arms then there is the option to specify that the target efficacy rate is relative to control, not absolute.\n\nCohort Expansion – (all designs except CRM(Efficacy) The user can specify that after the dose escalation phase of the trial has completed, the simulation is to include an ‘expansion cohort’ allocated at the final value of the target dose, and how large that expansion cohort will be.\n\nIf a Control arm has been included, then there can be a specific allocation to Control as well (these are taken from the overall cohort size, they are not additional to it).\nIn bCRM the user selects whether the expansion cohort is allocated to the MTD, MED or OSD.\n\n\nFigure 4 displays the Study Info sub-tab for a bCRM design. The functionality and look of the tab is similar for other CRM designs and mTPI.\n\n\n\n\n\n\nFigure 4: Study Info (bCRM)",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#treatment-arms",
    "href": "documentation/v71/userguides/de.html#treatment-arms",
    "title": "3+3 / mTPI",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nThe Treatment Arms sub-tab (Figure 5) provides an interface for specifying the various dose levels, and (except in “3+3”) a Control treatment arm.\nIf a control arm is included:\n\nTarget rates can be specified to be relative to control (otherwise they are absolute).\nA specific number of subjects are specified to be allocated to control in each cohort\nThe response on the Control arm can be included in the dose response model, or modeled separately (using a simple beta-binomial model) in which case monotonicity is not enforced.\n\nThe user may add doses either explicitly or by auto-generation, as depicted below. The user may also edit the Dose Names within the table by double clicking on any existing dose name, however the index cannot be edited.\nNote that the CRM designs use transformed dose levels, and unlike the Dose Finding designs do not have a relative dose level specified.\n\n\n\n\n\n\nFigure 5: Treatment Arms",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#explicitly-defined",
    "href": "documentation/v71/userguides/de.html#explicitly-defined",
    "title": "3+3 / mTPI",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\n\nToxicity [3+3, CRM(Toxicity), mTPI and bCRM]\nThe Toxicity sub-tab provides an interface for specifying one or more Toxicity profiles.\nToxicity profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 6. Toxicity values are entered directly into the Probability of Toxicity column of the table, and the graphical representation of these toxicity values updates accordingly. The graph may be modified by plotting the log of the dose strength as the x-axis, and by plotting the logit or the probability of toxicity as the y-axis.\nThis graph – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right-clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\n\n\n\n\n\n\nFigure 6: Toxicity Virtual Subject Response (bCRM)\n\n\n\nWhen using a bCRM design engine, if the Toxicity and Efficacy profiles are specified separately, then the results are simulated without correlation. To simulate correlation in the results it is necessary to specify joint profiles.\n\n\nToxicity with 2 Samples [CRM(Toxicity)]\nIf utilizing a CRM (Toxicity) design, the user has the option to model two sample populations (this option may be selected on the Study Info tab, as described here). If the user elects to enable the modeling of two samples, then the Toxicity sub-tab allows the user to input probabilities of Toxicity for each sample, as displayed in Figure 7 below.\n\n\n\n\n\n\nFigure 7: Toxicity Virtual Subject Response with 2 samples (CRM (Toxicity))\n\n\n\n\n\nOrdinal Toxicity [CRM(Ordinal)]\nFor CRM (Ordinal) designs, the user must specify the probability of toxicity at or above each category. Ordinal designs can use either three or four categories, with a category three toxicity corresponding to a toxic response in a CRM (Toxicity) design. Toxicity data must be entered for each category greater than 1 and must be monotonically decreasing with category (Figure 8).\n\n\n\n\n\n\nFigure 8: Toxicity Virtual Subject Response (CRM (Ordinal))\n\n\n\n\n\nEfficacy [CRM (Efficacy) and bCRM]\nSimilar to the Toxicity sub-tab, the Efficacy sub-tab provides an interface for specifying one or more Efficacy profiles.\nEfficacy profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen.\nEfficacy values are entered directly into the Probability of Efficacy column of the table, and the graphical representation of these efficacy values updates accordingly.\nIf utilizing a bCRM design, in which both Toxicity and Efficacy profiles are utilized, the user must ensure that profile names are unique. FACTS does not allow the user to enter Toxicity profiles and Efficacy profiles which are named identically.\nFinally, if utilizing a CRM (Efficacy) design, the user has the option to model two sample populations (this option may be selected on the Study Info tab, as described here). If the user elects to enable the modeling of two samples, then the Efficacy sub-tab allows the user to input probabilities of Efficacy for each sample, just as in the case of CRM (Toxicity) illustrated in Figure 9.\n\n\n\n\n\n\nFigure 9: Efficacy Virtual Subject Response (CRM Efficacy)\n\n\n\n\n\nJoint Efficacy / Toxicity [bCRM]\nThe Joint Efficacy / Toxicity sub-tab (Figure 10) allows the user to specify the probability of efficacy, toxicity, and success at each dose.\nJoint profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in (Figure 10). The user must ensure that any profile name entered on this tab is unique from all other profile names in the application, whether on this tab, or on the Toxicity sub-tab or the Efficacy sub-tab.\nProbabilities of toxicity, efficacy, and success are entered directly into the table, and the graphical representation of these probabilities updates accordingly. Success is the probability of observing efficacy without toxicity, thus the rate for success is naturally bounded:\n\nThe probability of success at a dose cannot exceed the probability of efficacy at that dose.\nThe probability of success at a dose cannot exceed (1 – the probability of toxicity) at that dose.\nThe probability of success cannot be less than the probability of efficacy minus the probability of toxicity at that dose.\nIf the user enters probabilities violating these limit, then FACTS mark them as invalid and will refuse to simulate the scenarios including the profile. FACTS reports to the user, on the Simulation tab, that the following error has been found: “True toxicity and Efficacy curve is not in range [0,1]” and will ask the user to resolve the error before running simulations.\n\nJoint Efficacy / Toxicity profiles may also be generated from models, rather than explicitly. This way of specifying Joint Efficacy / Toxicity profiles is described in this section.\n\n\n\n\n\n\nFigure 10: Joint Efficacy",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#external",
    "href": "documentation/v71/userguides/de.html#external",
    "title": "3+3 / mTPI",
    "section": "External",
    "text": "External\nSubject response data may be simulated from a PK-PD model in place of or in addition to choosing a response model from FACTS. The importing of subject response data (which must be in the form specified in the System Requirements Document) may be done from the External Files sub-tab depicted below (Figure 11).\nTo import an external file, the user must first add a profile to the table. After adding a profile, the user must click “Browse” to locate the externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 11: External Data",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#parametric-bcrm-sec-parametric-bcrm",
    "href": "documentation/v71/userguides/de.html#parametric-bcrm-sec-parametric-bcrm",
    "title": "3+3 / mTPI",
    "section": "Parametric [bCRM] (#sec-parametric-bcrm)",
    "text": "Parametric [bCRM] (#sec-parametric-bcrm)\nThe Parametric sub-tab allows the user to specify the Joint Efficacy / Toxicity profiles as modeled by Cox or Gumbel functions. This allowance is in contrast to the explicit definition of Joint Efficacy / Toxicity profiles, as may be done in the Joint Efficacy / Toxicity sub-tab of the Explicitly Defined Response tab (see this section).\nThe interface of the Parametric tab is displayed below (Figure 12). As with other Virtual Response sub-tabs, profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 12.\n\n\n\n\n\n\nFigure 12: bCRM Parametric Joint Toxicity & Efficacy VSR - Cox Model\n\n\n\n\n\n\n\n\n\nFigure 13: bCRM Parametric Joint Toxicity & Efficacy VSR - Gumbel Model",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-and-efficacy-response-tabs-crm-efficacy-crm-toxicity-and-bcrm",
    "href": "documentation/v71/userguides/de.html#toxicity-and-efficacy-response-tabs-crm-efficacy-crm-toxicity-and-bcrm",
    "title": "3+3 / mTPI",
    "section": "Toxicity and Efficacy Response tabs [CRM (Efficacy), CRM (Toxicity) and bCRM]",
    "text": "Toxicity and Efficacy Response tabs [CRM (Efficacy), CRM (Toxicity) and bCRM]\nOn the Toxicity and Efficacy Response sub-tabs, the user may choose toxicity and efficacy models from among a list available for this release. The supported models for modeling toxicity and efficacy response in this release are logistic, tanh, and power for one sample studies, and tanh (x-b) for two sample studies.\nThe user has the option to set the Minimum and Maximum parameter values, which define the toxicity/efficacy asymptotes. These values are used to rescale the probabilities and calculated the scaled dose values, X^ (X-hat). These default to 0 and 1, but if the endpoint being observed is expected to have a background rate (that would be observed even if placebo were administered) or a natural maximum rate (that would not be exceeded whatever the strength of dose used), then the model fitting will be improved\nThe Toxicity Response tab is depicted below in Figure 14; the Efficacy Response tab has a similar appearance.\n\n\n\n\n\n\nFigure 14: Toxicity Response (bCRM)\n\n\n\nThe user selects the model type:\n\nLogisitic, in which case the fixed value for the Alpha parameter is specified (usually this is set to 3 – giving a toxicity rate of 0.953 at a dose with a transformed dose strength of 0. Effective dose strengths should typically be in the range (-8, -1)\nTanh in which case the effective dose strengths should typically be in the range (-2, 1)\nPower in which case the effective dose strengths should typically be in the range (0,1)\n\nThe user specified the prior distribution of the estimated model parameter (Beta) either as an Exponential (1) distribution or a uniform distribution (0,U) where U is specified by the user.\nThe effective dose strengths of the doses are specified in the “Model Priors” table. They can be specified in one of two ways:\n\nBy defining a prior probability of toxicity for each dose. The corresponding transformed dose strength is then calculated as the value that would yield that toxicity rate from the model with Beta=1. The corresponding transformed dose strength is shown in the right hand column. Care needs to be taken with this approach that the resulting transformed dose strengths are well spaced out – this can be checked on the graph that shows the transformed dose strengths, and the corresponding toxicity rate on the graph of the Beta=1 model.\nBe specifying the effective dose strength explicitly. Care needs to be taken with this approach, depending on the model this may yield nonsensical toxicity rates, but once the correct range for the transformed doses is understood, it easier to ensure a good spacing of the transformed dose strengths.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-response-tab-crmordinal",
    "href": "documentation/v71/userguides/de.html#toxicity-response-tab-crmordinal",
    "title": "3+3 / mTPI",
    "section": "Toxicity Response tab [CRM(Ordinal)]",
    "text": "Toxicity Response tab [CRM(Ordinal)]\n\n\n\n\n\n\nFigure 15: CRM(Ordinal) Toxicity Response tab",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#design-tab-mtpi",
    "href": "documentation/v71/userguides/de.html#design-tab-mtpi",
    "title": "3+3 / mTPI",
    "section": "Design tab [mTPI]",
    "text": "Design tab [mTPI]\nThe mTPI method has two parameters that can be specified\n\nThe first is an option to prevent re-escalating to a dose (the “DU” or Do not Use category in the diagram) if the posterior probability that the toxicity rate is above the target toxicity plus the delta: pT+εU (on the Study &gt; Study Info tab this is the “Upper Bound” parameter) exceeds a specified threshold (0.95 is the default).\nThe second is an option that allows an early stopping rule to be specified that if the dose to be allocated next already has the maximum number of subjects on it then it is declared the MTD. Setting this value shrinks the size of the table displayed.\n\nGiven the target toxicity rate and the boundaries specified on the Study &gt; Study Info tab, and the parameters specified here the table of mTPI dosing decisions is displayed. This shows the dosing decision given the number of subject treated at the current dose and the number of toxicities observed:\n\nE Escalate\nS Stay\nD De-escalate\nDU De-escalate and do not revisit\n\n\n\n\n\n\n\nFigure 16: mTPI Design tab",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#allocation-rule",
    "href": "documentation/v71/userguides/de.html#allocation-rule",
    "title": "3+3 / mTPI",
    "section": "Allocation Rule",
    "text": "Allocation Rule\nThe Allocation Rule sub-tab is depicted below (Figure 17); it allows the user to set basic allocation parameters, including Maximum Dose Increment, Minimum Cohorts on Dose prior to Increment, and Maximum number of Cohorts used to determine MTD (bCRM only).\nThe dose range may be split with a greater maximum increment allowed in the upper part of the range than in the lower.\nWhen specifying the maximum dose increment, the user has two options of how to apply that increment; relative to the current dose or relative to the highest dose with a specified number of cohorts. In other words, the largest possible value for the next allocation at any point in the trial is either the current dose level plus the Maximum Dose Increment, or the highest dose that has been allocated to Max Subjects Before Incrementing plus the Maximum Dose Increment.\n\n\n\n\n\n\nFigure 17: Design & Allocation Rule tab (CRM Toxicity)\n\n\n\nAlso on this tab, the user may specify sampling rule parameters, namely the initial dose level for sampling. If using a “3+3” design, the sampling rule parameters will be the only parameters on this screen (and hence the tab will be called “Sampling Rule” in place of “Allocation Rule”), since allocation does not apply to the “3+3” design engine.\n\n\n\n\n\n\nFigure 18: Design & Allocation Rule tab (bCRM)\n\n\n\nIf utilizing a CRM (Toxicity) design, and if the option to model two sample populations has been enabled (this option may be selected on the Study Info tab, as described in this section), the user also has the option to set the initial dose for the second population.\nAdditionally, a single subject run-in can also be specified on the Allocation Rule tab. A single subject run-in starts at the specified starting dose and allocates a single subject to each dose until either a toxicity is observed or the maximum dose is reached. This option is available for CRM (Toxicity), CRM (Ordinal), and bCRM only. Note that subjects in the single subject run-in are not included in the “max trial size”, so with a single subject run-in, it is possible to end a simulation with more subjects that were specified in the max trial size.\nIf a Control treatment arm has been included, the user must also specify the number of subjects per cohort that should be allocated to Control. The Control dose is treated differently than the active doses, and cannot ever be found as the MTD or MED. Thus, it will never be assigned a cohort based on the fitted model, and must have subjects assigned per cohort.\nFinally, if utilizing a CRM (Efficacy) design, the user also has the option on this tab to allocate extra subjects to the min or max dose, as shown in Figure 19. “Probability gamma” is used to ensure that the numbers of subjects allocated to the maximum and minimum stay close to their specified values. Setting gamma (γ) to 0 turns this off, so the actual number allocated may drift away from the number expected. 2 is a good value to enable this correction.\nThe formula used is:\n\\(p = p_T^{\\gamma (p_O - p_T)}\\)\nwhere pT is the target probability as entered above, pO is the observed probability measured from the subjects allocated so far and p is corrected the probability that will be used in allocation. To prevent wild allocations, p is subject to the restrictions, p&lt;0.5 and pT/4 &lt; p &lt; 4pT.\n\n\n\n\n\n\nFigure 19: Allocation Rule (CRM (Efficacy))\n\n\n\nWhen a Control treatment arm is included, it is treated as the minimum dose, and gives the user another option of how to allocate to it. In this case, the user can specify a probability of allocating whole cohorts to the Control, or a number of subjects per cohort allocated to Control. The two options cannot be combined.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#stopping-criteria-crm-efficacy-crm-toxicity-crm-ordinal-n-crm-and-bcrm-only",
    "href": "documentation/v71/userguides/de.html#stopping-criteria-crm-efficacy-crm-toxicity-crm-ordinal-n-crm-and-bcrm-only",
    "title": "3+3 / mTPI",
    "section": "Stopping Criteria (CRM (Efficacy), CRM (Toxicity), CRM (Ordinal), N-CRM and bCRM only)",
    "text": "Stopping Criteria (CRM (Efficacy), CRM (Toxicity), CRM (Ordinal), N-CRM and bCRM only)\nFor any design, including “3+3”, study simulation will always stop when the maximum number of subjects has been achieved.\nThe study simulation may also stop early when the following conditions that have been enabled by the user are met:\n\nMTD finding (CRM (Toxicity), N-CRM and bCRM only)\n\nthe user-specified minimum number of subjects are allocated to the MTD\nAND the following optional rules either ANDed or ORed together.\n\nThe number of dose levels in the specified confidence interval meets the specified the threshold\nThere is a dose with a probability of being the MTD that is greater than the specified threshold.\nThe minimum number of cohorts have been accrued\n\n\nMED finding (CRM (Efficacy) and bCRM only)\n\nthe user-specified maximum number of subjects are allocated to the MED\nthe number of dose levels in the confidence interval meets the specified number\nthere is a dose with a probability of being MED that is greater than the specified threshold\nThe minimum number of cohorts have been accrued\n\nmTPI when the user specified number of subjects are already allocated to the next dose to be allocated – this is specified on the mTPI Design tab.\n\nWhen studying two samples, stopping rules will be evaluated and applied separately for each sample.\nThese parameters are set on the Stopping Criteria sub-tab as depicted below in Figure 20 Stopping tab CRM Toxicity.\n\n\n\n\n\n\nFigure 20: Stopping tab CRM Toxicity\n\n\n\nFor bCRM the stopping rules become the rules for stopping the MTD phase and starting the MED phase. There are then similar rules that can be used for stopping the MED phase early.\n\n\n\n\n\n\nFigure 21: Stopping Criteria (bCRM)\n\n\n\nFinally, if utilizing a CRM (Efficacy) design, the user may specify an additional set of parameters for stopping for futility, as displayed at the bottom of Figure 22. These allow the trial to be stopped early for futility. If the user enable these rules then the user can specify:\n\nThat a minimum number of cohorts must have been allocated to the minimum and maximum doses.\nThat the trial will stop unless there is a minimum difference in the mean response between the minimum and maximum doses.\n\n\n\n\n\n\n\nFigure 22: Stopping Criteria (CRM (Efficacy))",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#to-run-simulations",
    "href": "documentation/v71/userguides/de.html#to-run-simulations",
    "title": "3+3 / mTPI",
    "section": "To run simulations",
    "text": "To run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user may not modify any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#how-many-simulations-to-run",
    "href": "documentation/v71/userguides/de.html#how-many-simulations-to-run",
    "title": "3+3 / mTPI",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%).\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios.\n\nSimulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nAll: A window containing all the summary results columns\nHighlights: (all) a separate window with the results shown on the main tab\nAllocation, Observed: (all) summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity/efficacy/response: (all CRM) summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc / Pr(MED) etc: (all CRM) summary results of the posterior probabilities of the properties of interest. In bCRM these values are organised in three groupings called “Probabilities”, “Toxicity” and “Efficacy”.\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\nTwo Sample results: (CRM Toxicity)\n\nView Graph: opens the FACTS built in graph utility displaying the results for the currently selected scenario. See Section 11 below for a description of the graphs.\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options:\n\nOpen results folder: Opens a file browser in the results folder of the scenario, allowing swift access to any of the results files.\nSimulation results: Opens a window displaying the individual simulation results for each simulation of the currently selected scenario\nOpen in R: opens a control that will launch R, first loading the selected files in the results folder as data frames.\nView Graphs: launches the graph viewer to view the results of the currently selected scenario.\n\n\n\n\nMCMC Settings\n\n\n\n\n\n\nFigure 24\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nIf the Number of MCMC samples to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output, allowing the user to check convergence.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/de.html#facts-grid-simulation-settings",
    "title": "3+3 / mTPI",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#detailed-simulation-results",
    "href": "documentation/v71/userguides/de.html#detailed-simulation-results",
    "title": "3+3 / mTPI",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 25) will reveal more detailed results about the simulation.\n\n\n\n\n\n\nFigure 25: Detailed Simulation Results\n\n\n\nRight-clicking on a row displays a context menu from which the user can also view the corresponding cohort results:\n\n\n\n\n\n\nFigure 26: bCRM Parametric Joint Toxicity & Efficacy VSR - Gumbel Model",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#aggregation",
    "href": "documentation/v71/userguides/de.html#aggregation",
    "title": "3+3 / mTPI",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 27\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose, plus two extra rows for ‘below lowest’ and ‘above highest’.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nIf there is no value for the below lowest or above highest rows, they will be left blank.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. cohortsNNN.csv files are aggregated into a single agg_[pivot_]cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#highlights-all",
    "href": "documentation/v71/userguides/de.html#highlights-all",
    "title": "3+3 / mTPI",
    "section": "Highlights (All)",
    "text": "Highlights (All)\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nEngines\nDescription\n\n\n\n\nSelect\n1\nAll\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nAll\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nAll\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nNum Sims\n1\nAll\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nAll\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nAll\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPPn. Tox\n1\nAll except CRM Efficacy\nThis is the average proportion of the subjects recruited that experienced a toxicity in the simulations of this scenario.\n\n\nSD Ppn. Tox\n1\nAll except CRM Efficacy\nThis is the standard deviation of the proportion of toxicity across the simulations of this scenario.\n\n\nPpn Eff\n1\nCRM Efficacy & bCRM\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nCRM Efficacy & bCRM\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Tox\n1\nAll except CRM Efficacy\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nTrue Ppn Eff\n1\nCRM Efficacy & bCRM\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile of the scenario\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nFor each dose, this is proportion of the simulations where it was selected as the MTD (Maximum Tolerated Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MTD (closest dose to having the target toxicity rate – or “nearest below” or “nearest above” as selected by the user on the Study Info tab.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nCRM Efficacy & bCRM\nFor each dose, this is proportion of the simulations where it was selected as the MED (Minimum Efficacious Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MED (“closest” dose to having the target efficacy rate – or “nearest below” or “nearest above” as selected by the user on the Study Info tab.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nbCRM only\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. The dose chosen as the OSD will be the MED if that is below the MTD, otherwise it will be the MTD.\n\n\nPpn(All Tox)\n1\nAll\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nPpn(early Success)\n1\nAll\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nPpn(Cap)\n1\nAll\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#allocations-observed-all",
    "href": "documentation/v71/userguides/de.html#allocations-observed-all",
    "title": "3+3 / mTPI",
    "section": "Allocations, Observed (All)",
    "text": "Allocations, Observed (All)\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nEngines\nDescription\n\n\n\n\nScenario\n1\nAll\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Subj.\n1\nAll\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Tox\n1\nAll except CRM Efficacy\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nSD PPn. Tox\n1\nAll except CRM efficacy\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nPpn, Eff\n1\nCRM Efficacy and bCRM only\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nCRM Efficacy and bCRM only\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Tox\n1\nAll except CRM Efficacy\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nTrue Ppn Eff\n1\nCRM Efficacy and bCRM only\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile of the scenario\n\n\nNum Phase 1\n1\nbCRM Only\nThe mean (over the simulations) of the number of subjects allocated during phase 1 (the MTD finding part) of the trial.\n\n\nSubj. Per Dose: &lt;dose&gt;\nOne per dose\nAll\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subjects: &lt;dose&gt;\nOne per dose\nAll\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox. Per Dose: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nEff. Per Dose: &lt;dose&gt;\nOne per dose\nCRM Efficacy and bCRM only\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff. Per Dose: &lt;dose&gt;\nOne per dose\nCRM Efficacy and bCRM only\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\n80% Num Subj\n1\nAll\nThis is the 80th centile of the overall number of subjects recruited in each simulation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#fitted-toxicity-crm-toxicity-crm-ordinal-bcrm",
    "href": "documentation/v71/userguides/de.html#fitted-toxicity-crm-toxicity-crm-ordinal-bcrm",
    "title": "3+3 / mTPI",
    "section": "Fitted Toxicity (CRM Toxicity, CRM Ordinal, bCRM)",
    "text": "Fitted Toxicity (CRM Toxicity, CRM Ordinal, bCRM)\n\n\n\nColumn Title\nNumber of columns\nEngines\n\n\n\n\nScenario\n1\nAll\n\n\nMean Beta 1\n1\nAll\n\n\nSD Beta 1\n1\nAll\n\n\nMean Beta 2\n1\nbCRM only\n\n\nSD Beat 2\n1\nbCRM only\n\n\nPsi\n1\nbCRM only\n\n\nSD Psi\n1\nbCRM only\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nAll\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nTrue Toxicity: &lt;Dose&gt;\nOne per dose\nAll\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nMean Fit Tox Lower: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fit Tox Upper: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fit Eff Lower: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nMean Fit Eff Upper: &lt;dose&gt;\nOnse per dose\nbCRM only",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#fitted-efficacy-crm-efficacy",
    "href": "documentation/v71/userguides/de.html#fitted-efficacy-crm-efficacy",
    "title": "3+3 / mTPI",
    "section": "Fitted Efficacy (CRM Efficacy)",
    "text": "Fitted Efficacy (CRM Efficacy)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Beta 1\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response\n\n\nSD Beta 1\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy model for each dose\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nMean Fit Eff Lower: &lt;dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.\n\n\nMean Fit Eff Upper: &lt;dose&gt;\nOnse per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#prmtd-etc.-crm-toxicity-crm-ordinal-probabilities-etc.-bcrm",
    "href": "documentation/v71/userguides/de.html#prmtd-etc.-crm-toxicity-crm-ordinal-probabilities-etc.-bcrm",
    "title": "3+3 / mTPI",
    "section": "Pr(MTD) etc. (CRM Toxicity, CRM Ordinal, “Probabilities etc.” bCRM)",
    "text": "Pr(MTD) etc. (CRM Toxicity, CRM Ordinal, “Probabilities etc.” bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study. The dose chosen will be the dose with the highest posterior probability of having a toxicity rate that is closest to / nearest below / nearest above the target toxicity rate.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nFor each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose selected will be the MED if that is below the MTD, otherwise it will be the MTD.\n\n\nNum Stop Rule 1\n1\nNumber of times the minimum subjects on MTD was met.\n\n\nNum Stop Rule 2\n1\nNumber of times the number of doses in the credible interval of the estimate of the MTD was met.\n\n\nNum Stop Rule 3\n1\nNumber of times a dose met the required threshold for Pr(MTD).\n\n\nNum Eff Stop Rule 1\n1 (bCRM only)\nNumber of times the minimum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1 (bCRM only)\nNumber of times the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1 (bCRM only)\nNumber of times a dose met the required threshold for Pr(MED).\n\n\nMean Tox CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MTD.\n\n\nSD Tox CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MTD.\n\n\nMean Eff CI\n1 (bCRM only)\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1 (bCRM only)\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MTD): &lt;dose&gt;\nOne per dose\nThe mean (over the simulations) of posterior probability that a dose is the dose nearest / closest below / closest above the target toxicity rate.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose+&gt;\n1\nAs MTD Selection, but allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nMED+ Selection: minus\n1 (bCRM only)\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1 (bCRM only)\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nOSD+ Selection: minus\n1 (bCRM only)\nThe proportion of simulations where a dose below the tested range of doses is the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1 (bCRM only)\nThe proportion of simulations where a dose above the tested range of doses is the optimum selected dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being both at or above the MED and at or below the MTD.\n\n\nPr(MTD+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MTD.\n\n\nPr(MTD+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MTD.\n\n\nPost CE MTD+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as maximum tolerated dose.\n\n\nPost CE MTD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the maximum tolerated dose, allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nPost CE MTD+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as maximum tolerated dose.\n\n\nPost CE MED+: minus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of simulations where, after cohort expansion, the dose was selected as the minimum efficacious dose, allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nPost CE MED+: plus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of simulations where, after cohort expansion, the dose was selected as the optimum selected dose, allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nPost CE OSD+: plus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#prmed-etc.-crm-efficacy",
    "href": "documentation/v71/userguides/de.html#prmed-etc.-crm-efficacy",
    "title": "3+3 / mTPI",
    "section": "Pr(MED) Etc. (CRM Efficacy)",
    "text": "Pr(MED) Etc. (CRM Efficacy)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study. The dose chosen will be the dose with the highest posterior probability of having an efficacy rate in closest to / nearest below o/ nearest above the target efficacy rate..\n\n\nNum Eff Stop Rule 1\n1\nNumber of times the maximum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times a dose met the required threshold for Pr(MED).\n\n\nMean Eff CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MED): &lt;dose&gt;\n\nThe mean (over the simulations) of posterior probability that a dose is MED.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose+&gt;\nOne per dose\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nPpn Early Futility\n1\nThe proportion of simulations where the trials stopped because the early futility rule was met.\n\n\nPr(MED+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MED.\n\n\nPr(MED+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-bcrm",
    "href": "documentation/v71/userguides/de.html#toxicity-bcrm",
    "title": "3+3 / mTPI",
    "section": "Toxicity (bCRM)",
    "text": "Toxicity (bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nPpn Tox\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nSD Ppn Tox\n1\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nTrue Ppn Tox\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nMean Beta 1\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the toxicity response.\n\n\nSD Beta 1\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the toxicity response.\n\n\nPsi\n1\nThe mean estimate over the simulations of the overall probability of observing both a toxicity and efficacy response in a subject.\n\n\nSD Psi\n1\nThe standard deviation of the estimate of Psi over the simulations.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD (Maximum Tolerated Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MTD (closest dose to having the target toxicity rate – or nearest below or above as selected by the user.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. This will be the MED if it is below the MTD, otherwise it will be the MTD.\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nNum Stop Rule 1\n1\nNumber of times simulations stopped because maximum subjects on MTD was met.\n\n\nNum Stop Rule 2\n1\nNumber of times simulations stopped because the number of doses in the credible interval was met.\n\n\nNum Stop Rule 3\n1\nNumber of times simulations stopped because Pr(MTD) was met.\n\n\nMean Tox CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MTD.\n\n\nSD Tox CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MTD.\n\n\nPr(MTD): &lt;dose&gt;\nOne per dose\nThe posterior probability that a dose is the dose nearest / closest below / closest above the target toxicity rate.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose+&gt;\n1\nAs MTD Selection, but allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nOSD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose+&gt;\nOne per dose\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was the optimum selected dose.\n\n\nPpn Best: &lt;dose&gt;\nOne per dose\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being at or above the MED and at or below the MTD.\n\n\nPr(MTD+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MTD.\n\n\nPr(MTD+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MTD.\n\n\nTrue Toxicity: &lt;dose&gt;\nOne per dose\nThis is the true simulated toxicity rate for each dose in the scenario",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#efficacy-bcrm",
    "href": "documentation/v71/userguides/de.html#efficacy-bcrm",
    "title": "3+3 / mTPI",
    "section": "Efficacy (bCRM)",
    "text": "Efficacy (bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nPpn Eff\n1\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Eff\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nMean Beta 2\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response.\n\n\nSD Beta 2\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response.\n\n\nPsi\n1\nThe mean estimate over the simulations of the overall probability of observing both a toxicity and efficacy response in a subject.\n\n\nSD Psi\n1\nThe standard deviation of the estimate of Psi over the simulations.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED (Minimum Efficacious Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MED (closest dose to having the target toxicity rate – or nearest below or above as selected by the user.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. This will be the MED if it is below the MTD, otherwise it will be the MTD.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy model for each dose\n\n\nSD Fitted Effiacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nEff. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\nNum Eff Stop Rule 1\n1\nNumber of times simulations stopped because the maximum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times simulations stopped because the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times simulations stopped because a dose met the required threshold for Pr(MED).\n\n\nMean Eff CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MED): &lt;dose&gt;\nOne per dose\nThe mean (over the simulations) of the posterior probability that a dose is the dose nearest / closest below / closest above the target efficacy rate.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose+&gt;\nOne per dose\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nOSD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose+&gt;\nOne per dose\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was the optimum selected dose.\n\n\nPpn Best: &lt;dose&gt;\nOne per dose\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being both at or above the MED and at or below the MTD.\n\n\nPr(MED+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MED.\n\n\nPr(MED+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MED.\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nPost CE MED+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the minimum efficacious dose, allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nPost CE MED+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the optimum selected dose, allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nPost CE OSD+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#allocation-box-and-whisker-plot-all",
    "href": "documentation/v71/userguides/de.html#allocation-box-and-whisker-plot-all",
    "title": "3+3 / mTPI",
    "section": "Allocation Box and Whisker plot (All)",
    "text": "Allocation Box and Whisker plot (All)\n\n\n\n\n\n\nFigure 29: Allocation Box and Whisker plot\n\n\n\nThis graph displays a box and whisker plot of the number of subjects allocated to each dose over the simulations. These plots show:\n\nThe mean allocation over all simulations at each dose plotted as a solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#resp-and-subject-alloc-all-crm",
    "href": "documentation/v71/userguides/de.html#resp-and-subject-alloc-all-crm",
    "title": "3+3 / mTPI",
    "section": "Resp and Subject Alloc (all CRM)",
    "text": "Resp and Subject Alloc (all CRM)\n\n\n\n\n\n\n\nCRM Toxicity, CRM Ordinal\nCRM Efficacy\n\n\n\n\n\n\n\n\nbCRM\n\n\n\n\n\n\n\n\nThese graphs show the mean subject allocation to each dose as a blue bar, along with, as appropriate lines showing the mean estimated toxicity/efficacy and the simulated ‘true’ toxicity/efficacy. The ‘error’ bars on the mean estimated toxicity/efficacy are the 95% interval of the mean estimates across the simulations.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#distribution-of-mtd-med-osd-and-te",
    "href": "documentation/v71/userguides/de.html#distribution-of-mtd-med-osd-and-te",
    "title": "3+3 / mTPI",
    "section": "Distribution of MTD, MED, OSD and TE",
    "text": "Distribution of MTD, MED, OSD and TE\nMTD: Maximum Tolerated Dose\nMED: Minimum Effective Dose\nOSD: Optimum Selected Dose – if the MED is below the MTD then the OSD is the MED, otherwise it is the MTD.\nTE: Tolerated and Effective – the probability the dose is both below the MTD and above the MTD. In the results file this is under the column heading “Pr(Good)”.\n\n\n\n\n\n\n\n3+3 version\nCRM Toxicity, CRM Ordinal, bCRM - MTD\n\n\n\n\n\n\n\n\nCRM Efficacy, bCRM - MED\nbCRM – OSD and TE\n\n\n\n\n\n\n\nThese graphs show the proportion of times each dose has been selected as a particular target dose as a brown bar, along with lines showing the mean estimated toxicity/efficacy and the simulated ‘true’ toxicity/efficacy. The ‘error’ bars on the mean estimated toxicity/efficacy are the 95% interval of the mean estimates across the simulations.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-and-allocation-obs-tox-and-alloc-obs-efficacy-and-allocation",
    "href": "documentation/v71/userguides/de.html#toxicity-and-allocation-obs-tox-and-alloc-obs-efficacy-and-allocation",
    "title": "3+3 / mTPI",
    "section": "“Toxicity and Allocation” / “Obs Tox and Alloc” / “Obs Efficacy and Allocation”",
    "text": "“Toxicity and Allocation” / “Obs Tox and Alloc” / “Obs Efficacy and Allocation”\n\n\n\n\n\n\n\n3+3 version, CRM Toxicity, CRM Ordinal, bCRM\nCRM Efficacy, bCRM\n\n\n\n\n\n\n\n\n\nThese graph show the mean allocation to each dose and the mean number of toxicities/efficacies observed at each dose.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#mtd-credible-interval",
    "href": "documentation/v71/userguides/de.html#mtd-credible-interval",
    "title": "3+3 / mTPI",
    "section": "MTD Credible Interval",
    "text": "MTD Credible Interval\n\n\n\n\n\n\nFigure 30\n\n\n\nCurrently only available for CRM Toxicity, this histogram shows the distribution of the final number of doses in the MTD credible interval.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#sample-size-mtd-histogram-all-crm",
    "href": "documentation/v71/userguides/de.html#sample-size-mtd-histogram-all-crm",
    "title": "3+3 / mTPI",
    "section": "Sample Size MTD Histogram (All CRM)",
    "text": "Sample Size MTD Histogram (All CRM)\n\n\n\n\n\n\nFigure 31\n\n\n\nThis graph plots the distribution of the final sample sizes a a stacked bar chart with different shades of color indicating the proportion of simulations with that sample size had selected the different doses as MTD.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#simulation-allocation-history",
    "href": "documentation/v71/userguides/de.html#simulation-allocation-history",
    "title": "3+3 / mTPI",
    "section": "Simulation Allocation History",
    "text": "Simulation Allocation History\n\n\n\n\n\n\n\n\n\n\n\n\n\n3+3 version, CRM Toxicity, bCRM\nCRM Ordinal\n\n\n\n\n\n\nCRm Efficacy, bCRM\n\n\n\n\nThis graph shows the allocation and toxicity/efficacy history for an individual simulation. Each dot represents a subject, the dot is colored red if toxicity was observed, light pink for a mild toxicity, blue for efficacy and grey otherwise. The subjects’ outcomes are displayed left to right in the order in which they were dosed, and at a height corresponding to the dose they were given.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed. A history can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#cohort-responses-sample-all-crm-except-efficacy",
    "href": "documentation/v71/userguides/de.html#cohort-responses-sample-all-crm-except-efficacy",
    "title": "3+3 / mTPI",
    "section": "Cohort Responses Sample (All CRM except Efficacy)",
    "text": "Cohort Responses Sample (All CRM except Efficacy)\n\n\n\n\n\n\n\nCRM Toxicity\nCRM Ordinal\n\n\n\n\n\n\n\n\nbCRM\n\n\n\n\n\n\n\n\nThis graph shows the dose allocation and resulting toxicities/efficacies along with the fitted dose-toxicity/efficacy model for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#mtd-change-on-expansion",
    "href": "documentation/v71/userguides/de.html#mtd-change-on-expansion",
    "title": "3+3 / mTPI",
    "section": "MTD Change on Expansion",
    "text": "MTD Change on Expansion\n\n\n\n\n\n\nFigure 32: MTD change on expansion graph\n\n\n\nThis graph plots the dose selected as MTD before the results of the expansion cohort are available, against the dose selected as MTD after the expansion cohort results are available. Each simulation result is plotted as a ‘dot’, and the graph is divided up into a grid and dots located in the correct cell in the grid and then arranged within the cell depending on how many results fall into the cell. This allows the relative number of results in each cell to be appraised quickly by eye.\nThis graph allows the trial designer to see the degree of risk that the determination of the MTD will change as a result of the expansion cohort and whether the risk is predominantly that the determination will increase or decrease.\nThere are similar graphs showing the change in the estimate of the MED and OSD after the expansion cohort.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "3+3 / mTPI"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html",
    "href": "documentation/v71/userguides/FACTSfromR.html",
    "title": "FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "href": "documentation/v71/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "title": "FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#software-prerequisites",
    "href": "documentation/v71/userguides/FACTSfromR.html#software-prerequisites",
    "title": "FACTS from R",
    "section": "Software prerequisites",
    "text": "Software prerequisites\nTo call FACTS from R, you will need the following installed on your Windows machine on which you are using FACTS:\n\nA reasonably recent version of R (v3.5.3+).\nFACTS v6.4 or later, the command line executable versions of the FACTS simulation engines – these are currently available to Enterprise licensees.\nThe supplied R file: factR.R",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#factr.r",
    "href": "documentation/v71/userguides/FACTSfromR.html#factr.r",
    "title": "FACTS from R",
    "section": "factR.R",
    "text": "factR.R\nProvides an ‘R’ wrapper for accessing FACTS analysis models for:\n\nCore and Enrichment Design, allowing you to use the following FACTS analysis features:\n\nDose Response models\nLongitudinal models\nHierarchical Prior on Control (borrowing from historical data)\nTTE predictor endpoint\nBAC\n\nInputs\n\nFACTS param file with trial info and model specifications\nData file\n\nOutput\n\nMCMC file",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "href": "documentation/v71/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "title": "FACTS from R",
    "section": "Steps for calling FACTS from R",
    "text": "Steps for calling FACTS from R\nTo call FACTS from R, you will need to do the following sequence of steps:\n\nCreate a (non-adaptive) FACTS project for your Engine type with the general study info: Number of Arms, number and timing of Visits (if using), Dose response (& longitudinal if using) model specification and MCMC setup.\nConfigure VSR and Execution profiles to allow a simple simulation run.\nRun 1 simulation to produce:\n\nA ‘param’ file which will be passed as an input to the R function.\nA ‘patients’ file. This may be useful to illustrate data file format for the input data. See FACTS Execution Guides for details.\nAn ‘mcmc’ file. This will show you what to expect in the output MCMC file.\n\nIf using FACTS to analyze a data set, then\n\nput the data set into the required format\nwrite an R script to call FACTS with the data set\nprocess the MCMC output\n\nIf using FACTS within a simulation framework, then:\n\nWrite an R script that generates the data you wish to simulate and pass to FACTS to analyze\nWrite a loop that\n\ngenerates the data for a simulation\ncalls FACTS with generated data\nprocess the MCMC output\naccumulate the statistics for the overall operating characteristics to be computed\n\nOutput the resulting OCs",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#runfacts-usage-notes",
    "href": "documentation/v71/userguides/FACTSfromR.html#runfacts-usage-notes",
    "title": "FACTS from R",
    "section": "runFACTS() Usage Notes",
    "text": "runFACTS() Usage Notes\n\nRun FACTS MCMC Model from R\nrunFACTS(\n engine, \n data.file = “patients.dat”, \n param.file = “nuk1_e.param, \n mcmc.file.num = 0, \n rng.seed = 1, \n exec.path = getwd()\n)\nReturn Value: runFACTS returns a TRUE/FALSE to indicate a successful/failed execution. In case of errors, R error messages may be printed and in case of a FACTS execution error, a file called ‘error.txt’ will be output, containing the error description.\nArguments:\n\nengine: Name of the FACTS engine to use. Can be one of the following:\n\nFor Core Engines: “contin”, “dichot”, “ME”, “TTE”\nFor Enrichment Design Engines: “ed_contin”, “ed_dichot”, “ed_tte”\n\ndata.file: Name of the input data file. Default is “patients.dat”. This file format should exactly match the file format of the ‘patients’ file corresponding to the ones produced by FACTS for the design you setup in FACTS to specify the analysis model. (See the FACTS Execution Guide under the FACTS Help menu for details.)\nparam.file: Name of the FACTS ‘.param’ file that specifies the model setup. Default is ‘nuk1_e.param’.\nmcmc.file.num: The MCMC output is written to a file named ’mcmcNNNNN.csv. This argument set the NNNNN. Therefore, mcmc.file.num = 1 will create an MCMC output file called mcmc00001.csv. Default value is 0.\nrng.seed: Integer-valued random number generator seed. Will use the value from the ‘.param’ file if unspecified.\nexec.path The path to the directory where the FACTS executable program is available. Default is the current working directory.\n\n\n\nSet Up Files and Folders\nIt is important to pass files and parameters correctly, as there is not much in the way of helpful error messaging. Setting up the required folder and files is not hard but should be done carefully. The following example shows how.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "href": "documentation/v71/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "title": "FACTS from R",
    "section": "The “core.dichot.example.facts”",
    "text": "The “core.dichot.example.facts”\nIn this example we wish to use FACTS to fit a simple NDLM dose response model across 6 arms (control & 5 doses) with a dichotomous endpoint.\nWe have entered the following parameters:\n\nStudy:\n\nStudy Info:\n\nNon-adaptive\nRecruit subjects continuously\nMax subjects: 300\nResponse is a positive outcome\nTime to final endpoint: 4 weeks\n\nTreatment Arms:\n\nControl and 5 doses with strengths 1, 2, … 5\n\n\nVirtual Subject Response\n\nExplicitly defined\n\nDose Response\n\nresponses: 0.1, 0.1, 0.125, 0.15, 0.2, 0.25\n\n\n\nExecution\n\nAccrual\n\n1 region with mean accrual of 5 subjects per week\n\nDropout\n\nNo dropouts\n\n\nQuantities of Interest\n\nPosterior probability: Pr(P_d &gt; P_Control)\nProbability of being target: Pr(Max)\nDecision Quantity: Pr(P_d &gt; P_Contorl); d=Greatest Pr(Max)\n\nDesign\n\nDose Response\n\nSimple NDLM\n\nInitial Dose ~N(0,22)\nTau IG(1,1) “central value”, “weight”\n\n\nRequentist analysis: none\nAllocation: 1:1:1:1:1:1\nSuccess/Futility Criteria\n\nSuccess: Pr(P_d &gt; P_Control); d= Greatest Pr(Max) &gt; 0.9\n\n\n\nNot all these parameters will effect our analysis, but we have to enter sufficient parameters to be able to run a simulation and get a bin1_e.param file. This can be found in the scenario simulation results folder. We only need to run 1 simulation on order to have one written out. This file is copied to our “Example” directory. If we want to change something in the analysis – the model or the prior for example, we can modify this facts file, re-run one simulation, and copy the new bin1_e.param file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#dichot-demo.r",
    "href": "documentation/v71/userguides/FACTSfromR.html#dichot-demo.r",
    "title": "FACTS from R",
    "section": "dichot-demo.R",
    "text": "dichot-demo.R\nWe start by setting the current working directory to the “Example” folder, and setting up some file locations and sourcing the factR.R file.\n## Set up Folders and Paths\n\n# This is the directory where the parameter file and patient data must be located\n# It will be where the MCMCM files are written\nsetwd(\"Z:/FACTS test/FACTS 6 Training/FACTS R interface/Example\")\n\n# This must be the location of the factR.R file\nFactR.src = \"../factR.R\"\n\n# This must be the location of the executable files\nExec.dir = \"../WindowsExecutables\"\n\n# Load runFACTS\nsource(FactR.src)\nWe can then copy an example patients file from the simulation results and check that we can run facts.\n# Test to check its working\n# Copy an example patients file from the simulations results to this folder before running.\nsystem.time(\n  runFACTS(\n    engine='dichot', \n    data.file = 'patients00001.csv', \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 1, \n    rng.seed = 1, \n    exec.path = Exec.dir\n  )\n)\n\ngenBinaryData()\nWe now define a simple function to generate the data for a single run of FACTS.\n# generates a data frame that can be used to drive a FACTS analysis\n# dichotomous endpoint\n# no visits\n# n.per.arm: int, the number of subjects to be simulated for each arm\n# rates: int[], the response rate to be simulated for each arm\n# the length if rates defines the number of arms\n# returns a dataframe with n.per.arm * length(rates) simulated subjects\n\ngenBinaryData &lt;- function(nPerArm, rates) {\n  \n  patientID &lt;- 1:(nPerArm * length(rates)) # Generate a list of patients\n  region &lt;- rep(1, nPerArm * length(rates)) # all patients come from region 1\n  date &lt;- 1:(nPerArm * length(rates)) # Generate a list of enrolment dates - here simply one per day\n  doseAlloc &lt;- rep(1:length(rates), each = nPerArm) # Allocate patients equally to each dose\n  lastVisit &lt;- rep(1, nPerArm * length(rates)) # all patients have last visit data\n  dropout &lt;- rep(0, nPerArm * length(rates)) # no patients have dropped out\n  baseline &lt;- rep(-9999, nPerArm * length(rates)) # not simulating baseline\n  visit1 &lt;- rep(0, nPerArm * length(rates)) # create the outcome vector\n\n  for (d in 1:length(rates)){ # get responses for each dose\n  ix &lt;- which(doseAlloc == d) # get indices of patients on dose d\n  # assign them a final response based on the rate to simulate for dose d\n  if (length(ix) &gt; 0) {\n  visit1[ix] &lt;- \n    sample(\n     c(0,1), \n     size = length(ix), \n      replace = TRUE, \n      prob = c(1-rates[d], rates[d])\n    )\n  }\n  \n}\n\ndat &lt;- data.frame(\n  SubjectID = patientID, \n  Region = region, \n  Date = date, \n  Dose = doseAlloc, \n  LastVisit = lastVisit, \n  Dropout = dropout, \n  Baseline = baseline, \n  Visit1 = visit1, \n  row.names = NULL\n)\n\nreturn(dat)\n}\n\n\nrunSims\n########### Toy Example Trial Sim ##########\n### Constants\nDATAFILE = \"patients.csv\"\nMCMCFILE = \"mcmc00000.csv\"\n# function to simulate an example data set with dichotomous endpoint\n# nSims - the number of sims to run\n# nBurnin - the number of MCMC smaples to discard\n# (the number of MCMC samples is specified in the parameter file)\n# details - a boolean. If TRUE the function returns a data frame with\n# the results of each individual simulation,\n# otherwise just the win proportion and probabilities of being control\n\nrunSims &lt;- function(\n    nSims = 10, \n    nBurnin = 1000, \n    rates = c(0.1, 0.1, 0.125, 0.15, 0.2, 0.25),\n    details = FALSE\n) {\n\nwinPpn = 0\npr.gt.ctl.sum &lt;- rep(0, length(rates) - 1)\nif (details) {\nperSim &lt;- data.frame(Sim = 1)\n}\n\nfor(sim in 1:nSims) {\ndat = genBinaryData(nPerArm = 50, rates = rates)\nwrite.csv(dat,DATAFILE, row.names = FALSE)\nif (details) {\nperSim[sim, \"Sim\"] &lt;- sim\n# record true rates and observed rates\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"sim.rate.\", d, sep=\"\")] &lt;- rates[d]\n}\n\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"obs.rate.\", d, sep=\"\")] &lt;- mean(dat[dat[,\"Dose\"]==d, \"Visit1\"])\n}\n}\n\ncat(\"run FACTS: \", sim, \"\\n\")\n\nret &lt;- \n  runFACTS(\n    engine = 'dichot', \n    data.file = DATAFILE, \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 0, \n    rng.seed = sim, \n    exec.path = Exec.dir\n  )\n\ndat = read.csv(MCMCFILE, skip = 1)\n\n# discard burnin rows and just estimates of rate - the \"Pi\" columns\ndat = dat[(nBurnin + 1):nrow(dat), grep(\"Pi\", names(dat))]\n\nif (details) {\n# record est rate\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"est.rate.\", d, sep=\"\")] &lt;- mean(dat[,paste(\"Pi.\", d, sep=\"\")])\n}\n}\n\n# success if the first dose is not in the top 2 .. i.e. the resposnse on any 2 doses is &gt; control\n\nsuccess &lt;- apply(dat, 1, FUN = function(x) {ifelse(length(x) - which(order(x)==1) &gt;= 2, 1, 0)})\n\nif (details) {\nperSim[sim, \"Pr.Success\"] &lt;- mean(success)\nperSim[sim, \"Success.flag\"] &lt;- ifelse(mean(success) &gt; 0.9, 1,0)\n}\n\nwinPpn = winPpn + ifelse(mean(success) &gt; 0.9, 1,0)\n\n# example: calc pr(theta_d &gt; theta_ctl)\ngt.ctl.flag &lt;- apply(dat, 1, FUN = function(x){x[2:length(x)] &gt; x[1]})\npr.gt.ctl &lt;- apply(gt.ctl.flag,1,sum)\npr.gt.ctl &lt;- pr.gt.ctl / length(gt.ctl.flag[1,])\npr.gt.ctl.sum &lt;- pr.gt.ctl.sum + pr.gt.ctl\n\nif (details) {\nfor (d in 1:length(pr.gt.ctl)) {\nperSim[sim, paste(\"Pr.pi.\", d+1, \"&gt;pi_ctl\", sep=\"\")] &lt;- pr.gt.ctl[d]\n    }\n  }\n}\n\ncat(\"win proportion: \", winPpn/nSims, \"\\n\")\n\nif (details) {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims, perSim))\n} else {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims))\n}\n}\n\n\nRunning the Example\nHaving sourced the above functions and variables holding paths and fllenames we can run the simulations with the default scenario:\nrunSims(details=FALSE)\n\n&gt; run FACTS: 1\n&gt; run FACTS: 2\n&gt; run FACTS: 3\n&gt; run FACTS: 4\n&gt; run FACTS: 5\n&gt; run FACTS: 6\n&gt; run FACTS: 7\n&gt; run FACTS: 8\n&gt; run FACTS: 9\n&gt; run FACTS: 10\n&gt; win proportion: 0.3\n\n&gt; [[1]]\n&gt; [1] 0.3\n\n&gt; [[2]]\n&gt; Pi.2 Pi.3 Pi.4 Pi.5 Pi.6\n&gt; 0.30572 0.45824 0.51936 0.77944 0.92916\nIf “details” is set to TRUE then the list of results has a dataframe at the end that contains a row per simulation and details of that simulations results.\nHopefully this is sufficient to get you started.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/index.html",
    "href": "documentation/v71/userguides/enrichment/index.html",
    "title": "Enrichment Designs",
    "section": "",
    "text": "This document covers the design options that are common across the three FACTS Enrichment Design Engines: Continuous, Dichotomous and Time-to-Event. Some design elements are shared across design engines, in which case there is only a single description of them. Others differ based on the endpoint used, in which case separate pages have been created to describe each.\nThe screenshots provided are specific to a particular installation and  may not reflect the exact layout Screenshots from earlier versions of FACTS 6 are still used only when the tabs they show are unchanged in FACTS 7.1.  of the information seen by any particular user. They were taken from FACTS V7 & V6 installed on Windows 10 or 11. If FACTS is installed on different versions of Windows, or with different Windows ‘themes’ there will be some differences in appearance introduced by Windows. The contents of each tab, however, will generally be consistent with the screenshots in this document.\n\n\nThis is the version of the user guide for inclusion with the FACTS 7.1 release.\n\n\n\nPlease cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/index.html#facts-version",
    "href": "documentation/v71/userguides/enrichment/index.html#facts-version",
    "title": "Enrichment Designs",
    "section": "",
    "text": "This is the version of the user guide for inclusion with the FACTS 7.1 release.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/index.html#citing-facts",
    "href": "documentation/v71/userguides/enrichment/index.html#citing-facts",
    "title": "Enrichment Designs",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis is the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Odds Ratio: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of odds ratio for the study treatment in each group.\n\n\nMean Odds Ratio: Across groups\n1\nThis is the mean (over the simulations) of the across groups odds ratio (the estimate across the groups of the odds ratio between response rate on the study treatment and the historic control rate or the mean response rate on the control arm).\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response rate for the control arm in each group.\n\n\nMean Ctrl Response: Across Group\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response rate on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the response rate on the treatment arms for each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the average treatment response rates for each group.\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response rate for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response rate for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response rate for the scenario\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response rate in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations that each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups’ success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups’ futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\n\n\n\n\nThis is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall OR\n1\nThe mean (over the simulations) overall Odds Ratio\n\n\nSE Mean Overall OR\n1\nThe SE (over the simulations) of the mean overall Odds Ratio\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Chi-Square test in each group.\n\n\nppn Significant(Fisher Exact) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Fisher’s exact test in each group.\n\n\nPpn Significant (Binomial) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Binomial test in each group.\n\n\nMean Trt Effect (By Group) &lt;group&gt;\nG\nThe mean (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nSE Mean Trt Effect (by Group) &lt;group&gt;\nG\nThe SE (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\npSignificant (Ctl)\n1\nThe proportion of simulations with an overall significant control effect.\n\n\npSignificant (CMH)\n1\nThe proportion of simulations with an overall significant effect using Cochran-Mantel-Haenszel test.\n\n\npSignificant (Breslow-Day)\n1\nThe proportion of simulations with an overall significant effect using Breslow-Day test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#highlights",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#highlights",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis is the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#allocation",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#allocation",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#response",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#response",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Odds Ratio: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of odds ratio for the study treatment in each group.\n\n\nMean Odds Ratio: Across groups\n1\nThis is the mean (over the simulations) of the across groups odds ratio (the estimate across the groups of the odds ratio between response rate on the study treatment and the historic control rate or the mean response rate on the control arm).\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response rate for the control arm in each group.\n\n\nMean Ctrl Response: Across Group\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response rate on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the response rate on the treatment arms for each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the average treatment response rates for each group.\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response rate for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is hidden if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response rate for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response rate for the scenario\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response rate in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#observed",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#observed",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#probabilities",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#probabilities",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#stopping-rules",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#stopping-rules",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#evaluation-rules",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#evaluation-rules",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations that each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups’ success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups’ futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#hierarchical-prior-parameters",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#hierarchical-prior-parameters",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#simulation-results",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#simulation-results",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "This is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#frequentist-results",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#frequentist-results",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall OR\n1\nThe mean (over the simulations) overall Odds Ratio\n\n\nSE Mean Overall OR\n1\nThe SE (over the simulations) of the mean overall Odds Ratio\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Chi-Square test in each group.\n\n\nppn Significant(Fisher Exact) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Fisher’s exact test in each group.\n\n\nPpn Significant (Binomial) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Binomial test in each group.\n\n\nMean Trt Effect (By Group) &lt;group&gt;\nG\nThe mean (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nSE Mean Trt Effect (by Group) &lt;group&gt;\nG\nThe SE (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\npSignificant (Ctl)\n1\nThe proportion of simulations with an overall significant control effect.\n\n\npSignificant (CMH)\n1\nThe proportion of simulations with an overall significant effect using Cochran-Mantel-Haenszel test.\n\n\npSignificant (Breslow-Day)\n1\nThe proportion of simulations with an overall significant effect using Breslow-Day test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-summary.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Project\n1\nThe name of the “.facts” file in the FACTS GUI that was used to generate the simulations.\n\n\nScenario\n1\nThe name of the scenario – this is the various profile names that make up the scenario, concatenated together.\n\n\nTimestamp\n1\nThe date and time when the simulations started.\n\n\nVersion\n1\nThe version number of the FACTS GUI that ran the simulations.\n\n\nNsim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\n80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nMean Alloc &lt;Group&gt; \n2*G\nThe mean number of subjects recruited into each arm in each group. If a control arm is not included the column is still present, with value ‘0’.\n\n\nMean Odds Ratio &lt;Group&gt;\nG\nThe mean of the response odds ratio estimates between treatment and control.\n\n\nMean Odds Ratio (Overall)\n1\nThe mean of the means of the response odds ratio estimates across all groups.\n\n\nSE Odds Ratio &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of the response odds ratio in each group.\n\n\nSE Odds Ratio (Overall)\n1\nThe standard error, over the simulations, of the mean of the mean estimates of the response odds ratio across all groups.\n\n\nMean Trt Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response rate on the study treatment arm in each group.\n\n\nMean Trt Effect (Overall)\n1\nThe mean of the mean estimate of the across groups treatment response rate.\n\n\nSE Trt Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response rate on the study treatment arm in each group.\n\n\nSE Trt Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups treatment response rate.\n\n\nMean Control Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response rate on the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\nThe mean of the average of the control response rate across the groups.\n\n\nSE Control Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response rate on the control arm in each group.\n\n\nSE Avg. Control Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups control response rate.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\nTrue mean treatment response for the scenario.\n\n\nTrue Mean Control Resp &lt;Group&gt;\nG\nTrue mean control response for the scenario.\n\n\nMean Mu Theta\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean log-odds study treatment response rate from control in each group.\n\n\nSE Mu Theta\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean log-odds study treatment response rate from control in each group.\n\n\nMean Tau Theta\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the mean log-odds study treatment response rate from control in each group.\n\n\nSE Tau Theta\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the log-odds study treatment response rate from control in each group.\n\n\nMean Mu Gamma\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nSE Mu Gamma\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nMean Tau Gamma\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nSE Tau Gamma\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean log-odds response rate on the control arm in each group.\n\n\nMean Pr Ph3 Success &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of success in phase 3 of the study treatment in each group.\n\n\nMean Pr Ph3 Success 99\n1\nThe mean, across the simulations, of the mean probability of success in phase 3 of the across groups treatment difference.\n\n\nMean Pr CSD (Success) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for success in each group.\n\n\nMean Pr CSD (Success) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for success.\n\n\nMean Pr CSD (Futility) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for futility in each group.\n\n\nMean Pr CSD (Futility) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for futility.\n\n\nPpn CSD Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD success criterion.\n\n\nPpn Ph3 Success &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility &lt;Group&gt;\nG\nThe proportion of simulations that each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility 99\n1\nThe proportion of simulations that the across groups treatment effect met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success &lt;Group&gt;\nG\nThe proportion of simulations that each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success 99\n1\nThe proportion of simulations that the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations that each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThe proportion of simulations where the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations that each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThe proportion of simulations where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn CSD Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility (Final) 99\n1\nThe proportion of simulations in which after all data has been collected the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its futility criterion in each group.\n\n\nPpn Ph3 Futility (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its CSD success criterion.\n\n\nPpn Ph3 Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its success criterion in each group.\n\n\nPpn Ph3 Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility (Final) 99\n1\nThe proportion of simulations where the across groups treatment effect after all data has been collected met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success (Final) 99\n1\nThe proportion of simulations where after all data has been collected the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where after all data has been collected the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations after all data has been collected where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn Outcome 1\n1\nThe proportion of simulations that stopped early for success.\n\n\nPpn Outcome 2\n1\nThe proportion of simulations that reached full accrual and declared success on final evaluation.\n\n\nPpn Outcome 3\n1\nThe proportion of simulations that reached full accrual and declared futility on final evaluation.\n\n\nPpn Outcome 4\n1\nThe proportion of simulations that stopped early for futility.\n\n\nPpn Outcome 5\n1\nThe proportion of simulations that stopped early for success but were deemed futile on final evaluation.\n\n\nPpn Outcome 6\n1\nThe proportion of simulations that stopped early for futility but were deemed successful on final evaluation.\n\n\nPpn Outcome 7\n1\nThe proportion of simulations that reached full accrual and were inconclusive.\n\n\nMean Study Accrual Stop Week\n1\nThe mean study duration of accrual – from start of accrual to last patient first visit.\n\n\nMean BAC Mu &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Mu &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nMean BAC Tau &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Tau &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-summary_freq.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-summary_freq.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall OR\n1\nThe mean (over the simulations) overall Odds Ratio\n\n\nSE Mean Overall OR\n1\nThe SE (over the simulations) of the mean overall Odds Ratio\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Chi-Square Test in each group.\n\n\nppn Significant (Chisq) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Fisher exact test in each group.\n\n\nPpn Significant (Binomial) (By Group) &lt;group&gt;\nG\nThe proportion of simulations in which the treatment effect was significant according to a Binomial test in each group.\n\n\nMean Trt Effect (By Group) &lt;group&gt;\nG\nThe mean (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nSE Mean Trt Effect (by Group) &lt;group&gt;\nG\nThe SE (over the simulations) of the frequentist estimate of the mean treatment effect in each group.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\npSignificant (Ctl)\n1\nThe proportion of simulations with an overall significant control effect.\n\n\npSignificant (CMH)\n1\nThe proportion of simulations with an overall significant effect using Cochran-Mantel-Haenszel test.\n\n\npSignificant (Breslow-Day)\n1\nThe proportion of simulations with an overall significant effect using Breslow-Day test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n#Sim\n1\n✔\n\nSimulation number\n\n\nWeeks (Duration)\n1\n✔\n\nThe week of final analysis – the total duration of the simulation.\n\n\n#Week\n1\n\n✔\nWeek\n\n\nNo.Subj\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nAlloc &lt;Group&gt; \n2 * G\n✔\n✔\nThe number of subjects allocated to each arm in each group.\n\n\nMean Odds Ratio\nG\n✔\n✔\nThe estimated mean response odds ratio between the treatment and control arms\n\n\nMean Trt Resp &lt;Group&gt;\nG\n✔\n✔\nThe estimated mean response of the study treatment in the group.\n\n\nTrt Effect (Overall)\n1\n✔\n✔\nThe estimated mean treatment difference across the groups.\n\n\nSD Mean Trt Resp &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the study treatment in the group.\n\n\nSD Trt Effect (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the mean treatment difference across the groups.\n\n\nTrt Resp Lower\nG\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the treatment response.\n\n\nTrt Resp Lower (Overall)\n1\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the treatment response across the groups\n\n\nTrt Resp Upper\nG\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the treatment response.\n\n\nTrt Resp Upper (Overall)\n1\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the treatment response across the groups\n\n\nMu Theta\n1\n✔\n✔\nThe mean of the hierarchical distribution of the treatment differences over all the groups.\n\n\nTau Theta\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the treatment differences over all the groups.\n\n\nMean Control Resp &lt;Group&gt;\nG\n✔\n✔\nThe estimated mean response of the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\n✔\n✔\nThe average overall mean response of the control arm over all the groups. (Note: this is not part of the response model, but estimated separately).\n\n\nSD Mean Control Resp &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the control arm in each group.\n\n\nSD Avg. Control Resp (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the average response over all the control arms.\n\n\nControl Resp Lower\nG\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the control response.\n\n\nAvg. Control Resp Lower (Overall)\n1\n✔\n✔\nThe lower bounds of the confidence interval around the estimate of the control response across the groups\n\n\nControl Resp Upper\nG\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the control response.\n\n\nAvg. Control Resp Upper (Overall)\n1\n✔\n✔\nThe upper bounds of the confidence interval around the estimate of the control response across the groups\n\n\nMu Gamma\n1\n✔\n✔\nThe mean of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nTau Gamma\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\n✔\n✔\nTrue mean treatment response for the scenario\n\n\nTrue Mean Control Resp &lt;Group&gt;\nG\n✔\n✔\nTrue mean control response for the scenario\n\n\nNum Completed &lt;Group&gt; \n2 * G\n✔\n✔\nThe number of subjects completed (final endpoint available) in each arm in each group.\n\n\nComplete Info &lt;Group&gt; \n2 * G\n✔\n✔\nThe amount of complete information in each arm in each group – however that has been defined in the definition of interim timing – subjects enrolled, subjects complete at a certain visit or subjects who had the opportunity to complete at a certain visit.\n\n\nMean Raw Response &lt;Group&gt; \n2 * G\n✔\n✔\nThe mean raw response\n\n\nSE Raw Response &lt;Group&gt; \n2 * G\n✔\n✔\nThe standard deviation of the raw response\n\n\nNum Dropouts &lt;Group&gt; \n2 * G\n✔\n✔\nNumber of dropouts seen\n\n\nPr Ph3 Success &lt;Group&gt;\nG\n✔\n✔\nThe probability of success in phase 3 of the study treatment for each group.\n\n\nPr Ph3 Success 99\n1\n✔\n✔\nThe probability of success in phase 3 of the across groups treatment difference.\n\n\nPr CSD (Success) &lt;Group&gt;\nG\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the success NIM) of the study treatment for each group.\n\n\nPr CSD (Success) 99\n1\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the NIM) of the across groups treatment difference.\n\n\nPr CSD (Futility) &lt;Group&gt;\nG\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the study treatment for each group.\n\n\nPr CSD (Futility) 99\n1\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the across groups treatment difference.\n\n\nCSD Futility &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Futility 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Success 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nCSD Futility (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are met for the final evaluation for futility for each group, 0 = they are not all met. Whether the across group final futility condition is also met if required is not included in this flag.\n\n\nCombined Futility (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success (Final) &lt;Group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success for each group, 0 = they are not all met. Whether the across group final success condition is also met if required is not included in this flag.\n\n\nCombined Success (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met (Final) &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met (Final) &lt;Group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nFutile Study\n1\n✔\n✔\nA flag: 1 = the study was futile overall, 0 = otherwise.\n\n\nSuccessful Study\n1\n✔\n✔\nA flag: 1 = the study was successful overall, 0 = otherwise.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome: 1. Early success; 2. Late success; 3. Late futility; 4. Early futility; 5. Success to futility flip-flop; 6. Futility to success flip-flop; 7. Inconclusive\n\n\nGroup Outcome\nG\n✔\n✔\nA flag categorizing the group outcome: 1. Early success; 2. Late success; 3. Late futility; 4. Early futility; 5. Success to futility flip-flop; 6. Futility to success flip-flop; 7. Inconclusive\n\n\nGroup Stop Type &lt;Group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: 8. Success; 9. Group Cap; 10. Futility; 11. Other (group stopped because study stopped early); 12. Study cap\n\n\nGroup Stop Week &lt;Group&gt;\nG\n✔\n✔\nThe week the group stop decision was taken. There may be further follow-up time before the group analysis was completed.\n\n\nStudy Accrual Stop Week\n1\n✔\n✔\nThe week the study stop decision was taken. There may be further follow-up time before the study analysis was completed.\n\n\nBAC Mu &lt;Group&gt;\nG\n✔\n✔\nThe mean estimated value of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Mu &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nBAC Tau &lt;Group&gt;\nG\n✔\n✔\nThe mean estimated value of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Tau &lt;Group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nAlpha0  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Alpha for state 0 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nAlphaS  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Alpha for stable state (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nAlpha1  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Alpha for state 1 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nProb0  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Transition probability to state 0 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nProbS  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Probability of remaining stable (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nProb1  \nLM*V\n✔\n✔\nOnly if using the Restricted Markov longitudinal model: Transition probability to state 1 (Values are for the transition from visit v to v+1 so values for visit V are not used)\n\n\nBeta Binom Alpha0  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial alpha parameter for the probability that the final result will be 1 if the result at the visit is 0\n\n\nBeta Binom Beta0  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial beta parameter for the probability that the final result will be 1 if the result at the visit is 0\n\n\nProb0  \nLM*V\n✔\n✔\nIf using the Beta Binomial or Logistic Regression longitudinal models: the probability of 1 being the final result if the result at the visit is 0\n\n\nBeta Binom Alpha1  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial alpha parameter for the probability that the final result will be 1 if the result at the visit is 1\n\n\nBeta Binom Beta1  \nLM*V\n✔\n✔\nOnly if using the Beta Binomial longitudinal model: The beta binomial beta parameter for the probability that the final result will be 1 if the result at the visit is 1\n\n\nProb1  \nLM*V\n✔\n✔\nIf using the Beta Binomial or Logistic Regression longitudinal models: the probability of 1 being the final result if the result at the visit is 1",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations_freq.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-simulations_freq.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nOverall (Chisq) Group p-value\n1\nThe overall Chi-Squared p-value\n\n\nOverall OR\n1\nThe overall Odds Ratio\n\n\nExp[SD Overall log(OR)]\n1\nThe estimated SD of the Overall Log Odds Ratio\n\n\nOverall Grp in Trt p-value\n1\nThe overall treatment p-value\n\n\nGroup in Ctl p-value\n1\nThe treatment-group interaction p-value\n\n\nGroup (ChiSq) p-values\nG\nThe Chi-Squared p-value for each group\n\n\nGroup (Fisher Exact) p-values\nG\nThe Fisher Exact p-value for each group\n\n\nGroup (Binomial) p-values\nG\nThe Binomial p-value for each group\n\n\nGrp Trt Effects\nG\nThe treatment difference in rates compared to Control per group\n\n\nCMH p-value\n1\nCochran-Mantel-Haenszel test p-value\n\n\nBreslow-Day p-value\n1\nBreslow-Day test p-value\n\n\nAgresti Score Upper CI\n1\nUpper bound of Agresti Score interval\n\n\nAgresti Score Loser CI\n1\nLower bound of Agresti Score interval",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-patientsnnnnn.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nRegion index\n\n\nDateInWeeks\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nGroup\n1\nThe index number of the group the subject belongs to.\n\n\nArm\n1\nA flag indicating the arm the subject was randomized to: 0 = Control, 1 = Study Treatment.\n\n\nLastVisit#\n1\nThe index of the last visit for which subject data is available\n\n\nVisit &lt;visit&gt;\nV\nThe subject’s endpoint score for that visit: -9999 = not available.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/dichotomous.html#contents-of-mcmcnnnnn.csv",
    "title": "Simulation Tab - Dichotomous Endpoint",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContent if using a conventional Dichotomous endpoint\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nPi1 &lt;group&gt;\nG\nThe estimate of the response rate on the treatment arm in each group\n\n\nOverallTheta\n1\nThe overall treatment difference in log-odds\n\n\nPi0 &lt;group&gt;\nG\nThe estimate of the response rate on the control arm in each group.\n\n\nOverallGamma &lt;group&gt;\nG\nThe log odds of the response rate on control in each group\n\n\n\n\nLongitudinal parameters – the number and names of the longitudinal parameters will depend on which longitudinal model is being used in the analysis (if any) and how many model instances are being used. The reported parameter names can be matched up against the symbols used on the Design &gt; Longitudinal Model tab\n\n\n\n\n\nContents if using a Restricted Markov endpoint\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nPi1 &lt;group&gt;\nG\nThe estimate of the response rate on the treatment arm in each group\n\n\nOverallTheta\n1\nThe overall treatment difference in log-odds\n\n\nPi0 &lt;group&gt;\nG\nThe estimate of the response rate on the control arm in each group.\n\n\nOverallGamma &lt;group&gt;\nG\nThe log odds of the response rate on control in each group\n\n\nAlpha0 &lt;model&gt; &lt;visit&gt;\nM * V\nThe Dirichlet parameter: the number of observed fails in this model at this visit\n\n\nAlphaS &lt;model&gt; &lt;visit&gt;\nM * V\nThe Dirichlet parameter: the number of observed stable in this model at this visit\n\n\nAlpha1 &lt;model&gt; &lt;visit&gt;\nM * V\nThe Dirichlet parameter: the number of observed responses in this model at this visit\n\n\nProb0 &lt;model&gt; &lt;visit&gt;\nM * V\nThe probability of becoming a fail in this model in this visit\n\n\nProbS &lt;model&gt; &lt;visit&gt;\nM * V\nThe probability of remaining stable in this model in this visit\n\n\nProb0 &lt;model&gt; &lt;visit&gt;\nM * V\nThe probability of becoming a responder in this model in this visit",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ -&gt; Fut Flip Flop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut -&gt; Succ Flip Flop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nMean Events &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment in each group.\n\n\nMean Events Across groups, &lt;segement&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment.\n\n\nMean Exposure &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment in each group.\n\n\nMean Exposure Across groups, &lt;segment&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Trt.: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the hazard ratio on the treatment arms for each group.\n\n\nMean Trt.: Across groups\n1\nThis is the mean (over the simulations) of the average treatment hazard ratio across the groups.\n\n\nSD Trt : &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the estimate of the treatment hazard ratio in each group.\n\n\nSD Trt : Across groups\n1\nThis is the standard deviation (over the simulations) of the average of the estimates of the treatment hazard ratio across the groups.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response for the scenario\n\n\nMean Mu_theta\n1\nThis is the mean (over the simulations) of the estimate of the mean of hierarchical distribution of the log hazard ratios of the treatment arms in the groups.\n\n\nSD Mu_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the mean of the hierarchical distribution of the log hazard ratios in the groups.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios each group.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios in each group.\n\n\nAlpha: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Alpha: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nBeta: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Beta: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the mean (over the simulations) of the estimate of the control event rate in each group over each hazard model time segment.\n\n\nTrue Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the true mean control event rate for each group and each hazard time segment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSHRD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for success.\n\n\nProb. CSHRD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for success.\n\n\nProb. CSHRD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for futility.\n\n\nProb. CSHRD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for futility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn. CSHRD (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD success criteria.\n\n\nPpn. CSHRD (Success) Met: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSHRD success criteria.\n\n\nPpn. CSHRD (Futility) met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD futility criteria.\n\n\nPpn. CSHRD (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the CSHRD futility criteria.\n\n\nPpn. P3 (Futility) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria.\n\n\nPpn. P3 (Success) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\n\n\n\n\nThis is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nMean Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSE Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nCox PPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Cox proportion hazard test between treatment and control in each group was significant.\n\n\nLog Rank Ppn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Log Rank test between treatment and control in each group was significant",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#highlights",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#highlights",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations that each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ -&gt; Fut Flip Flop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut -&gt; Succ Flip Flop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#allocation-observed",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#allocation-observed",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nMean Events &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment in each group.\n\n\nMean Events Across groups, &lt;segement&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of the number of events observed in each arm in each hazard model time segment.\n\n\nMean Exposure &lt;group&gt;, &lt;segment&gt;, &lt;arm&gt;\nOne per group * segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment in each group.\n\n\nMean Exposure Across groups, &lt;segment&gt;, &lt;arm&gt;\nOne per segments * arms\nThe mean (over the simulations) of total exposure time in each arm in each hazard model time segment.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#response",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#response",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Trt.: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the hazard ratio on the treatment arms for each group.\n\n\nMean Trt.: Across groups\n1\nThis is the mean (over the simulations) of the average treatment hazard ratio across the groups.\n\n\nSD Trt : &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the estimate of the treatment hazard ratio in each group.\n\n\nSD Trt : Across groups\n1\nThis is the standard deviation (over the simulations) of the average of the estimates of the treatment hazard ratio across the groups.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response for the scenario\n\n\nMean Mu_theta\n1\nThis is the mean (over the simulations) of the estimate of the mean of hierarchical distribution of the log hazard ratios of the treatment arms in the groups.\n\n\nSD Mu_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the mean of the hierarchical distribution of the log hazard ratios in the groups.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios each group.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratios in each group.\n\n\nAlpha: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Alpha: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nBeta: &lt;segment&gt;\nOne per segment\nThis is the mean (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSD Beta: &lt;segment&gt;\nOne per segment\nThis is the standard deviation (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the mean (over the simulations) of the estimate of the control event rate in each group over each hazard model time segment.\n\n\nTrue Lambda: &lt;group&gt; &lt;segment&gt;\nOne per group * segments\nThis is the true mean control event rate for each group and each hazard time segment",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#probabilities",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#probabilities",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSHRD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for success.\n\n\nProb. CSHRD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for success.\n\n\nProb. CSHRD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the hazard ratio being better than the CSHRD for that group for futility.\n\n\nProb. CSHRD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group hazard ratio being better than the across group CSHRD for futility.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#stopping-rules",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#stopping-rules",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn. CSHRD (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD success criteria.\n\n\nPpn. CSHRD (Success) Met: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSHRD success criteria.\n\n\nPpn. CSHRD (Futility) met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSHRD futility criteria.\n\n\nPpn. CSHRD (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the CSHRD futility criteria.\n\n\nPpn. P3 (Futility) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Futility) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria.\n\n\nPpn. P3 (Success) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria.\n\n\nPpn. P3 (Success) Met: Across groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Futility Criteria Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#evaluation-rules",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#evaluation-rules",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final): Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final): Across Groups\n1\nThis is the proportion of simulations where across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#hierarchical-prior-parameters",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#hierarchical-prior-parameters",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#simulation-results",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#simulation-results",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "This is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#frequentist-results",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#frequentist-results",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nMean Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSE Trt Effect in Grp: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nCox PPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Cox proportion hazard test between treatment and control in each group was significant.\n\n\nLog Rank Ppn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the Log Rank test between treatment and control in each group was significant",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-summary.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Project\n1\nThe name of the “.facts” file in the FACTS GUI that was used to generate the simulations.\n\n\nScenario\n1\nThe name of the scenario – this is the various profile names that make up the scenario, concatenated together.\n\n\nTimestamp\n1\nThe date and time when the simulations started.\n\n\nVersion\n1\nThe version number of the FACTS GUI that ran the simulations.\n\n\nNsim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\n80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nMean Alloc &lt;group&gt; &lt;arm&gt;\n2*G\nThe mean number of subjects recruited into each arm in each group. If a control arm is not included the column is still present, with value ‘0’.\n\n\nMean Trt Resp &lt;group&gt;\nG\nThe mean of the estimates of hazard ratio on the study treatment arm in each group.\n\n\nMean Trt Effect (Overall)\n1\nThe mean of the estimate of the across groups treatment hazard ratio.\n\n\nSE Trt Resp &lt;group&gt;\nG\nThe standard error, over the simulations, of the estimate of hazard ratio on the study treatment arm in each group.\n\n\nSE Trt Effect (Overall)\n1\nThe standard error, over the simulations, of the estimate of the across groups treatment hazard ratio.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\nTrue mean treatment response for the scenario\n\n\nMean Pr Ph3 Success &lt;group&gt;\nG\nThe mean, over the simulations, of the mean probability of success in phase 3 of the study treatment in each group.\n\n\nMean Pr Ph3 Success 99\n1\nThe mean, across the simulations, of the mean probability of success in phase 3 of the across groups treatment difference.\n\n\nMean Pr CSD (Success) &lt;group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm hazard ratio being better than the CSHRD for success in each group.\n\n\nMean Pr CSD (Success) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups hazard ratio being better than the CSHRD for success.\n\n\nMean Pr CSD (Futility) &lt;group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm hazard ratio being better than the CSHRD for futility in each group.\n\n\nMean Pr CSD (Futility) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups hazard ratio being better than the CSHRD for futility.\n\n\nPpn CSD Futility &lt;group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSHRD futility criterion in each group.\n\n\nPpn CSD Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSHRD futility criterion.\n\n\nPpn Ph3 Futility &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success &lt;group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSHRD success criterion in each group.\n\n\nPpn CSD Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSHRD success criterion.\n\n\nPpn Ph3 Success &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility &lt;group&gt;\nG\nThe proportion of simulations where each group met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success &lt;group&gt;\nG\nThe proportion of simulations that each group met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met &lt;group&gt;\nG\nThe proportion of simulations where each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThe proportion of simulations where the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met &lt;group&gt;\nG\nThe proportion of simulations where each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile\n1\nThe proportion of simulations where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn CSD Futility (Final) &lt;group&gt;\nG\nThe proportion of simulations in which the estimate after all the data has been collected of the response on the study treatment arm met its CSHRD futility criterion in each group.\n\n\nPpn CSD Futility (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its CSHRD futility criterion.\n\n\nPpn Ph3 (Final) Futility &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability after all the data has been collected of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success (Final) &lt;group&gt;\nG\nThe proportion of simulations in which the estimate after all the data has been collected of the response on the study treatment arm met its CSHRD success criterion in each group.\n\n\nPpn CSD Success (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its CSHRD success criterion.\n\n\nPpn Ph3 Success (Final) &lt;group&gt;\nG\nThe proportion of simulations in which the predicted probability after all the data has been collected of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success (Final) 99\n1\nThe proportion of simulations in which after all the data has been collected the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility (Final) &lt;group&gt;\nG\nThe proportion of simulations where each group after all the data has been collected met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility (Final) 99\n1\nThe proportion of simulations where the across groups treatment effect after all the data has been collected met both the CSHRD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success (Final) &lt;group&gt;\nG\nThe proportion of simulations where after all the data has been collected each group met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success (Final) 99\n1\nThe proportion of simulations where after all the data has been collected the across groups treatment effect met both the CSHRD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met (Final) &lt;group&gt;\nG\nThe proportion of simulations where after all the data has been collected each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where after all the data has been collected the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met (Final) &lt;group&gt;\nG\nThe proportion of simulations where after all the data has been collected each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSHRD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where after all the data has been collected the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn Outcome 1\n1\nThe proportion of simulations that stopped early for success.\n\n\nPpn Outcome 2\n1\nThe proportion of simulations that reached full accrual and declared success on final evaluation.\n\n\nPpn Outcome 3\n1\nThe proportion of simulations that reached full accrual and declared futility on final evaluation.\n\n\nPpn Outcome 4\n1\nThe proportion of simulations that stopped early for futility\n\n\nPpn Outcome 5\n1\nThe proportion of simulations that stopped early for success but were deemed futile on final evaluation.\n\n\nPpn Outcome 6\n1\nThe proportion of simulations that stopped early for futility but were deemed successful on final evaluation.\n\n\nPpn Outcome 7\n1\nThe proportion of simulations that reached full accrual and were inconclusive.\n\n\nMean Study Accrual Stop Week\n1\nThe mean study duration of accrual – from start of accrual to last patient first visit.\n\n\nMean Mu Theta\n1\nThe mean, over the simulations, of the estimate of the mean of hierarchical distribution of the mean log hazard ratio in each group.\n\n\nSE Mu Theta\n1\nThe standard error, over the simulations, of the estimate of the mean of the hierarchical distribution of the mean log hazard ratio in each group.\n\n\nMean Tau Theta\n1\nThe mean, over the simulations, of the estimate of the standard deviation of the hierarchical distribution of the mean log hazard ratio in each group.\n\n\nSE Tau Theta\n1\nThe standard error, across the simulations, of the estimate of the standard deviation of the hierarchical distribution of the log hazard ratio in each group.\n\n\nMean Alpha &lt;segment&gt;\nS\nThis is the mean (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSE Alpha &lt;segment&gt;\nS\nThis is the standard deviation (over the simulations) of the estimate of the alpha parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean Beta &lt;segment&gt;\nS\nThis is the mean (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nSE Beta &lt;segment&gt;\nS\nThis is the standard deviation (over the simulations) of the estimate of the beta parameter to the gamma distribution for the hierarchical model over the control event rates in each hazard model time segment.\n\n\nMean BAC Mu &lt;group&gt;\nG\nThe mean, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nSE BAC Mu &lt;group&gt;\nG\nThe standard error, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nMean BAC Tau &lt;group&gt;\nG\nThe mean, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nSE BAC Tau &lt;group&gt;\nG\nThe standard error, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution of the log hazard ratios of the historical studies for each group.\n\n\nLambda &lt;group&gt; &lt;segment&gt;\nG * S\nThe mean, over the simulations, of the estimate of the event rate on the control arm in each group in each time segment of the hazard model.\n\n\nTrue Lambda &lt;group&gt; &lt;segment&gt;\nG * S\nThe true mean event rate on the control arm in each group in each time segment of the Control Hazard VSR profile.\n\n\nMean Events &lt;group&gt; &lt;segment&gt; &lt;arm&gt;\nG * S * A\nThe mean, over the simulations, of the number of events on each arm in each group in each time segment of the hazard model.\n\n\nMean Events 99 &lt;segment&gt; &lt;arm&gt;\nS * A\nThe mean, over the simulations, of the number of events on each arm across all the groups in each time segment of the hazard model.\n\n\nMean Exposure &lt;group&gt; &lt;segment&gt; &lt;arm&gt;\nG * S *A\nThe mean, over the simulations, of the subject weeks exposure time on each arm in each group in each time segment of the hazard model.\n\n\nMean Exposure 99 &lt;segment&gt; &lt;arm&gt;\nS * A\nThe mean, over the simulations, of the subject weeks exposure time on each arm across all groups in each time segment of the hazard model",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-summary_freq.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-summary_freq.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Overal Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nMean Trt Effect in Grp &lt;group&gt;\nG\nThe mean, over the simulations, of the estimate of the mean treatment effect in each group from the fit of an overall Cox model that includes a Group X Treatment interaction.\n\n\nSE Trt Effect in Grp &lt;group&gt;\nG\nThe standard error, over the simulations, of the estimate of the mean treatment effect in each group from the fit of an overall Cox model that includes a Group X Treatment interaction.\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations in which the treatment effect was significant.\n\n\nPpn Significnat Str. LogRank\n1\nThe proportion of simulations significant by the stratified log rank test\n\n\npSignificant (GrpxTrt)\n1\nThe proportion of simulations with a significant group and treatment effect.\n\n\nMean Trt Effect &lt;group&gt;\nG\nThe mean treatment effect per group.\n\n\nSE Trt Effect &lt;group&gt;\nG\nThe standard error of the treatment effect per group\n\n\nCox ppn significant\nG\nThe proportion of simulations with significant effect by Cox proportional hazards\n\n\nLogRank Significant &lt;group&gt;\nG\nThe proportion of simulations with significant effect by the Log Rank test",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n#Sim\n1\n✔\n\nSimulation number\n\n\n# Weeks (Duration)\n1\n✔\n\nThe week of final analysis – the total duration of the simulation.\n\n\n#Week\n1\n\n✔\nWeek\n\n\nNo.Subj\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nAlloc &lt;group&gt; \n2 * G\n✔\n✔\nThe number of subjects allocated to each arm in each group.\n\n\nMean Trt Resp (HR) &lt;group&gt;\nG\n✔\n✔\nThe estimated hazard ratio of the study treatment in the group.\n\n\nTrt Effect (HR) (Overall)\n1\n✔\n✔\nThe estimated mean hazard ratio across the groups.\n\n\nSD Mean Trt Resp &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the hazard ratio of the study treatment in the group.\n\n\nSD Trt Effect (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the hazard ratio difference across the groups.\n\n\nMu Theta\n1\n✔\n✔\nThe mean of the hierarchical distribution of the hazard ratios over all the groups.\n\n\nTau Theta\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the hazard ratios over all the groups.\n\n\nLambda &lt;group&gt; \nS * G\n✔\n✔\nThe estimate of the control event rate in each group in each time segment of the hazard model\n\n\nAlpha \nS\n✔\n✔\nThe estimate of the alpha parameter of the hierarchical gamma distribution of the control event rates across the groups, in each time segment of the hazard model\n\n\nBeta \nS\n✔\n✔\nThe estimate of the alpha parameter of the hierarchical gamma distribution of the control event rates across the groups, in each time segment of the hazard model.\n\n\nTrue Lambda &lt;group&gt; \nG * S\n✔\n✔\nThe true control even rate in each group in each time segment of the Hazard Rate VSR profile\n\n\nPr Ph3 Success &lt;group&gt;\nG\n✔\n✔\nThe probability of success in phase 3 of the study treatment for each group.\n\n\nPr Ph3 Success 99\n1\n✔\n✔\nThe probability of success in phase 3 of the across groups treatment difference.\n\n\nPr CSD (Success) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the success CSHRD (or as good as control by the success NIHRM) of the study treatment for each group.\n\n\nPr CSD (Success) 99\n1\n✔\n✔\nThe probability of being better than control by the success CSHRD (or as good as control by the NIHRM) of the across groups treatment difference.\n\n\nPr CSD (Futility) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the futility CSHRD (or as good as control by the futility NIHRM) of the study treatment for each group.\n\n\nPr CSD (Futility) 99\n1\n✔\n✔\nThe probability of being better than control by the futility CSHRD (or as good as control by the futility NIHRM) of the across groups treatment difference.\n\n\nCSD Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSHRD was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSHRD was below the threshold for early stopping for futility for the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSHRD was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSHRD was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Futility 99\n1\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for success for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Success 99\n1\n✔\n✔\nA flag: 1 = the required CSHRD, success in phase 3 and minimum sample size criteria are all met for early stopping for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s success criteria were met for the final evaluation, 0 = otherwise.\n\n\nSuccess Criteria Met (Final) 99\n1\n✔\n✔\nA flag: 1 = the Across Group success criteria were met for the final evaluation, 0 = otherwise\n\n\nFutility Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the group’s futility criteria were met for the final evaluation, 0 = otherwise.\n\n\nFutility Criteria Met (Final) 99\n1\n✔\n✔\nA flag: 1 = the Across Group futility criteria were met for the final evaluation, 0 = otherwise\n\n\nFutile Study\n1\n✔\n✔\nA flag: 1 = the study was futile overall, 0 = otherwise.\n\n\nSuccessful Study\n1\n✔\n✔\nA flag: 1 = the study was successful overall, 0 = otherwise.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome: 1. Early success; 2. Late success; 3. Late futility; 4. Early futility; 5. Success to futility flip-flop; 6. Futility to success flip-flop; 7. Inconclusive\n\n\nGroup Outcome &lt;group&gt;\nG\n✔\n✔\nA flag categorizing final group outcome, using the same codes as for the study outcome (above)\n\n\nGroup Stop Type &lt;group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: 1. Success; 2. Group Cap; 3. Futility; 4. Other (group stopped because study stopped early); 5. Study cap\n\n\nGroup Stop Week &lt;group&gt;\nG\n✔\n✔\nThe week the group stop decision was taken. There may be further follow-up time before the group analysis was completed.\n\n\nStudy Stop Week\n1\n✔\n✔\nThe week the study stop decision was taken. There may be further follow-up time before the study analysis was completed.\n\n\nBAC Mu &lt;group&gt;\nG\n✔\n✔\nThe estimated value of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Mu &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nBAC Tau &lt;group&gt;\nG\n✔\n✔\nThe estimated value of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Tau &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nEvents &lt;group&gt;  \nG * S * A\n✔\n✔\nThe number of events observed in each group, in each time segment of the hazard model, on each arm.\n\n\nEvents 99  \nS * A\n✔\n✔\nThe overall events observed over all the groups, in each time segment of the hazard model, on each arm.\n\n\nExposure &lt;group&gt;  \nG * S * A\n✔\n✔\nThe number of exposure in subject weeks in each group, in each time segment of the hazard model, on each arm.\n\n\nExposure 99  \nS * A\n✔\n✔\nThe overall exposure in subject weeks over all the groups, in each time segment of the hazard model, on each arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-simulations_freq.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-simulations_freq.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nOverall Group p-value\n1\nThe overall group effect p-vale\n\n\nOverall Trt Estimate\n1\nThe overall treatment effect estimate (cox)\n\n\nOverall Trt SD\n1\nThe SD of the overall treatment effect (Cox)\n\n\nEst Trt Effect [exp(coeff)] in Group &lt;group&gt;\nG\nThe estimate of the group treatment effect from fitting the full Cox model with Group x Treatment interaction\n\n\nSD Trt Effect [exp(SD)] in Group &lt;group&gt;\nG\nThe SD of the estimate of the group treatment effect from fitting the full Cox model with Group x Treatment interaction\n\n\nOverall Trt p-val\n1\nThe overall treatment effect p-value (Cox)\n\n\nStr. LogRank Statistic\n1\nStratified LogRank statistic\n\n\nStr. LogRank p-val\n1\nStratified LogRank p-value\n\n\nGroup x Trt p-value\n1\nGroup * Trt interaction p-value\n\n\nEst Trt Effect &lt;group&gt;\nG\nEstimated Trt Effect (Cox) or Exponential rate if control not included\n\n\nSD Trt Effect &lt;group&gt;\nG\nThe SD of the estimated treatment effect\n\n\nCox or Exponential MLE p-value &lt;group&gt;\nG\nTrt Effect (Cox) or rate if control not included – p-value\n\n\nLogRank p-value &lt;group&gt;\nG\np-value based on log-rank test statistic",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nThe accrual region index – where this subject was recruited.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nGroup\n1\nThe index number (1, …) of the group the subject belongs to.\n\n\nArm\n1\nA flag indicating the arm the subject was randomized to: 0 = Control, 1 = Study Treatment.\n\n\nDuration\n1\nThe time of observation of the subject (in weeks)\n\n\nOutcome\n1\nWhether an event was observed (1) or not (0)",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#exporting-the-results",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#exporting-the-results",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Exporting the Results",
    "text": "Exporting the Results\nUsing the menu item File -&gt; Export Project, the .facts file and all the results files can be saved as a single zip file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "title": "Simulation Tab - Time-to-Event Endpoint",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nHR\nG\nThe estimate of the Hazard Ratio in each group\n\n\nOverall HR\n1\nThe estimate of the overall Hazard Ratio\n\n\nLambda &lt;group&gt; &lt;seg&gt;\nG * S\nThe estimate of the event rate in the control arm in each of the Hazard Model observation segments in each group\n\n\nOverall Lambda &lt;group&gt;\nG\nThe estimate over all the observation segments of the event rate in the control arm in each group.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/continuous.html",
    "href": "documentation/v71/userguides/enrichment/vsr/continuous.html",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual subject responses generated from an external response model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/continuous.html#explicitly-defined",
    "href": "documentation/v71/userguides/enrichment/vsr/continuous.html#explicitly-defined",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\n\nBaseline\nThis tab is present only if “Include baseline data” was selected on the Study Info tab.\nBaseline profiles may be added, removed or renamed using the buttons and list on the left hand side of the screen. The baseline for each group is specified as a normal distribution, which may be truncated. If a truncated baseline is used, then samples will be drawn from the underlying normal and re-sampled if they fall outside the specified upper or lower bounds. [Note: clearly this will result in an observed baseline distribution with a different mean and SD from the underlying one].\n\n\n\n\n\n\nFigure 1\n\n\n\nOptionally we can simulate a baseline effect on subject’s responses. This will be an adjustment to the “change from baseline” or “final endpoint value” depending on which definition of Response is being used.\nIf adjusting the final response based on baseline, then the user selects “Use Baseline Adjustment for Subject Response” and supplies 3 parameters for each group:\n\nBeta - a coefficient that reflects the degree of influence of baseline on final score and the degree of variability in the final score due to baseline.\nC – a centering offset, typically the expected mean of the observed baseline scores\nS – a scaling element, typically set to the expected SD of the baseline.\n\nExample – in the above screenshot for the severe group, a baseline of mean 55 and SD 5 has been specified – so a centering of 55 and scaling of 5 is used. Wishing to simulate an overall SD of 5 in the final change from baseline and apportion two-thirds the variance to baseline, Beta has been set as follows:\n\nThe desired final variance is 25 (52), divided between 2/3rd dose response and 1/3rd baseline effects.\nThe SD of the simulated response is set to 4.08 \\(= \\sqrt{\\left( 25*\\frac{2}{3} \\right)}\\)\nThe SD of the scaled baseline score is 1, so to contribute one third the final variance of 25, Beta is set to 2.89 \\(= \\sqrt{\\left( 25*\\frac{1}{3} \\right)}\\)\nNote that when simulating a baseline effect in this way, limiting the range of baseline by specifying upper and lower cut-offs – which might be natural limits of the endpoint, or due to inclusion / exclusion criteria in the protocol – can significantly reduce the variance in the final endpoint due to the baseline effect.\n\n\n\nGroup Response\nResponse profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 2. Mean response, the change from baseline (unless baseline is included in the simulation and the “Response is: Final endpoint value” option has been selected on the Study &gt; Study Info, in which case the response simulated is the final endpoint value), values are entered for the treatment and control arms (if present) for each group directly into the “Treatment Response”/“Treatment change from baseline” and “Control Response”/“Control change from baseline” columns of the table. The graphical representation of these values updates accordingly.\nAs well as the mean of each distribution to sample responses from, it is necessary to specify the variance, by specifying the standard deviation of the observations, termed here the ‘SD response’/‘SD change from baseline’.\n\n\n\n\n\n\nNote\n\n\n\nThe ‘SD response’/‘SD change from baseline’ needs to be specified for each profile, initially for each new profile it will be set to the default value, which is 12. It is very easy to overlook setting this value! If unexpected operating characteristics are seen for any response profile in ED, it is advisable to first check first that ‘SD response’/‘SD change form baseline’ has been set correctly for the profile before looking for more sophisticated reasons.\n\n\nIn addition it is possible for the user to specify if a group “Should Succeed” using the “Should succeed” checkbox on each row. This is then used in the summary of the simulation results to compute how often the simulated trial was successful and groups that ‘Should Succeed’ were successful (reported in the column “Ppn Correct Groups”) and how often the simulated trial was successful and groups that were not marked ‘Should Succeed’ were successful (reported in the column “Ppn Incorrect Groups”).\n\n\n\n\n\n\nFigure 2: Virtual Subject Response – Explicitly Defined - Group Response sub-tab\n\n\n\nThe graph that shows the response to simulate that has been specified – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\nInstead of specifying a single value for the SD of all responses, it is possible to specify (still on a per profile basis) a different SD for the responses for each of the treatment and control arms:\n\n\n\n\n\n\nFigure 3\n\n\n\nOr a different SD for treatment and control in each group:\n\n\n\n\n\n\nFigure 4: Specifying different SD’s of the outcome measure buy group and arm\n\n\n\n\n\nLoad Scenario Means From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses.\nEach individual simulation uses one set of mean responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 5: Virtual Subject Response - Loading group response means from an external file\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual mean responses and the overall mean response over all the VSRs.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. The format is:\n\nIf a control arm is being used: Each line should contain columns [MT1, MT2, … , MTG, ST1, ST2, … , STG, MC1, MC2, … , MCG, SC1, SC2, … , SCG] giving the true Mean responses and SD’s for the Treatment arm in each of the G groups, followed by the Mean responses and SD’s in the Control arms.\nWithout a control arm, using Objective Control: Each line should contain columns [MT1, MT2, … , MTG, ST1, ST2, … , STG] giving the true Mean responses and SD’s for the Treatment arm in each of the G groups.\n\nFor example:\n# Alzheimer’s Example, 3 groups including control, SD of 5 for all arms\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.0, 0.0, 0.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.1, 0.2, 0.4, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.2, 0.4, 0.8, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.3, 0.6, 1.2, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.4, 0.8, 1.6, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n0.5, 1.0, 2.0, 5, 5, 5, 0.0, 0.0, 0.0, 5, 5, 5\n\n\nLongitudinal\nIf ‘Use longitudinal modeling’ has been checked on the ‘Study Info’ tab, and explicitly defined group responses have been specified, then it will be necessary to specify how to simulate the subjects’ responses at intermediate visits. This is done by selecting the correlation method to be combined with the response profiles to generate the intermediate results for subjects on treatment and control in all the groups.\nThe methods are parameterized so that their inclusion does not affect the mean and variance of the final endpoint to be simulated, and they scale automatically to suit each group response profile.\nThree longitudinal methods are provided:\n\nCorrelated: is specified in terms of the ‘amount of noise’ carried forward from the last visit and the amount that is generated ‘a new’.\nHierarchical: visits are correlated through the use of a subject-specific random effect.\nITP: responses follow an exponential model over time with a subject specific random effect.\n\nAll methods allow the fraction of the final response and the fraction of the final variance observed at each visit to be specified.\nBoth methods can be parameterized across the whole trial or separately parameterized for each group.\nClick here for an overview of longitudinal models for continuous endpoints in the FACTS Core engine.\n\nCorrelated Method\nThis method is defined by 3 values for each visit:\n\nft the fraction of the final mean response seen at visit t.\nφt the fraction of the final sigma seen at visit t.\nρt the visit-to-visit correlation in the noise from visit t-1 to t.\n\nThe method can be parameterized for each group individually (as shown) or with one set of parameters for all groups.\n\n\n\n\n\n\nFigure 6: Simulated Longitudinal Results - correlated method\n\n\n\nThe equations currently use a subscript of ‘1’ for the first visit, corresponding to the values shown for the index of each visit shown in the table where the values are entered.\nFor the first visit, for a subject in group g, treatment arm a, the mean response is given by: \\(\\mu_{g,\\ a,1} = f_{1}\\mu_{g,a}\\),\nand the observed variance by \\(\\sigma_{g,a,1}^{2} = \\varphi_{1}^{2}\\sigma_{g,a}^{2}\\)\nthus the expected response \\(y_{g,a,1}\\) for a subject is \\(y_{g,a,1} \\sim\\mu_{g,a,1} + N\\left( 0,\\sigma_{g,a,1}^{2} \\right)\\)\nWhere the final mean \\(\\mu_{g,a}\\), and variance \\(\\sigma_{g,a}\\), are as defined on the group response tab.\nFor the second visit, the mean response is given by: \\(\\mu_{g,a,2} = f_{2}\\mu_{g,a}\\),\nand the observed variance by \\({\\rho_{g,1}\\left( y_{i,1} - \\mu_{g,a,1} \\right)\\frac{\\sigma_{g,a,2}^{2}}{\\sigma_{g,a,1}^{2}} + \\sqrt{1 - \\rho_{g,2}^{2}}N\\left( 0,\\sigma_{g,a,2}^{2} \\right)}_{}^{}\\)\n(where \\(y_{i,1}\\) is the observed response at the ith subject’s first visit response) thus the variance is \\(\\sigma_{g,a,2}^{2} = \\varphi_{2}^{2}\\sigma_{g,a}^{2}\\) but made up of a component that has already been observed at the previous visit, scaled by the correlation factor and relative variance at the second visit compared to the first, plus a new suitably scaled random component.\n\n\nHierarchical Method\nThis method is defined by 2 values for each visit:\n\nft the fraction of the final mean response seen at visit t.\nφt the fraction of the final sigma seen at visit t.\n\nAnd an overall (or per group) fraction \\(\\nu_{g}\\) for how much of the final variance is due to inter-subject variance.\nThe method can be parameterized for each group individually (as shown) or with one set of parameters for all groups.\n\n\n\n\n\n\nFigure 7: Simulated Longitudinal Results - hierarchical method\n\n\n\nFor the visit t, for subject i, in group g, treatment arm a\nthe expected response \\(y_{g,a,t}\\) for a subject is \\(y_{g,a,t} \\sim f_{g,t}\\left( \\mu_{g,a} + \\delta_{i} \\right) + N\\left( 0,\\kappa_{g,a,t}^{2} \\right)\\)\nwhere \\(\\delta_{i}\\sim N\\left( 0,\\tau_{g,a}^{2} \\right)\\)\nand \\(\\tau_{g}^{2} = \\upsilon_{g}\\sigma_{g,a}^{2}\\),\nthe variance of \\(y_{g,a,t}\\) will be \\(\\kappa_{g,a}^{2} + {f_{g,t}^{2}\\tau}_{g}^{2}\\), and \\(\\kappa_{g,a}^{2}\\), will be set so that this \\(= \\varphi_{t}^{2}\\sigma_{g,a}^{2}\\)\nThus there is an inter-subject component to the variance \\(\\delta_{i}\\), and an intra-subject component \\(\\kappa_{g,a}^{2}\\).\n\n\nITP\nThis method is defined by 2 values:\n\nk which controls how quickly the response changes with visit.\nν the fraction of the final variance that is independent of visit.\n\nThese values may be specified per arm or per group and arm.\n\n\n\n\n\n\nFigure 8\n\n\n\nThe subject variability σ2 as specified on the Group Response tab is divided into a per subject component, νσ2 and a component which varies between visits (1-ν)σ2.\nThe response for each subject (including both noise terms and any baseline adjustment term) is scaled at each visit as:\n\\[\ny_{g,a,1} \\propto \\left( \\frac{1 - \\exp\\left( kv_{t} \\right)}{1 - \\exp\\left( kv_{T} \\right)} \\right)\n\\]\nwhere vt is the week of visit t and T is the final visit. The effect of some k values are shown below:\n\n\n\nK\nWeek 1\nWeek 2\nWeek 3\nWeek 4\n\n\n\n\n-1\n64%\n88%\n97%\n100%\n\n\n-0.5\n45%\n73%\n90%\n100%\n\n\n1\n3%\n12%\n36%\n100%\n\n\n\nThe later the final visit, the closer to zero interesting values of k will be, non-interesting values will be:\n\ntoo +ve: the scaling factor is close to 0 until the final visit, or\ntoo –ve and the scaling factor is almost 1 from the first visit onwards.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/continuous.html#external",
    "href": "documentation/v71/userguides/enrichment/vsr/continuous.html#external",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subjects’ responses within FACTS, they can be simulated externally, from a PK-PD model for instance, and imported into FACTS, and the supplied responses are sampled from (with replacement) to provide the subject responses in the simulation. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below.\nTo import an external file, the user must first add a profile to the table. After adding the profile, a file selector window is opened and the user must select the file of externally simulated data. To change the selection click the ‘Browse’ button.\n\n\n\n\n\n\nFigure 9: External Data\n\n\n\n\nRequired Format of Externally Simulated Detail\nThe supplied data should be in the following form: an ascii file with data in comma separated value format with the following columns:\n\nPatient id (must be positive and change from subject to subject)\nGroup index (1, 2, 3, … )\nArm Index (1 = Control, 2 = Treatment)\nVisit Id (1, 2, 3, …)\nResponse\n\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file. Note that all visits for each subject must be grouped together. Thus all the data for the first subject comes before that of the second, and so on.\n#Patient ID, Group Index, Arm Index, Visit, Response\n1, 1, 1, 1, 0.5\n1, 1, 1, 2, 0.8\n1, 1, 1, 3, 1.2\n1, 1, 1, 4, 0.9\n2, 1, 2, 1, 0.4\n2, 1, 2, 2, 0.6\n2, 1, 2, 3, 0.8\n2, 1, 2, 4, 0.9\n3, 2, 1, 1, 0.45\n3, 2, 1, 2, 0.55\n3, 2, 1, 3, 0.6\n3, 2, 1, 4, 0.5\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values. The treatment response analysis in FACTS will be based on the values supplied – if you require the analysis to be on patients change from baseline, then you should supply change from baseline values as the subjects’ responses in the file. Conversely if the analysis should be on the absolute value of the response then these are the values that should be supplied as the subject’s response.\nIf baseline is included, each subject must include a visit 0 for the baseline, and the absolute values of the subject responses should be supplied, the option on the Study &gt; Study Info tab to include baseline allows the specification of whether the analysis should be on\n\nthe absolute values,\nor change from baseline values and FACTS will calculate these and perform the analysis on them. For example the above data might become ….\n\n#Patient ID, Group Index, Arm Index, Visit, Response\n1, 1, 1, 0, 6.2\n1, 1, 1, 1, 6.7\n1, 1, 1, 2, 7.0\n1, 1, 1, 3, 7.4\n1, 1, 1, 4, 7.1\n2, 1, 2, 0, 5.4\n2, 1, 2, 1, 5.8\n2, 1, 2, 2, 6.0\n2, 1, 2, 3, 6.2\n2, 1, 2, 4, 6.3\n3, 2, 1, 0, 6.9\n3, 2, 1, 1, 7.35\n3, 2, 1, 2, 7.45\n3, 2, 1, 3, 7.5\n3, 2, 1, 4, 7.4",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/tte.html",
    "href": "documentation/v71/userguides/enrichment/vsr/tte.html",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual subject response from externally simulated PK/PD data. When simulations are executed, they will be executed for each profile specified by the user.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/tte.html#explicitly-defined",
    "href": "documentation/v71/userguides/enrichment/vsr/tte.html#explicitly-defined",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nUnlike other endpoints where simply a response (mean change from base line or rate) is specified, specification of subject responses for a time-to-event endpoint is by separately specifying a piecewise exponential event rate for the control population and then hazard ratios for the treatment arms. For simplicity and consistency, this means of specifying the simulated event rates is also used when no control arm is present and the comparison is with historic control rates.\n\nControl Hazard Rates\nControl hazard rate profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 1. Event rates values are entered for control subjects in each group. By specifying segment breakpoints (in weeks) different event rates for different periods of follow-up can be specified. The graphical representation of these values updates accordingly.\nTo remove an unwanted segment breakpoint, select the row where the segment starts with that breakpoint and click on ‘Delete’. Note that segments are always defined in weeks. The selection of the unit time only applies to the specification of the event rate on this page.\n\n\n\n\n\n\nFigure 1: Control Hazard Rates tab\n\n\n\n\n\nGroup Response\nResponse profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 2. Hazard ratios are entered for the study treatment arm for each group directly into the Hazard ratio column of the table. The graphical representation of these values updates accordingly.\nIn addition it is possible for the user to specify if a group “Should Succeed” using the “Should succeed” checkbox on each row. This is then used in the summary of the simulation results to compute how often the simulated trial was successful and groups that ‘Should Succeed’ were successful (reported in the column “Ppn Correct Groups”) and how often the simulated trial was successful and groups that were not marked ‘Should Succeed’ were successful (reported in the column “Ppn Incorrect Groups”).\n\n\n\n\n\n\nFigure 2: Virtual Subject Response – Explicitly-Defined - Group Response sub-tab\n\n\n\nThis graph that shows the response to simulate that has been specified – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right-clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\n\n\nLoad Scenario Response From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses.\nEach individual simulation uses one set of responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\nFor TTE the user must supply 2 files – one for the Control Hazard Rates and one for the Hazard Ratios in each group. MVSR hazard rates are only combined with MVSR control hazard rates. MVSR hazard rates are only combined with MVSR control hazard rates. The lines from each file are paired up for each simulation, so the first control hazard rate is used with the first group response hazard ratio, the second control hazard rate is used with the second dose response hazard ratio and so on. There must be the same number of lines in each file.\n\n\n\n\n\n\nFigure 3: Virtual Subject Response - Loading scenario hazard rates from a MVSR file\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual control hazard rates for each group.\n\n\n\n\n\n\nFigure 4: Virtual Subject response – Loading group response using an MVSR file\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual hazard ratios for each group.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. The VSR parameters are provided in two separate files, (the number of lines in the files must be the same for the two files). The formats are:\n\nThe Control Hazard Rate File: Each line should contain columns [L1,1, L1,2, … , L1,S, L2,1, L2,2, … , L2,S, … LG,1, LG,2, … , LG,S] giving the true control hazard rates (lambda) for each of the S segments for each of the G groups. (Note: FACTS assumes that the unit of time is weeks)\nThe Hazard Ratio File: Each line should contain columns [HR1, HR2, … , HRG] giving the true treatment arm Mean Hazard Ratios for each of the G groups.\n\nFor example:\n# TTE control hazard rates, 8 groups 1 segment\n# Same number of rows as in Group Response MVSR file\n#G1\n0.082, 0.087, 0.083, 0.085, 0.091, 0.090, 0.087, 0.087\n0.083, 0.086, 0.085, 0.083, 0.090, 0.091, 0.088, 0.087\n0.084, 0.085, 0.087, 0.082, 0.089, 0.088, 0.089, 0.087\n0.085, 0.084, 0.082, 0.087, 0.088, 0.089, 0.090, 0.087\n0.086, 0.083, 0.084, 0.086, 0.087, 0.086, 0.091, 0.087\n0.087, 0.082, 0.086, 0.084, 0.086, 0.087, 0.086, 0.087\n0.088, 0.091, 0.088, 0.090, 0.085, 0.084, 0.085, 0.087\n0.089, 0.090, 0.090, 0.088, 0.084, 0.085, 0.084, 0.087\n0.090, 0.089, 0.089, 0.091, 0.083, 0.083, 0.083, 0.087\n0.091, 0.088, 0.091, 0.089, 0.082, 0.082, 0.082, 0.087\n# TTE 8 Groups, HR for each group\n# 5 Null cases\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n# increasing mixed strong XX\n1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.3\n1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 1.0, 0.2\n1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4\n1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5\n1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/tte.html#external",
    "href": "documentation/v71/userguides/enrichment/vsr/tte.html#external",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subject response within FACTS they can be simulated externally, from a PK-PD model for instance, and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below.\nTo import an external file, the user must first add a profile to the table. After adding the profile a file selector window is opened and the user must select the file of externally simulated data. To change the selection, click the ‘Browse’ button .\n\n\n\n\n\n\nFigure 5: External Data\n\n\n\n\nRequired Format of Externally Simulated Detail\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id\nGroup index (1, 2, 3,… )\nArm Index (1 = Control, 2 = Treatment)\nUncensored time to event in weeks\n\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n#Patient ID, Group Index, Arm Index, Time to Event\n1, 1, 1, 8.87\n2, 1, 2, 9.34\n3, 2, 1, 6.78\n4, 2, 1, 10.23\n5, 2, 1, 9.96\n6, 2, 2, 5.6\n7, 1, 1, 37.01\n8, 1, 2, 28.67\n9, 1, 1, 39.70\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/dichotomous.html",
    "href": "documentation/v71/userguides/enrichment/study/dichotomous.html",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement, and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether a response is subject improvement or subject worsening. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a response indicates subject improvement, then success criteria will look for the response rate of subjects in the treatment arm to be greater than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will look for the response rate in the treatment arm failing to exceed the response rate in the control arm by more than the Target Rate Difference for Futility (TRDF).\nIf a response indicates subject worsening, then success criteria will look for a response rate of subjects in the treatment arm to be less than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS). Futility criteria will look for a response rate in the treatment arm in failing to be less than the response rate of the control arm less the Target Rate Difference for Futility (TRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 0.5 for success corresponds to lowering the rate by 0.5”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\n\nIn ‘Special Longitudinal Features’ the user specifies whether to use the restricted Markov model for longitudinal aspects of the design. This special longitudinal model determines both the method used to simulate subject response data as well as the model used in the design analysis. See SPEC for more information about the model details.\n\nWithin the restricted Markov model, the user must specify whether subjects with a final assessment of “stable” should be counted as a success or failure.\n\n\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/dichotomous.html#study-info",
    "href": "documentation/v71/userguides/enrichment/study/dichotomous.html#study-info",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement, and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether a response is subject improvement or subject worsening. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a response indicates subject improvement, then success criteria will look for the response rate of subjects in the treatment arm to be greater than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will look for the response rate in the treatment arm failing to exceed the response rate in the control arm by more than the Target Rate Difference for Futility (TRDF).\nIf a response indicates subject worsening, then success criteria will look for a response rate of subjects in the treatment arm to be less than that of subjects in the control arm (or of a specified rate based on historical data) by some Target Rate Difference for Success (TRDS). Futility criteria will look for a response rate in the treatment arm in failing to be less than the response rate of the control arm less the Target Rate Difference for Futility (TRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 0.5 for success corresponds to lowering the rate by 0.5”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\n\nIn ‘Special Longitudinal Features’ the user specifies whether to use the restricted Markov model for longitudinal aspects of the design. This special longitudinal model determines both the method used to simulate subject response data as well as the model used in the design analysis. See SPEC for more information about the model details.\n\nWithin the restricted Markov model, the user must specify whether subjects with a final assessment of “stable” should be counted as a success or failure.\n\n\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/dichotomous.html#sec-groups",
    "href": "documentation/v71/userguides/enrichment/study/dichotomous.html#sec-groups",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Group Info",
    "text": "Group Info\nOn this tab values to define the analysis for each group are specified.\n\nTrials to show Superiority\nIn a trial to show superiority, for each group the Target Clinically Significant Difference can be set). Optionally the parameters for a phase 3 may be specified to allow the predictive probability of subsequent success in such a phase 3 trial to be used as decision criteria.\n\nWith a dichotomous endpoint the Clinically Significant Difference is in terms of Target Rate Difference – that is a difference between the probability of response in the treatment arm and the probability of response in the control arm. Separate differences can be set for determining success and futility, and separate differences can be set for each group and for the across groups analysis. Their use in practice is specified on the “Design &gt; Stopping Criteria” and “Design &gt; Evaluation Criteria” tabs.\n\n\n\n\n\n\n\nFigure 3: The Group Info sub-tab.\n\n\n\n\nThe phase 3 success criteria are:\n\nPhase 3 total number of subjects per arm\nThe required one-sided alpha\nWhether the phase 3 is to use a test for superiority or non-inferiority (set independently from whether the ED trial is for superiority or non-inferiority)\nA super-superiority margin / non-inferiority margin (depending on whether the phase 3 trial is for superiority or non-inferiority), this margin is independent from any margins specified for the ED trial itself.\n\nGiven these criteria FACTS calculates the predicted probability of success in such a trial for each treatment arm given the estimate of the treatment difference, integrated over the uncertainty in that estimate. The conventional expected power of the specified phase 3 is calculated for the treatment effect in each MCMC sample and then averaged. The resulting predicted probability of success in phase 3 can then be used in the stopping criteria and final evaluation criteria.\n\nSeparate ‘Target Rate Differences’ can be set for success and for futility, specifying the TRD for success and TRD for futility. We use the terms Target Rate Difference here as it makes it clearer that the value to be entered should be positive. Elsewhere (in column headings for instance) the more conventional term CSD may be used.\n\nIf the “Posterior probability” criteria is used for stopping a group for success or judging if a group is successful in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\pi_{d} - \\pi_{0} &gt; CSD\\ for\\ success \\right) &gt; Success\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target difference in rates or success is greater than a specified threshold (set on the Design tabs).\nIf the “Posterior probability” criteria is used for stopping a group for futility or judging if a group is futile in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\pi_{d} - \\pi_{0} &gt; CSD\\ for\\ futility \\right) &lt; Futility\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target mean difference for futility by is less than a specified threshold (set on the Design tabs).\nIf the endpoint is such that a response means subject worsening, then the comparison is reversed (the treatment difference becomes \\(\\pi_{0} - \\pi_{d}\\)) and if the trial is non-inferiority then the test for “being greater than the CSD” is replaced with testing for “being less than the NIM” (Non-Inferiority Margin). Thus the meaning of the user specified Difference or Margin is interpreted taking into account both whether a response means ‘better’ or ‘worse’ and whether the trial aim is ‘superiority’ or ‘non-inferiority’. The result is that for normal usage the value entered will be +ve, as the following diagrams should make clear:\n\n\n\n\n\n\nFigure 4\n\n\n\n\nNote that in Superiority trials the required TRD for success will be greater than or equal to the TRD for futility, whereas in the Non-Inferiority trials the required NIM for futility will be greater than or equal to the NIM for success.\n\n\nNotes on setting Target Mean Differences\nTarget Rate Differences (TRD) are also referred to as Clinically Significant Differences (CSD). A “standard” hypothesis test for demonstrating superiority to control uses a CSD of 0. Testing with a non-zero CSD is different, and the implications need to be carefully understood.\nThe first mistake to avoid is setting the target rate difference for success too large. The decisions for success are in terms of the estimated posterior probability that the rate difference of the response on the treatment arm from the response on the control arm is greater than the target. If the CSD is set to what might be the treatment difference in the “alternate hypothesis” of a standard hypothesis test, we could only expect on average to have a posterior probability that the treatment difference is greater than the CSD of 50%.\nTo achieve posterior probabilities of &gt; 50%, we must set a CSD that we expect the treatment to exceed. To achieve the desired power by instead lowering the required posterior probability threshold would be a mistake, as posterior probability thresholds of &lt; 50% have the undesirable characteristic that the criteria can be met in circumstances where it can be seen that if further data was gathered consistent with what had been seen already, it would lead the threshold no longer being met. The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a target rate difference for success that, should the treatment have the value that it is hoped to achieve, we would expect to see some &gt;50% probability of being greater than it. Thus rather than using what might be termed ‘the target value’ for the TRDS, it is better to use ‘the minimum acceptable value’.\nThe same target difference can be used to decide futility, requiring a &lt;&lt; 50% confidence that the difference of the response rate on the treatment arm from response rate on the control arm is greater than the target. However, particularly if there are other endpoints not being explored in the simulation, or other properties of the treatment (such as convenience, compliance, cost, tolerability etc.) that might justify continued development even if it is not an outright winner on the primary endpoint, it may be that a lower target rate difference needs to be set as the threshold for determining futility – for example sufficient to demonstrate that development even on the basis of non-inferiority on the primary endpoint is likely to fail. Hence FACTS allows separate CSDs to be set for assessing success and futility.\n\n\nNon-inferiority\nIn a trial to show non-inferiority, the tab is the same except that ‘Target Rate Differences’ are now ‘Target Non-inferiority Margins’.\n\n\n\n\n\n\nFigure 5: Group Info tab, in a non-inferiority design",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/dichotomous.html#visits",
    "href": "documentation/v71/userguides/enrichment/study/dichotomous.html#visits",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Visits",
    "text": "Visits\nIf longitudinal modeling is not being used, then simply the time to the final visit is specified:\n\n\n\n\n\n\nFigure 6: Time to endpoint\n\n\n\nOtherwise the visit schedule needs to be specified. The last visit in the schedule is taken to be when the final endpoint is observed. Visits can be specified one at a time by entering the week of the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week of the visit and the visit index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 7: Visit schedule",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/dichotomous.html#variants",
    "href": "documentation/v71/userguides/enrichment/study/dichotomous.html#variants",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of subjects).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different maximum subjects for each group that has had a cap specified on the Study &gt; Study Info tab and maximum “Total Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\nIn FACTS Enrichment Designs, as well as trial Success and Failure rates, a major Operating Characteristic that we often wish to estimate is the ability of the design (if the trial is successful) to select the ‘right’ groups, depending of course on the scenario being simulated. To enable FACTS to report this the user must specify on the Virtual Subject Response &gt; Explicitly Defined &gt; Group Response profiles which of the groups “Should succeed”, that is, it would constitute a ‘correct selection’ by the design in that scenario.\n\n\n\n\n\n\nFigure 8: The Variants tab, specifying 3 variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/execution.html",
    "href": "documentation/v71/userguides/enrichment/execution.html",
    "title": "Execution Tab",
    "section": "",
    "text": "The Execution tab allows the user to specify profiles for subject accrual rates and dropout rates.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/execution.html#accrual",
    "href": "documentation/v71/userguides/enrichment/execution.html#accrual",
    "title": "Execution Tab",
    "section": "Accrual",
    "text": "Accrual\nThe Accrual sub-tab provides an interface for specifying accrual profiles; these define the mean recruitment rate week by week during the trial. During the simulation, the simulator uses a Poisson process to simulate the random arrival of subjects with the specified mean accrual rate.\nAccrual profiles are displayed in left-most table on the screen, as depicted below. These accrual profiles may be renamed by double-clicking on them, and typing a new profile name. After creating a profile the user must create at least one recruitment region. Early in the trial design process, detailed simulation of the expected accrual pattern is typically not necessary and a single region with a simple mean accrual rate is sufficient.\n\n\n\n\n\n\nTip\n\n\n\nIf using a hierarchical model across groups and you know the group accruals are going to be staggered, this should be incorporated in the simulations earlier rather than later.\n\n\nTo model more accurately the expected accrual rates over the trial, the user may specify multiple regions for each accrual profile and separately parameterise them. Regions are added via the table in the center of the screen (Figure 8‑1). Within this table, the user may specify:\n\nthe peak, mean weekly recruitment rate,\nthe start date (in weeks from the start of the trial) for this recruitment region,\nwhether the region will have a ramp up phase and if so when the ramp up will be complete (in weeks from the start of the trial),\nwhether the region will have a ramp down, and if so when the ramp down start and when the ramp down will complete (in weeks from the start of the trial).\n\nRamp up and ramp down define periods of simple linear increase and decrease in the mean recruitment rate from the start to the end of the ramp. Note that simulation of accrual is probabilistic but ramp downs are defined in terms of time, so even if ramp downs are planned so that at the average accrual rate they will occur as the trial reaches cap, there is a risk in simulations with slow accrual, that ramp downs occur before the full sample size is reached. It is advisable to have at least one region that doesn’t ramp down to prevent simulations being unable to complete.\nA graph of the overall recruitment rate profile of and the currently selected region is displayed. As the recruitment parameters are changed, the graph will update to show the time at which full accrual is reached. An accrual profile that does not reach full accrual is invalid and cannot be used to run simulations.\nIn ED, as well as the overall recruitment rate, the relative recruitment into each group must be defined. This is interpreted as defining the relative proportion of the total population that would fall into each particular group. It is also possible to specify a delay (in weeks from the start of the trial) in accepting subjects of a particular group.\nWhen a group is not being recruited – either because recruitment in that group has not yet started or because that group is no longer being recruited – then the overall recruitment rate is reduced by the fraction corresponding to the relative proportion of the population constituted by that group.\n\n\n\n\n\n\nFigure 1: Accrual\n\n\n\nIn the screenshot above you can see\n\nThe two step ramp up in accrual from the two regions – one starting later than the other.\nIn the group accrual that the mild group (who constitute 50%of the population) will complete accrual first at about week 65.\nThe overall recruitment shows a drop after about week 65 because after then only the Moderate and Severe sub-populations will be recruited as the Mild subgroup should have reached its cap.\n\nNote that the accrual profile graph is only the mean expectation; actual accrual is simulated using exponential distributions for the intervals between subjects, derived from the mean accrual profile specified here. Thus some simulated trials will recruit more quickly than this and some more slowly.\nThere are commands to import and export region details from/to simple external XML files. When importing, the regions defined in the external file are added to the regions already defined, they don’t replace them. Import/export allows an accrual profile with a large specification of regions to be copied to another FACTS design by exporting it from this design and importing it into the other.\nThe simple XML format means that these region definitions could be shared with other software. This is an example of a simple region file defining with 4 regions:\nRegion 1: has a peak rate of 1 and starts on week 0\nRegion 2: has a peak rate of 1 and starts on week 4, with a ramp up that completes on week 8\nRegion 3: has a peak rate of 1 and starts on week 6, with a ramp down starting week 40 that completes on week 50\nRegion 4: has a peak rate of 2 and starts on week 10, with a ramp up that completes on week 14 and a ramp down starting week 45 that completes on week 55.\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;regions&gt;\n&lt;region&gt;\n&lt;name&gt;Region 1&lt;/name&gt;\n&lt;rate&gt;1&lt;/rate&gt;\n&lt;start&gt;0&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;region&gt;\n&lt;name&gt;Region 2&lt;/name&gt;\n&lt;rate&gt;1&lt;/rate&gt;\n&lt;start&gt;4&lt;/start&gt;\n&lt;ramp-up&gt;\n&lt;ramp-complete&gt;8&lt;/ramp-complete&gt;\n&lt;/ramp-up&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;region&gt;\n&lt;name&gt;Region 3&lt;/name&gt;\n&lt;rate&gt;1&lt;/rate&gt;\n&lt;start&gt;6&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down&gt;\n&lt;ramp-start&gt;40&lt;/ramp-start&gt;\n&lt;ramp-complete&gt;50&lt;/ramp-complete&gt;\n&lt;/ramp-down&gt;\n&lt;/region&gt;\n&lt;region&gt;\n&lt;name&gt;Region 4&lt;/name&gt;\n&lt;rate&gt;2&lt;/rate&gt;\n&lt;start&gt;10&lt;/start&gt;\n&lt;ramp-up&gt;\n&lt;ramp-complete&gt;14&lt;/ramp-complete&gt;\n&lt;/ramp-up&gt;\n&lt;ramp-down&gt;\n&lt;ramp-start&gt;45&lt;/ramp-start&gt;\n&lt;ramp-complete&gt;55&lt;/ramp-complete&gt;\n&lt;/ramp-down&gt;\n&lt;/region&gt;\n&lt;/regions&gt;\n\n\n\n\n\n\nFigure 2: Accrual",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/execution.html#drop-out-rates",
    "href": "documentation/v71/userguides/enrichment/execution.html#drop-out-rates",
    "title": "Execution Tab",
    "section": "Drop-out Rates",
    "text": "Drop-out Rates\nThe Dropout Rate sub-tab provides an interface for specifying dropout profiles; these define the probability of subjects dropping out during the trial.\nDropout profiles are listed on the left of the screen, as depicted below. These profiles may be renamed by double-clicking on them, and typing a new profile name. The default dropout scenario is that no subjects drop out of the study before observing their final endpoint data.\nThe continuous and dichotomous engines allow specification of dropout rate slightly differently than the time-to-event engine.\n\nContinuous/DichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 3: Dropout Rate specification tab for a continuous or dichotomous endpoints.\n\n\n\nIn the Continuous and Dichotomous engines, the following options for dropouts exist:\n\nHaving no dropout modelling (“No Dropouts”);\nSpecifying dropouts as a fraction of all patients per visit (“Dropouts per visit”);\nSpecifying dropouts for each arm and visit (“Dropouts per arm and visit”).\nSpecifying dropouts for each group, arm and visit (“Dropouts per group, arm and visit”).\n\nMissing Data - A subject that drops out after the first visit will contribute to the Bayesian modeling and frequentist analysis. Final response estimates are imputed based on the completed visits and the selected longitudinal model for Bayesian modeling, and LOCF for the frequentist analysis. However, a subject that drops out before the first visit will be included in subject randomization counts, but excluded from the frequentist analysis and will make no net contribution to the Bayesian modeling. When the longitudinal modeling option has been disabled, no intermediate visits are simulated and for all dropouts the latter approach applies.\n\n\n\n\n\n\n\n\nFigure 4: Dropout Rate specification for time-to-event endpoints.\n\n\n\nIn the Time-to-Event engine, the following options for dropouts exist:\n\nHaving no dropouts included in the simulation (“No Dropouts”);\nSpecifying dropouts by group;\nSpecifying dropouts by group and visit;\nSpecifying dropouts for each group, and segment.\n\nDropout rates can be specified either by the mean time or a hazard rate (in weeks). When specified by group and segment, a set of dropout specific time segments can be entered; these do not have to match the segments used in defining the control hazard rate (or segments used in the design).\nMissing Data – Only subjects that drop out before experiencing an event matter, such a subject’s data becomes censored, contributing only to the overall exposure time.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html",
    "href": "documentation/v71/userguides/crm.html",
    "title": "CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system.\n\n\n\nThis document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nPlease cite FACTS wherever applicable using this citation.\n\n\n\nAn overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-purpose",
    "href": "documentation/v71/userguides/crm.html#sec-purpose",
    "title": "CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-scope",
    "href": "documentation/v71/userguides/crm.html#sec-scope",
    "title": "CRM",
    "section": "",
    "text": "This document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-citing",
    "href": "documentation/v71/userguides/crm.html#sec-citing",
    "title": "CRM",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-definitions",
    "href": "documentation/v71/userguides/crm.html#sec-definitions",
    "title": "CRM",
    "section": "",
    "text": "An overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 7.1 Changes to N-CRM",
    "text": "FACTS 7.1 Changes to N-CRM\nIn FACTS 7.1 there were new features added to N-CRM:\n\nIt is now possible to backfill to the current escalation dose (also known as “frontfilling”).\nIt is now possible to specify a third queue concept – maximum number of patients in their DLT period on the current MTD estimate.\nIt is now possible to define the concept of “near” target/MTD as part of stopping rules, for both fine-grained and regular dosing.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 7.0 Changes to N-CRM",
    "text": "FACTS 7.0 Changes to N-CRM\nIn FACTS 7.0 the only change was on the Analysis tab, when entering subject data manually, the FACTS GUI now ensures that a cohort number is entered for every subject.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.5 Changes to N-CRM",
    "text": "FACTS 6.5 Changes to N-CRM\nIn FACTS 6.5 there was a new feature added to N-CRM:\n\nIt is now possible to generate a design report – a Word document describing design - once the design has been simulated. In FACTS 6.5 there was two small changes to the functionality:\nWhen deriving toxicity/efficacy priors from quantiles at the lowest and highest doses, the “middle” probability option now refers to the median dose rather than the reference dose and is now optional. When pre-6.5 FACTS files are loaded in FACTS 6.5, the “middle” probability option will be ignored and replaced by 0.5.\nWhen deriving toxicity/efficacy priors from specific quantiles the specification of at least two dose levels is now required whereas previously the specification of at least three dose levels was required.\n\nIn FACTS 6.5 there were some improvements in the simulated behavior:\n\nDesigns which include efficacy, the “Maximum cohorts used to determine MTD” parameter on the Allocation Rule tab is now observed, in FACTS 6.4 and earlier it was ignored.\nIn N-CRM designs using open enrollment without accrual pausing, FACTS will now output entries in the cohort file for subjects that have been lost following stopping rules being met.\nIn N-CRM designs using open enrollment, FACTS will no longer prematurely apply stopping conditions when there are still subjects whose outcomes have been observed, but not yet processed.\nIn N-CRM designs using open enrollment, FACTS will now report the selected MTD/MED/OSD and associated state to be the dose where the stopping conditions were first met, unless the MTD has subsequently decreased post stopping rules being met. This is to prevent a dose above the one selected when the stopping conditions were met being reported as the MTD when it is very likely that there is insufficient data on this higher dose to justify its selection. If rather than reporting the MTD at the point when the stopping rules where met, you would like the trial to resume if the dose selected as MTD has changed (and this the stopping rules possibly no longer met), ensure that the ”Pause accrual and wait for completers” option is selected on the “Stopping Criteria” tab. This allows the trial to resume if the recruitment cap has not been met.\nFACTS Command Line mode and FLFLL (FACTS Linux File Loader Lite) now correctly handle the processing of N-CRM designs whose prior was derived using the “Legacy Prior” option.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.4 Changes to N-CRM",
    "text": "FACTS 6.4 Changes to N-CRM\nIn FACTS 6.4 there were no changes to the Dose Escalation simulators.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.3 Changes to N-CRM",
    "text": "FACTS 6.3 Changes to N-CRM\nIn FACTS 6.3 a number of changes were made to improve facilities in N-CRM, or improve the way existing facilities were implemented. These were:\n\nNew run-in options: the existing run-in scheme is available as “simple run-in”, “custom run-in” allows a specific sequence of doses and number of subjects to test at each dose to be specified, “small cohort pre-escalation” allows a run that uses a smaller cohort size but follows the dose escalation rules and over dose control.\nNew “backfill” options in open enrolment. Backfill allows subjects that become available at a time when they can’t be allocated to the current dose (because the maximum number of subjects without final results have already been allocated to the current dose).\nImproved handling of “maximum subjects without final results” in open enrolment. In earlier versions of FACTS this was a “global” maximum, which led to a suboptimal allocation pattern and overly cautious rejection of subjects that became available. The new model applies a maximum “per dose” so that once the trial has escalated to a new dose strength, any subjects without final results on lower doses do not block allocation to the new dose, in addition it is possible to specify two different maximums – one for when a dose has just been escalated to but has not been “cleared” (typically smaller and more cautious), and one when a dose has been cleared but we continue to allocate to it because it is the target dose (typically larger and more confident). This method is such an improvement that we recommend moving any design using open enrolment to this new version of FACTS.\nImproved Ordinal Toxicity model – the way the likelihood is calculated has been improved – reducing the uncertainty in the model fit. Any design using an ordinal model will need to re-calibrate the prior if you move the design to FACTS 6.3. If you have a design already complete, or in execution we recommend you remain using the earlier version of FACTS for that trial.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.2 Changes to N-CRM",
    "text": "FACTS 6.2 Changes to N-CRM\nIn FACTS 6.2 features available separately in the other FACTS CRM engines (CRM (Toxicity), bCRM & CRM Ordinal) were all incorporated into N-CRM. This allowed these features to be used in conjunction with N-CRM’s target toxicity band methodology, overdose control and open enrollment features, as well as in conjunction with each other for the first time.\nThe new features are:\n\nFrom CRM (Toxicity) the option to specify that the data is coming from ‘two groups’ and for the toxicity experienced in the two groups to be modelled with a joint model [CRM 2 Sample]. This allows a trial where there are two patient populations (such as adults and children) or where there are two versions of the treatment to be simulated.\nFrom bCRM the option to model a second binary efficacy endpoint [bCRM] and the for dose allocation to proceed in two stages – the first to establish an MTD and the second to establish an MED.\nFrom CRM Ordinal the option for the toxicity endpoint to be modelled not as binary endpoint, but one with different categories of toxicity, and with a joint model applied to the different categories [CRM Ordinal]. The endpoint can be to model either 3 or 4 categories of toxicity:\n\ncategory 1 is “no toxicity”,\ncategory 2 is “mild toxicity”,\ncategory 3 is “toxicity”\ncategory 4 (if included) is “severe toxicity”\n\n\nAll decision making is made in terms of the probability of observing a category 3 (or worse) toxicity.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "title": "CRM",
    "section": "FACTS 6.1 Changes to N-CRM",
    "text": "FACTS 6.1 Changes to N-CRM\nIn FACTS 6.1 N-CRM has the new feature of ‘design variants’ that allow the user to easily simulate and evaluate an N-CRM design at different sample sizes. This change includes 4 elements:\n\nUnder the ‘Study’ tab the user can now specify the number of design variants, and for each variant the maximum study size in Cohorts.\nOn the simulation tab FACTS will display a copy of each simulation scenario for each variant.\nThe simulation results now include the Ppn of trials that stopped for each stopping reason: stopping because all doses are too toxic (the toxicity estimates exceed the overdose criteria), because a stopping rule was met or because the study cap was reached.\nThere are now a set of cross variant graphs that show trellis plots of the key summary graphs by design variant and scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#overdose-control",
    "href": "documentation/v71/userguides/crm.html#overdose-control",
    "title": "CRM",
    "section": "Overdose Control",
    "text": "Overdose Control\nOverdose control can be specified on the Study &gt; Toxicity tab. Overdose control specifies a limit on the probability that a dose has a toxicity rate above a certain level. After fitting the Bayesian logistic regression model, all doses for which the posterior probability that their toxicity rate lies above the specified level exceeds the specified limit are ineligible for allocation. Because the Bayesian Logistic regression is monotonic, this means that after every analysis either all doses are permitted for allocation or there will be a dose level above which no dose is permitted for allocation.\n\n\n\n\n\n\nFigure 1: Setting the overdose control limit\n\n\n\nThe overdose control is specified in terms of the “toxicity bands” (concept of allowing ranges for the target toxicity, excess toxicity, unacceptable toxicity and under-dosing explained in more detail in this section) and can either be in terms of the “excess and unacceptable toxicity bands” or just the “unacceptable toxicity band”. The “excess and unacceptable toxicity band” is every toxicity rate above the upper bound of the target band. Care should be taken when setting the permitted threshold for this joint band. If set below 0.5, it will likely exclude doses whose mean expected toxicity rate is within the target band with the risk that this makes the escalation decision in the design too cautious. Initially it might be recommended to just use the “unacceptable band” for specifying the overdose control. This allows an overdose control that is more strict – for example: “exclude any dose where the probability that the toxicity rate is above 0.6, is greater than 20%“. The lower bound for the unacceptable band can be set wherever desired, its only role is in defining this band for overdose control. It is also possible to specify that the limit changes over the course of the trial, allowing the overdose control to become stricter as more information becomes available. For example, one could reduce the permitted probability of a dose having a toxicity rate in the unacceptable band from 50% to 25% in steps of 2.5% after every cohort.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#dose-escalation-rules",
    "href": "documentation/v71/userguides/crm.html#dose-escalation-rules",
    "title": "CRM",
    "section": "Dose Escalation Rules",
    "text": "Dose Escalation Rules\nThe dose escalation could be solely controlled by the overdose control (as originally proposed in (Neuenschwander, Branson, and Gsponer 2008)), however this means that the escalation behavior is very dependent on the interplay between the prior and the observed data. Usually, teams prefer to have a fixed set of rules in place ensuring the escalation behavior is sufficiently cautious. FACTS has an option to just use overdose control or to use a combination of overdose control and a set of fixed escalation rules. In the latter case, the following rules can be set in the Design &gt; Allocation Rule &gt; Allocation tab:\n\n\n\n\n\n\nFigure 2: Dose escalation rules\n\n\n\nWe introduce the notion of whether a dose has been “cleared”. A dose is cleared once we have sufficient data on it (usually, but not necessarily, the results of one cohort, but if the cohort size is small, for example 2 subjects, perhaps more than one cohort will be required). This can be supplemented by a rule that if the observed raw toxicity rate at the dose exceeds a certain limit, then the dose is not counted as cleared (this rule is usually unnecessary if overdose control limits have been set). Once a dose has been cleared, it stays cleared, meaning there is “maximum cleared dose”. The number of dose increments or the factor of dose strength above the current cleared dose that can be allocated to is then specified. For example, with doses of 12.5, 25, 50, 100, 150, 200, 250, we might allow escalation at two dose increments a time. In the figure below, you see the combination of settings used to achieve this behaviour alongside the “Fastest Possible Dose Escalation” plot on the right:\n\n\n\n\n\n\nFigure 3: Escalation by number of dose increments\n\n\n\nAlternatively, we can specify the permitted escalation as a ratio, for example we might allow the dose strength to be at most tripled at each escalation, which, with the example dose strengths, makes the initial escalation more cautious:\n\n\n\n\n\n\nFigure 4: Escalation by dose strength factor\n\n\n\nThe escalation rules can be adjusted so that instead of a single increment rule, there are different increments depending on the dose, or depending on the number of observed toxicities. To modify our earlier example, we can allow escalation by 2 dose levels while no toxicities have been observed, but limit it to only one dose level once one or more toxicities have been observed:\n\n\n\n\n\n\nFigure 5: Escalation increment varying by number of toxicities\n\n\n\nLastly escalation can be relative to the highest cleared dose, or relative to the last dose allocated.\nTo summarize the allocation procedure:\n\nThe current maximum cleared dose is identified.\nThe current data is analyzed using the Bayesian Logistic Regression model.\nThe overdose rules are evaluated and all doses exceeding the overdose control limit are excluded from this escalation selection.\nFrom the remaining doses, the dose best meeting the target MTD or target toxicity interval objective based on the model is selected as the “target dose” (TD).\nIf the TD is at or below the current maximum cleared dose, the next cohort is allocated to the TD.\nIf the TD is within the escalation rules of the current maximum cleared dose, the next cohort is allocated to the TD.\nOtherwise, the next cohort is allocated to the highest dose above the current maximum cleared dose as allowed by the escalation rules.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#initial-run-in",
    "href": "documentation/v71/userguides/crm.html#initial-run-in",
    "title": "CRM",
    "section": "Initial Run-in",
    "text": "Initial Run-in\nThe purpose of defining a run-in is to define a fixed allocation behavior to be followed up to the first toxicity being observed. The specified number of subjects to allocate to each dose in the run-in and which doses to test are specified. This scheme is followed until a toxicity is observed or we reach the end of this fixed scheme.\nThree forms of run-in specification are available:\n\nSimple: allocates a small cohort to every defined dose in ascending order (unless fine grain doses - see this section – have been specified, in which case the escalation rules are followed).\nCustom: allocates a defined number of subjects (possibly varying by dose) to selected doses in ascending order.\nSmall cohort pre-escalation: allocates a small cohort, but follows the escalation rules assuming just a single small cohort is required to clear a dose.\n\nAll run-in schemes can be modified in a number of ways:\n\nSpecifying a maximum dose at which the run-in stops if no toxicities are observed until that dose.\nIf ordinal toxicities are being simulated, the run-in may should at the first observed category 2 toxicity (rather than a category 3 toxicity)\nWhether the subjects used in the run-in should be counted towards the trial sample size or not.\nWhen a toxicity is observed the standard behavior is to allocate to the minimum of: the last dose tested in the run-in, the current TD or the highest dose that can be allocated to by the overdose rules. This can be replaced by expanding the allocation on the current dose to make it a full cohort as specified in Study &gt; Study Info tab (this option is particularly useful in conjunction with stopping for a category 2 toxicity).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#two-groups",
    "href": "documentation/v71/userguides/crm.html#two-groups",
    "title": "CRM",
    "section": "Two Groups",
    "text": "Two Groups\nFACTS has the option to model the subjects in the trial as belonging to two different groups, these can be either:\n\nTwo groups distinguished by a baseline property of the subjects, for example adults and paediatrics.\nTwo groups separated by a difference in treatment (and selected randomly), for example the study drug alone or in combination with an additional drug.\n\nThere are options for when group 2 starts enrolling:\n\nThey can be recruited sequentially – group 1 then group 2.\nThey can be recruited in parallel\nThe second group can be started when the allocation to the first group reaches a particular dose\nThe second group can be started when the number of subjects allocated to group 1 reaches a particular threshold.\n\nA joint model is fitted to the two groups.\nThe first group is modeled:\n\\[\nlogit(p_{1j}) = \\alpha + \\beta \\hat{x}_j\n\\]\nThe second group is modeled:\n\\[\nlogit(p_{2j}) = (\\alpha + a) + (\\beta + b) \\hat{x}_j\n\\] With separate priors and some optional constraints on \\(a\\) and \\(b\\). Dose escalation and stopping are judged independently for the two groups.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-efficacy",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-efficacy",
    "title": "CRM",
    "section": "Efficacy",
    "text": "Efficacy\nFACTS has the option to additionally model an efficacy endpoint. There are currently two limitations in simulation:\n\nOnly a binary efficacy endpoint can be simulated\nThe efficacy endpoint is assumed to be available at the same time as the toxicity endpoint.\n\nThe efficacy and toxicity endpoints are modelled separately. There are options to specify early stopping rules for finding the MTD, and to specify a cap on the sample size that can be spent finding the MTD. Once these rules are met, then allocation is towards the Minimum Efficacious Dose (MED) – if this is below the MTD. If the estimated MED lies at or above the estimated MTD, the allocation is at the estimated MTD.\nIf while allocating to the estimated MED further toxicity results change the estimate of the MTD, and if there is now insufficient information on the MTD as specified by the early stopping rules for finding the MTD, allocation switches back to allocating to the estimated MTD, if the sample size cap for finding the MTD allows.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-fgd",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-fgd",
    "title": "CRM",
    "section": "Fine Grain Dosing",
    "text": "Fine Grain Dosing\nIn some settings, e.g. when the drug is delivered in solution by IV or when manufacturing allows any dose in a range from say 100mg to 400mg in steps of 10mg, dose strengths need not be restricted to just a small number of pre-defined levels. FACTS has a feature that allows this to be simulated, not with a continuous range of doses, but with “fine grain” dosing.\nFACTS supports the specification of a range of doses from a minimum to a maximum with doses either equally spaced or spaced with equal ratio. Using dose ratio makes most sense it you want to use the dose strength whilst believing the effect will be roughly log-dose. Using dose ratios, it’s necessary to accept FACTS reporting dose strengths only close to those desired. As an example, if the main doses followed a dose doubling scheme: 12.5, 25, 50, etc., one might use fine grain dosing with dose space ratios of approximately the 4th root of 2 (1.1892). The resulting doses are 12.5, 14.865, 17.677, 21.022, 24.999, 29.729, 35.354, 42.043, 49.998, etc., which means there are three dose levels between each of the original doses.\nThere are two alternatives:\n\nUse nominal dose strengths 1, 2, 3, 4, … (i.e. assuming the dose spacing is linear in expected effect) and label the doses according to their actual strength.\nUse a fixed dose interval (e.g. 12.5 resulting in doses of 12.5, 25, 37.5, 50, 62.5, etc.) so the lower doses (of the original scheme) have fewer (or no) intermediate doses and the higher doses have many more. The dose escalation rules can be specified in terms of dose strength ratio to achieve the required escalation, for example allowing dose escalation with a dose strength ratio of 2 will result in the initial escalation using doses 12.5, 25, 50, 100, etc.\n\nAs well as possibly adjusting the dose escalation step size to accommodate the new dose levels on the Design &gt; Allocation Rule tab, there are two other rules that may need modification:\n\nTo count a dose as “cleared”, we might now count cohorts on nearby doses to count towards the required clearing total. This is specified as the “Max ratio of dose strengths considered as near” (if dose allocation rules apply to ratio of dose strength) or “Delta in dose strength considered as near” (if dose allocation rules apply to dose strength) on the Design &gt; Allocation Rule tab.\n\nFor example, if we have doses at roughly 4th root of 2 intervals, we might count any dose within a ratio of 1.2 as “near” so that any cohorts allocated to immediate neighbor doses count towards clearing a dose.\nAlternatively, if we have doses every 12.5mg from 12.5 to 400, counting any dose within a ratio of 1.1 will mean that from dose 125 and above, immediate neighbor doses (within 12.5) count towards clearing a dose, and from dose 250 and above, doses within 25mg (two immediate neighbor doses) count towards clearing a dose.\n\nThe concept of “near doses” in fine grained dosing allows us to skip certain doses in the escalation phase, which might make sense if there is reason to believe that doses of similar dose strengths behave similarly and don’t provide enough additional information to justify assigning more cohorts to.\n\n\n\n\n\n\nFigure 6: Doses from 12.5 to 400mg, with fixed spacing of 12.5. Showing dose escalation by dose doubling.\n\n\n\nWhen requiring a certain number of cohorts to have been allocated to the estimated MTD before the trial can stop / to allow the trial to stop, we might now count cohorts on doses near the estimated MTD as counting towards that total. This is set on the Design &gt; Stopping Criteria tab. In considering which doses are near, the same logic as on the Design &gt; Allocation Rule tab regarding Dose Strength or Ratio of Dose Strength will be used.\n\nIf Dose Strength is used, then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength. For example, by +/- 12.5 mg:\n\n\n\n\n\n\nFigure 7\n\n\n\nIf ratio of Dose Strength is used then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength ratio. For example, by +/- 10%:\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nNote that with Fine Grain dosing, if a band is specified for a dose to count as cleared, then the maximum cleared dose will be the maximum dose within that band, and if incrementing relative to the Maximum cleared dose, then the maximum permitted increment will be relative to the maximum dose within the cleared band.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-oe",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-oe",
    "title": "CRM",
    "section": "Open Enrollment",
    "text": "Open Enrollment\nOpen enrollment (Broglio et al. 2015) can be used instead of cohort enrollment. Cohort enrolment enrolls a fixed number of subjects to a given dose, then waits until their treatment and follow up is complete (so their final status – whether they suffered a DLT (Dose Limiting Toxicity) or not – is known) before deciding on the next dose to allocate to and then recruiting the next cohort. This likely means that subjects become available for inclusion in the trial, but have to be turned away as the trial waits for the current cohort to complete. Open enrollment attempts to address this by allowing subjects to be enrolled whilst the current “cohort” is completing. However, this may come with some risk – more than a cohort’s worth of subjects may now be exposed to a new dose before we have any estimate of its DLT rate. To allow this risk to be managed, open enrollment introduces two new concepts:\n\nWhen allocating to an uncleared dose, a cap can be set on the number of subjects that can be allocated to that dose who have not yet got their final results (“OE cap 1”). For example, if this number is set to 3 (to be the same as a common cohort size), after 3 subjects have been recruited and allocated to the current dose, no more subjects will be allocated to this dose until at least one of these subjects has completed. Until then, potential subjects that become available will be turned away unless backfilling is enabled (see next point below). But unlike cohort enrolment, as soon as the first of the subjects on the dose completes, a subject that comes available could now be allocated to the dose, depending on further rules explained below (frontfilling) – unless of course that subject’s result has changed the estimated MTD. Note that the trial won’t escalate beyond the current dose until the required number of subjects to clear the dose have completed. By default, the trial won’t allocate more than the number of subjects required to clear the dose until the dose is cleared, meaning if 3 subjects are required to clear a dose and 3 subjects have been allocated to this dose, even when 1 or 2 of these subjects have their final results and a new subjects is enrolled, they won’t be allocated to this dose. If this is regarded as over cautious, it can be modified by enabling frontfilling, allowing 3 subjects without final results simultaneously. In the above example, this would mean we could place a fourth subject on the dose when the result of the first subject has come in and a fifth subject as soon as the result of the second subject has come in.\nWhen the cap on the number of subjects without final results on an uncleared dose has been reached (“OE cap 1”) new potential subjects will be turned away, unless backfilling is enabled. Enabling backfilling allows these subjects to instead be included, allocating them to a lower dose that has already been cleared. Whilst such an allocation may not contribute as much to identifying the MTD as allocating to the current dose would, it can still contribute by:\n\nIncreasing the information on the next lower dose can inform the estimate of toxicity on the current dose through the Bayesian logistic model.\nProviding additional information on a dose that it may be necessary to de-escalate too if the current dose turns out to be too toxic.\n\nIt can also contribute information on other endpoints (such as efficacy). Once backfilling has been enabled, it is also possible to enable frontfilling. For more information on backfilling and frontfilling, see this section.\n\nAssume at a given point in time we want to allocate a subject to a specific dose, denoted by “candidate dose”. FACTS allows 3 different caps to be specified on how many subjects who have not yet got their final results (i.e. are not yet complete) can be allocated to this candidate dose:\n\n\n\n\n\n\nFigure 9\n\n\n\n\nMaximum subjects without final results if dose is uncleared: As described early in this section, we encounter this cap during escalation when the candidate dose is not yet cleared. This cap takes into account subjects not yet complete on the candidate dose and any higher dose (“OE cap 1”).\nMax subjects without final results if dose is cleared and below MTD: We encounter this cap when the candidate dose is cleared and below the estimated MTD (which can happen if the estimated MTD is beyond the range of available doses, when backfilling, or when allocating during the efficacy phase of a toxicity plus efficacy trial). This cap takes into account subject not yet complete on the candidate dose and any lower doses (“OE cap 2”).\nMax subjects without final results if dose is cleared and at MTD: We encounter this cap when the candidate dose is cleared and the current model estimated MTD (which can happen when after clearing a dose we decide not to escalate, or after de-escalating) (“OE cap 3”).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-bf",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-bf",
    "title": "CRM",
    "section": "Backfilling and Frontfilling",
    "text": "Backfilling and Frontfilling\nAs described in the preceding section, Backfilling is the allocation of subjects to a lower dose when, due to restrictions, it is not possible to allocate a subject who comes available to the current dose (Dehbi, O’Quigley, and Iasonos 2021). FACTS provides a number of options to configure how backfilling behaves. Backfilling can be enabled in the Study &gt; Study Info tab. The total sample size can be divided between the subjects allocated as part of conventional dose escalation and those allocated using backfill. When backfill is enabled, it is important to increase the total sample size and then limit the number that can be allocated using backfill, as subjects allocated using backfilling will not contribute to the escalation and the confirmation of the MTD and it’s usually important to retain sufficient sample size to achieve this aim.\n\n\n\n\n\n\nFigure 10\n\n\n\nWhen enabling backfilling, several options can be specified in the Design &gt; Backfill Allocation tab.\n\n\n\n\n\n\nFigure 11\n\n\n\n\nTwo maximum caps can be specified on the number of subjects that are assigned in the process of backfilling to a given dose:\n\nan overall cap on subjects per dose that cannot be exceeded by backfill, counting also subjects that were assigned to that dose through regular allocation\na cap on the number of subjects per dose that were allocated by backfill, counting only subjects that were assigned to that dose using backfilling.\n\nHow many dose levels below the current dose can be allocated to when backfilling. Backfilling will always be to the highest dose possible (which might be the current dose if frontfilling is enabled, otherwise it will be below the current dose). Allocation to the next highest dose might be limited either by an open enrolment cap if there are already subjects allocated to that dose who have not yet completed, or it might be limited by the backfill caps described above. If allocation to the dose below the current dose is not possible, backfilling will by default look at the dose below that (two levels below the current dose) and so on. Using this option can ensure no backfilling happens to doses that are too far below the current dose.\nThe lowest dose that can be allocated to when backfilling. This option is particularly useful when there is reason to believe doses below a certain level will not be effective.\nWhether frontfilling is allowed – frontfilling allows allocating more subjects to uncleared doses than the number required to clear that dose (see this section).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "href": "documentation/v71/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "title": "CRM",
    "section": "Open Enrollment, Backfilling and Fine Grain Dosing",
    "text": "Open Enrollment, Backfilling and Fine Grain Dosing\nWhen using open enrolment and fine grain dosing, the interval defined on the Design &gt; Allocation Rule tab “Delta in dose strength considered as near +/-“, or “Max ratio of dose strengths considered as near” is crucial: it is used to define the range of doses where subjects allocated to any of them count towards clearing a dose.\n\nIf dose allocation rules are selected to apply to “Dose strength”, the interval is defined “Delta in dose strength considered as near +/-“. Thus, for example, if this is set to 2, then subjects complete on doses with strength in the range 4-8 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 8.\nIf dose allocation rules are selected to apply to “Ratio of strength”, the interval is defined “Max ratio of dose strengths considered as near“. Thus, for example if this is set to 1.5, then subjects complete on doses with strength in the range 4-9 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 9. These ranges also apply when assessing “OE cap 1-3”, and how many subjects have been allocated overall, or via backfilling. In backfilling, FACTS checks each dose strength and the doses in its “near” interval range, at (if frontfilling) or below the current dose, until the first dose strength is found where backfilling can take place.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-studyinfo",
    "href": "documentation/v71/userguides/crm.html#sec-studyinfo",
    "title": "CRM",
    "section": "Study Info",
    "text": "Study Info\nThe Study Info sub-tab provides parameters for specifying:\n\nWhether the trial has an Efficacy endpoint as well as a Toxicity one.\nWhether recruitment is in Cohorts or uses Open Enrolment,\nWhether the trial data is being analyzed as a single population (single group) or two groups (which could be 2 different patient types, or 2 different treatment types).\nThe option to specify that the trial should include an expansion cohort once the MTD has been identified.\nThe option (if using open enrolment) to specify the use of backfill.\n\nIncluding an efficacy endpoint – this allows the trial to include a binary efficacy outcome that is observed at the same time as the toxicity endpoint. Once the MTD has been sufficiently determined further cohorts are allocated to determine the MED (Minimum Effective Dose) as long as that is below the MTD, until the maximum sample size or MED stopping rules have been reached.\nCohort versus Open enrolment: cohort enrolment is the standard way of running a phase 1 trial, a cohort of subjects of pre-determined size are treated at the current dose and the trial pauses until all the subjects in the cohort are complete, then the dose for the next cohort is determined. A phase 1 trial using Open Enrolment recruits ‘all comers’ with a cap on the number of subjects who can be treated but have not yet had a final result. Subjects that arrive whilst this cap has been reached are deemed to be ‘lost’ to the study.\nIf Open Enrolment and Efficacy Endpoint options are being used together, then subjects who arrive who cannot be allocated to the MTD (because the cap number of subjects awaiting a final result has been reached), can be allocated to the MED (as long as that is below the MTD).\nIf the trial is analyzing 2 Groups then a joint statistical model is used with options to constrain the group 2 difference in the intercept term to be +ve or -ve, and options as to whether a common or separate estimates of the slope term are used.\nIf an expansion cohort is included, the this is a single cohort (or one per group, if 2 groups is being used) typically much larger than used during the dose escalation, that is assigned at the end of the study to the target dose. FACTS simulates the results that arise from this cohort and a final analysis.\nIf open enrolment is being used, the further option to use backfill becomes available. The parameters on this tab for backfill, are to specify the maximum number of subjects that can be allocated for escalation, and the maximum that can be allocated in backfill. These two maximums should not total less than the overall “Max subjects” that can be enrolled. If adding backfill to a trial, usually the previous “Max subjects” becomes the “Max study allocation for escalation”, and an additional sample is allowed for backfill and added to the overall Max subjects.\nIf the trial has two groups the backfill maximums are the sum of the subjects in the two groups.\nFor Cohort enrolment, the parameters are:\n\nMaximum Study Size, in cohorts: the maximum number of cohorts the trial can use, though designs can include conditions that cause them to stop earlier.\nIf the trial has two groups, the maximum number of cohorts of the second group.\nCohort size: The number of subjects in each cohort. The maximum sample size for the trial is simply: Max Trial Size (cohorts) * Cohort Size.\nCohort information delay: Normally left at zero, this allows for the trial to be simulated with cohorts recruited more quickly than if waiting for the last cohort to complete before treating the next cohort. This allows the trial to complete in shorter time, but is likely to lead to a larger incidence of toxicities. If the delay is set to 1, the second cohort will be recruited before the results for the first cohort are available, the third cohort will be recruited after the results of the first cohort are available but before the results for the second cohort are available, and so on.\nExecution rate: the time taken to recruit, treat and complete the observation of each cohort (in weeks). The value of this parameter does not affect the behavior of the simulations, but it allows a nominal “duration” of each simulation to be calculated. Unlike other FACTS simulations, this duration is not simulated stochastically, it is simply the number of cohorts * this duration. Its purpose is to give a figure to compare with open enrolment designs of the same trial.\n\n\n\n\n\n\n\nFigure 15: Study Info - Cohort Enrolment\n\n\n\n\n\n\n\n\n\nFigure 16\n\n\n\nIf rather than Cohorts, subjects are recruited using open enrolment, the parameters are:\n\nMax subjects: the maximum number of subjects who can be recruited into the study.\nTime unit – this is a text string that will change the “units” label for time on graphs. This allows data to be more easily entered when the natural time unit is not “weeks”, but “days” or “months”.\nMean recruitment rate: for simulation purposes this is the average number of subjects who will be expected to be available for recruitment during each time period. Recruitment is simulated using a standard Poisson process.\nTime until final result: the time for each subject for their final result to become available. This is simulated as a fixed length of time that is the same for all subjects.\nMaximum subjects without a final result: the cap on the number of subjects on the current dose or a backfill dose (if backfill is enabled) who can be recruited but not yet completed. This is also known as the ‘maximum queue length’. Subjects arriving and available for recruitment while the number of subjects on this dose who have been treated but have not yet completed is at this maximum, are dropped and assumed no longer available for recruitment. Once the current subjects complete the study has to await further new subjects to become available. There are three caps:\n\nMaximum subjects without final results if the dose is uncleared. This allows the design to be cautious when a new dose is used for the first time.\nMaximum subjects without final results if dose is cleared and below MTD. This allows a larger number of subjects without final results to be recruited at backfill doses or at the current escalation dose if backfill to the current escalation dose (“frontfilling”) is enabled, or at the MED in the efficacy phase of a trial that includes an efficacy endpoint.\nMaximum subjects without final results if dose is cleared and at MTD. This allows us to be more cautious if the model thinks all doses are toxic or if we are allocating at the model MTD and don’t want to expose too many subjects.\n\nBackfill – this can be enabled. Backfilling is the allocation of subjects to a dose below the current target dose, if the number of subjects allocated to the current target dose without final results has reached the maximum. Further parameters for backfill are set on the “Backfill” tab under the “Design” tab. On this tab, if backfill is enabled, two sub-maximums can optionally be specified:\n\nthe maximum number of subjects who can be allocated as part of usual allocation for escalation and MTD determination (and MED if efficacy is included in the trial),\nand the maximum number of subjects who can be allocated as part of “backfill”.\n\n\n\n\n\n\n\n\nFigure 17: Study Info - Open Enrolment\n\n\n\nGroups: a trial can be analyzed as a “single group” or as “two groups”. If analyzed as a single group, then all subjects are assumed to be the same and treated the same (except for the difference in the dose strength). If analyzed as two groups this allows either:\n\nThe subjects can be simulated as coming from two similar but distinct groups such as: adults and children, first line or recurrent, having some concomitant treatment or not. The separation into the two groups is based on some property of the subject.\nOr the subjects can be simulated as having been allocated (possibly randomized) to one of two versions of the treatment, with the same rang of dose strengths and differing in some other way such as dosing schedule, treatment duration or combination with an additional treatment. The separation of the subjects into the two groups is under the control of the protocol.\n\nIn either case the same analysis options are available (hence we use the generic term “groups” to describe this feature).\nIf enrolment is by cohort, the there are two separate “maximum study sizes” in cohorts – one for each group.\nThe Group 2 recruitment, while it overlaps in time with the Group 1 recruitment, is simulated as being in lock-step and the recruitment of the cohort in each group is concurrent and analyzed when both are complete. In the ‘cohorts.csv’ files that are output, the cohort numbers indicates which cohorts were concurrent. The options are:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the first group1 cohort has been allocated the specified dose (if cohorts can be accrued before the cohort before has completed, the group 2 is accrued too – it does not wait until the group 1 cohort completes unless the next group 1 also waits).\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the specified number of subjects had been recruited into group 1.\n\nIf enrolment is open then there are options similar to the cohort enrolment to control when enrolment into group 2 starts:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 subject can be recruited after the first group1 subject has been allocated the specified dose.\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 subject can recruited after the specified number of subjects had been recruited into group 1.\n\nBut in addition the user specifies the maximum number of subjects in Group 2 and how the recruitment is to be simulated:\n\nWith the group membership a property of the subject – along with a mean recruitment rate for group 2.\nWith subjects randomized between the two groups (the randomization is fixed at 1:1).\n\nThe user specifies the three “Maximum subjects without final results …” for the second group.\nIf backfill is enabled, the backfill totals apply to the total of the subjects on both groups.\n\n\n\n\n\n\nFigure 18: Study info - 2 group options with open enrolment\n\n\n\nEnable Final Expansion Cohort: if this is enabled a final cohort of specified size will be allocated the dose selected as MTD at the end of the N-CRM phase of the study:\n\nIf the study includes a control arm, the number of subjects in this expansion cohort to be allocated to control is also specified.\nIf the study has two groups, two separate expansion cohorts will be allocated, their sizes are set separately.\nIf the study includes observing efficacy then the target dose can be changed from MTD to MED or OSD.\n\n\n\n\n\n\n\nFigure 19\n\n\n\nSimulating an additional efficacy outcome is simply specified by checking the “include efficacy” checkbox.\n\n\n\n\n\n\nFigure 20: Study tab with “Include efficacy” checked\n\n\n\nSimulating an efficacy endpoint can be combined with all the other features (two groups, open enrolment, backfilling) already discussed, as well as with ordinal toxicity and fine grain dosing that are described below.\nCurrently there are two significant limitations to the simulation of an efficacy endpoint:\n\nThe endpoint is assumed to be dichotomous.\nThe endpoint is assumed to be observable at the same time as toxicity.\n\nWe hope to lift these restrictions in a later version of FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity",
    "href": "documentation/v71/userguides/crm.html#toxicity",
    "title": "CRM",
    "section": "Toxicity",
    "text": "Toxicity\nThe Toxicity sub-tab provides parameters for specifying:\n\nWhether toxicity should be simulated as dichotomous or ordinal. If simulated as Ordinal, toxicity can be simulated as a 1-3 scale or 1-4 scale. In both cases ‘1’ is “no-toxicity”, 2 is “mild toxicity” and 3 is the toxicity of interest (from the point of view of defining the MTD, target toxicity band and Overdose Control. If a four category scale is selected, 4 is “sever toxicity” or death. Choosing whether to model the ordinal response is a separate option and is present on the Design &gt; Toxicity Response tab.\nType of target: this allows the user to specify whether the dose selection targets “the dose who’s estimated toxicity rate is closest to a specified target rate” or “the dose with the highest posterior probability of having a toxicity rate in a target band”. The former is the target rule used in the original CRM papers ((O’Quigley, Shen, and Gamst 1999), (deMoor et al. 1996), and the latter rule was introduced in (Neuenschwander, Branson, and Gsponer 2008).\nToxicity target (only displayed if the type of target is “a single dose”): this allows the target toxicity rate to be specified and whether the target dose is the one nearest, the one nearest but with a lower rate or the one nearest but with a higher rate.\nTarget: this panel allows the target toxicity bands to be specified along with overdose control limits. The panel is displayed and enabled even if the target type is “a single dose” to allow overdose control limits to be specified.\nType of Target: controls the selection of the dose for the next cohort – this can be to target a single dose (to replicate the original CRM behavior, see this section) or to target the dose with the highest probability that its toxicity rate lies in a target band.\n\n\n\n\n\n\n\nFigure 21: Toxicity tab targeting a toxicity band\n\n\n\n\nTargeting a Toxicity Interval\nTargeting a toxicity band or interval is an innovation introduced with the N-CRM design, unlike other CRM designs that select the dose that is expected to have a toxicity response closest to the desired tolerated limit, the N-CRM selects the dose that has the highest posterior probability of having a toxicity rate in a target toxicity band. This has the advantage of a) having a clearer probability statement and b) having in addition probability statements about the probability of under and overdosing (the toxicity rate being below or above the target toxicity band).\n\nThe uncertainty in the estimate of toxicity at each dose is expressed by calculating the posterior distribution of the estimate of the rate of toxicity at each dose and calculating the proportion of that distribution that falls in to each of 4 bands of toxicity: ‘Under-dosing’ (toxicity so low that it is likely that a higher dose could be used), ‘Target’ toxicity (we want to select doses whose toxicity rate is most likely to be in this band), ‘Excess’ toxicity (toxicity higher than desired) and ‘Unacceptable’ toxicity.\n\nUnder-dosing: this band always starts at 0.0; the user specifies the upper bound.\nTarget band: this band always starts at the upper-bound of the under-dosing band; the user specifies the upper bound.\nExcess toxicity: this band always starts at the upper-bound of the target band; the user specifies the upper bound.\nUnacceptable toxicity: this band always starts at the upper-bound of the target band, with an upper bound of 1.0. The graph shows the width of the different bands using a simple, fixed, example posterior probability distribution of a toxicity rate.\n\nIntervals are relative to control: if a control arm is included in the study, then toxicity bands can be defined as the difference in toxicity rate relative to control. Negative differences (a lower toxicity rate than control) are always treated as under-dosing. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\nLimit max excess/unacceptable toxicity: the ‘overdose control’ in terms of a maximum allowed posterior probability that a dose’s toxicity rate lies in either the ‘Excess’ or the ‘Unacceptable’ toxicity bands. Any dose with a posterior probability of having a toxicity rate in either of these two bands that is higher than the specified limit, cannot be selected for allocation to the next cohort, nor selected as MTD at the end of the study (this gives rise to slightly different results compared to those in (Neuenschwander, Branson, and Gsponer 2008) where the overdose control was not applied to final selection).\n\nIt is possible to have the overdose control limit vary with the number of cohorts allocated. In particular this can be used to reduce the overdose limit as the number of cohorts (and the amount of information) grows. For example for a particular prior and final level of overdose control, it may be that initial escalation is excessively constrained, one way to allow early escalation in this setting is to use these parameters to allow an higher initial overdose control limit and gradually reduce it over time to the final desired limit. Tuning the parameters will require some iteration and simulation. A varying limit is specified by the specifying amount to change the limit by per cohort and the final limit. The amount to change by is always entered as a number in the range (0,1), whether this is an increment or decrement depends on whether the target limit is greater or less than the initial limit. Leaving the change in limit at its default of 0 means the limit does not vary.\nLimit max unacceptable toxicity: as for the previous parameter, but here the overdose control is only in terms of the posterior probability that a dose’s toxicity rate lies in the ‘Unacceptable’ toxicity band.\nAs or the limit on excess/unacceptable toxicity it is possible to have the overdose control limit vary with the number of cohorts, see the description above.\n\n\n\nTargeting a single dose\nIt is possible to use the N-CRM design engine with a conventional CRM allocation strategy - to “allocate to the nearest / highest dose below the maximum tolerated toxicity”; this allows conventional CRM design to be simulated with some of the additional features of N-CRM:\n\nOverdose control\nEstimate both parameters of the 2 parameter logistic\nThe “Recommender” to analyze a specific data set.\n\nThe target is calculated by:\n\nIn the MCMC sampling loop finding the dose that meets the target criteria, a doses probability of being the target is then the proportion of times that dose meets the target criteria across the MCMC sampling.\nRather than selecting the dose with the highest probability, the dose at the 50% quantile is used. The cumulative probability of being the target is calculated over the doses in ascending dose strength, and the dose when the cumulative probability passes 50% is selected. This addresses some problems that can arise when very little data is available: that the dose with the highest probability is at one end of the dose range, but that probability is not that high, or that doses are not evenly spaced and a dose close to both its immediate neighbors may never have greater probability than both of them.\n\nSetting the Type of Target option to Target a single dose, modifies the tab thus:\n\n\n\n\n\n\nFigure 22: N-CRM, targeting a single dose, not a toxicity band\n\n\n\nWhen targeting a single dose FACTS allows the user to specify:\n\nThe target toxicity rate\nWhether to allocate to the dose with the mean estimate of its toxicity rate nearest the target, highest dose with a mean estimate of its toxicity rate below the target or lowest dose with a mean estimate of its toxicity rate above the target.\nAn option to use the toxicity rate relative to control, rather than the default of the absolute toxicity rate. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\n\nThe definition of the boundaries of the toxicity bands is still included in order to allow the specification of overdose control limits. These are calculated and applied in exactly the same way as when targeting a toxicity interval.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#efficacy",
    "href": "documentation/v71/userguides/crm.html#efficacy",
    "title": "CRM",
    "section": "Efficacy",
    "text": "Efficacy\nIf include efficacy has been checked on the Study Info tab, a simple additional input page is included:\n\n\n\n\n\n\nFigure 23: The Efficacy tab\n\n\n\nIf simulating and modelling an efficacy endpoint is included there are two items to be specified on this tab:\n\nWhether a subject experiencing a toxicity can also count towards efficacy or not. If unchecked patients outcomes are simply sampled separately and a patient can both have a toxicity and an efficacy response. If checked, a patient’s toxicity outcome is sampled first, and only if there is no toxicity is an efficacy outcome sampled.\nThe efficacy target – this consists of:\n\nThe target efficacy rate required for the Minimum Efficacy Dose (MED).\nWhether the target dose is the nearest dose to the MED rate, the lowest dose above the MED rate or the highest dose below the MED rate.\nIf a control arm has been included, whether the target rate is absolute or relative to the observed rate on the control arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#treatment-arms",
    "href": "documentation/v71/userguides/crm.html#treatment-arms",
    "title": "CRM",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nOn this tab the number of treatment arms (doses) available to the study is specified. The user can either define a set of specific doses that can be used or a continuous dose range with some granularity.\nSelecting Explicit Doses allows the user to specify the specific doses that can be used on the trial:\n\nA single new dose or multiple doses can be added either by clicking “Add” or “Generate”. Initially each dose is defined by a simple integer name and level. The dose levels and dose names can then be edited on by clicking on them and entering the desired value. The dose level can also be set later on in the Design &gt; Toxicity Response tab.\nThere is also the option to include a control arm. Including a control arm allows the toxicity rate to be relative to the control arm.\n\n\n\n\n\n\n\nFigure 24: The Treatment Arms tab specifying explicit doses\n\n\n\n\nFinely Spaced Doses\nSelecting Finely Spaced Doses allows the user to specify the dose range that can be used on the trial:\n\nThe minimum and maximum dose strength to be used\nThe ‘granularity’ of the actual dose used, either as a fixed delta (Fixed spacing) or a dose ratio (the ratio specified must be greater than 1) (Ratio spacing).\nThe number of ‘bins’ or ‘doses for which to report’ – this is because FACTS will still produce summary statistics in columns, many with a “column per dose” – it is possible to use more doses than it is practical to report on (and a limit in MS Windows of 32K pixels for the width of a table means that the GUI can only display simulation results for a maximum of ~40 doses). However this limitation is only for reporting summary statistics; the dose strengths modeled and allocated in the simulations are unaffected.\n\nSelecting ‘Finely Spaced Doses’ will also affect how some of the other parameters are specified in the FACTS GUI.\n\n\n\n\n\n\nFigure 25: The Treatment Arms tab, specifying a finely spaced dose range",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-variants",
    "href": "documentation/v71/userguides/crm.html#sec-variants",
    "title": "CRM",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of cohorts).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with different maximum numbers of cohorts.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Max Cohorts” for each variant.\nThese will then appear on the simulations tab.\nIf open enrolment is being used, then the enrolment cap is specified by the number of subjects.\nIf there are two groups then separate caps are specified for each group.\n\n\n\n\n\n\nFigure 26: The Variants tab, specifying 5 variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#explicitly-defined",
    "href": "documentation/v71/userguides/crm.html#explicitly-defined",
    "title": "CRM",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nToxicity profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 28. The user enters the toxicity rate to simulate at each dose into the Probability of Toxicity column of the table, and the graphical representation of these toxicity values updates accordingly.\nThis form of toxicity profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter toxicity rates for all of them. When using “finely spaced” doses the toxicity rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 27: Explicitly defined toxicity - binary endpoint\n\n\n\nIf the design is using ordinal toxicity, the toxicity response rates can be specified either:\n\nOn the “Toxicity” tab by specifying the category 3 or greater toxicity rate at each dose and then two offsets – one for the category 2 or greater rate and one for the category 4 rate.\nOn the “Ordinal Toxicity” tab by separately specifying the toxicity rates for each category of toxicity at each dose.\n\nSpecifying offsets: to ensure that the specified category 3 rate plus the category 2 offset doesn’t sum to more than 1, or the category 3 rate plus the -ve category 4 offset sum to less than 0, the offsets are applied to the logit of the category 3 toxicity rate.\nThus for the category 2+ rate:\n\\[\nln(\\frac{p_{2+}}{1-p_{2+}}) = ln(\\frac{p_{3}}{1-p_{3}} + \\Delta_2)\n\\]\nwhere:\n\n\\(p_{2+}\\) is the probability of observing a category 2 or greater toxicity at a dose\n\\(p_3\\) is the probability of observing a category 3 or greater toxicity at a dose\n\\(\\Delta_2\\) is the difference in the log odds between the two probabilities\n\nThe offset is defined at the lowest dose and highest dose and then varied linearly with dose strength at the intermediate doses. The plot of the curve can either use Pr(Tox) or Log-odds(Tox) as the y-axis and dose strength or log(dose strength) as the x-axis. A graph is displayed of the toxicity rates that have been entered, and the category 2+ and category 4 toxicity rates if applicable. This graph, as with all graphs in the application, may be copied to the clipboard or to a file using the “right-click” menu.\n\n\n\n\n\n\nFigure 28: Virtual Subject Response – Explicitly-Defined – ordinal endpoint\n\n\n\n\nExplicitly defined – Ordinal Toxicity\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 29: Virtual Subject Response – Explicitly-Defined: Ordinal Toxicity tab\n\n\n\n\n\nExplicitly defined toxicity – when simulating 2 groups\nIf simulating toxicity as a binary outcome, when simulating 2 groups, the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group.\n\n\n\n\n\n\nFigure 30: Explicitly defined toxicity - 2 groups\n\n\n\nIf simulating 2 groups and ordinal toxicity, then on the explicitly defined tab once again the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group. As for a single group the Category 2 toxicity and Category 4 (if using) toxicity rates are defined by defining log odds offsets at the lowest and highest dose. Specification is limited to a single set of offsets that are applied to both groups.\n\n\n\n\n\n\nFigure 31: Explicitly defined toxicity - 2 groups and ordinal offsets\n\n\n\nAs in the single group case in addition to the Category 3 toxicity rates that are editable, columns showing the Pr(Tox 2+) and Pr(Tox 4) are shown, but these are not editable and derived from the Pr(Tox) rates and the offsets that have been specified. The ordinal toxicity rates are only shown for group 1.\n\n\nExplicitly defined – Ordinal Toxicity with 2 groups\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 32: Explicitly Defined, Ordinal Toxicity with 2 Groups\n\n\n\n\n\nEfficacy response profiles\nEntering efficacy response profiles is very similar to entering toxicity profiles. FACTS will construct scenarios to simulate of every combination of toxicity and efficacy response profiles.\n\n\nExplicitly Defined – Efficacy\nEfficacy profiles may be added, deleted, and renamed just like toxicity profiles. The user enters the efficacy rate to simulate at each dose into the Probability of Efficacy column of the table, and the graphical representation of these efficacy values updates accordingly.\nAs with toxicity this form of efficacy profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter efficacy rates for all of them. When using “finely spaced” doses the efficacy rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 33: Efficacy virtual subject response, explicitly defined\n\n\n\n\n\nExplicitly Defined – Efficacy with two groups\nIf the design included 2 groups, when explicitly defining an efficacy response profile, there is simply a second column of efficacy response rates to enter:\n\n\n\n\n\n\nFigure 34: Explicity defined efficacy response profile with 2 groups",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-parametric",
    "href": "documentation/v71/userguides/crm.html#sec-parametric",
    "title": "CRM",
    "section": "Parametric",
    "text": "Parametric\nToxicity scenarios may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen. The user selects the model to use to determine the toxicity rate to simulate at each dose, and specifies the values of the model’s parameters. The graphical representation of these toxicity values updates accordingly.\nThe graph may be copied using the context menu functionality described in the previous section.\nFour models are available:\n\nLogistic: the probability of toxicity at dose x is given by: \\(P_x=\\frac{1}{1+e^{-s(x-x_{50})}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with linear effective doses \\(\\hat{x}=x-x_{ref}\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*(x_{ref}-x_{50})\\)\nEmax: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{x}{x+x_{50}}\\) with user specified parameter \\(x_{50}\\).\nLog Logistic: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{1}{1+e^{-s(ln(x)-ln(x_{50}))}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with log effective doses \\(\\hat{x}=ln(\\frac{x}{x_{ref}})\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*ln(\\frac{x_{ref}}{x_{50}})\\)\nPiecewise linear: with the probability of toxicity specified at a series of knots, with the probability linearly interpolated between knots.\n\n\n\n\n\n\n\nFigure 35: Virtual Subject Response - Parametric Toxicity tab\n\n\n\nIf Ordinal toxicity is being simulated then the category 2 and greater toxicity rates and category 4 toxicity rates are specified using the logit offset methods as on the Explicitly-Defined &gt; Toxicity tab.\n\n\n\n\n\n\nFigure 36: Virtual Subject Response - Parametric Toxicity tab with Ordinal toxicity\n\n\n\nIf Ordinal Toxicity and 2 groups are being simulated then both the Cat 2+ and Cat 4 toxicities and the Group 2 toxicities are defined using the logit offset methods.\n\n\n\n\n\n\nFigure 37: Parametric definition of ordinal toxicity response with 2 groups\n\n\n\n\nParametric efficacy response\nParametric efficacy response profiles function exactly like toxicity profiles, with the same parametric models to choose from and if 2 groups are present the response of the second group is again defined by 2 log-odds offsets, one at the lowest dose and one at the highest.\n\n\n\n\n\n\nFigure 38: Parametric efficacy response profile",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#external",
    "href": "documentation/v71/userguides/crm.html#external",
    "title": "CRM",
    "section": "External",
    "text": "External\nSubject response data may be simulated from a PK-PD model in place of, or in addition to, choosing a response model from FACTS. The importing of subject response data (which must be in the form specified in the System Requirements Document) may be done from the External Files sub-tab depicted below (Figure 39).\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id\nDose index (1, 2, 3,… if a Control is to be included it should be index 0) this is not the user settable dose name or dose level\nToxicity (0,1)\nEfficacy (0. 1) even if efficacy not being simulated this value must be present\nGroup (*1, 2) only required if groups are being simulated\n\nThe GUI requires that the file name has a “.dat” suffix.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.\nTo import an external file, the user must first add a scenario to the table. After adding a scenario, the user must click “Browse” to locate the externally simulated data via a standard file browser dialog.\n\n\n\n\n\n\nFigure 39: External Data",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#x-hats",
    "href": "documentation/v71/userguides/crm.html#x-hats",
    "title": "CRM",
    "section": "X-Hats",
    "text": "X-Hats\nOn this tab the user specifies the reference dose \\(d^*\\) for use in calculating the adjusted dose value (the “x-hat” values). The default value to use is Median dose is reference, this uses the median of the dose range for the reference dose, minimizing the correlation in the sampled values of \\(\\alpha\\) and \\(\\beta\\). Note though that when allocation is restricted to explicit doses it is also recommended that the value of the reference dose is not the same as an actual dose that can be used (at this dose \\(\\hat{x}\\) will be 0 and the data on this dose can have undue weight on the estimate of \\(\\alpha\\)).\nDifferent reference doses can easily be used however – between the two doses thought most likely to be MTD, just below the lowest dose, just above the highest dose. The bi-variate Normal prior for \\((\\alpha, ln(\\beta))\\) will need to be recalibrated to take the change into account.\nX-Hats are log(dose strength) allows the user to select between:\n\nlinear effective dose \\(\\hat{x}_j = d_j - d^*\\)\n\\(log(\\hat{x}_j) = ln(\\frac{d_j}{d^*})\\)\n\nIf you have entered linear dose strengths for the doses (1, 2, 3, 4, … or 100, 150, 200, 250, …) then use the linear effective dose. If however the dose strengths that have been entered are non-linear (12.5, 25, 50, 100, …) but expected to be roughly linear in effect, then use the log of the dose ratio.\n\n\n\n\n\n\nFigure 40: Specifying the dose transformation - the “x-hats”.\n\n\n\n\nThe Pro’s & Con’s of using the median dose as the reference dose\nThe reason the median dose is recommended as the reference is that this minimizes the correlation in the fit of \\(\\alpha\\) and \\(ln(\\beta)\\), the parameters of the BLRM, and it maximises the flexibility of the fit of the model over the dose range.\nHowever care needs to be taken that the prior on \\(\\alpha\\) is not more restrictive than that on \\(ln(\\beta)\\) in order to avoid a phenomena observed when preparing tutorials: observing “no toxicities” below the reference dose resulted in a model with increased probability of toxicity above the reference dose compared to observing a toxicity below the reference dose. For a given value of \\(\\alpha\\), higher values of \\(ln(\\beta)\\) correspond to lower toxicity below the reference dose – as the \\(\\hat{x}\\)̂ values are -ve below the reference dose. The fitted curve thus “pivots” about the value of \\(\\alpha\\) at the reference dose.\nThere are two solutions to this:\n\nmove the reference dose, which involves a choice between two options\n\nmoving it to the first dose or below (normally allowing a relatively constrained prior around a low value for \\(\\alpha\\)),\nor to the highest dose or above (with a relatively uninformative prior).\n\nWe have seen both solutions perform well against the chosen scenarios – but the choice needs checking and refining with a full range of scenarios that represent the full uncertainty in the true response.\nor modify the priors on \\(\\alpha\\) and \\(ln(\\beta)\\) making the prior on \\(\\alpha\\) less informative (in particular increase the probability of low values) and make the prior on \\(ln(\\beta)\\) more informative (in particular lower the probability of high values less). Because the prior distribution on \\(\\beta\\), is on \\(ln(\\beta)\\), it is easy to make large values of \\(\\beta\\) more probable than intended.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-response",
    "href": "documentation/v71/userguides/crm.html#toxicity-response",
    "title": "CRM",
    "section": "Toxicity Response",
    "text": "Toxicity Response\nThe parameters that can be specified on this page are:\n\nThe parameters of the bivariate Normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\). Specifying the mean and standard deviation of \\(\\alpha\\) \\((\\mu_{\\alpha}, \\sigma_{\\alpha})\\), and \\(ln(\\beta)\\) \\((\\mu_{ln(\\beta)},\\sigma_{ln(\\beta)})\\) and the correlation coefficient \\(\\rho\\).\n\nIf ordinal toxicity is being simulated, it is possible to model the ordinal toxicity, specifying the mean and standard deviation of \\(\\alpha_2\\) and \\(\\alpha_4\\). These priors are separate from the \\(\\alpha_3\\) and \\(ln(\\beta)\\) prior, there is no correlation term in the prior. There is the constraint in the model that \\(\\alpha_2 &gt; \\alpha_3 &gt; \\alpha_4\\).\nUse fixed Alpha: the value of Alpha can be fixed to allow the N-CRM model to behave like the traditional CRM models. [Where \\(\\alpha\\) was set to 3 and the reference dose is set above the top of the available dose range]\n\n\nRather than entering the priors directly, they can be derived based on indirect prior information or beliefs, see ‘Deriving the Prior’ below.\n\nThe Minimum and Maximum rates that the model is to be fitted too. The model fits the range \\((0,1)\\), asymptotically approaching each limit as the adjusted dose value tends to \\(-\\infty\\) or \\(+\\infty\\). By specifying an alternative minimum and maximum, inside the range \\((0,1)\\), the user can have the model scaled to fit data to fit event rates where the asymptotic rates are not \\(0\\) or \\(1\\). For instance if the event being observed has a non-zero background rate (probability of being observed in placebo treated subjects), then the model may fit better if the minimum is set to the lower limit of this expected rate. Similarly if, even at the most toxic dose the event being observed is only expected to effect a proportion of subjects, the model may fit better if the maximum is set to the upper limit of this expected rate.\nIf a control arm is present, the user can specify to have this modelled separately, and if so the user specifies the parameters for a prior Beta distribution – in terms of numbers of prior observations on control of subjects with and without a toxicity.\nGroup 2 priors: if a second ‘Group’ is being simulated – whether this is a subset of subjects, or a modified treatment that subjects can be randomized to, then the BLRM is jointly fitted to the responses for both groups, with group 2 having offsets \\(a\\) and \\(b\\) from the first group’s \\(\\alpha\\) and \\(\\beta\\). The priors for \\(a\\) and \\(b\\) can be full bivariate Normal or can use constraints such as \\(b = 1\\), or \\(a &gt; 0\\) or \\(a &lt; 0\\).\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\nDeriving the prior\nThe priors of \\(\\alpha\\) and \\(ln(\\beta)\\), can be specified directly or derived in one of four ways. When entered explicitly, the user specifies the parameters of the prior bivariate-normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\): the means, standard deviations and the correlation term \\(\\rho\\).\nAlternatively, the user may click the ‘derive prior’ button and select from:\n\nQuantiles at the lowest and highest dose: (based on the “uninformative prior” given in the paper (Neuenschwander, Branson, and Gsponer 2008), for details see this section) - the user specifies the probability of an unacceptable toxicity at the lowest dose, and the probability of under-dosing at the highest dose (0.1 for both is the default, and 0.05 for both is the value used in the paper). Optionally the probability that toxicity is less than the mid-point of the target toxicity band at the median dose can be specified. (Prior to FACTS 6.5 this third data point was not optional and constrained to be at the reference dose, but this had problems if the reference dose was not the media dose – it might also be the lowest dose for example).\nNote this method does not work so well if the reference dose is outside the dose range.\n\n\n\n\n\n\nFigure 42\n\n\n\nScenarios: the model is fitted to each of the toxicity response scenarios (MLE), the parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\n\n\n\n\n\nFigure 43\n\n\n\nSpecific quantiles: The user selects which doses and toxicity rates to provide an expectation – a prior probability that the toxicity rate on the dose will be the specified rate or less. At least 3 such expectations using at least 2 different doses strengths must be supplied. If a large number of specific quantiles are specified (e.g. reproducing the all quantiles method) the large number of different beta distributions sampled from, with the monotonicity constraint applied, results in losing too much variability. So this should only be used quantiles specified at 2-4 doses.\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\nAll quantiles: the user specifies the prior expected toxicity rate at the 2.5%, 50% and 97% quantiles for each dose. (Only available when using explicitly defined doses, not a continuous dose range). Note that using Create Prior with this option will require the facts file to be saved and for there to be at least one virtual subject response profile.\n\n\n\n\n\n\n\nFigure 45\n\n\n\nIn all cases once prior values have been derived they are displayed along with a graph of 100 sampled curves from the prior. The user can accept the values, change derivation method, or cancel the derivation.\nThe plot of the samples can either be viewed as Pr(Tox) or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\nDerivation of the Prior from Quantiles\nDerivation of the parameters of the bivariate Normal prior for \\(\\alpha\\) and \\(ln(\\beta)\\)) in the Quantiles at lowest and highest dose, Specific quantiles and All quantiles cases:\n\nMinimally informative unimodal Beta distributions are fitted for each of the doses where a prior expectation of a toxicity has been specified. For doses where no prior expectation has been specified, the median expected toxicity rate are derived by assuming that the median expected toxicity is linear in log dose on the logit scale, and again a minimally informative unimodal Beta distribution is fitted with the same median.\nPreviously and following (Neuenschwander, Branson, and Gsponer 2008), the parameters of the bivariate Normal distribution were found using a stochastic fit to the prior expectations of toxicity, minimizing the error in the prior toxicity rates at the 2.5%, 50% and 97.5% quantiles. This is still used in the All quantiles and Legacy prior cases. However experience with this method with the standard priors (previously called “uninformative”) showed that it yielded priors with too little uncertainty in the \\(ln(\\beta)\\)) and too high a value for the correlation parameter for many cases and certainly for the prior to be called “uninformative”.\nConsequently, in the Quantiles at lowest and highest dose and Specific quantiles cases, the prior is now derived by sampling from the minimally informative unimodal Beta distributions, and fitting the model to each set of sampled toxicity rates. The parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\nIf a control arm has been included, it may be included in the model, or modelled separately using a beta-binomial model, the user specifies the prior values for the Beta distribution.\n\n\nDerivation of the prior from the scenarios\nIn this screenshot, priors have been derived from the scenarios:\n\n\n\n\n\n\nFigure 46: Design - Toxicity Response sub-tab (after prior derivation)\n\n\n\nClearly in this example the scenarios have a very high correlation between the toxicity at the reference dose (\\(\\alpha\\)) and the log gradient (\\(ln(\\beta)\\)) giving a high value for the correlation in the prior (\\(\\rho\\)). As there are very few scenarios, and they didn’t include extreme cases, the SDs of the priors of parameters will be underestimated.\nSo In this instance we might round the prior means to 2 decimal places, double the SD of \\(\\alpha\\), slightly increase the SD of \\(ln(\\beta)\\) and reduce the correlation to 0.5.\nHowever, it is much better to use Drive from Scenarios after entering a large number of varied and credible scenarios. Indeed is such a collection of scenarios exists, deriving the prior from the scenarios is the simplest approach and often very effective. Only if the performance in the simulations in some scenarios does the prior need re-visiting (usually to slightly increase the \\(ln(\\beta)\\) SD and/or reduce the correlation).\nWe strongly recommend checking the performance of the prior across a wide range of scenarios, and of entering the reported derived values of the fitted prior as an explicit prior and then manually modifying them in the light of the model’s performance on the various scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-legacy-prior-methods",
    "href": "documentation/v71/userguides/crm.html#sec-legacy-prior-methods",
    "title": "CRM",
    "section": "Legacy prior methods",
    "text": "Legacy prior methods\nIn old versions N-CRM (pre-FACTS 4.0) , the design could be left at the stage where the method of deriving the prior had been specified but the derivation postponed to the simulation stage. We now require that the derivation be performed first, this\n\nEnables the actual prior that results from the derivation to be inspected\nMakes simulation and recommendation faster, as the derivation is not repeated every time the design engine starts.\n\nWhen opening a FACTS N-CRM file created using a pre-FACTS 4.0 version of FACTS, to continue to use the original prior, select “Derive prior”, and the derive prior window will display the old prior and allow an explicit prior to be derived from it using the now deprecated methods in the older versions of FACTS N-CRM.\n\n\n\n\n\n\nFigure 47: Derivation from a legacy prior",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-efficacy-response",
    "href": "documentation/v71/userguides/crm.html#sec-efficacy-response",
    "title": "CRM",
    "section": "Efficacy Response",
    "text": "Efficacy Response\nThe Efficacy Response tab is displayed if an efficacy endpoint is included in the trial.\nThe Efficacy Response is specified separately from the Toxicity Response. The Toxicity and Efficacy models are completely separate except for the x-hat values for the transformed dose strengths, where a common set of values is used for both models. Note that the use of a common set of x-hats for both endpoints is a difference from FACTS bCRM, that means it may not be possible to exactly replicate a bCRM design in FACTS N-CRM, however we think that the additional features and options in FACTS N-CRM will make it possible to create an overall superior design in FACTS N-CRM.\nThe features available for specifying the Efficacy Response model are the same as the Toxicity Response model (see above) – with the exception that the Toxicity Response includes an option for modeling ordinal toxicity, there is no corresponding ordinal efficacy option.\n\n\n\n\n\n\nFigure 48\n\n\n\nThe same options for deriving the prior are available as for the Toxicity Response Model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prior-pseudo-subjects",
    "href": "documentation/v71/userguides/crm.html#sec-prior-pseudo-subjects",
    "title": "CRM",
    "section": "Prior Pseudo-subjects",
    "text": "Prior Pseudo-subjects\nThis option allows “prior data” or “pseudo data” to be specified that will be included in every analysis. This is equivalent to the model being fitted with the parameter prior to this pseudo data and the resulting posterior being the new prior, but it is easier and quicker to include the pseudo subject with the real data and do one analysis.\nThe user selects which dose levels at which to include the data and specifies the number of pseudo/prior subjects/observations and the number of toxicities. These are allowed to be fractional, and observations can be at dose levels not being tested in the trial. In each analysis the observed data is augmented with this specified data and the parameters of the toxicity response estimated.\nIf there is an efficacy endpoint as well as toxicity endpoint, pseudo subject data is specified separately for the two endpoints (not surprisingly!). If the data is to be analyzed as “2 groups” pseudo subject data is also specified separately for the two groups.\nThe effect of this data on the prior can be visualized by the “Update Plot” function that estimates the parameters of the toxicity response and plots the curves of 100 samples drawn from the posterior estimates of the parameters of the model.\nThe plot of the samples can either be viewed as Pr(Tox) vs Dose Strength or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\n\n\n\n\nFigure 49: Specifying prior pseudo subjects\n\n\n\n\n\n\n\n\n\n\nExample of effect of pseudo subjects on prior\n\n\n\n\n\nPrior Only\n\n\n\nPrior plus 0.5/0 subjects toxicities on dose 1 and 1/0.5 on dose 8.\n\n\n\nPrior plus 3/0 subjects / toxicities on dose 1 and 3/1.5 on dose 8.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation-rules",
    "href": "documentation/v71/userguides/crm.html#allocation-rules",
    "title": "CRM",
    "section": "Allocation Rules",
    "text": "Allocation Rules\nThe Allocation Rule sub-tab is depicted below (Figure 50); it allows the user to select and set the parameters for the allocation rules.\nThere is an option to mimic the original Neuenschwander paper and rely on just the toxicity model and the overdose control to guide allocation – i.e. allocate to as close to the estimate of the MTD as we can limited only by the overdose control limits. This is “Use only overdose control”. Normally however we think the clinical will want to impose additional allocation rules.\nThe next dose allocated is a combination of the current target dose, and 2 possible maximum allocatable doses:\n\nThe target dose is the dose with the highest probability of being the target: the dose with the greatest estimated probability that its toxicity rate lies in the target toxicity band, or the dose nearest or highest below the MTD depending on the options selected on the Study &gt; Toxicity tab.\nThe highest dose that meets the overdose criteria (if any): the highest dose that does not have a posterior probability that its toxicity rate lies in the excess & unacceptable toxicity bands or unacceptable toxicity bands above the threshold specified on the Study &gt; Toxicity tab.\nThe current cleared dose and how far above that dose can be allocated to as defined by the specified allocation rules (if any).\n\nThe next dose to be allocated to is the lowest of these 3 doses.\nIt is possible to specify a “run-in” phase before this dose escalation phase applies. A run-in phase has a fixed sequence of doses and cohort sizes (typically smaller than the cohort size used in the escalation phase) and lasts up to the first observed toxicity or the end of the sequence of doses.\n\n\n\n\n\n\nFigure 50\n\n\n\nIf the user selects to use an “Initial run-in” then there are 3 run-in types that can be selected from:\n\n“Simple run” (the only type available before FACTS 6.3): cohorts are of a single size and the dose sequence up to the “Run-in cannot go beyond” dose (if specifed) is either:\n\nat every dose starting at the specified “initial dose” if explicit doses have been defined on the the Study &gt; Treatment Arms tab,\nat the dose increment intervals defined in the allocation rules if finely spaced doses have been defined on the Study &gt; Treatment Arm tab.\n\n“Custom run-in” where the user specifies which doses are selected (leaving the “number of subjects” at a dose at 0 mans it is not selected) and at each dose how many subjects are allocated in the cohort at that dose.\nSmall cohort pre-escalation, cohorts are of a single size and the dose sequence follows the dose increment intervals defined in the allocation rules up to the “Run-in cannot go beyond” dose (if specifed).\n\n“Simple run-in” and “Small cohort pre-escalation” the user specifies the “small cohort size” and there is an option to specify a top dose that the “Run-in cannot go beyond”.\nFor all run-in schemes there are options:\n\nEnd run-in on 1st category 2 toxicity: The default for the simulation is to stop the run-in when the first full toxicity is observed, there is an option to instead stop when a lower grade toxicity is observed – a category 2 toxicity. If this option is selected, the simluation of virtual subjects is extended to include the simulation of category 2 tocxicity as well as category 3.\nInclude run-in subjects in overall maximum\nWhen run-in ends expand last cohort to full size\n\nIf this is not set then N-CRM model is applied and the next cohort is allocated as close to the target as possibly, restricted by the overdose restrictions and not allocating any higher than the dose reached in the run-in.\nIf this option is set then last allocated small cohort is treated as the start of a full cohort and the remaining subjects are allocated at the same dose. The N-CRM model is then applied and the next cohort allocated according to the overdose restrictions and allocation rules. [Note it is possible that after observing no toxicities in the run-in or in the expanded cohort, that the overdose control will force a de-escalation in dose, depending on the priors for the model parameters and the overdose control limits]\n\n\nIf simulating 2 groups, and a run-in is specified, group 2 will only use a run-in if on the Study tab the option “Recruit Group 1 and Group 2 together” has been selected. Otherwise only group 1 will use the specified run-in, once group 2 starts it starts with the full cohort (or if open enrollment is being used, the full number of subjects to clear the dose). If using a run-in and recruiting groups 1 and 2 together then both use the same run-in rules, and both will stop on the first toxicity regardless of which group the toxicity occurs in.\nIf used, the Allocation rules have the effect of setting a Highest Allocatable Dose that specifies the highest dose level that can be allocated to by the allocation rules.\n\nAt the start the Highest Allocatable Dose is the user specified Starting dose level.\nA dose is not ‘cleared’ until ‘The minimum cohorts on/near a dose before cleared’ have been allocated to it. If this is set to greater than one, then the specified number of cohorts must have been allocated to the current dose before it is ‘cleared’. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nIf using Open enrolment, this parameter is ’The minimum subjects on a dose before cleared’ and refers to the number of complete subjects.\nThe user can specify that Dose not cleared if proportion toxic on dose &gt; and a maximum level of toxicity that can be tolerated for the current dose to be ‘cleatred’. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe user specifies whether the dose increment rules are to be specified in terms of the number of dose levels that can be incremented or in terms of the ratio by which the dose strength can be increased. If using “dose strength” it is up to the user to ensure that the specified ratio is sufficient to allow all doses to be reached. For example if in the dose range there is a dose of strength 100 and the next dose is of strength 200, then if any dose increment rule is specified that doesn’t allow the dose strength to at least double (using “ratio of dose strength” of less than 2) then the rule will prevent escalation from the 100 dose to the 200 dose.\nThe maximum amount the dose can be incremented can be specified in 3 ways:\n\nSingle value: For all doses, once a dose has been cleared the next ‘highest allocatable dose’ is the dose above the cleared dose by the specified increment – either a number of dose levels or by a proportion of the dose strength.\nBy total number of toxicities: With this rule the maximum dose increase allowed depends on the total number of toxicities that have been observed in the trial so far. The user specifies the maximum increment (in terms of the number of dose levels or the maximum proportional increase in dose strength) when no toxicities, one toxicity, or more than one toxicity has been observed.\n\n\nFigure 9-5 Maximum dose increment varying by number of toxicities observed\n\n\n\n\n\n\nFigure 51: Maximum dose increment varying by number of toxicities observed\n\n\n\n\nBy dose levels: the maximal permitted increment is defined in terms of the number of dose levels or the proportional increase in dose strength that the dose can be increased, in up to 3 bands of dose strength. The user defines the upper and lower doses of the middle increment range, the lower range is then from the lowest up to this band and the upper band is from the top of the middle band to the highest dose. The user then specifies the maximum number of dose levels or the maximum proportional dose strength that can be incremented if the current dose is within each of these bands.\n\n\n\n\n\n\n\nFigure 52: Maximum dose increment varying by dose level\n\n\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently allocated dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\nThe “Fastest Possible Dose Escalation” graph shows the dose allocation permitted by the rules if no toxicities are observed. Or the graph can be changed to show the allocation if a toxicity occurs at a specific dose during the run-in.\nIf there are 2 groups and recruitment to the second group is delayed (either until the first group is complete, has reached a specific dose, or tested a specified number of subjects). Then starting dose for the second dose can be specified as\n\n\n\n\n\n\nFigure 53: Design - Allocation Rule Initial Dose Second Group\n\n\n\n\nAllocation rules when using a fine grained dose range\nWhen a fine grained dose range is being used, the specification of the allocation rules change to accommodate the fact that the trial is no longer stepping up a few pre-defined dose levels. As with the explicit doses, the allocation rules work with the notion of a Currently Permitted Maximum Dose (CPMD).\n\nAt the start the highest allocatable dose is the user specified Starting dose strength.\nA Run-in phase can be specified, this will always begin at the start dose and allocate ‘small cohorts’ following the maximum increment rules (this is different from when there are explicit doses – with explicit doses, the simple run-in simply allocates successive small cohorts to successive doses, ignoring the maximum permitted increment) for a simple run-in or small cohort pre-escalation, or the specified allocation pattern for a custom run-in, until the first toxicity is observed.\nThe degree of increment and specification of what counts as a ‘close’ dose, can be done either in Dose Strength or Ratio of dose strength.\nA dose is not cleared until ’The minimum cohorts on a dose before incrementing’ have been allocated to it. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nThe user can specify that Dose not cleared if ppn toxic on/near dose &gt; and a maximum level of toxicity that can be tolerated before the dose is cleared. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe Max ratio of dose strengths considered as near or Delta in dose strength considered as near defines a margin such that when evaluating whether a dose that has been allocated to counts as “cleared”, then cohorts allocated within that margin all count as being “on” the dose to allow incrementing.\nThe Maximum increment selected by option allows the user to specify varying maximum increments either dependent on the current cleared dose (Dose strength), or on the Total number of toxicities that have been observed, as follows:\n\nSingle value: the amount by which the highest allocatable dose can be above the current cleared doses is constant throughout the trial. (This simple rule can work well when combined with overdose control. The two more complex rules are essentially trying to achieve the same thing as overdose control but are more simplistic and it may be confusing as to whether the allocation rule or overdose control is preventing escalation at any given moment).\nTotal number of toxicities: the amount by which the highest allocatable dose is above the current cleared dose is specified separately for whether zero, one or two or more toxicities have been observed in the entire study.\nDose strength: The user specifies:\n\nthe increment at low doses;\nthe increment at medium doses, along with the upper and lower dose strengths that define the bounds of what constitutes a ‘medium dose’; and\nthe increment at high doses.\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently cleared dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\n\n\n\n\n\n\nFigure 54: Allocation rules with a finely spaced doses",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-backfill-allocation",
    "href": "documentation/v71/userguides/crm.html#sec-backfill-allocation",
    "title": "CRM",
    "section": "Backfill Allocation",
    "text": "Backfill Allocation\nIf the trial is using open enrolment then “Backfill” allocation is an option. Backfilling is enabled on the Study &gt; Study Info tab, where the maximum number of subjects that can be allocated when backfilling is specified.\nBackfilling is the allocation of subjects to a dose below the current dose when the maximum number of subjects on the current dose without final results has been reached.\nThe Backfill Allocation tab allows detailed control of when backfilling can be used. The user specifies:\n\nThe maximum overall number of subjects that can be on a dose for backfilling to be allowed to that dose.\nThe maximum number of subjects that can be allocated to a dose by backfill.\nThe maximum number of dose levels below the current dose that can be used for backfilling (the highest that can be backfilled to will be used)\nThe lowest dose strength that can be backfilled to.\nWhether or not the current escalation dose is a candidate for backfilling as long as the maximum number of subjects in their DLT period (set in the Study &gt; Study Info tab) is not exceeded (also known as “frontfilling”).\nIf frontfilling is enabled, whether these subjects should count towards the backfill allocation cap or the regular study allocation cap (specified in the “Study/Study Info” tab).\n\n\n\n\n\n\n\nFigure 55: Backfill Allocation",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-stopping-criteria",
    "href": "documentation/v71/userguides/crm.html#sec-stopping-criteria",
    "title": "CRM",
    "section": "Stopping Criteria",
    "text": "Stopping Criteria\nThe simulation will always stop when the maximum number of cohorts or subjects has been allocated.\nThe user may also specify that the study may stop early in terms of the amount of information that has been gathered about the target dose. In the N-CRM, the target dose can either be the Maximum Tolerated Dose or the “the allowed dose that has the highest posterior probability of having a toxicity rate in the Target Toxicity Band”; we use the label MTD (Maximum Target toxicity Dose) for this target dose too, for brevity and familiarity from previous CRM methods.\nNote: if overdose limits have been set (see this section), then doses with posterior probabilities of having a toxicity rate in the Excess Toxicity or Unacceptable Toxicity bands that are greater than the specified thresholds, are disallowed for both dose allocation and selection as MTD.\nTo enable early stopping, the user must select “Rules for stopping trial early” there are then 2 rules which if selected are always “AND”ed together, and a further block of rules, the result of which (if specified) are “AND”ed with the first two rules. If more than one rule is selected within the block these may be either logically “AND”ed together or “OR”ed together to give the result of the rules in the block.\nThe two first standalone rules are:\n\nIf a “Required number of cohorts/subjects near MTD” rule is set then the trial will only stop early if at least this number of cohorts or subject has been allocated to the MTD dose or nearby doses. In the box above the “Count as MYD doses differing from MTD by less than or equal” allows how far away a dose can be and for cohorts/subjects on those doses to count towards this total. This can be set to 0 to count only cohorts/subjects on the MTD. It is provided for when fine grain dosing or a large number of explicit doses have been specified. If specified this rule this must always be met along with any other rules set for the trial to stop early.\nIf a “Minimum number of cohorts/subjects accrued” has been set then this specifies a lower limit on the sample size before early stopping is allowed. It is provided to allow a higher number of subjects on the MTD if one of the first doses appears to be the MTD. If specified this rule this must always be met along with any other rules set for the trial to stop early.\n\n\n\n\n\n\n\nFigure 56: Design - Stopping Criteria sub tab\n\n\n\nThe available stopping rules are:\n\nRequired number of cohorts/subjects on/near MTD: Once the specified number of cohorts has been allocated to the dose currently determined to be the MTD, the trial may stop.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\nThe required number of cohorts/subjects can be “near” (rather than “on”) the MTD if a “Count as MTD” interval is defined. This interval is either defined by an interval of “dose strength” using the dose strengths defined on the Study &gt; Treatment Arms tab, or by an interval of “dose strength ratios”. Which is used will correspond to the Selection for “Dose allocation rules apply to” on the Allocation Rules tab. A difference of 0, or factor of 1 can be used to only count cohorts/subjects actually on the selected MTD dose.\nIf open enrolment is being used, there is an option to only pause accrual not stop it when the stopping rules have been met, in case the final results of any subjects that were not complete at the time the stopping rules were met cause the rules to be no longer met (for example by having results that change the dose estimated to be the MTD). If this option is selected then the stopping rules are re-assessed when all the current subjects are complete and the trial resumed if the rules are no longer met. Note this option is not required for cohort enrolment as stopping rules are only evaluated between cohorts.\n\nMinimum cohorts/subjects accrued: this rule ensures that a minimum overall number of cohorts have been tested before the trial is allowed to stop. It makes no sense to use this rule on its own (it would effectively just lower the overall study cap) it should only be used in conjunction with other stopping rules.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\n\nThe Additional Stopping Rules:\n\nRange/Ratio of dose strengths within the credible interval is less than or equal to: This is only enabled if on the Study &gt; Study Info tab, the Type of Target has been set to “Target a single dose”. In this case the credible interval of the dose range that might contain the MTD is calculated, and this option allows the user to specify the width of the credible interval in “dose strength” to consider. How this works is a little counter intuitive: in each MCMC sample the engine determines which dose is the MTD and each doses’ Pr(MTD) is the proportion of the samples it was the MTD. The engine then calculates the minimum range of dose strengths required so that the sum of Pr(MTD) of the doses in the range exceeds 1 – Alpha. (So if a single dose has Pr(MTD) &gt; 1-Alpha, the CI has zero width). If the xhats are log(dose strength) then the minimum range of log(dose strength) is found and exp() of this range returned.\n\nThe stopping rule is met if the width of the CI returned is less than that specified by the user.\nIf fine grain dosing is being used then the width of the target credible interval is defined as a dose strength ratio rather than a number of doses.\n\nStop if adding another DLT free cohort does not alter the MTD: this rule is evaluated by analyzing the existing data supplemented by an additional cohort of a specified size where all the responses are no-toxicity; if this results in no change in selection of MTD then this stopping rule is met. If the study is using Open Enrolment, the user additionally specifies the size of the ‘virtual’ cohort to use.\nProbability of dose being in the target band greater than: in order to stop, the MTD’s posterior probability that its toxicity rate lies in the Target Toxicity band must be at or above the required threshold.\nMaximum Cohorts/Subjects near MTD: this option is supplied for use if the additional stopping rules are being OR’d together. (When they are AND’d together it simply functions the same way as the “Required Cohorts on MTD and stopping will not occur until the higher of the two targets is met).\nJoin condition: if more than one additional stopping rule is selected, whether only any one of them needs to be met for the trial to be able to stop (Join condition = “OR”), of if all of the selected rules need to be met for the trial to be able to stop (Join condition = “AND”).\n\n\nThe study will also be stopped if there are no allowed doses by the overdose rules. However this can occur early in the study if a toxicity is observed in the first or second cohort. It is likely that in practice the clinical team would override the design stopping the study. Whilst it is difficult to fully represent the team’s decision making, a simple rule is included that is intended to approximate it:\n\nMinimum toxicities required before stopping: This allows a requirement to be specified to observe a ‘minimum number of toxicities’ before the trial stops. If no doses are allowed by the overdosing rules, cohorts are assigned to the lowest dose until the minimum number of toxicities are observed, the stopping rules are met, or doses become allowable again after the model is updated after seeing no toxicities.\n\n\n\n\n\n\n\nFigure 57: Stopping Criteria tab with open enrolment and finely spaced doses\n\n\n\nIf an additional efficacy endpoint is used, the MTD stopping criteria refer to when the trial jumps from assigning subjects with the aim of finding the MTD (“MTD phase”) to assigning subjects with the aim of finding the MED (“MED phase”). In the MED phase, it is possible that because of new data being observed, the MTD stopping rules are no longer met and the trial switches back to the MTD phase. A new stopping rule for the MTD phase is added, “Maximum subjects used to determine the MTD”. After this number of subjects has been enrolled, the trial switches from MTD to MED phase and there is no going back.\nIn the MED phase, there are several rules for stopping early (i.e. before the maximum sample size of the trial):\n\nMaximum cohorts/subjects on MED. This behaves analogous to Required number of cohorts/subjects on/near MTD in the MTD stopping rules, with the exception of not using a concept of “near” doses.\nNumber of doses within the credible interval is less than or equal to with the sub-option Alpha for width of credible interval. This behaves analogous to Range/Ratio of dose strengths within the credible interval is less than or equal to in the MTD stopping rules.\nProbability of dose being MED greater than. This behaves analogous to Probability of dose being in the target band greater than in the MTD stopping rules.\n\nA special case arises when the MED estimate is larger than the MTD estimate. If that is the case, subjects are allocated to MTD or the highest cleared dose (whatever is smaller) even in the MED phase. The option “If MED &gt; MTD: Continue until subjects near MTD reach” specifies how many subjects should be assigned to MTD in the MED phase before stopping the trial (and therefore giving up hope that the MED is a safe dose).\n\n\n\n\n\n\nFigure 58: Stopping Criteria tab with open enrolment and both a toxicity and efficacy endpoint",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-simulation-with-variants",
    "href": "documentation/v71/userguides/crm.html#sec-simulation-with-variants",
    "title": "CRM",
    "section": "Simulation with Variants",
    "text": "Simulation with Variants\nThe only difference that specifying design variants (see this section) introduces is to create additional scenarios – one for each Virtual Subject Response (VSR) profile for each variant. For example if there were 5 VSR profiles and then 4 variants (different numbers of maximum cohorts) specified then would now be 20 scenarios in total. The scenario names have “_Var1”, “_Var2”, … appended to them. Once simulations have been run and the .facts file has been saved and re-opened the scenarios are listed in alphabetical order.\n\n\n\n\n\n\nFigure 60: Simulation tab showing variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-to-run-simulations",
    "href": "documentation/v71/userguides/crm.html#sec-to-run-simulations",
    "title": "CRM",
    "section": "To run simulations",
    "text": "To run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user may not modify any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-how-many-simulations-to-run",
    "href": "documentation/v71/userguides/crm.html#sec-how-many-simulations-to-run",
    "title": "CRM",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%). This degree of accuracy usually unnecessary for dose escalation designs.\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-simulation-results",
    "href": "documentation/v71/userguides/crm.html#sec-simulation-results",
    "title": "CRM",
    "section": "Simulation results",
    "text": "Simulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nOpen Results Folder: Opens the windows “File Explorer” tool in the results folder of the currently selected scenario. This makes it very easy to locate and open results files.\nCopy to Clipboard: will copy the values displayed in the summary to the clipboard as “CSV” data, enabling it to be pasted straight into a spreadsheet.\nAll: A window containing all the summary results columns\nHighlights: a separate window with the results shown on the main tab\nAllocation, Observed: summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity: summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc: summary results of the posterior probabilities of the properties of interest\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\n\nExplore Results: offers three options:\n\n“Show Per Scenario Highlighted Scenario Graphs” that shows the graph of the simulation results for a specific scenario (see Section 12 below for a description of these graphs)\n“Show Across Scenario Graphs” that displays a trellis plot of summary graphs for each variant and each scenario (see Section 13 below for a description of these graphs).\n“Compare Scenarios in AIRSHIP” to open the simulation results in R with the AIRSHIP R-shiny graphing app. See the AIRSHIP User Guide for details.\n\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nDesign Report: it uses an R script and R libraries to generate a MS Word document describing the design. See the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.\n\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options corresponding to the above options that can be reached from the buttons, in what is sometimes a more ergonomic manner.\n\nMCMC Settings\n\n\n\n\n\n\nFigure 61\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nThe Number of samples per imputation value only applies to analyses using imputed data from a longitudinal model and is irrelevant for N-CRM, hence it is greyed out.\nIf the Number of MCMC files to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output, allowing the user to check convergence.\nThe MCMC output thinning parameter can be used to reduce the amount of data output to the MCMC file. It does not reduce the amount of MCMC samples used within the model fitting.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/crm.html#sec-facts-grid-simulation-settings",
    "title": "CRM",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-detailed-simulation-results",
    "href": "documentation/v71/userguides/crm.html#sec-detailed-simulation-results",
    "title": "CRM",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 62) will reveal more detailed results about the simulation.\n\n\n\n\n\n\nFigure 62: Detailed Simulation Results for a particular scenario\n\n\n\nRight-clicking on a row displays a context menu from which the user can view other columns (the default are the “highlihgts” columns) and also view the corresponding cohort results:\n\n\n\n\n\n\nFigure 63: Cohort Results for a particular simulation\n\n\n\nRight clicking on the cohort results, displays a context menu from which the user can view other columns (the default are the “highlights” columns).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-sim-aggregation",
    "href": "documentation/v71/userguides/crm.html#sec-sim-aggregation",
    "title": "CRM",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 64\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose, plus two extra rows for ‘below lowest’ and ‘above highest’.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nIf there is no value for the below lowest or above highest rows, they will be left blank.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. cohortsNNN.csv files are aggregated into a single agg_[pivot_]cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-design-report",
    "href": "documentation/v71/userguides/crm.html#sec-design-report",
    "title": "CRM",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "href": "documentation/v71/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "title": "CRM",
    "section": "Results when 2 groups have been simulated",
    "text": "Results when 2 groups have been simulated\nWhen 2 groups have been simulated, the results displayed on the Simulation tab and all the directly viewable summary tables under the “Show other columns” button are from Group 1.\nTo see the results from Group 2 you need to first display the Group 2 highlights – either by selecting “Group 2” on the “Show other columns” menu or after right clocking on a row of results.\nOnce the Group 2 “highlights” results are being displayed, the other sets of summary results can be viewed by right clicking on a row of results in the Group 2 “hightlights” window.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-highlights",
    "href": "documentation/v71/userguides/crm.html#sec-highlights",
    "title": "CRM",
    "section": "Highlights",
    "text": "Highlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nSettings\n1\nDisplays an icon showing the status of the simulation results.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the “MTD” at the end of the study. In CRM this is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on “excess” or “unacceptable” toxicity have been specified, then doses with posterior probabilities above these limits are excluded from being chosen as MTD. (Note this is different from the calculation of MTD used in Neuenschwander, Branson, and Gsponer (2008) where doses with posterior probabilities above these limits were not excluded from being selected as MTD.)\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nIncluded if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the “MED” at the end of the study. This is the dose with the highest posterior probability of being the dose that is the ‘highest dose below’ / ‘nearest’ / ‘lowest dose above’ (as specified on the Study &gt; Effiacy tab) to the target efficacy rate.\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration in weeks of the simulated trials.\n\n\nSD Duration\n1\nThe standard deviation over the simulations of the duration in weeks of the simulated trials.\n\n\nMean Lost\n1\nIf the trial uses open enrollment, this is the number of subjects that could not be allocated a dose because the number of subjects treated but not yet complete had reached the specified “Maximum subjects without final result” limit. Otherwise the value is 0.\n\n\nSD Lost\n1\nThe standard deviation over the simulations of the number of subjects lost.\n\n\nPpn(All Tox)\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nPpn(early Success)\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nPpn(Cap)\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nPpn MTD Under\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Underdosing” toxicity range.\n\n\nPpn MTD Target\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Target” toxicity range.\n\n\nPpn MTD Excess\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Excess” toxicity range.\n\n\nPpn MTD Unacc\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Unacceptable” toxicity range.\n\n\nPpn Correct Under\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Underdosing” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Target\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Target” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Excess+Unacc\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Excess” or “Unacceptable” ranges. This will be 0 if none of the doses in that scenario had a true toxicity that fell in those ranges.\n\n\nPpn MED Under\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or less. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over))\n\n\nPpn MED Over\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or more. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over)",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-allocations-observed",
    "href": "documentation/v71/userguides/crm.html#sec-allocations-observed",
    "title": "CRM",
    "section": "Allocations, Observed",
    "text": "Allocations, Observed\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nNum Phase 1\n1\nIncluded if efficacy is being simulated. This is the mean (over the simulations) of the number of subjects recruited in the first phase (up to the MTD stopping criteria being met) in this scenario.\n\n\nSubj. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subjects: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nCat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nCat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\n80% Num. Subj.\n1\nThe 80th percentile of the distribution of the number of subjects recruited in the simulations",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-fitted-toxicity",
    "href": "documentation/v71/userguides/crm.html#sec-fitted-toxicity",
    "title": "CRM",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nSD Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nMean Alpha Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nSD Mean Alpha Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nAlpha[234] Tox\n2-3\nIf Ordinal toxicity is being simulated, instead of Mean Alpha and SD Mean Alpha columns, there are pairs of columns Mean Alpha2, SD Mean Alpha2, … for Alpha2 & Alpha3 if a 3 point ordinal scale is being used and Alpha2, Alpha3 & Alpha4 if a 4 point ordinal scale is being used. These are the means (over the simulations) of the mean and standard deviation of the various Alpha coefficients in the BLRM model when fitting to the ordinal outcome.\n\n\nSD Alpha[234] Tox\n2-3\nsee row above\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Toxicity: &lt;Dose&gt;\nOne per dose\nThis is the true simulated toxicity rate for each dose in the scenario\n\n\nMean Fit Tox Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose.\n\n\nMean Fit Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prmtd-etc",
    "href": "documentation/v71/userguides/crm.html#sec-prmtd-etc",
    "title": "CRM",
    "section": "Pr(MTD) Etc.",
    "text": "Pr(MTD) Etc.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study. The dose chosen as MTD is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on excess or unacceptable toxicity have been specified, then doses with posterior probabilities above the specified limit, of having a toxicity rate in those bands are excluded from being chosen as MTD.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum [Tox] Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MTD was rule was met at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 1” (see below).\n\n\nNum [Tox] Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MTD is less than the specified number. [This stopping rule only evaluated if targeting an MTD rather than a toxicity band] If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 2” (see below).\n\n\nNum [Tox] Stop Rule 3\n1\nNumber of times the Pr(MTD) – that the probability that the dose was MTD or dose’s toxicity rate lies in the target toxicity rate band - met the stopping rule threshold at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 3” (see below).\n\n\nNum Stop Rule 4\n1\nNumber of times that observing another cohort with no toxicities would not change the selected MTD stopping rule was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nNum Stop Rule 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose&gt;\nOne per dose\nAs MTD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nOSD+ Selection: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. As OSD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nOSD+ Selection: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose above the tested range of doses was selected as the optimum selected dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(Under): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPost CE MTD+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the maximum tolerated dose.\n\n\nPost CE MTD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was the maximum tolerated dose allowing for a probability that the MTD is at a dose below or above the range of tested doses.\n\n\nPost CE MTD+: plus\n1\nThe posterior probability, after the results of the Cohort Expansion, that a dose above the tested range of doses was the maximum tolerated dose.\n\n\nPost CE OSD+: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose was selected as the optimum selected dose allowing for a probability that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose above the tested range of doses was selected as the optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-fitted-efficacy",
    "href": "documentation/v71/userguides/crm.html#sec-fitted-efficacy",
    "title": "CRM",
    "section": "Fitted Efficacy",
    "text": "Fitted Efficacy\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nSD Beta Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nAlpha Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nSD Alpha Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy response model for each dose.\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates of the efficacy rate across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Efficacy: &lt;Dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nMean Fit Eff Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.\n\n\nMean Fit Eff Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prmed-etc",
    "href": "documentation/v71/userguides/crm.html#sec-prmed-etc",
    "title": "CRM",
    "section": "Pr(MED) Etc.",
    "text": "Pr(MED) Etc.\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study. The dose chosen as MED is the dose with the highest posterior probability of having a efficacy rate nearest the target rate / is the highest dose with a rate below the target rate / is the lowest dose with a rate above the target rate – as specified on the Study &gt; Efficacy tab.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum Eff Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Eff CI\n1\nThe mean (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nSd Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nPr(MED) &lt;dose&gt;\n\nThe mean (over the simulations) of the posterior probability that each dose is the MED.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose&gt;\nOne per dose\nAs MED Selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(MED+): minus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose below the tested range of doses was the MED.\n\n\nPr(MED+): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities that each dose was the MED. As Pr(MED) but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nPr(MED+): plus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose above the tested dose range is the MED.\n\n\nPost CE MED+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was selected as the minimum efficacious dose allowing for the possibility that the MED is at a dose below or above the range of tested doses.\n\n\nPost CE MED+: plus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after the results of the Cohort Expansion, that each dose was selected as the optimum selected dose allowing for the possibility that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation-box-and-whisker-plot",
    "href": "documentation/v71/userguides/crm.html#allocation-box-and-whisker-plot",
    "title": "CRM",
    "section": "Allocation Box and Whisker Plot",
    "text": "Allocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 68: Allocation box and whisker plot\n\n\n\nThis graph displays a box and whisker plot of the number of subjects allocated to each dose over the simulations. These plots show:\n\nThe mean allocation over all simulations at each dose plotted as a solid line.\n\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\n\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#response-and-subject-allocation",
    "href": "documentation/v71/userguides/crm.html#response-and-subject-allocation",
    "title": "CRM",
    "section": "Response and Subject Allocation",
    "text": "Response and Subject Allocation\n\n\n\n\n\n\nFigure 69: Response and subject allocation graph\n\n\n\nThis plot shows the mean subject allocation to each dose as a blue bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation and the mean fitted toxicity separately.\nIf efficacy is being simulated, then lines for the mean fitted efficacy and true efficacy are also shown.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#response-and-mtd-distribution",
    "href": "documentation/v71/userguides/crm.html#response-and-mtd-distribution",
    "title": "CRM",
    "section": "Response and MTD Distribution",
    "text": "Response and MTD Distribution\n\n\n\n\n\n\nFigure 70: MTD distribution and response graph\n\n\n\nThis plot shows the proportion of times each dose has been selected as the MTD as a brown bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nThere is an options to display the “MTD+” distribution. This is the proportion of times each dose has been selected as the MTD when a dose below the lowest dose and a dose above the highest dose is included.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately.\nIf efficacy is being simulated then there are versions of the graph showing the histograms showing the distribution of selection of MTD, MED, OSD, and TE targets. The plot of the MTD shows the true and mean fitted toxicity, the plot of the MED shows the true and mean fitted efficacy and the plots of the OSD and TE show both the true and mean fitted toxicity and efficacy.\n\n\n\n\n\n\nFigure 71: Response and TE Distribution for a group",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities",
    "href": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities",
    "title": "CRM",
    "section": "Toxicity Interval Probabilities",
    "text": "Toxicity Interval Probabilities\n\n\n\n\n\n\nFigure 72: Toxicity interval probabilities graph\n\n\n\nThis plot shows the posterior probability for each dose that it’s toxicity rate lies in each of the four toxicity intervals as a stacked bar chart.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#observed-toxicity-and-allocation",
    "href": "documentation/v71/userguides/crm.html#observed-toxicity-and-allocation",
    "title": "CRM",
    "section": "Observed Toxicity and Allocation",
    "text": "Observed Toxicity and Allocation\n\n\n\n\n\n\nFigure 73: Observed Toxicities and Allocation Histogram\n\n\n\nThis graph shows the mean allocation to each dose and the mean number of toxicities observed at each dose. The total height of the bar shows the total allocation, and the red section of the bar shows the proportion that experienced toxicity.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sample-size-mtd-histogram",
    "href": "documentation/v71/userguides/crm.html#sample-size-mtd-histogram",
    "title": "CRM",
    "section": "Sample Size MTD Histogram",
    "text": "Sample Size MTD Histogram\n\n\n\n\n\n\nFigure 74: Sample Size MTD Histogram\n\n\n\nThis graph shows the number of times different sample sizes (number of subjects tested) were observed across the simulations. Each bar is shown as a stacked plot with each color indicating the proportion of times a particular dose was selected as the MTD in the simulations that ended with a particular sample size.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#alloc-history-summary",
    "href": "documentation/v71/userguides/crm.html#alloc-history-summary",
    "title": "CRM",
    "section": "Alloc History Summary",
    "text": "Alloc History Summary\n\n\n\n\n\n\nFigure 75: Allocation History Summary Graph\n\n\n\nThis graph overlays all the allocation histories of those simulations for which “cohorts” files have been output. The lines show the “route” the dose escalation followed and the circles show the dose selected as MTD at the end of the simulation. The lines are heavier and the circles darker the more simulations followed the same route or made the same selection.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nThis graph allows a quick appraisal of how well the design and priors allow the dose escalation to reach and then stop in the target band.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#per-sim-alloc-and-tox-history",
    "href": "documentation/v71/userguides/crm.html#per-sim-alloc-and-tox-history",
    "title": "CRM",
    "section": "Per Sim Alloc and Tox History",
    "text": "Per Sim Alloc and Tox History\n\n\n\n\n\n\nFigure 76: Allocation and toxicity history plot\n\n\n\nThis graph shows the allocation and toxicity history for an individual simulation. Each dot represents a subject, the dot is colored red if toxicity was observed, grey otherwise. The subjects’ outcomes are displayed left to right in the order in which they were dosed, and at a height corresponding to the dose they were given.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed. A history can only be displayed for trials for which a ‘cohorts’ file has been written out.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nOn the left hand side, a black triangle next to the y-axis indicates the highest cleared dose. Green/red triangles show the Model MTD/MED+ and finally the selected MTD/MED.\nIf the trial uses open enrolment this graph is slightly changed.\n\n\n\n\n\n\nFigure 77: Open Enrolment Allocation and Toxicity History graph\n\n\n\nIf the trial uses open enrolment then this graph has an “Interim” picker that shows the data available at a specific interim. Subjects whose outcome has not been observed at this interim are shown as grey squares. Subjects who were not included in the trial because there were already the maximum number of subjects treated but who had not attained their final toxicity / non-toxicity status are shown as yellow crosses at the level below the lowest dose. As the interim displayed is increased subjects symbol will change as their endpoint data becomes available.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#cohort-responses",
    "href": "documentation/v71/userguides/crm.html#cohort-responses",
    "title": "CRM",
    "section": "Cohort Responses",
    "text": "Cohort Responses\n\n\n\n\n\n\nFigure 78: Cohort response plot\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the fitted dose-toxicity model for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIt the trial uses open enrolment then subjects whose outcome has not been observed at the time of the interim being displayed are shown as a light grey part of the “allocated subjects” bar.\nIf trial simulates 2 groups then there are two graphs one for each group.\nIf the trial simulates efficacy, then the bar shows both toxicities and efficacies using half width bars of different colors.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED.\n\n\n\n\n\n\nFigure 79: Cohort Responses for one of two groups showing efficacy and toxicity",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#cohort-band-probabilities",
    "href": "documentation/v71/userguides/crm.html#cohort-band-probabilities",
    "title": "CRM",
    "section": "Cohort Band Probabilities",
    "text": "Cohort Band Probabilities\n\n\n\n\n\n\nFigure 80: Cohort band probabilities graph\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the posterior probabilities that the toxicity rate lies in each of the toxicity bands for each dose, for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#mtd-change-on-expansion",
    "href": "documentation/v71/userguides/crm.html#mtd-change-on-expansion",
    "title": "CRM",
    "section": "MTD Change on Expansion",
    "text": "MTD Change on Expansion\n\n\n\n\n\n\nFigure 81: MTD change on expansion\n\n\n\nThis graph plots the dose selected as MTD before the results of the expansion cohort are available, against the dose selected as MTD after the expansion cohort results are available. Each simulation result is plotted as a ‘dot’, and the graph is divided up into a grid and dots located in the correct cell in the grid and then arranged within the cell depending on how many results fall into the cell. This allows the relative number of results in each cell to be appraised quickly by eye.\nThis graph allows the trial designer to see the degree of risk that the determination of the MTD will change as a result of the expansion cohort and whether the risk is predominantly that the determination will increase or decrease.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#selected-mtd-graph",
    "href": "documentation/v71/userguides/crm.html#selected-mtd-graph",
    "title": "CRM",
    "section": "Selected MTD graph",
    "text": "Selected MTD graph\nThe Selected MTD “Across Scenarios” graph shows for each scenario and each variant a histogram of the proportion of times each dose was selected as the MTD at the end of the simulations in that scenario. The bars are colored to reflect the toxicity band that the “true” toxicity rate of the dose falls into in that scenario.\n\n\n\n\n\n\nFigure 83",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities-graph",
    "href": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities-graph",
    "title": "CRM",
    "section": "Toxicity Interval Probabilities graph",
    "text": "Toxicity Interval Probabilities graph\nThe Toxicity Interval Probabilities “Across Scenarios” graph shows for each scenario and each variant a stacked bar chart of the posterior probability that the toxicity rate at each dose falls into one of the user defined 4 toxicity bands.\n\n\n\n\n\n\nFigure 84",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#fitted-toxicity-1",
    "href": "documentation/v71/userguides/crm.html#fitted-toxicity-1",
    "title": "CRM",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\nThe Fitted Toxicity “Across Scenarios” graph shows for each scenario and each variant, the mean fitted toxicity and the 95-percentile spread of the fitted toxicities across the simulations.\n\n\n\n\n\n\nFigure 85",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation",
    "href": "documentation/v71/userguides/crm.html#allocation",
    "title": "CRM",
    "section": "Allocation",
    "text": "Allocation\nThe Allocation “Across Scenarios” graph shows for each scenario and each variant, a box plot of the spread of the number of subjects allocated to each dose across the simulations. As N-CRM may only be allocating a small number of cohorts the number of subjects allocated to each dose is often not a smooth distribution, but somewhat discontinuous.\n\n\n\n\n\n\nFigure 86",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sample-size",
    "href": "documentation/v71/userguides/crm.html#sample-size",
    "title": "CRM",
    "section": "Sample Size",
    "text": "Sample Size\nThe Sample Size “Across Scenarios” graph is the only “Across Scenario” graph that is not a trellis plot. It is a single graph with a line plotted per scenario of the mean sample size at each maximum sample size.\n\n\n\n\n\n\nFigure 87",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#entering-the-data-directly",
    "href": "documentation/v71/userguides/crm.html#entering-the-data-directly",
    "title": "CRM",
    "section": "Entering the data directly",
    "text": "Entering the data directly\nThe data can be entered directly in the data grid – each row containing the date for a single subject.\nThe format of the grid is the same as though viewing the “subject.csv” file in a spreadsheet.\nFor historical reasons the data file format contains 5 columns that are now unused but have been retained. Thus the columns are:\n\nFirst column is the subject ID. This column can be left blank, FACTS does not used the value, it is there to allow the data to be cross-referenced to an external data source. If not required there is no harm simply entering ‘1’ on each row.\nThe next 5 columns are unused and can be left blank. Do not enter text containing comma’s in these fields, these will be read as column separators if the data is saved and read back in.\nCohort number, this should be an integer indicating which cohort the subject belonged to (and hence the order in which they entered the trial). This data is sometimes used when determining what doses the allocation rules permit to be used. The FACTS GUI now checks to ensure that this value has been entered.\nDose Strength, if explicit doses are being used this value must match the dose strength of one of the doses defined on the ‘Treatment Arms’ tab – as the dose escalation rules are defined in terms of “number of doses”. If doses have been defined using ‘finely spaced doses’ then this column can contain any value as dose escalation rules defined using “dose strength”. The value entered will be used as the strength of the dose the subject was administered.\nApart from the requirement that the dose strength corresponds to one of the planned doses if the design uses explicit doses, there is no need for the data entered to represent a dose escalation permitted by the design. The team can have been more or less cautious, and there can be more or less data, than originally planned.\nToxicity – if a binary endpoint is being used this must be 0 (not-toxicity) or 1 (toxicity), if Ordinal toxicity is being used then this must be 1, 2, 3 or 4; where 1 is now no toxicity, 2 mild toxicity, 3 toxicity, and 4 severe toxicity.\nEfficacy – this must be 0 (no efficacy) or 1 (efficacy), even if efficacy is not being modelled in the design. If it is not being modelled then whether the value is 0 or 1 is immaterial.\n\n\n\n\n\n\n\nFigure 91\n\n\n\nIf the data entered and then ‘Run Analysis’ clicked, the data is saved to a file called ‘subject.csv’ and the analysis results saved to a folder called ‘Analysis’ within the “_results” folder of the design.\n\n\n\n\n\n\nFigure 92\n\n\n\nThe ‘Save As’ button can be used to save the file with a different name, but it will still be saved within the _Results folder.\nA specific results folder is also created, called ‘Analysis_’.\n\n\n\n\n\n\nFigure 93\n\n\n\n\n\n\n\n\n\nFigure 94",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#loading-data-from-a-file",
    "href": "documentation/v71/userguides/crm.html#loading-data-from-a-file",
    "title": "CRM",
    "section": "Loading Data From a File",
    "text": "Loading Data From a File\nAs well as entering the data via FACTS its possible to load the data from a ‘subject.csv’ file. These can be cerated within FACTS or outside of FACTS and once created can be edited FACTS or outside of FACTS.\nWe have tried to make it particularly easy to enter, modify and analyze data in N-CRM because this is a useful way to explore the properties of the design in addition to simulation.\n\nThe subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nFor historic reasons the CRM subject.csv file format includes fields for patient identification data, however the FACTS design engine does not use this data, but does require that the columns contain data of the expected format. This is the first 6 columns: patient ID, Patient Initials, Year, Month, Day, Time & Cohort. The simplest thing to do is enter simple default values as shown below.\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Toxicity, Efficacy\n1, , , , , , 1, 12.5, 0, 0\nIn FACTS only the last four columns are ever significant: Cohort, Dose, Toxicity and Efficacy. “Dose” needs to contain the dose level (not the dose index)) of the dose given to the subject, For example if the dose levels were specified as 12.5, 25, 50, 100, 150, 200 and 250 so this column should contain one of these values.\nIf the toxicity endpoint is dichotomous then “Toxicity” needs to contain either a ‘1’ (to indicate toxicity observed) or a ‘0’ (for no toxicity). If its ordinal, then it needs to contain a ‘1’, ‘2’, ‘3’ or ‘4’ corresponding to the observed level of toxicity for that subject, where ‘1’ is now “no toxicity’ and ‘3’ is the ‘target toxicity’, ‘2’ is ‘mild toxicity’ and ‘4’ is ‘severe toxicity’.\nA value in the final “Efficacy” column is also required whether or not efficacy is being modelled in the design. The column should contain either a ‘1’ (to indicate efficacy observed) or a ‘0’ (for no efficacy).\nThus if the first cohort had been allocated the lowest dose ’12.5’ and no subjects experienced toxicity (‘0’), the subjects.csv file looks like this:\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Tox, Efficacy\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "href": "documentation/v71/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "title": "CRM",
    "section": "Data file management on the Analysis tab",
    "text": "Data file management on the Analysis tab\nFrom FACTS 6.2 onwards FACTS now supports multiple subject data files and analysis folders.\nButtons that allow the subject data file to be changed:\n\n’Select File to Create New Analysis: launches a file browser that allows the user to select a new “.csv” file from any location. The selected file is copied to the “_Results” folder (retaining its current name) and made the current subject data file.\n‘Rename Current Analysis’ allows the name of the current subject data file to be changed.\n‘Select Difference Analysis’ allows a different subject data file that is in the “_Results” folder to be made the current subject data file.\n‘Delete Analysis’ allows any of the subject data files that are in the “_Results” folder to be deleted.\n\nThe name of the current subject data file and the name of the corresponding analysis folder are shown below the subject data file buttons.\nThere are five buttons that allow the currently loaded subject data file to be modified:\n\n‘Delete Row’ deletes the currently selected row in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Delete All’ clears all the data in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Reload data’ replaces the data in the data grid with the data that is still in the current subject data file.\n‘Save As’ saves the current data in the data grid to a new subject data file in the “_Results” folder, and makes that the current subject data file.\n“Save” saves the current data in the data grid to the current subject data file.\n\nRunning an analysis performs a ‘Save’ before running the analysis.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#running-an-analysis",
    "href": "documentation/v71/userguides/crm.html#running-an-analysis",
    "title": "CRM",
    "section": "Running an Analysis",
    "text": "Running an Analysis\nOnce data has been loaded or entered, the user can click the ‘Run Analysis’ button.\nOnce the analysis has run, FACTS displays the recommendation, and a graph showing the data the fitted toxicity.\n\n\n\n\n\n\nFigure 95: Analysis tab - analysis results\n\n\n\nThe available analysis parameters are\n\nMCMC Burn-in: how many of the initial MCMC samples are discarded before accumulating samples to estimate the posterior distributions of the values of interest.\nNumber of samples: the number of MCMC samples to take in-order to estimate the posterior distributions of the values of interest.\nRandom Seed: the seed to be used initialize the random number sequence, with the same design data and random seed FACTS will return the same results.\nEdit command parameters. This allows the command line string to the design engine to modified, this is an advanced option. The command line options are described in the “FACTS DE User Guide for Trial Execution”.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#analysis-graphs",
    "href": "documentation/v71/userguides/crm.html#analysis-graphs",
    "title": "CRM",
    "section": "Analysis Graphs",
    "text": "Analysis Graphs\nThree graphs are available, the first shows the subject allocation, observed toxicities and resulting fitted curve:\n\n\n\n\n\n\nFigure 96\n\n\n\nThe second shows the posterior probabilities for each dose that its toxicity rate falls in each of the 4 toxicity bands:\n\n\n\n\n\n\nFigure 97\n\n\n\nThe third simply shows the observed data:\n\n\n\n\n\n\nFigure 98\n\n\n\nA fourth graph is available if “Generate MCMC file” is checked before running the analysis, we can now view MCMC trace plots of the fitted parameters such as Alpha and Beta:\n\n\n\n\n\n\nFigure 99",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-summary",
    "href": "documentation/v71/userguides/crm.html#contents-of-summary",
    "title": "CRM",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe columns in summary.csv are common across all the FACST Dose Finding design engines.\nSome columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate summary files. The first summary file “summary.csv” contains the results for the first group, the second summary file “summary2.csv” contains the result for the second group.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber of Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nMean num subjects\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario (including any single patient run-in, but excluding any expansion cohort..\n\n\nPpn Tox\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn Eff\n1 – Efficacy only\nThis is the average proportion of the subjects recruited that experienced a efficacy in this scenario.\n\n\nSD Ppn Tox\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1 – Efficacy only\nThis is the standard deviation of the proportion of efficacy across the simulations.\n\n\nTrue Mean Tox\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile that we are simulating from.\n\n\nMean Beta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\ns.d.Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\nMean Alpha 3 Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\ns.d.Alpha 3 Tox3\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\nMean Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\ns.d.Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\nMean Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\ns.d.Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\nMTD Selection &lt;dose index&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study, as defined by the Selected MTD simulation results column. Index starts at 0 if a control arm is included.\n\n\nMED Selection &lt;dose index&gt;\nOne per dose – Efficacy only\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study, as defined by the Selected MED simulation results column. Index starts at 0 if a control arm is included.\n\n\nOSD Selection &lt;dose index&gt;\nOne per dose\nUnused [it contains values that are copies of the MTD selection. OSD = Optimum Safe Dose, it differs from the MTD only if there is an efficacy endpoint to take into account too]\n\n\nMean num Ph1\n1 – Efficacy only\nThis is the mean (over the simulations) of the number of subjects dosed during the first phase of the trial that targets the MTD.\n\n\nMean Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Fitted Efficacy &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Efficacy &lt;dose index&gt;\nOne per dose -\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Subj per dose&lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subj per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nMean Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nMean Eff per dose &lt;dose index&gt;\nOne per dose -Efficacy only\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\nMean Known per dose &lt;dose index&gt;\nOne per dose\nThe number of known patient outcomes for a dose. In N-CRM there is no simulation of drop-outs so this will always be the same as “Mean subj per dose”\n\n\nSD Known per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of known patient outcomes for each dose across the simulations. In N-CRM there is no simulation of drop-outs so this will always be the same as “SD subj per dose”\n\n\nNum subj 80%ile\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nTox Stopping 1\n1\nThe number of times that toxicity stopping rule 1 was true at the end of the simulation. This is the rule that the trial should stop when the specified minimum number of cohorts has been allocated to the dose currently selected as the MTD.\n\n\nTox Stopping 2\n1\nThe number of times that toxicity stopping rule 2 was true at the end of the simulation. This is the rule that the trial should stop when no more than the specified number of doses within the credible interval for the target toxicity.\n\n\nTox Stopping 3\n1\nThe number of times that toxicity stopping rule 3 was true at the end of the simulation. This is the rule that the trial should stop when a dose achieves the minimum posterior probability of having a toxicity rate within the target band/of being MTD.\n\n\nTox Stopping 4\n1\nThe number of times that toxicity stopping rule 4 was true at the end of the simulation. This is the rule that the trial should stop if testing another cohort and observing no toxicities does not change the dose selected as the MTD.\n\n\nTox stopping 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation.\n\n\nEff Stopping 1\n1 – Efficacy only\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nEff Stopping 2\n1 – Efficacy only\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nEff Stopping 3\n1 – Efficacy only\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Tox CI\n1 – MTD target only\nThe mean number of doses in the MTD CI\n\n\nSD Tox CI\n1 – MTD target only\nThe SD of the number of doses in the MTD CI\n\n\nMean Eff CI\n1 – Efficacy only\nThe mean number of doses in the MED CI\n\n\nSD Eff CI\n1 – Efficacy only\nThe SD of the number of doses in the MED CI\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe mean probability of the dose being MTD\n\n\nPr(MED)\nOne per dose – Efficacy only\nThe mean probability of the dose being the MED\n\n\nMTD+ &lt;1\n1\nThe number of times the MTD was deemed to be less than the lowest dose\n\n\nMTD+ &lt;dose index&gt;\nOne per dose\nThe number of times each dose was selected as the MTD\n\n\nMTD+ &gt;&lt;D&gt;\n1\nThe number of times the MTD was deemed to be above the highest dose\n\n\nMED+ &lt;1\n1 – Efficacy only\nThe number of times the MED was deemed to be less than the lowest dose\n\n\nMED+ &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of times each dose was selected as the MED\n\n\nMED+ &gt;&lt;D&gt;\n1 – Efficacy only\nThe number of times the MED was deemed to be above the highest dose\n\n\nOSD+ &lt;1\n1\nThe same as MTD+ &lt;1, can be ignored\n\n\nOSD+ &lt;dose index&gt;\nOne per dose\nThe same as MTD+ &lt;dose index&gt;, can be ignored\n\n\nOSD+ &gt;&lt;D&gt;\n1\nThe same as MTD+ &gt; &lt;D&gt;, can be ignored\n\n\nDE Version\n1\nThis is the version of the N-CRM design engine that simulated these trial results.\n\n\nGUI Version\n1\nThis is the version of the FACTS GUI that was used to specify the parameters for the trials to be simulated.\n\n\nProject\n1\nThe name of the project or design\n\n\nScenario\n1\nThe name of the scenario within the project or design\n\n\nDate/Time\n1\nThe date and time the simulations were started\n\n\nBest &lt;dose index&gt;\nOne per dose – Efficacy only\nThe probability that the dose is ‘the best’ – that is it has the highest probability of efficacy without toxicity.\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe mean probability that the MTD is below the lowest dose\n\n\nPr(MTD+) &lt;D+1&gt;\n1 – MTD target only\nThe mean probability that the MTD is above the highest dose\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe mean probability that the MED is below the lowest dose\n\n\nPr(MED+) &lt;D+1&gt;\n1 –Efficacy only\nThe mean probability that the MED is above the highest dose\n\n\nPr(Under) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPrior Mean ln(Beta) Tox\n1\nThe value used for the mean value for the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior s.d.(Beta) Tox\n1\nThe value used for the standard deviation of the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior Mean Alpha Tox\n1\nThe value used for mean value for the prior distribution of Alpha in the toxicity model for all the simulations\n\n\nPrior s.d.Alpha Tox\n1\nThe value used for the standard deviation of the prior distribution of Alpha in the toxicity model for all simulations\n\n\nPrior Mean Rho Tox\n1\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the toxicity model for all simulations\n\n\nPrior Mean ln(Beta) Eff\n1 – Efficacy only\nThe value used for the mean value for the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior s.d.(Beta) Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior Mean Alpha Eff\n1 – Efficacy only\nThe value used for mean value for the prior distribution of Alpha in the efficacy model for all the simulations\n\n\nPrior s.d.Alpha Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of Alpha in the efficacy model for all simulations\n\n\nPrior Mean Rho Eff\n1 – Efficacy only\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the efficacy model for all simulations\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration of the trial.\n\n\ns.d.Duration\n1\nThe SD (over the simulations) of the duration of the trial.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe actual toxicity rate being simulated for each dose\n\n\nMean Lost\n1 – Open Enrolment only\nThe mean (over the simulations) of the number of subjects who were available for treatment but could not be included in the trial because the number of treated subjects for whom the final result is not available equals the maximum queue length. This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\ns.d.Lost\n1 – Open Enrolment only\nThe SD (over the simulations) of the number of subjects ‘lost’ (see above). This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\nPostCE MTD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nAll Tox Stop\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nEarly Success\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nCap Stop\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nMean Fitted Tox Lower &lt;D&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the estimates of the toxicity rate across the simulations, at each dose.\n\n\nMean Fitted Tox Upper &lt;D&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the estimates of the toxicity rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "href": "documentation/v71/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "title": "CRM",
    "section": "Contents of simulations.csv and cohortsNNN.csv",
    "text": "Contents of simulations.csv and cohortsNNN.csv\nMost of the columns are common to the two file types, but the first few are different.\nAs with the summary.csv file, some columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate simulation.csv and cohorts.csv files. The first simulations file “simulations.csv” contains the results for the first group, the second file “simulations2.csv” contains the result for the second group.\nFor the cohorts files if two groups are being simulated, the files names ‘cohortsNNN.csv’ contain the results for the first group, and the files ‘named cohorts2_NNN.csv’ contain the results for the second group\n\nsimulations.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber\n1\nThe number of the simulation.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nNo.Subjects\n1\nThe number of subjects recruited in this trial in the small cohort run-in and in the cohorts used to locate the MTD, but not those in the expansion cohort.\n\n\nPpn Tox\n1\nThis is the proportion of the subjects recruited that experienced a toxicity in this trial\n\n\nPpn Eff\n1 – Efficacy only\nThis is the proportion of the subjects recruited that had an efficacious outcome in this trial\n\n\nTrue Mean Tox\n1\nThis is the average probability of toxicity for the subjects in the trial, given the doses they were treated with and the toxicity rate being simulated for each of those doses.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average probability of efficacy for the subjects in the trial, given the doses they were treated with and the efficacy rate being simulated for each of those doses.\n\n\nSeeds\n2\nThe two 32 bit numbers that make up the random number seed at the end of the simulation. To exactly re-simulate a specific simulation (e.g. in order to generate an mcmc file or cohorts file for that simulation) enter these values from the line above the simulation to be re-simulated.\n\n\n\n\n\ncohortsNNN.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe number of the cohort in the simulation.\n\n\nAlloc Dose\n1\nThe index of the dose assigned to that cohort. If using open enrolment, -2 means the subject was ‘lost’, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nNumToxic\n1\nThe number of subjects in that cohort that experienced a toxicity\n\n\n\n\n\nCommon simulations.csv and cohortsNNN.csv columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nmean Beta Tox\n1\nThe mean fitted value for the toxicity model Beta parameter\n\n\ns.d.Beta Tox\n1\nThe standard deviation of the toxicity model fitted Beta parameter\n\n\nMean Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\ns.d.Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\nMean Alpha 3 Tox\n1\nThe mean fitted value for the toxicity model Alpha parameter for category 3 (or the only category) toxicity or higher\n\n\ns.d.Alpha 3 Tox\n1\nThe standard deviation of the toxicity model Alpha parameter for category 3 toxicity (or the only category) or higher\n\n\nMean Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 4 toxicity\n\n\ns.d.Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 4 toxicity\n\n\nMean Beta Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Beta parameter\n\n\ns.d.Beta Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model fitted Beta parameter\n\n\nMean Alpha Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Alpha parameter\n\n\ns.d.Alpha Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model Alpha parameter\n\n\nModel MTD\n1\nThe dose index of the model MTD\n\n\nModel MED\n1 – Efficacy only\nThe dose index of the model MED\n\n\nModel OSD\n1 [simulations only]\nThe index of the dose selected as the Optimal Safe Dose (OSD), in N-CRM this will always be the same as the MTD as there is no efficacy to take into consideration.\n\n\nHighest Cleared Dose\n1\nThe highest cleared dose\n\n\nSelected MTD\n1\nThe dose index of the selected MTD (minimum of the Highest Cleared Dose and the model MTD+)\n\n\nSelected MED\n1\nThe dose index of the selected MED (minimum of the Highest Cleared Dose and the model MED+)\n\n\nSelected OSD\n1 [simulations only]\nThe dose index of the selected OSD (minimum of the Highest Cleared Dose and the model OSD+)\n\n\nNo. Ph1\n1 – Efficacy only\nThe number of subjects enrolled during the first, MTD locating, phase, before switching to the MED locating phase\n\n\nToxicity &lt;dose index&gt;\nOne per dose\nThe mean of the final posterior estimate of the toxicity rate at each dose.\n\n\nEfficacy\nOne per dose – Efficacy only\nThe mean of the final posterior estimate of the efficacy rate at each dose.\n\n\nNo. Subj &lt;dose index&gt;\nOne per dose\nThe number of subjects that have been allocated to each dose.\n\n\nTox per dose &lt;dose index&gt;\nOne per dose\nThe number of toxicities that have observed at each dose\n\n\nEff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of efficacies that have observed at each dose\n\n\nKnown per dose &lt;dose index&gt;\nOne per dose\nThe number of subjects for whom final results are available at each dose.\n\n\nTox CI\n1 – MTD target only\nOnly used if targeting a single dose. It’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nEff CI\n1 – Efficacy only\nIt’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nFlags\n1\nA flag value comprising a number of (possible) flag values ’OR’d together to show the current allocation or stopping decisions. See the Flag Values table below for details\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe posterior probability for each dose that that dose is the MTD.\n\n\nPr(MED) &lt;dose index&gt;\nOne per dose – Efficacy only\nThe posterior probability for each dose that that dose is the MED.\n\n\n1st full size\n1 [simulations only]\nThe number of the first full sized cohort, this will be ‘1’ unless the small cohort run-in is enabled. If the small cohort run-in is enabled, then this is the index of the first full size cohort.\n\n\nCohort size\n1 [cohorts only]\nThe number of patients in the cohort.\n\n\nMTD+\n1\nThe dose index of the model MTD+, this is the model MTD using the dose range extended by one dose either end. Dose 0 will be selected if all doses are too toxic and dose D+1 will be selected if no doses are toxic enough.\n\n\nMED+\n1 – Efficacy only\nThe dose index of the model MED+, this is the model MED using the dose range extended by one dose either end. Dose 0 will be selected if all doses are efficacious enough and dose D+1 will be selected if no doses are efficacious enough.\n\n\nOSD+\n1 [simulations only]\nThe dose index of the model OSD+, will be the same as the model MED+ unless this is above the model MTD+, in which case it is same as the model MTD+. If there is no efficacy to take into consideration this is the same as the model MTD+..\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe probability that the model MTD+ is at a dose below the lowest tested doses.\n\n\nPr(MTD+) D+1\n1 – MTD target only\nThe probability that the model MTD+ is at a dose above the highest tested doses.\n\n\nPr(Good) &lt;dose index&gt;\n1 – Efficacy only\nThe posterior probability for each dose of observing efficacy without observing toxicity.\n\n\nBest\n1 – Efficacy only\nThe index of the dose with the highest Pr(Good)\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe probability that the model MED+ is at a dose below the lowest tested doses.\n\n\nPr(MED+) D+1\n1 – Efficacy only\nThe probability that the model MED+ is at a dose above the highest tested doses.\n\n\nPr(Under) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Under-dosing’ toxicity band.\n\n\nPr(Target) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Acceptable’ toxicity band.\n\n\nPr(Excess) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Excessive’ toxicity band.\n\n\nPr(Unacc) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Unacceptable’ toxicity band.\n\n\nDuration\n1 (simulations only)\nDuration of the trial in weeks.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe toxicity rate being simulated for that dose in the scenario.\n\n\nTrue Efficacy &lt;Dose&gt;\nOne per dose – Efficacy only\nThe efficacy rate being simulated for that dose in the scenario.\n\n\nRec Time\n1 - Cohorts only\nIf the trial is using open enrolment this column record the time the subject was available to be dosed.\n\n\nNum Lost\n1 – Open enrolment only (simulations only)\nIf the trial is using open enrolment this is the total number of subjects ‘lost’ during the simulation, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nPostCE MTD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MTD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED+\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPr(Tox) Lower &lt;Dose&gt;\nOne per dose\nThe lower bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Tox) Upper &lt;Dose&gt;\nOne per dose\nThe upper bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Eff) Lower &lt;Dose&gt;\nOne per dose – Efficacy only\nThe lower bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\nPr(Eff) Upper &lt;Dose&gt;\nOne per dose – Efficacy only\nThe upper bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\n\nThe ‘Flags’ column is a number comprised of a number of binary flags that are used to indicate the ‘state’ of the simulator at different analyses. These flags are:\n\n\n\n\n\n\n\n\nFlag Values\n\n\n\n\n\n\nBits used\nValue\nMeaning\n\n\n0x001F\n0x0001\nAllocating to MTD\n\n\n0x001F\n0x0002\nAllocating to MED\n\n\n0x001F\n0x0003\nAllocating to initial dose\n\n\n0x001F\n0x0004\nAllocating new single patient cohort\n\n\n0x001F\n0x0005\nAllocating 1st dose of 2nd sample to dose below MTD from 1st sample\n\n\n0x001F\n0x0006\nAllocating 1st dose of 2nd sample to MTD from 1st sample\n\n\n0x001F\n0x0007\nAllocating expansion cohort\n\n\n0x001F\n0x0008\nAllocating to MTD because can’t allocate to MED because its above MTD\n\n\n0x001F\n0x0009\nExpanding at the current dose\n\n\n0x001F\n0x000A\nExpanding at the dose below\n\n\n0x001F\n0x000D\nAllocating as a backfill\n\n\n0x001F\n0x000E\nAllocating as a frontfill\n\n\n0x001F\n0x000F\nAllocating to max/min for fixed probability\n\n\n0x001F\n0x0011\nStopping for early futility\n\n\n0x001F\n0x0012\nStopping because MED is found\n\n\n0x001F\n0x0013\nStopping because all doses are toxic\n\n\n0x001F\n0x0014\nStopping because MTD is found\n\n\n0x001F\n0x0015\nStopping because MTD is found but there is no MED\n\n\n0x0020\n0x0020\nReached max subjects on MTD\n\n\n0x0030\n0x0030\nReached min required subjects on MTD\n\n\n0x0040\n0x0040\nToxicity confidence interval test passed\n\n\n0x0080\n0x0080\nPr(MTD) test passed\n\n\n0x0100\n0x0100\nReached max subjects on MED\n\n\n0x0200\n0x0200\nEfficacy confidence interval test passed\n\n\n0x0400\n0x0400\nPr(MED) test passed\n\n\n0x1000\n0x1000\nNo MED so using maximum instead\n\n\n0x2000\n0x2000\nUsing maximum permitted dose in place of MTD\n\n\n0x4000\n0x4000\nUnable to escalate due to part filled dose\n\n\n0x8000\n0x8000\nUnable to allocate during open enrolment because maximum permitted subjects without final result reached\n\n\n0x10000\n0x10000\nReached max subjects on MTD\n\n\n0x20000\n0x20000\nUnable to allocate during open enrolment because reached max subjects on MTD, but not all subjects on MTD have final results, so trial is ‘paused’ – resuming if the final results move the MTD so the trial continues, or resuming to allocate an expansion cohort.\n\n\n0x40000\n0x40000\nMax subjects reached and no other stopping rules met\n\n\n0x80000\n0x80000\nMaximum cohorts used to determine MTD has been met when there is also an efficacy endpoint, switching to searching for the MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-mcmcnnnnn",
    "href": "documentation/v71/userguides/crm.html#contents-of-mcmcnnnnn",
    "title": "CRM",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file, if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (excluding the burnin) and the samples from all the analyses (i.e from every cohort – or if using open enrollment, every subject) in the simulation are included. The first two columns are the cohort’s index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta\n1\nThe estimate of the slope of the logistic regression\n\n\nAlpha &lt;O&gt;\nO\nThe estimate of intercept of the logistic regression – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the second group.\n\n\na\n1\nIf a second group is included this is the estimate of the offset for the intercept of the cat-3 toxicity model.\n\n\n\n\nMCMC File if Efficacy is included\nIf an efficacy endpoint is included in the design, then all the model parameters columns described above are included suffixed with “Tox” and then duplicate, suffixed with “Eff”.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta Tox\n1\nThe estimate of the slope of the logistic regression of the toxicity model\n\n\nAlpha &lt;O&gt; Tox\nO\nThe estimate of intercept of the logistic regression of the toxicity model – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb Tox\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the toxicity model for the second group.\n\n\na Tox\n1\nIf a second group is included this is the estimate of the offset of the intercept for the cat-3 toxicity model.\n\n\nBeta Eff\n1\nThe estimate of the slope of the logistic regression of the efficacy model\n\n\nAlpha Eff\n1\nThe estimate of intercept of the logistic regression of the efficacy model.\n\n\nB Eff\n1\nIf a second group is included this is the estimate of ‘B’ the slope offset for the efficacy model for the second group.\n\n\nA Eff\n1\nIf a second group is included this is the estimate of the offset of the intercept for the efficacy model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#exporting-the-results",
    "href": "documentation/v71/userguides/crm.html#exporting-the-results",
    "title": "CRM",
    "section": "Exporting the Results",
    "text": "Exporting the Results\nUsing the menu item File -&gt; Export Project, the .facts file and all the results files can be saved as a single zip file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html",
    "href": "documentation/v71/userguides/platform.html",
    "title": "Platform Trials",
    "section": "",
    "text": "FACTS Platform Trials is useful for simulating trials in which there are a number of treatments being tested against a common control arm. Unlike in the FACTS Core engine, each of these treatments is meant to stand alone, and decisions are made on individual arms rather than at the trial level. As a result, there is not expected to be a dose-response relationship between the treatments. Two common goals of platform trials are to find many effective treatments among a set of treatments or to find a single effective treatment arm as quickly as possible. Treatments in a platform trial can leave the trial when a decision has been made for that arm or they have reached their sample size cap, and new treatment arms can enter when they are available to begin randomizing.\nFACTS V7.0 was the first version of FACTS to feature a Platform Trial simulator.\nThis first version of the Platform Trial simulator FACTS provides fairly limited options for the statistical analysis:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#trial-info",
    "href": "documentation/v71/userguides/platform.html#trial-info",
    "title": "Platform Trials",
    "section": "Trial Info",
    "text": "Trial Info\nThe Trial Info sub-tab provides parameters for specifying characteristics of the platform trial, including whether the design is adaptive, the sample size, the number of arms allowed to enroll simultaneously, and the time to observe the endpoint.\n\nDesign Options:\n\nEnable adaptive features\nSpecify whether the design is adaptive. If adaptive features are not enabled, some adaptive-specific parameters and tabs are hidden, such as the tabs for defining trial updates, early stopping criteria, and adaptive allocation options on the allocation tab.\n\n\nUse longitudinal modeling\nCurrently, longitudinal modelling is not implemented for the platform trials engine.\n\n\nEndpoint Specific Inputs\n\nContinuousDichotomous\n\n\n\nInclude simulation of baseline\nWhether to simulate participant’s baseline score or simply change from baseline. If simulating baseline, whether the analysis is based on change from baseline or final endpoint value.\n\n\n\n\nEnable special longitudinal options\nSpecial longitudinal options are not enabled currently in FACTS platform trials.\n\n\n\n\n\n\n\nTrial Information\nThe platform trial will automatically terminate when all of the defined treatments have had a chance to enter the trial and complete their enrollment and follow-up. Arms that attempt to join the platform, but are not allowed to, are considered complete.\n\nMax enrollment time (wks)\nYou may limit the total enrollment time of the platform as a whole. If the trial reaches this time limit it stops and a final analysis is performed.\n\n\nMax number of participants\nYou may limit the total number of subjects that can be enrolled to the platform. If the trial reaches this number of subjects it stops accruing, follows up subjects, and performs a final analysis.\n\n\nMax successful treatments\nYou may stop the platform when a specified number of successful treatments have been found. This is useful for platfroms only interested in finding a specific number of successful treatments.\n\n\nMax participants per treatment\nEach arm is only allowed a specified maximum number of subjects to be enrolled to them. When an arm reaches this cap, it stopps accruing new subjects, but continues collecting follow-up on those that have been accrued. At full follow-up the arm specific final analysis is performed.\n\n\nMax concurrent treatments\nThis option allows you to specify the maximum number of non-control treatments that can be enrolling during the platform. If there are fewer than this number of active arms enrolling, then new arms are allowed to enter the platform. If there are already arms randomizing equal to the value provided here, then no new treatment arms are allowed to join the platform. Treatment arms that are ready to begin enrolling, but are not allowed to start, begin a waiting period.\n\n\nResponse\nSpecify whether a higher response indicates the subject improving or worsening (and thus whether a higher or lower mean response is a good thing).\n\n\n\nSchedule of Post Baseline Visits\nEnter the time, in weeks, it takes to observe the final response for a subject after enrollment. No visit schedule is currently available, since longitudinal modeling has not been included in FACTS Platform Trials.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#trial-arms",
    "href": "documentation/v71/userguides/platform.html#trial-arms",
    "title": "Platform Trials",
    "section": "Trial Arms",
    "text": "Trial Arms\n\nDefinition\nThis sub-tab is used to specify the set of arms that will be considered for participation throughout the course of the platform trial. Not all of these arms must accrue subject in each simulated trial, and which arms do accrue subjects depends on the state of each individual trial.\n\n\n\n\n\n\nFigure 5: The Trial &gt; Trial Arms &gt; Definition tab.\n\n\n\nIn the current implementation, a control arm is required.\nClicking the “Add” button creates a new active arm. Selecting a row, and clicking “Delete Arm” will remove the selected active arm. Clicking “Clear all arms” will delete all active arms from the table. The arms names can be changed in the table.\nArms are considered independent in the platform trial engine, so Index (d) ordering is irrelevant.\nThe number of arms defined here determines the maximum number of treatments that can enter the simulated trials. If the simulation does not stop through meeting one of the optional maximums that can be specified on the Trial &gt; Trial Info tab, then it will stop when the last treatment from this list to complete enrollment completes the follow-up of the last patient allocated to it.\n\n\nArrivals\nThe arrivals sub-tab is used to specify the timing for treatment arms becoming available in the trial simulation. In the current implementation, the control arm is always available starting at time 0. Multiple arrival schedules may be set up by adding multiple profiles.\n\n\n\n\n\n\nFigure 6: The Trial &gt; Trial Arms &gt; Arrivals tab.\n\n\n\nFor each arm defined on the Trial Arm &gt; Definition tab, the Arrivals tab requires 3 inputs.\n\nEarliest/Latest Possible Arrival (wks)\nThese column specify the range of times (in weeks) that an arm becomes available to enter the trial. For a given simulation, the actual arrival time is simulated uniformly between the earliest and latest possible times. To simulate a trial with an arm that always enters at the exact same time provide the same value for the earliest and latest possible arrivals for the arm.\nOnce an arm has reached its time of arrival, it is allowed to enter the trial if:\n\nThere are less than ‘max concurrent treatments’ currently enrolling in the trial, and\nThe trial is performing:\n\nupdates at milestones,\nupdates on a regular schedule and it is executing an update,\nupdates on a regular schedule and arms can enter between updates.\n\n\n\n\nWithdrawn If Not Used Within\nThis column specifies how long an arm will wait to begin enrolling in the trial before it “gives up” and withdraws from the trial. That is, if the arm becomes available while there are already more active treatment arms than the max concurrent treatments (as specified on the Trial &gt; Trial Info tab), then the arm must wait before being allowed to enter the trial. In that case, if no arm leaves the trial before the end of the waiting period, the waiting arm will withdraw and never enter the trial.\nIn the case of a tie in arrival times, the lower-indexed arm is entered into the trial first.\nTo create a trial in which arms never “time out” and withdraw from the study, enter a very large value in the Withdrawn if not used within column.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#variants",
    "href": "documentation/v71/userguides/platform.html#variants",
    "title": "Platform Trials",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only two design features that can be changed are\n\nthe maximum sample size per treatment.\nThe maximum number of concurrent treatments at any time.\n\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Maximum Participants per Treatment” and “Max Concurrent Treatments” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\n\n\n\n\n\n\nFigure 7: The Variants tab for platform trials.\n\n\n\nIn the above screenshot, 6 variants have been created testing 3 x 2 combinations of Max participants per treatment of 80, 90 and 100 and Max concurrent treatments of 3 and 4. These will override the values for these parameters that have been specified on the Trial Info tab. It there are for example 2 response profiles to simulate, this will give 6 x 2 scenarios to simulate:\n\n\n\n\n\n\nFigure 8: The simulation tab showing 6 different variants creating extra scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#treatment-classification",
    "href": "documentation/v71/userguides/platform.html#treatment-classification",
    "title": "Platform Trials",
    "section": "Treatment Classification",
    "text": "Treatment Classification\nMany outputs and graphs depend on a classification of the treatment effect as “good,” “mediocre,” or “unacceptable.” The thresholds provided on this tab are used in this classification. These treatment effect thresholds are applied to the true effect size for the simulation. For fixed effects, a treatment arm will be categorized the same in all simulations, but if sampled from a distribution, its categorization may vary from simulation to simulation. This categorization is used for some operating characteristic output such as “The number of good arms that were successful” and plots.\n\n\n\n\n\n\nFigure 9: The Virtual Response &gt; Treatment Classification tab.\n\n\n\nAn example of how this can be important is considering a quantity like power: the probability of success given the treatment has some assumed level of effect. When the treatment effect is assumed to have some fixed value this calculation is easy. When the treatment effect is drawn from a distribution, sometimes the effect is strong, but sometimes it’s weak or even null/negative. Creating a classification system allows for the replacement of a traditional “power” with the probability of success given that the arm’s efficacy falls into the “good” class.\nIt’s still typical to consider an arm’s Type I error to be the probability of success given the arm has a fixed efficacy exactly equal to the control arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#response-specification",
    "href": "documentation/v71/userguides/platform.html#response-specification",
    "title": "Platform Trials",
    "section": "Response Specification",
    "text": "Response Specification\nThis tab allows for the specification of the efficacy assumptions for the control and treatment arms in the platform. It is expected that multiple VSR scenarios will be created, so that the design can be simulated across various treatment effect assumptions.\nFor a continuous endpoint, the control arm can have a known mean response or a distribution of potential control rates. The standard deviation of the final response is always assumed as a fixed value. The treatment effects for a continuous trial can be specified as known means, known changes from the control arm, or simulated from a distribution of effects.\nFor a dichotomous endpoint, the control arm can have a fixed assumed response rate or a response rate drawn from a distribution. The treatment arms can be specified has response rates, change from control on the probability scale, change from control on the log-odds scale, or have a change from baseline drawn from a distribution.\n\nArm Response\nThis sub-tab allows the user to set up several scenarios for how the true response to be simulated for each treatment arm.\n\nContinuous EndpointDichotomous Endpoint\n\n\nTwo types of inputs are needed for a given scenario:\n\nThe mean response for each treatment arm. There are three ways that the mean response can be specified.\n\nFixed mean response. This specifies the actual mean value for the arm in the Value column. This option is somewhat redundant with the fixed mean effect option, except in the case when the control arm is sampled from a distribution.\nFixed mean effect. This specifies the difference between the treatment arm and the control arm in the Value column. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative effect value.\nThis option is not available for the control arm.\nA distribution. Rather than specifying a specific value for the mean value, a distribution of values can be chosen. Each simulated trial will then have a (possibly) different value for the mean response for the arm. Distributions will be available for selection in the drop-down menu for the treatment arms, with one option for each distribution profile set up on the Virtual Response &gt; Explicitly Defined &gt; Treatment Distribution sub-tab. If a distribution is chosen, the value in the Value column is ignored.\nIf this selection is made for the control arm, the selection is called “Sampled”, and the distribution is specified on this tab, in the “Control Sampled Mean Response” box.\n\n\n\n\n\n\n\n\nContinuous Response Simulation\n\n\n\nIf a baseline value is not being simulated, the final response simulated is equivalent to ‘change from baseline,’ where baseline is always assumed to be 0. If baseline is being simulated, the user can select whether the response to be analyzed is the change from baseline or absolute response (the option is selected on the Study tab). Depending on that selection, the response specified on this tab is either change from baseline or absolute response.\n\n\n\nThe standard deviation of the response – either through a common SD of response for all treatment arms, or by specifying the standard deviation for the response on each treatment arm separately. [Note that the total variance in the observed final responses can be greater than this if a baseline adjustment for subject response is specified]\n\n\n\n\n\n\n\nFigure 10: The Virtual Response &gt; Response Specification &gt; Arm Response tab for a continuous endpoint.\n\n\n\n\n\nThis sub-tab differs for a dichotomous endpoint in that there are options for how to specify the treatment effect, and no specification of a standard deviation or baseline value is required. The response rate must be specified for each arm. There are four ways that the mean response can be specified.\n\nFixed response rate. This specifies the actual response rate for the arm in the Value column. This option is somewhat redundant with the fixed effect option, except in the case when the control arm is sampled from a distribution.\nFixed effect (rate). This specifies the response rate difference between the treatment arm and the control arm in the Value column. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative effect value. If an effect size would cause a value to be outside of the range [0, 1], then it is set at the appropriate boundary.\nThis option is not available for the control arm.\nFixed effect (log-odds). This specifies the Value column as the difference between the logit of the response rate for the treatment arm and logit of the response rate for the control arm.\nThis option is not available for the control arm.\nA distribution. Rather than specifying a specific value for the response rate, a distribution of values can be chosen. Each simulated trial will then have a (possibly) different value for the response rate for the arm. Distributions will be available for selection in the drop-down menu for the treatment arms, with one option for each distribution profile set up on the Virtual Response &gt; Explicitly Defined &gt; Treatment Distribution sub-tab. If a distribution is chosen, the value in the Value column is ignored.\nIf this selection is made for the control arm, the selection is called “Sampled”, and the distribution is specified on this tab, in the “Control Sampled Response Rate” box.\n\n\n\n\n\n\n\nFigure 11: The Virtual Response &gt; Response Specification &gt; Arm Response tab for a dichotomous endpoint.\n\n\n\n\n\n\n\n\nTreatment Distribution\nThis tab is used to set up distribution profiles, so that the arm response is drawn randomly for each simulated trial. Any distribution profile that is added on this sub-tab will appear as an option in the drop-down menu for the Mean Fixed/Sampled column in the Arm Response sub-tab. There is an auto created treatment distribution created by default on this tab, so “Sampled” from a distribution can be selected on the Arm Response tab before visiting the Treatment Distribution tab.\n\nContinuousDichotomous\n\n\nFor each distribution profile, three components must be established by the user:\n\nResponse vs Effect Size. This selection determines whether the value being simulated is the actual mean response or a difference from the control arm response. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative value if Effect Size is the selection.\nProb same as control / Prob effect size is 0: The distribution that is simulated will be a mixture distribution with a point mass on the value of the control arm, and the remaining probability assigned to a continuous distribution. This value specifies the probability assigned to the point mass. Values of 0 are allowed, meaning that only the continuous distribution would be utilized.\nThe distribution for the continuous portion of the mixture. Currently, three distributions types are available.\n\nThe normal distribution, which requires the specification of the mean and standard deviation of the distribution.\nThe truncated normal distribution, which requires the specification of the mean and standard deviation of the (non-truncated) distribution, along with the minimum and maximum values that are to be allowed. Note: if the maximum (minimum) value is left blank in the gui, the value is interpreted as infinity (negative infinity), allowing for one-sided truncation.\nThe generalized beta distribution, which requires the specification of the minimum, maximum, mean, and standard deviation. Note that not all combinations of values lead to a valid specification of the beta distribution. The mean must be between the minimum and maximum, and the standard deviation must then be no larger than:\n\\[\\sqrt{(Maximum - Mean) \\bullet (Mean - Minimum)}\\]\n\n\n\n\n\n\n\n\nFigure 12: The Virtual Response &gt; Response Specification &gt; Treatment Distribution tab for continuous endpoints showing specification of a distributional treatment effect.\n\n\n\n\n\nThis tab is used to set up distribution profiles, so that the arm response rate is drawn randomly for each simulated trial. Any distribution profile that is added on this sub-tab will appear as an option in the drop-down menu for the Response Rate Fixed/Sampled column in the Arm Response sub-tab. The options here are somewhat expanded relative to the continuous endpoint version. The user must specify three components:\n\nResponse Rate, Effect (Response Rate), Log-Odds, Effect (Log-Odds). This selection determines both whether the value being simulated is: a) the rate or log-odds (logit of rate) and b) the actual value or an effect - difference from the control arm response. Note: FACTS does not use the convention that positive effects always indicate improvement. I.e., if “Lower response is improvement” is specified on the Trial &gt; Trial Info tab, an effective arm should have a negative value if Effect Size is the selection.\nProb same as control / Prob effect size is 0: The distribution that is simulated will be a mixture distribution with a point mass on the value of the control arm, and the remaining probability assigned to a continuous distribution. This value specifies the probability assigned to the point mass. Values of 0 are allowed, meaning that only the continuous distribution would be utilized.\nThe distribution for the continuous portion of the mixture. Currently, three distributions types are available.\n\nThe normal distribution, which requires the specification of the mean and standard deviation of the distribution – available only for if log-odds or effect (log-odds) is selected in step 1.\nThe truncated normal distribution, which requires the specification of the mean and standard deviation of the (non-truncated) distribution, along with the minimum and maximum values that are to be allowed. Note: if the maximum (minimum) value is left blank in the gui, the value is interpreted as infinity (negative infinity), allowing for one-sided truncation. Allowed for all selections in step 1.\nThe generalized beta distribution, which requires the specification of the minimum, maximum, mean, and standard deviation. Note that not all combination of values lead to a valid specification of the beta distribution. The mean must be between the minimum and maximum, and the standard deviation must then be no larger than:\n\\[\\sqrt{(Maximum - Mean) \\bullet (Mean - Minimum)}\n\\]This option is allowed for all selections in step 1.\n\n\nNote that if the selection in step 1 is Effect (Response Rate), it is possible to specify a distribution that has support beyond the range of [0, 1] for the response rate (particularly if the control rate is sampled, since the control rate for any simulation may draw a rate near the boundary). In this case, the distribution in truncated to ensure a rate in the range [0, 1].\n\n\n\n\n\n\nFigure 13: The Virtual Response &gt; Response Specification &gt; Treatment Distribution tab for dichotomous endpoints showing specification of a distributional treatment effect on the probability scale.\n\n\n\n\n\n\n\n\nBaseline (Continuous Only)\n\nResponse is Change from Baseline\nIf the endpoint is continuous, then there will be a Baseline sub-tab within the Response Specification tab.\nIf simulation of baseline has been included on the Study &gt; study Info tab, if response is change from baseline then the explicitly defined dose response is still in terms of change from baseline, and a new virtual subject response tab is available for specifying the baseline score.\n\n\n\n\n\n\nFigure 14: The Virtual Response &gt; Response Specification &gt; Baseline tab showing the specification of a baseline VSR with a baseline adjustment to the response.\n\n\n\nThe simulation of distribution of baseline scores is specified using a normal distribution –with user specified mean and standard deviation and optionally applied upper and lower bounds to reflect limitations on the score range or screening criteria. Note that if the observed baseline score is truncated, then the true mean and SD of the baseline are likely to be different from these values of the mean and SD which are before truncation.\nEither baseline is simulated separately from the final response (for use with BOCF), or the simulation of the final response can include a baseline dependent element.\nIf adjusting the final response based on baseline, then the user selects “Use Baseline Adjustment for Subject Response” and supplies 3 parameters:\n\n\\(\\beta\\)\n\na coefficient that reflects the degree of influence of baseline on final score and the degree of variability in the final score due to baseline.\n\nc\n\na centering offset, typically the expected mean of the observed baseline scores\n\ns\n\na scaling element, typically set to the expected SD of the baseline.\n\n\nExample – in the above screenshot a baseline of mean 25 and SD 10 has been specified – so a centering of 25 and scaling of 10 is used. Wishing to simulate an overall SD of 5 in the final change from baseline and apportion two-thirds the variance to baseline, Beta has been set as follows:\n\nThe desired final variance is 25 (52), divided between 1/3rd dose response and 2/3rd baseline effects.\nThe SD of the simulated response is set to 2.89 \\(\\sqrt{\\left( 25*\\frac{1}{3} \\right)}\\)\nThe SD of the scaled baseline score is 1, so to contribute half the final variance of 25, Beta is set to 4.08 \\(\\sqrt{\\left( 25*\\frac{2}{3} \\right)}\\)\nNote that when simulating a baseline effect in this way, limiting the range of baseline by specifying upper and lower cut-offs – which might be natural limits of the endpoint, or due to inclusion / exclusion criteria in the protocol – can significantly reduce the variance in the final endpoint due to the baseline effect.\n\n\n\nResponse is Final endpoint\nIf the response is specified to be final endpoint score, then baseline is specified as above, but the explicitly defined dose response is now defined in terms of final endpoint.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#dropout-rate",
    "href": "documentation/v71/userguides/platform.html#dropout-rate",
    "title": "Platform Trials",
    "section": "Dropout Rate",
    "text": "Dropout Rate\nThe probability of a subject dropping out before observing their final endpoint value is provided by arm. There is no longitudinal model, so Dropouts Per Arm Per Visit cannot be selected.\n\n\n\n\n\n\nFigure 15: The Virtual Response &gt; Dropout Rate tab, specifying the dropout rate per arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#control-response",
    "href": "documentation/v71/userguides/platform.html#control-response",
    "title": "Platform Trials",
    "section": "Control Response",
    "text": "Control Response\nThis sub-tab is used to set up the normal prior distribution for the control arm. There are two separate priors to create - one for the all participants model, and one for the concurrent controls only model.\nIn the all participants model, only the prior mean and standard deviation for the shared control arm must be entered. The treatment effect priors are specified on the Treatment Response tab. All arms are modeled independently in the platform trials engine.\nAdditionally, the Concurrent Control box allows for the specification of the fixed normal prior for the control arm estimate for all of the individual concurrent controls models. The concurrent control arm esimate for all arms has the same prior. There is also a box to specify a window before the arms begin randomizing in which the randomized control participants should be considered concurrent with an active arm. So, if it’s specified that FACTS should include participants up to “8” weeks from treatment entering trial, then all control subjects randomized between 8 weeks before an arm started randomizing to when an arm stopped randomizing (or the current time if still randomizing) are considered concurrent controls.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#treatment-response",
    "href": "documentation/v71/userguides/platform.html#treatment-response",
    "title": "Platform Trials",
    "section": "Treatment Response",
    "text": "Treatment Response\nThis sub-tab is used to set up the prior distribution for the treatment arms and for the common standard deviation of the response. All treatment means are modeled independently. The user can specify a single normal prior that applies to each treatment arm or specify normal priors individually for each treatment arm.\nThis tab also contains the prior distribution for the standard deviation of the responses. The variance is modeled as an inverse-gamma distribution.\nThis tab also contains the method for handling missing data due to dropouts. Since longitudinal modeling is not yet implemented, the only method that currently applies is BOCF, if the endpoint is continuous and baseline data is simulated. Without longitudinal modeling to incorporate post-baseline visit data, the Bayesian multiple imputation from post-baseline method will ignore dropouts.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#allocation",
    "href": "documentation/v71/userguides/platform.html#allocation",
    "title": "Platform Trials",
    "section": "Allocation",
    "text": "Allocation\nAs long as “Enable Adaptive Features” is selected on the Trial &gt; Trial Info tab, a radio button at the top of the Design &gt; Allocation sub-tab allows choice between the two options for specifying arm allocation in the Platform Trial simulator.\n\nFixed Allocation\nIf Fixed Allocation is selected, then the blocking for randomization can either be done by creating a control:treatment block for each active arm, or by specifying a specific block size based on the number of arms currently randomizing. A radio button allows a choice between the two options.\n\n\n\n\n\n\nFigure 19: The fixed allocation tab for the platform trials engine.\n\n\n\n\nConstant proportion allocated to control\nFor this option, the proportion of participants that are allocated to the control arm remains constant, regardless of the number of treatment arms that are active in the trial. The user specifies the number of control participants and treatment participants per sub-block. A full block consists of a number of sub-blocks corresponding to the number of active treatment arms being allocated. Within each sub-block, the number of control participants is fixed, while the participants for any particular active treatment arm may be spread across the sub-blocks.\nAs an example, suppose the inputs for constant proportion allocated to control are\n\nAllocation to control per sub-block = 2\nAllocation to treatments per sub-block = 6\n\nThen, “Sub-block size per treatment” is calculated to be 8. Suppose that there are currently 3 active arms accruing and 1 control arm.\nWhen randomizing participants, the blocks that subjects allocations are drawn from are then a total size of \\(8*4=32\\). It is guaranteed that each sub-block of 8 within the total 32 will have exactly 2 control. The other 24 slots in the block are completely suffled, so there is no guarantee that any number of a non-control arm will be contained within the sub-blocks.\n\n\nAllocation dependent on the number of treatments\nFor this option, the proportion allocated to the control arm is allowed to vary depending on the number of active treatment arms in the trial. For each potential number of active treatment arms, the user specifies the allocation to each treatment and control by specifying Y and X for that allocation Y:Y:…:Y:X. That is, each treatment arm gets an equal number of participants per block, while the control has a (possibly) different allocation. The entire block of size X + Y*(# treatments) is then shuffled.\n\n\n\nAdaptive Allocation\nIf Adaptive Allocation is selected, then the blocking for randomization can either be done by creating a block with a fixed proportion of controls and the same number of adaptively randomized slots per block, or by specifying a specific block size based on the number of arms currently randomizing. A radio button allows a choice between the two options.\nA radio button allows a choice between two options for adaptive allocation.\n\nConstant proportion allocated to control\nThis option works in the same way as the fixed allocation constant proportion allocated to control, except that the non-control parts of the block are randomly assigned to active treatment arms based on the response-adaptive randomization probabilities.\n\n\nAllocation dependent on the number of treatments\nFor this option, the proportion allocated to the control arm is allowed to vary depending on the number of active treatment arms in the trial. For each potential number of active treatment arms, the user specifies the allocation to treatments and control for each block by specifying Y and X for that allocation Y:X of treatment to control. In this case (differing from the fixed allocation version), Y is the total allocation to treatment arms that are being adaptively allocated to. Notice that this Y:X differs from the Y:Y:…:Y:X pattern used for fixed allocation.\n\n\nRAR Probabilities\nThe response-adaptive randomization (RAR) follows the same scheme as FACTS Core with respect to specifying the quantity(ies) of interest to use in determining allocation by arm. However, one important difference is the definition of the burn-in period.\nUnlike FACTS Core, the burn-in period applies to each treatment arm separately rather than to the beginning of the trial.\nThe fixed allocation period for each treatment arm is specified at the bottom of the Allocation sub-tab. A treatment arm is not part of the RAR scheme until this number of participants have been enrolled and an analysis has been run to update the relevant QOIs. This latter aspect is distinct from FACTS Core, where an analysis is always run as soon as the allocation burn-in period has completed. The user may need to consider this aspect when determining how interim timing is to be set up. While in the burn-in phase, a treatment arm is allocated a fixed 1/T ratio of the adaptive allocation slots, where T is the number of active treatment arms.\n\n\n\n\n\n\nFigure 20: The Design &gt; Allocation tab with an Adaptive Allocation target specified.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#interims",
    "href": "documentation/v71/userguides/platform.html#interims",
    "title": "Platform Trials",
    "section": "Interims",
    "text": "Interims\nThe Interims sub-tab is used to specify the timing of trial update analyses and to specify which set of stopping criteria (if any) are to be applied at the update.\n\n\n\n\n\n\nFigure 21: The Interims tab with information specified as opportunity to complete, updates every 4 weeks, follow-up before a final analysis if the decision for the arm is success, and decision milestones at 30, 55, and 80 subjects with the opportunity to complete.\n\n\n\n\nUpdate Frequency\nTrial updates can be specified by time, information, a mixture of time and information, or by milestones reached.\nInformation is defined in the same way as FACTS Core, either by number of participants enrolled, number of participants with complete data (at a particular visit when longitudinal modeling is used), or number of participants who have had the opportunity to provide complete data (at a particular visit when longitudinal modeling is used). This definition of information is used for milestones as well as (optionally) update timing.\nThere are two distinct modes for trial update timing. A radio button toggles between “Updates occur whenever a treatment milestone is met” and “Updates occur on a regular schedule”. The former option is only available if Fixed Allocation is chosen on the Design &gt; Allocation sub-tab.\n\nUpdates occur whenever a treatment milestone is met\nIn this mode, the only time an analysis is performed is when a treatment arm reaches a milestone – i.e. when a particular level of information has been reached, as specified in the Treatment Milestones portion of the sub-tab. The analysis will include all patients enrolled up to that point in time and provide model output for all arms, but decisions will be made only for the treatment arm whose milestone triggered the analysis. An analysis will also occur if a treatment arm has reached full follow-up on its maximum number of enrolled participants.\n\n\nUpdates occur on a regular schedule\nIn this mode, analyses are performed on a regular schedule based on time or information at the trial level (based on all participants). The timing of the first update can be specified as either time from the start of the trial or a fixed level of information. If adaptive allocation is being utilized, it may begin after this first analysis, provided that at least one individual treatment arm burn-in has been reached. After the first update, updates can be specified to occur at either fixed time intervals or after a fixed incremental level of information has accrued.\nNote that in this mode, it is possible for treatment milestones to be skipped. E.g. if milestones were set up to occur at enrollment of 30, 55, and 80 participants, it is possible that a treatment arm could have 20 participants enrolled at one trial update and 60 participants at the next. If this were the case, any decisions associated with milestone 1 would be skipped and milestone 2 decisions would be evaluated instead.\nAn exception to the regular schedule is that an analysis is always performed whenever a treatment arm has reached its time for a final analysis. When this happens, an “off-schedule” trial update is performed, but no actions are taken except for the declaration of success or futility for the completed arm.\n\n\n\nSubject Follow-up Options\nThe follow-up options mimic those of FACTS Core. However, in the Platform Trial simulator, the follow-up decision applies only to those participants that were allocated to the arm that is being stopped. Control participants and participants on other treatment arms continue to be followed.\n\n\nTreatment Milestones\nTreatment milestones are benchmarks that a treatment can reach that determine which success/futility fules should be used on the arm at that point in time. Before an arm has reached a milestone, there are no success or futility rules for that arm to evaluate. Once it reaches the first milestone, any time its status is evaluated it will use the set of success/futility rules that correspond to the most recent milestone that has been reached.\nIf updates occur every time a treatment milestone is met, then there is an update performed immediately every time an arm crosses a milestone threshold. This update is specific to the arm reaching the threshold, and while all arm data is used in the analysis models, only the arm that reached the threshold can make a decision at this milestone based update.\nWhen updates are regularly scheduled, an update triggers based on the specified rules, and all arms have an opportunity to make a decision as long as they have reached at least the first milestone. Each arm keeps track of its own milestone benchmark and uses its own rules based on its progress through the platform. When regularly scheduled updates are selected, further input from the user is required in the Treatment Milestones box to dictate if milestone specific criteria can be assessed many times, or only once.\n\nEvaluate milestone criteria at each update\n\nWith this selection, decisions associated with a particular milestone may be evaluated more than once. E.g., suppose milestones occurred at enrollment of 30 and 55 participants, and at a trial update with 32 participants enrolled, an early futility decision was evaluated. At the next trial update, if only 50 participants are enrolled, the early futility decision associated with milestone 1 would be re-evaluated with the updated model results.\n\nEvaluate milestone criteria when milestone first reached\n\nWith this selection, a given milestone can be evaluated at most once – the first time it is reached (if a higher milestone hasn’t already been reached). In the example above, there would be no futility evaluation at 50 participants because the milestone had already been evaluated to 32 participants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#successfutility-criteria",
    "href": "documentation/v71/userguides/platform.html#successfutility-criteria",
    "title": "Platform Trials",
    "section": "Success/Futility Criteria",
    "text": "Success/Futility Criteria\nThe success/futility criteria sub-tab is used to specify the rules for early stopping and final evaluation. The basic structure of defining criterion with a QOI, a direction, and a threshold follows the conventions used in FACTS Core. However, the Platform Trial Simulator has some differences and additions, as outlined below.\n\n\n\n\n\n\nFigure 22: The Success/Futility Criteria tab, specifying the milestone 1 criteria for all subjects.\n\n\n\nEarly stopping criteria can be set up for each milestone defined on the Design &gt; Interims sub-tab. Use the Create button to add a sub-tab for early stopping for a chosen milestone. There are check-boxes to indicate if early futility or early success should be evaluated at the milestone. If the same decision criteria are to be used for multiple consecutive milestones, then only the first in the series needs to be created, and subsequent milestones will use the criteria until a new milestone decision rule applies. E.g., if the first and second milestones use early futility with the same stopping criteria, while early success is not allowed until the third milestone, the user need only set up the Milestone 1 and Milestone 3 criteria. For convenience, the “Copy from” button can be used to copy all criteria from a different milestone. The Final Evaluation criteria are applied for a treatment arm only if:\n\nthe treatment arm has enrolled participants and followed up its maximum number of participants, or\nthe treatment arm had previously hit an early stopping criteria and has completed all expected follow-up on its participants.\n\nOne fundamental difference from FACTS Core in setting up a decision criterion is that all QOIs are available, not the scalar “Decision QOIs” that FACTS Core uses. The value utilized is always the one corresponding to the treatment arm whose milestone is being evaluated.\nThe Platform Trial simulator also allows for differential specification of evaluation criteria by treatment arm. Any criteria that are specified on the “All Treatments” sub-tab apply to each treatment arm, and only on this sub-tab can the checkboxes for early stopping be checked. The combination of “All Treatments” criteria are combined with the criteria for an individual treatment in the following way.\n\nThe criteria on the “All Treatments” tab are combined with the AND/OR as specified on that tab to determine an “All Treatments” TRUE or FALSE.\nThe criteria on the individual treatment arm’s tab are combined with the AND/OR as specified on that tab to determine an individual treatment arm TRUE or FALSE.\nThe “All Treatments” and individual treatment results are then combined differently for early success or futility:\n\nFor success, the two are combined with an AND – both must be met (as typically a specific treatment arm would be allowed to have a stricter rule for success but not a laxer rule).\nFor futility, the two are combined with an OR – either can be met (as typically a specific treatment arm might be allowed more leeway to withdraw from a trial).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#simulation-options",
    "href": "documentation/v71/userguides/platform.html#simulation-options",
    "title": "Platform Trials",
    "section": "Simulation Options",
    "text": "Simulation Options\n\nNumber of simulations\nThis box allows the user to enter the number of simulations that they would like FACTS to run for each scenario listed in the table at the bottom of the simulation tab. There is no set number of simulations that is always appropriate.\n\n10 simulations\n\nYou might want to run 10 simulations if you just want to look at a few simulated trials and assess how the decision rules work and if FACTS is simulating what you expected based on what you input on the previous tabs. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\n\n100 simulations\n\nYou might want to run 100 simulations if you want to look at many individual trials to make sure that what you want to happen is nearly always happening. You can also start to get a very loose idea about operating characteristics like power based on 100 sims. 100 simulations is also usually sufficient to spot big problems with the data analysis such as poor model fits or significant bias in the posterior estimates.\n\n1,000 simulations\n\nYou might want to run 1,000 simulations if you want estimates of operating characteristics like power, sample size, and Type I error for internal use or while iterating the design. This generally isn’t considered enough simulations for something like a regulatory submission. With 1,000 simulations the standard error for a typical type I error calculation is on the order of \\(0.005\\).\n\n10,000 simulations\n\nYou might want to run 10,000 simulations per scenario if you are finalizing a design and are preparing a report. This is generally enough simulations for a regulatory submission, especially in non-null simulation scenarios. The standard error for a typical type I error calculation using 10,000 simulations is on the order of \\(0.0015\\).\n\n&gt; 10,000\n\nYou might want to run more than 10,000 simulations if you want to be very certain of an operating characteristic’s value - like Type I error. And plan to use the measurement of the quantity for something important like a regulatory submission. The standard error of a Type I error calculation with 100,000 simulations, e.g., is on the order of \\(0.0005\\).\n\n&gt; 100,000\n\nYou probably don’t want to run more than 100,000 simulations per scenario. Maybe your finger slipped an hit an extra 0, or you thought there were 5 zeroes in that number when there were actually 6. If the simulated trial is adaptive, this is going to take a while.\n\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. Not all scenarios must be run with the same number of simulations. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run.\n\n\nStart at Simulation\nThe “Start at simulation” option allows for the simulation of a particular trial seen in a previous set of simulations without having to simulate all of the previous trials in that previous set to get to it.\nThe initial random seed for FACTS simulations is set in the simulation tab. The first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. Thus, it is possible to re-run a specific simulation out of a large set without re-running all of them. For example, say the 999th simulation out of a set displayed some unusual behavior, in order to understand why, one might want to see the individual interim analyses for that simulation (the “weeks” file), the sampled subject results for that simulation (the “Subjects” files) and possibly even the MCMC samples from the analyses in that simulation. You can save the .facts file with a slightly different name (to preserve the existing simulation results), then run 1 simulation of the specific scenario, specifying that the simulations start at simulation 999 and that at least 1 weeks file, 1 subjects file and the MCMC samples file (see the “MCMC settings” dialog) are output.\n\nThe parallelization packet size, this allows simulation jobs to be split into runs of no-more than the specified number of trials to simulate. If more simulations of a scenario are requested than can be done in one packet, the simulations are started as the requisite number of packets and the results combined and summarized when they are all complete – so the final results files look just as though all the simulations were run as one job or packet. FACTS tries to set a sensible default size given the overall number of simulations to be run.\nWhen running simulations on the local machine FACTS enterprise version will process as many packets in parallel as there are execution threads on the local machine. The overhead of packetization is quite low so a packet size of 10 to 100 can help speed up the overall simulation process – threads used to simulate scenarios that finish quicker can pick up packets for scenarios that take longer, if the number of scenarios is not directly divisible by the number of threads packetization uses all threads until the last few packets have to be run and finally the “Simulations complete” figure can be updated at the end of each packet, so the smaller the packet the better FACTS can report the overall progress.\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run.\n\n\nParallelization Packet Size\nThe parallelization packet size option allows simulation jobs to be split into runs of no-more than the specified number of trials that are run in parallel. If more simulations of a scenario are requested than can be done in one packet, the simulations are broken into the requisite number of packets, run, and combined and summarized when they are all complete. The final results files will look just as though all the simulations were run as one job or packet.\nThe packet size must be a perfect divisor of the number of simulations. This is usually easy since common numbers of simulations are multiples of 100, but don’t try to use a prime number for Number of Simulations or you’re stuck with only 2 packet size options.\nBy default (if the check box with Choose Parallel Packet Size is not checked) the number of simulations per packet depends on the number of simulations per scenario. If the number of simulations is less than 1000, then each scenario is packaged as a single packed and simulated. If the number of simulations per scenario is greater than or equal to 1000, the default packet size is 10 and all simulations are decomposed into packets of size 10.\n\n\n\n\n\n\nPacketization with VSRs using .mvsr files\n\n\n\nIf an external file is used to create explicit VSRs (a .mvsr file), then the packet size should be a multiple of the number of rows in that MVSR file. Each packet will get passed the entire .mvsr file to run. If there are multiple .mvsr files with differing numbers of lines then only the VSR scenarios that have a .mvsr file that has a number of rows that is a divisor of the packet size will be run. The rest will error. The packet size can then be modified to get each of the .mvsr specified VSR files to be run.\nCare should be taken when packetizing a scenario that includes an external data file to supply the virtual subject responses; in this situation, a of copy of the external file is included in each packet which can cause the packetisation process to run out of memory as the packets are being created. In this case, use a smaller number of larger packets, such as packets that are 1/10th of the total number of simulations.\n\n\nWhen running simulations, FACTS will create and run as many packets in parallel as there are execution threads on the local machine. In general, the overhead of packetization is quite low, so a packet size of 10 to 100 can help speed up the overall simulation process. Threads used to simulate scenarios that finish quickly can pick up packets for scenarios that take longer. The progress bar updates as simulation packets complete, so the smaller the packet size, the more accurately FACTS can report the overall progress of the simulation execution.\n\n\nRandom Seed\nRandom number generation plays a huge role in FACTS’s virtual patient generation and statistical analyses. In order to exactly reproduce a statistical set of results, it is necessary to start the random number generation process from an identical “Random Seed”. Using the same random seed in the same version of FACTS guarantees that simulated trials will always be reproducible. Changing the design parameters or the version of FACTS may or may not remove this reproducibility depending on the change.\nEven a small change in the random seed will produce very different simulation results.\nIn addition to setting the seed, the user can choose whether they want the “Same seed for all scenarios” or “Different seed” for different scenarios. If “Same seed for all scenarios” is selected, the subjects generated for each simulated trial will match for the different scenarios. This induces a correlation among the simulation output for different scenarios. This can be good if you’re trying to compare operating characteristics for different scenarios, but it can also be misleading. To disable this option select the “Different Seed” option. If “Different seed” is selected, then each scenario has its own seed that samples a different set of subjects than any other scenario. This uncorrelates the simulation output across scenarios, which can be advantageous if the absolute value of the operating characteristics are more valuable to you than the comparison of operating characteristics across scenarios.\n\n\nMCMC Settings\nTo set advanced settings for simulation, the user may click the “MCMC Settings” button, which will display a number of additional specifiable parameters for simulation in a separate window.\n\n\n\n\n\n\nFigure 24: Advanced MCMC settings.\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the MCMC chain to reach its equilibrium distribution.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest.\n\nThe third parameter controls the number of MCMC samples taken after each imputation of missing data using the longitudinal model. Since there is no longitudinal modeling in the Platform Trials engine currently, this parameter does nothing. \nThe next parameter concerns the output of the MCMC samples to a file. It is possible to have the design engine output the sampled values from the MCMC in all of the interims of the first N simulated trials of each scenario by specifying the “Number of MCMC files to output” to be greater than 0. The resulting files, ‘mcmcNNNN.csv’, will be in the results directory with all the other results files for that scenario. These files include the burn-in samples from the MCMC chains.\nThe final parameter in MCMC Settings is the thinning parameter. This parameter will only keep every \\(N^{th}\\) sample taken during MCMC where \\(N\\) is the thinning parameter. Thinning MCMC samples can reduce the autocorrelation of consecutive MCMC iterations, which increases the effective samples per retained sample, but also results in needing many more MCMC iterations to reach the same number of retained samples. Generally, we do not recommend thinning for standard simulation runs.\n\n\n\n\n\n\nWarning about thinning\n\n\n\nUnlike other software that performs MCMC, when you choose to thin by a value, FACTS does not increase the number of MCMC iterations it performs in order to retain the value specified in “Number of Samples”. So if you leave “Number of Samples” at its default value, \\(2500\\), and thin by \\(10\\), you will be left with \\(250\\) retained samples. You should adjust for this by increasing the “Number of Samples” if you choose to thin.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#results-output",
    "href": "documentation/v71/userguides/platform.html#results-output",
    "title": "Platform Trials",
    "section": "Results Output",
    "text": "Results Output\nThe results output section of the Simulation tab allows for the specification of how many output files should be generated for files that are individually created for each simulation. Summary files (summary.csv) that have 1 line per scenario are always created. Simulations files (simulations.csv) that have 1 line per simulation are always created. Weeks files (weeksXXXXX.csv), patients files (patientsXXXXX.csv), and frequentist weeks files (weeks_freq_{missingness}_XXXXX.csv) are not created for every single simulation. Instead, the number of simulation specific output files can be set per type. This limits the amount of output files that FACTS will save.\nSome plots in FACTS that are created based on weeks files, and if very few weeks files are saved, the plots will not be as accurate or descriptive.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#run-simulations",
    "href": "documentation/v71/userguides/platform.html#run-simulations",
    "title": "Platform Trials",
    "section": "Run simulations",
    "text": "Run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user is prevented from modifying any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario.\n\nFACTS Grid Simulation Settings\nA user with access to a computational grid, may choose to run simulations on the grid instead of running them locally. This frees the user’s computer from the computationally intensive task of simulating so that they can continue other work or even shutdown their PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#simulation-results",
    "href": "documentation/v71/userguides/platform.html#simulation-results",
    "title": "Platform Trials",
    "section": "Simulation Results",
    "text": "Simulation Results\nIn the center of the simulation tab, the summary simulation results are displayed. Once simulations have been run the table will be populated with results. The results that are shown by default are highlights of the operating charactersics for each scenario run. FACTS outputs many columns of results, and they are organized into related groups of sub-windows, which can be displayed by clicking on the “Show More Columns” button or right clicking on a row of the table.\n\n\n\n\n\n\nFigure 25: The “Show More Columns” menu on the Simulation tab.\n\n\n\nThese options will open windows that show:\n\n\n\nName\nColumns that will be shown\n\n\n\n\nAll\nShow all available summary columns.\n\n\nHighlights\nShow only the columns shown on the main tab. See below.\n\n\nTiming\nThe columns indicating timing information for treatment arms entry and exit.\n\n\nAllocation\nThe columns that report on participant recruitment and allocation.\n\n\nResponse\nThe columns that report the Bayesian model based estimates of the response for each treatment.\n\n\nObserved\nThe raw endpoint output and the dropout rates by arm and visit.\n\n\nProbabilities\nThe final estimates for the QOIs that were computed for the trial.\n\n\nModel Parameters\nThe columns that report the estimates of the values of the model parameters.\n\n\nSimulation Results\nOpens a window that displays the individual simulation results for that scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results (see below) the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of the Simulation Results highlights window.\n\n\nSimulation Duration\nA window that displays simulation run time information.\n\n\n\n\nRight Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 26: The Right Click menu for the simulations table.\n\n\n\nThese provide access to all the actions available from the “Results Options” – except aggregation. These will respectively:\n\nOpen windows showing the various “Other Columns” options described above\nShow Per Scenario Graphs: Opens the FACTS graph control displaying the graphs for that scenario. See section 14 for details.\nShow Across Scenario Graphs: Opens the FACTS graph control displaying the graphs comparing all scenarios.\nCompare Scenarios in Airship: Opens the R AIRSHIP package for comparing results across scenarios, see the AIRSHIP User Guide for details.\nOpen Results Folder: Open a new Windows directory browser window showing the contents of the simulation results for that scenario.\nCopy to Clipboard: Copies all the rows of the summary to the clipboard, as tab-delimited text.\nOpen in R: Opens R, loading in the result files for the currently selected scenario as separate data-frames (doesn’t load the aggregated files if they exist, for that use the “Open in R” button).\nDesign Report: Create a design report (a word document describing the design), see the Design Report User Guide for details.\n\n\n\nDouble Click\nDouble clicking on a row of simulation results opens a window listing the results of each individual simulation for that scenario. See section 15 below for a description.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#results-options",
    "href": "documentation/v71/userguides/platform.html#results-options",
    "title": "Platform Trials",
    "section": "Results Options",
    "text": "Results Options\n\nExplore Results\nThis button displays the following options:\n\n\n\n\n\n\nFigure 27: The Explore Results button menu.\n\n\n\n\nShow Per Scenario Graphs: Opens the FACTS graph control displaying the graphs for that scenario. See section 14 for details.\nShow Across Scenario Graphs: Opens the FACTS graph control displaying the graphs comparing all scenarios.\nCompare Scenarios in Airship: Opens the R AIRSHIP package for comparing results across scenarios, see the AIRSHIP User Guide for details.\nOpen Results Folder: Open a new Windows directory browser window showing the contents of the simulation results for that scenario.\nCopy to Clipboard: Copies all the rows of the summary to the clipboard, as tab-delimited text.\n\n\n\nOpen in R\nThe “Open in R” button allows for the creation of an R script that has pre-populated code for loading in output files created by the FACTS simulations.\nBy default, any/all of the simulation output files can be included in the created script. If “Aggregation” (see below) has been performed, then only the aggregated files will be available for being loaded in R.\nWhen the button is clicked, FACTS will create an R script with the correct file paths to load in the data, as well as creating a function that will read the files in correctly. The file is then opened in the default R editor for the user. If there is no default program for opening a .R file, your operating system should ask how you want to open the file.\n\n\nAggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 28: Window that appears when aggregating simulation results.\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nValues in columns that are independent of dose will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. WeeksNNNNN.csv files are aggregated into a single agg_[pivot_]weeks.csv file. PatientsNNNNN.csv files are aggregated into a single agg_patients.csv file, but they are never pivoted because each row already refers to a single dose.\nRegionIndex.csv is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nRecruitment Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g. External Subjects Profile if there are no external scenarios)\n\n\nDropouts Profile\n\n\n\nVirtual Response Profile\n\n\n\nTreatment Arm Arrival Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in weeks and patients files.\n\n\nTreatment\nTreatment Arm. Only present if pivoted.\n\n\n\n\n\nDesign Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#simulation-output-table",
    "href": "documentation/v71/userguides/platform.html#simulation-output-table",
    "title": "Platform Trials",
    "section": "Simulation Output Table",
    "text": "Simulation Output Table\n\nHighlights\nThese are the care completed. They can also be displayed in the separate “Highlights” columns displayed on the simulations tab after simulations results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Tr eatments Started\n1\nThe average number (over the simulations) of non-control treatments that entered the trial and were eligible to enrollment participants.\n\n\nMean Tr eatments Analyzed\n1\nThe average number (over the simulations) of non-control treatments that reached a final analysis within the trial – either because they reached an early stopping decision with no follow-up or because they completed follow-up on all participants.\n\n\nMean Good Tr eatments Analyzed\n1\nThe average number (over the simulations) of “good” treatment arms that reached a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable Tr eatments Analyzed\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that reached a final analysis, “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable Tr eatments Analyzed\n1\nThe average number (over the simulations) of “mediocre” treatment arms that reached a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean S uccesses\n1\nThe average number (over the simulations) of treatment arms that were declared successful at a final analysis.\n\n\nMean Good T reatment S uccesses\n1\nThe average number (over the simulations) of “good” treatment arms that were declared successful at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable T reatment S uccesses\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared successful at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre T reatment S uccesses\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared successful at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Fu tilities\n1\nThe average number (over the simulations) of treatment arms that were declared futile at a final analysis.\n\n\nMean Good T reatment Fu tilities\n1\nThe average number (over the simulations) of “good” treatment arms that were declared futile at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable T reatment Fu tilities\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared futile at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre T reatment Fu tilities\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared futile at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Incon clusives\n1\nThe average number (over the simulations) of treatment arms that were declared inconclusive at a final analysis.\n\n\nMean Good T reatment Incon clusives\n1\nThe average number (over the simulations) of “good” treatment arms that were declared inconclusive at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Unac ceptable T reatment Incon clusives\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared inconclusive at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre T reatment Incon clusives\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared inconclusive at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn S uccesses | T reatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Fu tilities | T reatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Incon clusives | T reatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn S uccesses | T reatment Unac ceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Fu tilities | T reatment Unac ceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Incon clusives | T reatment Unac ceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn S uccesses | T reatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Fu tilities | T reatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Incon clusives | T reatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Tr eatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable Tr eatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Tr eatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Tr eatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable Tr eatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Tr eatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Tr eatments | Inco nclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable Tr eatments | Inco nclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Tr eatments | Inco nclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn 1+ S uccesses\n1\nThe proportion of simulations that had at least one treatment arm declared successful.\n\n\nPpn Good Tr eatments | 1+ S uccesses\n1\nThe proportion of simulations that had at least one “good” treatment arm declared successful, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Su ccesses: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared successful.\n\n\nPpn Fut ilities: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared futile.\n\n\nPpn Inconc lusives: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared inconclusive.\n\n\nPpn Good T reatment | E nrolled: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable T reatment | E nrolled: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre T reatment | E nrolled: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good T reatment | A nalyzed: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Unac ceptable T reatment | A nalyzed: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre T reatment A nalyzed: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Duration\n1\nThe average (in weeks) over the simulations of the duration of the trial from the start to completion of the trial.\n\n\nMean First Success Time\n1\nAmongst simulations that had at least one success, the average time (in weeks) at which the first success was declared.\n\n\nMean Part icipants\n1\nThe average number of participants enrolled across all simulations.\n\n\nMean A vailable Time: &lt;Arm&gt;\n1\nThe average time across simulations that the treatment arm became available to enter the trial (regardless of whether the trial ended prior to it becoming available).\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nTiming\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Duration\n1\nThe average (in weeks) over the simulations of the duration of the trial from the start to completion of the trial.\n\n\nMean First Success Time\n1\nAmongst simulations that had at least one success, the average time (in weeks) at which the first success was declared.\n\n\nMean A vailable Time: &lt;Arm&gt;\n1 per arm\nThe average time (in weeks) across all simulations at which the treatment because available for entry into the trial. Note: the available time is reported whether or not it became available after the end of the trial.\n\n\nMean Start Time: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became available to enroll participants, the average time (in weeks) that the arm became eligible for enrollment.\n\n\nMean End Time: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became available to enroll participants, the average time (in weeks) that the arm became stopped enrolling.\n\n\nMean Final Analysis Time: &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis, the average time (in weeks) that the arm’s final analysis occurred.\n\n\n\n\n\nAllocation\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Part icipants\n1\nThe average number (over the simulations) of participants enrolled in the trial.\n\n\nSD Mean Part icipants\n1\nThe standard deviation of the number (over the simulations) of participants enrolled in the trial.\n\n\nMean Alloc.: &lt;Arm&gt;\n1 per arm\nThe average (over the simulations) of the number of participants enrolled onto the arm.\n\n\nSD Alloc.: &lt;Arm&gt;\n1 per arm\nThe standard deviation (over the simulations) of the number of participants enrolled onto the arm.\n\n\n\n\n\nResponse\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Resp.: &lt;Arm&gt;\n1 per arm\nAverage (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nSD Resp.: &lt;Arm&gt;\n1 per arm\nStandard deviation (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nMean Sigma\n1\nAverage (over the simulations) of the estimate of the SD of the dose response across all the treatment arms (‘sigma’)\n\n\nSD Mean Sigma\n1\nStandard deviation (over the simulations) of the estimate of the SD of the dose response across all the treatment arms.\n\n\nTrue Mean Resp: &lt;Arm&gt;\n1 per arm\nTrue mean response from which the simulated participant data was sampled for the arm.\n\n\nSD True Resp.: &lt;Arm&gt;\n1 per arm\nTrue standard deviation of the simulated participant data for the arm.\n\n\nMean Baseline Beta\n\nThis is the mean (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nSD Baseline Beta\n\nThis is the SD (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nMean Baseline\n1\nAverage (over the simulations) of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\nStandard deviation (over the simulations) of the estimate of the mean baseline score.\n\n\nTrue Mean Baseline\n1\nTrue mean from which baseline scores where simulated (including accounting for possible truncation of the baseline scores)\n\n\nTrue SD Baseline\n1\nTrue standard deviation of the distribution from which baseline scores were simulated (including accounting for possible truncation of the baseline scores)\n\n\n\n\n\nObserved\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Arm&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Arm&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean D ropouts: &lt;Arm&gt;,\n&lt;Visit&gt;\n1 per arm per visit\nAverage (across the simulations) of the number of dropouts for the arm by visit.\n\n\n\n\n\nProbabilities\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt;: &lt;Arm&gt;\n1 per arm per QOI\nFor each Posterior Probability, Predictive Probability, p-value, or Target QOI, this is the mean over the simulations of the estimate of the probability of the QOI for each dose.\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition. The probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial).\n\n\n\n\n\nModel Parameters\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Virtual Response &gt; Dropout Rate’,\n‘Virtual Response &gt; Arm Response’,\n‘Trial &gt; Trial Arms &gt; Arrivals’\nThis is the same name as used for the results directory\n\n\nMean Sigma\n1\nAverage (over the simulations) of the posterior estimate of sigma, the SD in the participant’s final responses.\n\n\nSD Mean Sigma\n1\nStandard deviation (over the simulations) of the posterior estimate of sigma, the SD in the participant’s final responses.\n\n\nMean Baseline Beta\n1\nAverage (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nSD Baseline Beta\n1\nStandard deviation (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\n\n\n\nDetailed Per Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulations output in the table by double-clicking on the appropriate row. A separate window pops up and displays the results for each individual simulation. This is a “prettier” view of the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 29: The simulation results pop out viewer showing simulation highlights.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#graphs-of-simulation-results",
    "href": "documentation/v71/userguides/platform.html#graphs-of-simulation-results",
    "title": "Platform Trials",
    "section": "Graphs of Simulation Results",
    "text": "Graphs of Simulation Results\nTo enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described in the Output Files section below.\n\n\n\n\n\n\nBox and whisker plot conventions\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\nPer Scenario Graphs\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select that row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks. (Not displayed if the ‘y’ value must lie in the interval 0-1.\nShow Truth – this toggles whether the true response by arm is plotted.\n\n\nSimulation Summary Graphs\nThese graphs provide output averaged across all trials simulated for the scenario.\n\nOutcome by Treatment\n\n\n\n\n\n\nFigure 30: A stacked bar plot of outcomes for each arm in the trial.\n\n\n\nThis graph summarizes the final status of each treatment arm in the trial, including availability, completeness, and conclusion. The “Trt. Classification” drop-down menu in the Controls box allows the user to specify whether the plot should be restricted to treatment effects with a particular classification.\n\n\nSimulated Treatment Classification\n\n\n\n\n\n\nFigure 31: A stacked bar plot of the treatment classification (Good, Mediocre, or Unacceptable) for each arm in the trial. Arms with fixed treatment effects will always be 1 color.\n\n\n\nThis graph summarizes the treatment effect classification of each treatment arm in the trial. If no sampling is used for treatment arms, these bars will all be one solid color, but may show variation if either the control arm or the arm itself is sampled. The classification into “good,” “unacceptable,” and “mediocre” uses the definitions provided on the Virtual Response &gt; Treatment Classification tab. The “General Outcome” drop-down menu in the Controls box allows the user to restrict the plot to treatment arms that reached a particular outcome. For example, choosing “Success” will show the distribution of treatment classifications for the arms only amongst arms that achieved a “Early Success,” “Late Success,” or “Futility to Success Flip” outcome.\n\n\nAllocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 32: A box plot of the sample size per arm.\n\n\n\nThis graph displays a box and whisker plot of the number of subjects enrolled into each arm. This provides the distribution, over all simulations, of the number of subjects allocated to each arm. If a treatment arm is not able to enter the trial in a simulation it gets a 0 allocation for that simulation. In very simple platform trials, the number may be the same in every simulation, and the box and whiskers collapse to a single line.\n\n\nResponse and Participant Allocation\n\n\n\n\n\n\nFigure 33: A two-axis plot showing the mean allocation per arm and estimated response rate with the credible interval.\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black dot-and-whisker shows the distribution of the true mean response across the simulations, as 2.5%, 50%, and 97.5% quantiles. This will only be a black dot if the response is fixed.\nThe green dot-and-whisker shows the distribution of the estimated response across the simulations, as 2.5%, 50%, and 97.5% quantiles.\n\n\n\nPer Treatment: QOIs\n\n\n\n\n\n\nFigure 34: Boxplots across all simulations of a QOI value, separated by treatment.\n\n\n\nThese plots show box and whisker plots to display the distribution (across the simulations) of the final values for the QOIs. The drop-down menu in the Controls panel allows the user to change which QOI is displayed.\n\n\nCumulative Operating Characteristics Plot\nThere are two graphs, one that shows the cumulative proportion of simulations that have a duration less than the x-axis provided value across all simulations, and another that shows the cumulative proportion of simulations that had fewer than the x-axis provided number of subjects. A radio button allows for selection between the two graphs.\n\nCumulative Proportion of DurationCumulative Proportion of Subjects\n\n\n\n\n\n\n\n\nFigure 35: The proportion of trials that have a duration lower than the x-axis value.\n\n\n\n\n\n\n\n\n\n\n\nFigure 36: The proportion of trials that have a total sample size lower than the x-axis value.\n\n\n\n\n\n\n\n\n\nPer Sim Graphs\nThe Per Sim graphs allow the user to select a particular Simulation to examine in detail. The simulation number is selected in the Controls box.\n\n\n\n\n\n\nFigure 37: The control for selecting which simulation to display in the Per Sim graph.\n\n\n\n\nPer Sim: Arm and Participant Arrivals\n\n\n\n\n\n\nFigure 38: A plot summarizing the platform trial. It shows arm entry times, exit times, decisions made, and the classification of the arm’s true effect.\n\n\n\nThis graph depicts an overview of a single simulated trial, providing timing information (in weeks). The graph contains several components:\n\nA dashed line from the time that the arm became available for entry into the trial until the time time that it either entered the trial (began enrolling) or stopped waiting to enter the trial and withdrew. Note: if the arm began enrolling as soon as it became available, the dashed line will not appear.\nA thick solid line from the beginning to end of the enrollment period.\nA dot-dashed line from the end of enrollment to the time of final analysis. If there is no follow-up after an early stopping decision is made, then this line will not appear.\nA symbol at the end of each treatment arm’s line indicating the final status of the treatment arm.\nA symbol above the final status symbol indicating the treatment effect classification.\nOptionally, checking the “Show Participant Arrivals” checkbox in the Controls box will add additional lines to the plot: number of participants accrued versus time.\n\n\n\nPer Sim: Response and Participant Alloc.\n\n\n\n\n\n\nFigure 39: For a single simulated trial, this graph provides the number of subjects accrued on each arm as well as the estimated response rate and credible intervals.\n\n\n\nThis graph mimics the other Response and Participant Allocation graph but is for a single simulation, as selected in the Controls box. In this graph, the ochre dot-and-whiskers represent a 95% credible interval for the response, based on the Bayesian model fit, while the green dot-and-whiskers represents a frequentist 95% confidence interval for the response.\n\n\nPer Sim: Posterior Quantities\n\n\n\n\n\n\nFigure 40: For a single simulated trial, this graph provides the value of a selected QOI for each arm as well as the estimated response rate and credible intervals.\n\n\n\nThis graph mimics the Per Sim Response and Participant graph but replaces the enrollment bar chart with a bar chart for a QOI value at the trial’s final analysis. The QOI Value to display is chosen via a drop-down menu in the Controls box.\n\n\n\nPer Update Graphs\nThese graphs mimic previous graphs but allow the user to look at quantities as of a specific trial update within a simulation. The user can select the simulation and update in the Controls box. The updates are labeled with both the number of the trial update and the time (week) in which the update took place. Note: final analyses for arms at the end of follow-up are considered updates for the purpose of this numbering, though they may have no adaptive decisions associated.\nThe per update graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\n\n\n\n\n\n\nFigure 41: The controls for selecting which simulation and update to display in the Per Update graph.\n\n\n\n\nPer Update: Response and Participant Alloc.\nThis graph mimics the Per Update: Response and Participant Alloc. graph, but showing the outcomes at a single update, rather than the final analysis.\n\n\nPer Update: Posterior Quantities\nThis graph mimics the Per Update: Posterior Quantities graph, but showing the outcomes at a single update, rather than the final analysis.\n\n\nPer Update: Allocation Probability\nThe only new graph in the per update set is the allocation probability graph. This graph is intended primarily to examine the behavior of adaptive allocation but can also be used to see which arms are enrolling and how allocation changes with the number of available arms even in the non-adaptive allocation case. This graph is identical to the Per Update: Response and Participant Allocation graph, except that in place of a bar chart for past enrollment, it gives the current probability of allocation for the arms.\n\n\n\n\n\n\nFigure 42: This graph shows the estimated response and credible interval for each arm, as well as the allocation rate resulting from these estimated values. If using RAR, this provides the RAR probabilities, otherwise it is fixed based on the number of accruing arms.\n\n\n\n\n\n\nExplore graphs\nTwo graphs, one for futility and one for success, are available to help calibrate a design. For a particular treatment arm and a particular QOI, the line graph displays the proportion of simulated trials for which the QOI exceeds (or falls below) a threshold, indicating a potential way to set up early stopping rules to achieve desired operating characteristics.\nThese graphs require the use of weeks files to get detailed information updates, so it may be critical to increase the number of weeks files saved (as set on the Simulation tab, defaulting to 100) to provide sufficient information in these graphs.\nSince the graphs rely on the existing simulations, then any early stopping that is applied will limit the data available for later interims, and make the data conditional on not stopping early, which may make interpretation difficult. Thus, it may better to run simulations with no early stopping when utilizing these graphs. However, unlike the Core engine where removing early stopping lead to identical results up to the stopping point of interest, removing early stopping from all arms may affect the trajectory of the platform trial, leading to fewer arms enrolling, a difference in proportion of controls, and other changes in behavior of the platform trial beyond the early stopping of arms. The graphs may provide rough estimates of where thresholds should be set, but ultimately, these will need to be calibrated in the context of early stopping for all arms.\n\n\n\n\n\n\n\n\n\n\n\n(a) The Cumulative Proportion of Simulations Satisfying the Futility Criteria for a specific treatment.\n\n\n\n\n\n\n\n\n\n\n\n(b) The Cumulative Proportion of Simulations Satisfying the Success Criteria for a specific treatment.\n\n\n\n\n\n\n\nFigure 43\n\n\n\n\n\n\nAcross Scenario Graphs\n\nSelect Scenarios and Variants to Display\nThere is only 1 Across Scenario Graph available in the platform trials engine currently. The column facets separate the simulation scenarios, and the row facets separate the design variants.\nThe Across Scenario Graphs have a control labelled, “Select Scenarios/Variants” that allows for the filtering to only certain scenarios or certain variants. This is especially helpful if there are many simulation scenarios.\nThe control simply presents a list of the scenarios and a list of the variants, with a checkbox alongside each one allowing it to be de-selected:\n\n\n\n\n\n\nFigure 44: The Select Scenarios/Variants pop-up allowing for filtering of results shown in the Across Scenarios graphs.\n\n\n\n\nQOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\n\n\n\n\n\nFigure 45: For each scenario and each design variant, this graph provides the distribution of the selected QOI for each arm in the trial.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/platform.html#output-files",
    "href": "documentation/v71/userguides/platform.html#output-files",
    "title": "Platform Trials",
    "section": "Output Files",
    "text": "Output Files\nFACTS stores the results of simulations as ‘.csv’ files in a results folder. For each row in the simulations table, there is a folder named by the profiles that make up the scenario, which contains the corresponding ‘.csv’ files.\n\n\n\n\n\n\nFigure 46: Output files in the Windows file viewer.\n\n\n\nThese files can be opened using Microsoft Excel, but versions of Excel before 2007 are restricted to 256 columns, which is too few to view some files in their entirety. The ‘Calc’ application in ‘OpenOffice’ will show all the columns (and will open two files that have the same name at the same time!). Because Excel takes out a file lock on any file it has open, while a file is open in Excel it cannot be deleted or modified by another application. The most common cause for an error to be reported when simulating trials in FACTS is because the user has one of the previous results files is still open in Excel.\nIn the scenario directory there are the following types of results file:\nSummary.csv Contains a single row of data that summarizes the simulation results. This is the source of the shown on the simulations tab.\nSimulations.csv Contains one row per simulation describing the final state of each simulation for every trial simulated.\nPatientsNNNNN.csv Contains one row per patient in a simulation, where NNNNN is the number of the simulation. By default this file is written out only for the first simulation, but this can be changed on the simulations tab.\nWeeksNNNNN.csv Contains one row for each trial update during a simulation where NNNNN is the number of the simulation. By default this file is written only for the first 100 simulations, but this can be changed via the simulation tab. The values in the last row of the cohorts file will be the same as the final values for that simulation in the simulations.\n\nContents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the column headings.\n\nContents of the summary.csv file.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNsim\n1\nThe number of simulations contributing to the summary file.\n\n\nMean Treatments Started\n1\nThe average number (over the simulations) of non-control treatments that entered the trial and were eligible to enroll participants.\n\n\nMean Treatments Analyzed\n1\nThe average number (over the simulations) of non-control treatments that reached a final analysis within the trial – either because they reached an early stopping decision with no follow-up or because they completed follow-up on all participants.\n\n\nMean Good Treatments Analyzed\n1\nThe average number (over the simulations) of “good” treatment arms that reached a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatments Analyzed\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that reached a final analysis, “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatments Analyzed\n1\nThe average number (over the simulations) of “mediocre” treatment arms that reached a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Successes\n1\nThe average number (over the simulations) of treatment arms that were declared successful at a final analysis.\n\n\nMean Good Treatment Successes\n1\nThe average number (over the simulations) of “good” treatment arms that were declared successful at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatment Successes\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared successful at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre Treatment Successes\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared successful at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Futilities\n1\nThe average number (over the simulations) of treatment arms that were declared futile at a final analysis.\n\n\nMean Good Treatment Futilities\n1\nThe average number (over the simulations) of “good” treatment arms that were declared futile at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatment Futilities\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared futile at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre Treatment Futilities\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared futile at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean In conclusives\n1\nThe average number (over the simulations) of treatment arms that were declared inconclusive at a final analysis.\n\n\nMean Good Treatment In conclusives\n1\nThe average number (over the simulations) of “good” treatment arms that were declared inconclusive at a final analysis, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean U nacceptable Treatment In conclusives\n1\nThe average number (over the simulations) of “unacceptable” treatment arms that were declared inconclusive at a final analysis, “unacceptabl” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Mediocre Treatment In conclusives\n1\nThe average number (over the simulations) of “mediocre” treatment arms that were declared inconclusive at a final analysis, “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Successes | Treatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Futilities | Treatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn In conclusives | Treatment Good\n1\nAmongst the “good” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Successes | Treatment U nacceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Futilities | Treatment U nacceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn In conclusives | Treatment U nacceptable\n1\nAmongst the “unacceptable” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “unacceptable” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Successes | Treatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared successful – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Futilities | Treatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared futile – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn In conclusives | Treatment Mediocre\n1\nAmongst the “mediocre” treatment arms that reached final analysis across all simulations, the proportion of those arms that were declared inconclusive – “mediocre” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatments | Success\n1\nAmongst the treatment arms that reached final analysis and were declared successful (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatments | Futility\n1\nAmongst the treatment arms that reached final analysis and were declared futile (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatments | I nconclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “good” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatments | I nconclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “unacceptable” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatments | I nconclusive\n1\nAmongst the treatment arms that reached final analysis and were declared inconclusive (across all simulations), the proportion of those arms that were “mediocre” treatment arms, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn 1+ Successes\n1\nThe proportion of simulations that had at least one treatment arm declared successful.\n\n\nPpn Good Treatments | 1+ Successes\n1\nThe proportion of simulations that had at least one “good” treatment arm declared successful, “good” as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Success &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared successful.\n\n\nPpn Futility &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared futile.\n\n\nPpn In conclusives &lt;Arm&gt;\n1 per arm\nAmongst simulations in which a final analysis was reached for the arm, the proportion of simulations where the arm was declared inconclusive.\n\n\nPpn Good Treatment | Started &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatment | Started &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatment | Started &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm was open to enroll participants in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Good Treatment | Analyzed &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “good”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn U nacceptable Treatment | Analyzed &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “unacceptable”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nPpn Mediocre Treatment Analyzed &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis in the trial, the proportion of simulations in which the arm was actually “mediocre”, as defined on the Virtual Response &gt; Treatment Classification tab.\n\n\nMean Duration\n1\nThe average (in weeks) over the simulations of the duration of the trial from the start to completion of the trial.\n\n\nMean First Success Time\n1\nAmongst simulations that had at least one success, the average time (in weeks) at which the first success was declared.\n\n\nMean Available Time &lt;Arm&gt;\n1 per arm\nThe average time (in weeks) across all simulations at which the treatment because available for entry into the trial. Note: the available time is reported whether or not it became available after the end of the trial.\n\n\nMean Start Time &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became open to enroll participants, the average time (in weeks) that the arm began enrolling.\n\n\nMean End Time &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm became open to enroll participants, the average time (in weeks) that the arm stopped enrolling.\n\n\nMean Final Analysis Time &lt;Arm&gt;\n1 per arm\nAmongst simulations in which the arm reached a final analysis, the average time (in weeks) that the arm’s final analysis occurred.\n\n\nMean No. P articipants\n1\nThe average number of participants enrolled across all simulations.\n\n\nSE P articipants\n1\nThe standard error of the number of participants enrolled across all simulations.\n\n\nMean P articipants\n1\nThe average number (over the simulations) of participants enrolled in the trial.\n\n\nSE Mean P articipants\n1\nThe standard error (over the simulations) of the number of participants enrolled in the trial.\n\n\nMean Alloc &lt;Arm&gt;\n1 per arm\nThe average (over the simulations) of the number of participants enrolled onto the arm.\n\n\nSE Alloc &lt;Arm&gt;\n1 per arm\nThe standard error (over the simulations) of the number of participants enrolled onto the arm.\n\n\nMean resp &lt;Arm&gt;\n1 per arm\nAverage (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nSE resp &lt;Arm&gt;\n1 per arm\nStandard error (over the simulations) of the estimate of the mean response for the arm at the end of the trial.\n\n\nMean Sigma\n1\nAverage (over the simulations) of the estimate of the SD of the dose response across all the treatment arms (‘sigma’)\n\n\nSE Mean Sigma\n1\nStandard error (over the simulations) of the estimate of the SD of the dose response across all the treatment arms.\n\n\nTrue Mean resp &lt;Arm&gt;\n1 per arm\nTrue mean response from which the simulated participant data was sampled for the arm.\n\n\nTrue SD resp &lt;Arm&gt;\n1 per arm\nTrue standard deviation of the simulated participant data for the arm.\n\n\nN o.Dropouts: &lt;Arm&gt;, &lt;Visit&gt;\n1 per arm per visit\nAverage (across the simulations) of the number of dropouts for the arm by visit.\n\n\nMean Baseline Beta\n\nThis is the mean (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model. [Continuous endpoint only]\n\n\nSD Baseline Beta\n\nThis is the SD (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model. [Continuous endpoint only]\n\n\nQOIs (named according to QOI naming convention, as described in the QOIMappi ngFile.csv)\n1 per arm per QOI\nFor each Posterior Probability, Predictive Probability, p-value, or Target QOI, this is the mean over the simulations of the estimate of the probability of the QOI for each dose.\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition. The probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial).\n\n\n\n\n\nContents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the last analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each update in the trial and a row for each final analysis that occurs off-cycle from regular updates.\nThe first line is a header line, starting with a ‘#’, and containing\n\nThe FACTS GUI version number\nThe name of the FACTS file\nThe name of the scenario\nThe time stamp of the start of the simulation\n\nThe second and third lines are header lines. Most header names are identical in the second and third lines, but differ for some quantities, particularly QOIs. The alternate names for QOIs are summarized in the QOIMappingFile.csv that is produced in the main results directory.\nMost of the columns are common to the simulations and weeks file types, except as noted below.\n\nContents of the simulations/weeks files.\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nUpdate Number\n1\n\n✔\nThe index of the interim analysis being performed.\n\n\nStage\n1\n✔\n✔\n0 = Accruing (standard trial update)1 = Final (final analysis for a particular arm)2 = Accrued (standard trial update after full enrollment)4 = Paused (accrual is paused – currently not allowed)\n\n\n# Participants\n1\n✔\n✔\nNumber of participants enrolled in the trial\n\n\nAlloc &lt;Arm&gt;\n1 per arm\n\n\nNumber of participants enrolled on the arm\n\n\nStatus &lt;Arm&gt;\n1 per arm\n✔\n✔\nStatus of the treatment arm:-99 = Turned Away (could not enter trial)-98 = Waiting In Treatment Queue-97 = Never Arrived, (trial ended before arm arrived)-1 = Not Started (weeks file only)0 = Enrolling1 = In Followup (done enrolling)2 = Paused (not currently available)99 = Complete\n\n\nAvailable Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) at which the treatment becomes available for entry into the trial. Note: the available time is always reported, even before the trial has reached that point.\n\n\nStart Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) that the arm became eligible to enroll participants. If not reached, -9999 is used.\n\n\nEnd Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) that the arm stopped enrolling participants. If not reached, -9999 is used.\n\n\nFinal Analysis Time &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe time (in weeks) that the arm’s final analysis occurred. If not reached, -9999 is used.\n\n\nMilestone &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe milestone being evaluated for the treatment arm at the given trial update. -1 indicates no milestone is being evaluated. 9999 indicates a final analysis.\n\n\nOutcome &lt;Arm&gt;\n1 per arm\n✔\n✔\nA flag categorizing final study outcome:-9999 = Not applicable (for control arm)-1 = Not available (arm enrolling but no outcome yet)0 = Not started (arm still hasn’t entered the trial)1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive8 = Success to inconclusive flip-flop9 = Futility to inconclusive flip-flop10 = Incomplete (trial ended without reaching final analysis)\n\n\nDesired Outcome &lt;Arm&gt;\n1 per arm\n✔\n✔\nTreatment effect classification for the treatment arm for this simulation.0 = “Unacceptable”1 = “Mediocre”2 = “Good”\n\n\n#Enrolled Treatments\n1\n✔\n✔\nThe number of arms that have been open to enrollment in the trial.\n\n\n#Analyzed Treatments\n1\n✔\n✔\nThe number of arms that have reached a final analysis in the trial.\n\n\nFirst Success Time\n1\n✔\n✔\nIf any treatment has been declared successful, the time of the first successful final analysis. Otherwise, -9999.\n\n\n#Successes\n1\n✔\n✔\nThe number of arms that have been declared successful.\n\n\n#Futilities\n1\n✔\n✔\nThe number of arms that have been declared futile.\n\n\n#Inconclusive\n1\n✔\n✔\nThe number of arms that have been declared inconclusive.\n\n\n#GoodTrt\n1\n✔\n✔\nThe number of arms with treatment effects considered “good.”\n\n\n#GoodSucc\n1\n✔\n✔\nThe number of “good” arms that have been declared successful.\n\n\n#GoodFut\n1\n✔\n✔\nThe number of “good” arms that have been declared futile.\n\n\n#GoodInconc\n1\n✔\n✔\nThe number of “good” arms that have been declared inconclusive.\n\n\n#UnaccTrt\n1\n✔\n✔\nThe number of arms with treatment effects considered “unacceptable.”\n\n\n#UnaccSucc\n1\n✔\n✔\nThe number of “unacceptable” arms that have been declared successful.\n\n\n#UnaccFut\n1\n✔\n✔\nThe number of “unacceptable” arms that have been declared futile.\n\n\n#UnaccInconc\n1\n✔\n✔\nThe number of “unacceptable” arms that have been declared inconclusive.\n\n\n#MedTrt\n1\n✔\n✔\nThe number of arms with treatment effects considered “mediocre.”\n\n\n#MedSucc\n1\n✔\n✔\nThe number of “mediocre” arms that have been declared successful.\n\n\n#MedFut\n1\n✔\n✔\nThe number of “mediocre” arms that have been declared futile.\n\n\n#MedInconc\n1\n✔\n✔\nThe number of “mediocre” arms that have been declared inconclusive.\n\n\nPr(Alloc) &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe probability of allocation to the different arms following the update.\n\n\nMean resp &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe estimated response (or response rate for Dichotomous endpoints) of each treatment arm.\n\n\nSD resp &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nMean resp (lower CI) &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe lower bound of the 95% credible interval for response rate. (Dichotomous only)\n\n\nMean resp (upper CI)\n1 per arm\n✔\n✔\nThe upper bound of the 95% credible interval for response rate. (Dichotomous only)\n\n\nTrue Mean &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe true response for the arm for this simulation.\n\n\nTrue SD resp\n1 per arm\n✔\n✔\nThe true standard deviation of the observed response for the arm. (Continuous only)\n\n\nSigma\n1\n✔\n✔\nThe (posterior mean) estimate of sigma, the pooled standard deviation for observations. (Continuous only)\n\n\nSD_Sigma\n1\n✔\n✔\nThe posterior standard deviation of the sigma parameter. (Continuous only)\n\n\nBeta\n1\n✔\n✔\nThe estimate of ‘Beta’ the baseline adjustment coefficient, if baseline adjustment is being used. (Continuous only)\n\n\nSD_Beta\n1\n✔\n✔\nThe SD of the estimate of ‘Beta’ the baseline adjustment coefficient. (Continuous only)\n\n\nMean Baseline\n1\n✔\n✔\nThe estimate of the mean baseline score. (Continuous only)\n\n\nSE Mean Baseline\n1\n✔\n✔\nThe standard error of the estimate of the mean baseline score. (Continuous only)\n\n\nSD Baseline\n1\n✔\n✔\nThe estimate of the SD of the baseline score. (Continuous only)\n\n\nTrue Mean Baseline\n1\n✔\n✔\nThe true mean of the simulated baseline score. (Continuous only)\n\n\nTrue SD Baseline\n1\n✔\n✔\nThe true SD of the simulated baseline score. (Continuous only)\n\n\nMean Raw Response &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe observed mean response on each treatment arm (unadjusted by any modeling).\n\n\nSE Mean Raw Response &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe standard error of the estimate of the mean raw response on each dose.\n\n\nComplete &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis.\n\n\nComplete Information &lt;Arm&gt;\n1 per arm\n✔\n✔\nThe number of subject who count as complete for the purposes of update timing, as defined on the Design &gt; Trial Updates tab: enrolled, complete, or opportunity to complete.\n\n\n#Dropouts &lt;Arm&gt; &lt;Visit&gt;\n1 per arm per visit\n✔\n✔\nThe number of dropouts on the arm, broken out into the visit at which they dropped. (The current implementation has only a single visit.)\n\n\nQOI Columns\n1 per arm per QOI*\n✔\n✔\nAll QOI values are reported for each arm. The row 1 and row 2 names for the columns are given in the QOIMappingFile.csv that is output in the top-level “_results” folder. There is an additional column for each Target QOI that specifies the arm index of the arm with the highest posterior probability of being the target arm.\n\n\n\n\n\nContents of PatientsNNNNN.csv\n\nContents of the patients files.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Participant\n1\nThe participant id number, starting at 1.\n\n\nArm\n1\nThe index of the arm on which the subject was enrolled.\n\n\nRegion\n1\nThe index of the region the subject was recruited in, based on the regions defined on the Accrual tab.\n\n\nDateInWeeks\n1\nThe time (in weeks) from the start of the trial, of the subject enrollment (and if relevant, baseline visit).\n\n\nLastVisit#\n1\nThe index of the last visit for which the participant’s data was collected. (The current implementation only allows for a single visit.)\n\n\nDropout\n1\n1 = dropout, 0 = no dropout.\n\n\nBaseline\n1\nParticipant baseline if simulated.\n\n\nVisit &lt;visit&gt;\nV\nParticipant response at each visit. (Currently only one visit allowed.)\n\n\n\n\n\nContents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContinuousDichotomous\n\n\n\nContents of the MCMC output files for Continuous Platform Trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis in the simulation. This corresponds to the rows in the weeksNNNNN.csv file.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nTheta &lt;Arm&gt;\n1 per arm\nThe estimate of the mean response for each arm.\n\n\nSigma\n1\nThe estimate of the standard deviation of the endpoint.\n\n\n\n\n\n\nContents of the MCMC output files for Continuous Platform Trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis in the simulation. This corresponds to the rows in the weeksNNNNN.csv file.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Arm&gt;\n1 per arm\nThe estimate of the mean response rate for each arm.\n\n\n\n\n\n\n\n\nMapping Files\nWhen any simulations are run, two files are generated, which are placed in the top-level “_results” folder. These files apply to any results produced for individual scenarios and are intended to help with tracking the QOIs in use for the simulation.\n\nQOIMappingFile.csv\nThis file provides a list of all QOIs that are computed for the simulations, along with details about the QOI definition. This file utilizes the same structure as for the Core engine, though it is largely redundant for the Platform engine, since there is a row for each treatment for each individual QOI.\n\nContents of the QOIMappingFile\n\n\n\n\n\n\nColumn Title\nDescription\n\n\n\n\nFACTS filename\nThe .facts file from which the QOI mapping was constructed.\n\n\nQOI Category Index\n1 = Posterior Probability, 2 = Predictive Probability, 3 = p-value, 4 = Target Probability\n\n\nQOI Category (text)\nText version of the QOI category index.\n\n\nQOI Alternative Name\nThe base text name for the QOI, which will be written in the second row of header columns for the simulations.csv and weeksNNNNN.csv files.\n\n\nCondition\nFor posterior probabilities, the direction of the comparison of the inference parameter.\n\n\nResponse Relation\nFor posterior probabilities, whether the parameter is being compared relative to another arm’s parameter or to an absolute reference value.\n\n\nRelative To (Treatment Index)\nFor posterior probabilities, index of the treatment arm being compared to (if any).\n\n\nRelative To (Treatment Name)\nFor posterior probabilities, text name of the treatment arm being compared to (if any).\n\n\nDelta\nFor posterior probabilities, the absolute reference value being compared to, or for relative comparisons, the additional delta value to add to the relative treatment arm parameter.\n\n\nPhase\nFor predictive probabilities, trial phase being predicted. (Currently only future trial is available.)\n\n\nTest Type\nFor predictive probabilities or p-values, the type of frequentist test being computed/predicted.\n\n\nSample Size\n(Currently unused for platform trials.)\n\n\nAlpha\nFor predictive probabilities or p-values, one-sided alpha level for the frequentist test.\n\n\nSubjects Per Arm\nFor predictive probabilities of future trials, the number of participants enrolled on each arm in the trial being predicted.\n\n\nMargin\nFor predictive probabilities, the margin of superiority being tested.\n\n\nTarget Type\nFor target probabilities, the type of target – currently only the Max is available.\n\n\nTarget Dose\nCurrently unused.\n\n\nTarget Dose Treatment Index\nCurrently unused.\n\n\nTarget Dose QOI Alternative Name\nCurrently unused.\n\n\nQOI Treatment Index\nIndex of treatment arm QOI is being computed for.\n\n\nQOI Treatment Name\nText name of treatment arm QOI is being computed for.\n\n\nQOI Variable\nFull label name of the QOI, which is used in the first header row of the simulations.csv and weeksNNNNN.csv files.\n\n\nQOI Label\nFull alternate label name of the QOI, which is used in the second header row of the simulations.csv and weeksNNNNN.csv files.\n\n\nQOI Category Subindex\nIndexed value of QOI within the QOI category\n\n\n\n\n\nDecisionMappingFile.csv\nThis file provides a comprehensive list of all QOIs that are used in futility and success decisions throughout the trial.\n\nContents of the DecisionMappingFile.\n\n\n\n\n\n\nColumn Title\nDescription\n\n\n\n\nFACTS filename\nThe .facts file from which the QOI mapping was constructed.\n\n\nDecision Category Index\nIndex value of the type of decision for which the QOI is being used. 1 = Futility, 2 = Success.\n\n\nDecision Category (text)\nText version of the Decision Category Index\n\n\nQOI Category Index\nIndex of the QOI category for the QOI being used in the decision\n1 = Posterior Probability, 2 = Predictive Probability, 3 = p-value, 4 = Target Probability\n\n\nQOI Category (text)\nText version of the QOI category index.\n\n\nQOI Alternate Name\nText name of the QOI, as defined by QOI Alternate Name in QOIMappingFile.csv.\n\n\nCondition\nThe direction of comparison (&gt; or &lt;).\n\n\nThreshold\nFor threshold the QOI is being comparing to.\n\n\nDecision Variable\nFor posterior probabilities, the absolute reference value being compared to, or for relative comparisons, the additional delta value to add to the relative treatment arm parameter.\n\n\nDecision Label\nA full text description of the decision and the comparison being computed.\n\n\nStart At Milestone Index\nThe first milestone at which this decision is evaluated.\n\n\nEnd Before Milestone Index\nThe first milestone at which this decision is no longer evaluated.\n\n\nTreatment\nThe treatment arm to which this decision applies, either given by the treatment name or “All Treatments” if it applies to all arms.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Platform Trials"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/multendpt.html",
    "href": "documentation/v71/userguides/core/qois/multendpt.html",
    "title": "Multiple Endpoint QOIs",
    "section": "",
    "text": "The multiple endpoint engine allows the user to specify the quantities of interest for each endpoint in the same way that the continuous and dichotomous engines allow for specifications of QOIs.\nThe QOI tab in the multiple endpoint engine has a set of sub-tabs that are not present in the single endpoint engines. All of the sub-tabs except the last correspond to one of the endpoints. These tabs create QOIs that are only calculated for a single endpoint.\nThe last sub-tab on the multiple endpoint QOI page is the “Utility” tab. The utility tab exists so that decision making quantities can be calculated base on the distribution of the overall utility.\n\n\n\n\n\n\nFigure 1: Execution &gt; Quantities of Interest &gt; Utility tab for multiple endpoints.\n\n\n\n\nUtility QOIs\nSince the utility of an arm has a distribution, as discussed here, FACTS can calculate probabilistic quantities based on that utility. Examples of this include (among others):\n\nPr(U_d &gt; 0) - The probability that an arm’s combined utility is greater than 0\nPr(U_d - U_(Control) &gt; 1) - The probability that an arm’s conbined utility is greater than the control arm’s combined utility by at least 1 point.\nPr(UMax) - The probability that each arm has the highest utility among all arms.\nPr(UMin relative to Control: Delta=-1) - The probability that each dose is the dose with the smallest utility that is greater than the control utility minus 1.\n\nPredictive probabilities and p-values based on utilities are not supported.\nThe Decision Quantities on the “Utility” tab operate in the same way as they do in the other core engines. In order to get a single value of a QOI, rather than a vector of QOIs, you must specify a decision QOI, which includes the method of selecting a scalar value from the vector QOI.\nThe standard evaluation variables section of the QOI tab only contains an entry for the Clinically significant minimum utility (CSMU). This value only changes the value of the default posterior probability utility QOI that is created automatically.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest",
      "Multiple Endpoint QOIs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/index.html",
    "href": "documentation/v71/userguides/core/index.html",
    "title": "Phase II & III Designs",
    "section": "",
    "text": "This document covers the design options that are common across the four FACTS Core Design Engines: Continuous, Dichotomous, Time-to-Event, and Multiple Endpoint. Some design elements are shared across design engines, in which case there is only a single description of them. Others differ based on the endpoint used, in which case separate pages have been created to describe each.\nThe screenshots provided are specific to a particular installation and  may not reflect the exact layout Screenshots from earlier versions of FACTS 6 are still used only when the tabs they show are unchanged in FACTS 7.1.  of the information seen by any particular user. They were taken from FACTS V7 & V6 installed on Windows 10 or 11. If FACTS is installed on different versions of Windows, or with different Windows ‘themes’ there will be some differences in appearance introduced by Windows. The contents of each tab, however, will generally be consistent with the screenshots in this document.\n\n\nThis is the version of the user guide for inclusion with the FACTS 7.1 release.\n\n\n\nPlease cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/index.html#facts-version",
    "href": "documentation/v71/userguides/core/index.html#facts-version",
    "title": "Phase II & III Designs",
    "section": "",
    "text": "This is the version of the user guide for inclusion with the FACTS 7.1 release.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/index.html#citing-facts",
    "href": "documentation/v71/userguides/core/index.html#citing-facts",
    "title": "Phase II & III Designs",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described below.\nTo view the graphs, select a row of scenario results by clicking on it and then click on the ‘View Graph’ button.\nBelow we have often shown full screen shots of the graphs in the graph manager, but the graph display supports copying just the graph to the clipboard to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘png’ format to a file.\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select that row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nTo view multiple graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot. You can select the graph type, filter the design variants and filter which scenarios displayed:\n\n\n\n\n\n\nFigure 1: Pop up to select scenarios and variants to display.\n\n\n\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks (Not displayed if the ‘y’ value must lie in the interval 0-1.)\nSpace doses evenly – if the x-value is ‘dose’, then data for each dose can either be spaced equally or proportionate to the doses’ effective dose strength.\nIf using a 2D treatment arm model then it is possible to display graphs where the different doses (or “arms”) form the x-axis, then there is an option to show the row factors as separate series – otherwise the different combinations are displayed in effective strength order\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Arms shown in effective strength order\n\n\n\n\n\n\n\n\n\n\n\n(b) Arms shown with row factors as a separate series\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Arms shown in effective strength order\n\n\n\n\n\n\n\n\n\n\n\n(b) Arms shown with row factors as a separate series\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 6: Response and Subject Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Response and Subject Allocation.\n\n\n\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black line shows the true mean response being simulated.\nThe green dashed line shows the mean of the estimated response across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated mean response across the simulations (if less than 20 simulations have been run it simply shows the full spread).\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 8: Response and QOI Selection.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Response and QOI Selection.\n\n\n\n\n\n\nThese plots show the true simulated mean response and the mean and 95% spread of the mean fitted mean responses along with bars showing the proportion of simulations the different treatment arms are the selected target arm for each of the “Probability of being target” QOIs, such as Pr(Max), Pr(EDq relative to ..: Quantile=q) and Pr(MED relative to…: Delta=\\(\\delta\\)).\n\n\n\nThis set of graphs show the distribution over the simulations of the posterior probability estimates of quantities of interest for each dose. All the Posterior Probability, Predictive Probability, P-value and Probability of being Target QOIs that have been defined are available for plotting, via a drop down list.\nNote that in any one analysis the Probability of being Target will sum to 1 across the doses (and can sum to less than one in the case of the MED target if none of the doses look likely to be better than the CSD), whereas all the other QOIs are assessed individually for each dose.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(\\theta_d\\) - \\(\\theta_{Control}\\) &gt; 0.1) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.1) Across Simulations.\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(P_d\\) - \\(P_{Control}\\) &gt; 0.15) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.2) Across Simulations.\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 12: Posterior Mean Response at P(MED relative to Control: Delta}=0.1) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior Mean Response at P(MED relative to Control: Delta}=0.2) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\nThis graph shows a scatter plot of trial outcomes with the selected Target QOI as the y-axis and total number of subjects recruited as the x-axis. The trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as it is possible?\nWhen and with what mean response trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\n\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial. The user can select whether time or subject enrollment is used as the x-axis.\n\n\n\nIf the design has used arm dropping, the following graphs are available.\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 18: Response and Proportion Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Response and Proportion Arm Dropped\n\n\n\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 20: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 22: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 24: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\nThese graphs are available if frequentist analysis is enabled.\n\n\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 26: Distribution of P(LOCF. Bonferroni Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Distribution of P(LOCF. Unadjusted Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from Unadjusted, Bonferroni or Dunnett’s and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 28: Response and Ppn LOCF. Bonferroni Significance\n\n\n\n\n\n\n\n\n\n\n\nFigure 29: Response and Ppn Fail. Dunnett Significance\n\n\n\n\n\n\n\n\n\n\nThis graph shows the final analysis at the end of individual simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 30: Response and Subject Allocation\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Response and Subject Allocation\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBlue bars showing the number of subjects allocated on each treatment arm.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 32: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\nControl to select the individual simulation results to be displayed\n\n\n\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 34: Response and Pr(MED relative to Control: Delta = 0.1) Probability\n\n\n\n\n\n\n\n\n\n\n\nFigure 35: Response and Pr(EDq relative to Control: Quantile = 0.9) Probability\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot.\n\n\n\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\nNote the ‘Interim’ control in both of the below figures.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 36: Response and Pr(MED relative to Control: Delta = 0.1) Probability with side panel allowing for selection of simulation, interim, and quantity of interest.\n\n\n\n\n\n\n\n\n\n\n\nFigure 37: Response and Subject Allocation with side panel allowing for selection of simulation and interim number.\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 39\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: e.g. Pr(Max), and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis). The graphs show for all the decision QOIs defined for the selected Target Dose the proportion of simulations that would have met the threshold values.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (unlike the examples above where the shape of the “existing stopping rules” line indicates that early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which decision QOI is evaluated and from which interim onwards stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 42: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\nFigure 43: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 45\n\n\n\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the decision QOI to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are successful/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates.\n\n\n\n\n\nDichotomous\n\n\n\n\n\n\n\n\nFigure 46: Trace plot of an MCMC parameter.\n\n\n\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC.\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot.\n\n\n\n\n\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 47: Across scenario graph for selected arms with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 48: Across scenario graph for selected arms with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 49: Across scenario graph for showing QOIs per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50: Across scenario graph for showing QOIs per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 51: Across scenario graph for showing the proportion of trials that result in success with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 52: Across scenario graph for showing the proportion of trials that result in success with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 53: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 54: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 55: Across scenario graph for showing the distribution of allocation per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 56: Across scenario graph for showing the distribution of allocation per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 57: Across scenario graph for showing the mean sample size of the study with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 58: Across scenario graph for showing the mean sample size of the study with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 59: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\n\n\nFigure 60: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 61: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.\n\n\n\n\n\n\n\n\n\n\n\nFigure 62: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#per-scenario-graphs",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#per-scenario-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "ContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 6: Response and Subject Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Response and Subject Allocation.\n\n\n\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black line shows the true mean response being simulated.\nThe green dashed line shows the mean of the estimated response across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated mean response across the simulations (if less than 20 simulations have been run it simply shows the full spread).\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 8: Response and QOI Selection.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Response and QOI Selection.\n\n\n\n\n\n\nThese plots show the true simulated mean response and the mean and 95% spread of the mean fitted mean responses along with bars showing the proportion of simulations the different treatment arms are the selected target arm for each of the “Probability of being target” QOIs, such as Pr(Max), Pr(EDq relative to ..: Quantile=q) and Pr(MED relative to…: Delta=\\(\\delta\\)).\n\n\n\nThis set of graphs show the distribution over the simulations of the posterior probability estimates of quantities of interest for each dose. All the Posterior Probability, Predictive Probability, P-value and Probability of being Target QOIs that have been defined are available for plotting, via a drop down list.\nNote that in any one analysis the Probability of being Target will sum to 1 across the doses (and can sum to less than one in the case of the MED target if none of the doses look likely to be better than the CSD), whereas all the other QOIs are assessed individually for each dose.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(\\theta_d\\) - \\(\\theta_{Control}\\) &gt; 0.1) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.1) Across Simulations.\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(P_d\\) - \\(P_{Control}\\) &gt; 0.15) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.2) Across Simulations.\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 12: Posterior Mean Response at P(MED relative to Control: Delta}=0.1) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior Mean Response at P(MED relative to Control: Delta}=0.2) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\nThis graph shows a scatter plot of trial outcomes with the selected Target QOI as the y-axis and total number of subjects recruited as the x-axis. The trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as it is possible?\nWhen and with what mean response trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\n\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial. The user can select whether time or subject enrollment is used as the x-axis.\n\n\n\nIf the design has used arm dropping, the following graphs are available.\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 18: Response and Proportion Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Response and Proportion Arm Dropped\n\n\n\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 20: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 22: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 24: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\nThese graphs are available if frequentist analysis is enabled.\n\n\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 26: Distribution of P(LOCF. Bonferroni Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Distribution of P(LOCF. Unadjusted Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from Unadjusted, Bonferroni or Dunnett’s and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 28: Response and Ppn LOCF. Bonferroni Significance\n\n\n\n\n\n\n\n\n\n\n\nFigure 29: Response and Ppn Fail. Dunnett Significance\n\n\n\n\n\n\n\n\n\n\nThis graph shows the final analysis at the end of individual simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 30: Response and Subject Allocation\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Response and Subject Allocation\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBlue bars showing the number of subjects allocated on each treatment arm.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 32: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\nControl to select the individual simulation results to be displayed\n\n\n\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 34: Response and Pr(MED relative to Control: Delta = 0.1) Probability\n\n\n\n\n\n\n\n\n\n\n\nFigure 35: Response and Pr(EDq relative to Control: Quantile = 0.9) Probability\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot.\n\n\n\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\nNote the ‘Interim’ control in both of the below figures.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 36: Response and Pr(MED relative to Control: Delta = 0.1) Probability with side panel allowing for selection of simulation, interim, and quantity of interest.\n\n\n\n\n\n\n\n\n\n\n\nFigure 37: Response and Subject Allocation with side panel allowing for selection of simulation and interim number.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#post-simulation-boundary-finding-graphs",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#post-simulation-boundary-finding-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "ContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 39\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: e.g. Pr(Max), and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis). The graphs show for all the decision QOIs defined for the selected Target Dose the proportion of simulations that would have met the threshold values.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (unlike the examples above where the shape of the “existing stopping rules” line indicates that early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which decision QOI is evaluated and from which interim onwards stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 42: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\nFigure 43: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 45\n\n\n\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the decision QOI to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are successful/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#mcmc-trace-plots",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#mcmc-trace-plots",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "Dichotomous\n\n\n\n\n\n\n\n\nFigure 46: Trace plot of an MCMC parameter.\n\n\n\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC.\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#across-scenario-graphs",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#across-scenario-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "This graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 47: Across scenario graph for selected arms with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 48: Across scenario graph for selected arms with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 49: Across scenario graph for showing QOIs per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50: Across scenario graph for showing QOIs per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 51: Across scenario graph for showing the proportion of trials that result in success with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 52: Across scenario graph for showing the proportion of trials that result in success with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 53: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 54: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 55: Across scenario graph for showing the distribution of allocation per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 56: Across scenario graph for showing the distribution of allocation per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 57: Across scenario graph for showing the mean sample size of the study with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 58: Across scenario graph for showing the mean sample size of the study with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 59: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\n\n\nFigure 60: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 61: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.\n\n\n\n\n\n\n\n\n\n\n\nFigure 62: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#summary-per-scenario",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#summary-per-scenario",
    "title": "Continuous and Dichotomous Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after continuous or dichotomous simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn S uc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn F ut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inco nclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt; Dose&gt;&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn I ncorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation columns provided in FACTS output for continuous or dichotomous trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nResponse\n\nContinuousDichotomous\n\n\n\nResponse columns available in FACTS output for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the mean response on this dose.\n\n\nSD Trt.: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this dose.\n\n\nMean Sigma\n1\nThe mean (over the simulations) of the estimate of the SD of the dose response across all the treatment arms (‘sigma’)\n\n\nSD Mean Sigma\n1\nThe SD (over the simulations) of the estimate of the SD of the dose response across all the treatment arms.\n\n\nTrue Mean Resp: &lt;Dose&gt;\nOne per arm\nThis is the true mean response from which the simulation data was sampled for each treatment arm.\n\n\nSD True Resp.: &lt;Dose&gt;\nOne per arm\nThis is the true SD of the dose response for each treatment arm\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit, based on the independent estimate of the dose response (omega_d), scaled by the longitudinal model's estimate of the proportion of the final effect observed at the visit.\n\n\nSD Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the SD ( over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Baseline Beta\n\nThis is the mean (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nSD Baseline Beta\n\nThis is the SD (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nMean Baseline\n1\nThis is the mean (over the simulations) of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\nThis is the SD (over the simulations) of the estimate of the mean baseline score.\n\n\nTrue Mean Baseline\n1\nThis is the true mean from which baseline scores where simulated (including accounting for possible truncation of the baseline scores)\n\n\nTrue SD Baseline\n1\nThis is the true SD of the distribution from which baseline scores were simulated (including accounting for possible truncation of the baseline scores)\n\n\n\n\n\n\nResponse columns available in FACTS output for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the response on this dose.\n\n\nSD Trt.: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this dose.\n\n\nTrue Mean Resp: &lt;Dose&gt;\nOne per arm\nThis is the true response rate from which the simulation data was sampled.\n\n\n\n\n\n\n\n\nObserved\n\nObserved columns available in FACTS output for continuous or dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the mean (over the simulations) of dropouts per arm per visit in this scenario.\n\n\n\n\n\nProbabilities\n\nProbabilities columns available in FACTS output for continuous or dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\nModel Parameters\n\nContinuousDichotomous\n\n\n\nModel Parameters columns available in FACTS output for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the posterior estimate of sigma, the SD in the subject’s final responses.\n\n\nSD Mean Sigma\n1\nThis is the SD (over the simulations) of the estimate of sigma.\n\n\nMean Baseline Beta\n1\nThis is the mean (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nSD Baseline Beta\n1\nThis is the SD (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nBAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\n\n\n\n\nModel Parameters columns available in FACTS output for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nD*V\nThese are only calculated and written out if the ITP longitudinal model is being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nBAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#detailed-per-simulation-results",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#detailed-per-simulation-results",
    "title": "Continuous and Dichotomous Output",
    "section": "Detailed Per Simulation Results",
    "text": "Detailed Per Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulations output in the table by double-clicking on the appropriate row. A separate window pops up and displays the results for each individual simulation. This is a “prettier” view of the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 63: The simulation results pop out viewer showing simulation highlights.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;Dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;Dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nMean Sigma\n1\nThe mean of the estimate of sigma – the average standard deviation of the dose response\n\n\nSE Mean Sigma\n1\nThe standard error of the estimate of sigma – the average standard deviation of the dose response\n\n\nTrue Mean resp &lt;Dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nTrue SD resp &lt;Dose&gt;\nD\nThe true SD of the response for each treatment arm of the simulated subject responses\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nMean Beta\n1\nThe mean of the estimates of the coefficient of baseline adjustment\n\n\nSE beta\n1\nThe standard error of the estimates of the coefficient of baseline adjustment\n\n\nMean Baseline\n1\nThe mean of the estimate of the mean of the baseline score\n\n\nSE Mean Baseline\n1\nThe standard error of the estimate of the mean of the baseline score\n\n\nSD Baseline\n1\nThe mean of the estimate of the SD of the baseline score\n\n\nSE SD Baseline\n1\nThe standard error of the estimate of the SD of the baseline score\n\n\nTrue Mean Baseline\n1\nThe true mean of the baseline score (accounting for possible truncation)\n\n\nTrue SD Baseline\n1\nThe true SD of the baseline score (accounting for possible truncation)\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\nContents of the summary.csv file for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;Dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;Dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary_freq_missingnesstype.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary_freq_missingnesstype.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of Summary_freq_{missingnessType}.csv",
    "text": "Contents of Summary_freq_{missingnessType}.csv\nThere is a frequentist summary file for each type of treatment of missing values.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\nSE Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\nMean Beta (b aseline)\n1\nThe mean of the estimate of the baseline coefficient.\n\n\nSE Mean Beta (b aseline)\n1\nThe standard error of the estimate of the baseline coefficient\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.\n\n\n\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for dichotomous simulations.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\n\nSE Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of | simulations where the unadjusted p-value is | less than the user specified one-sided alpha. | | The marginal probability of significance per | dose (using unadjusted p-values).\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nRandom Number Seed\n1\n✔\n✔\nBase random number seed.\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nMean resp &lt;Dose&gt;\nD\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation.\n\n\nTrue SD resp &lt;Dose&gt;\nD\n✔\n✔\nThe true SD of the response of each treatment arm for this simulation\n\n\nSigma\n1\n✔\n✔\nThe pooled estimate of the SD of the response across all the treatment arms\n\n\nSD_Sigma\n1\n✔\n✔\nThe SD of the pooled estimate of the SD of the response across all the treatment arms\n\n\nBeta\n1\n✔\n✔\nThe estimate of ‘Beta’ the baseline adjustment coefficient, if baseline adjustment is being used.\n\n\nSD_Beta\n1\n✔\n✔\nThe SD of the estimate of ‘Beta’ the baseline adjustment coefficient.\n\n\nMean Baseline\n1\n✔\n✔\nThe estimate of the mean baseline score.\n\n\nSE Mean Baseline\n1\n✔\n✔\nThe standard error of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\n✔\n✔\nThe estimate of the SD of the baseline score.\n\n\nTrue Mean Baseline\n1\n✔\n✔\nThe true mean of the simulated baseline score\n\n\nTrue SD Baseline\n1\n✔\n✔\nThe true SD of the simulated baseline score\n\n\nMean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe observed mean response on each treatment arm (unadjusted by any modeling).\n\n\nSE Mean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe SE of the estimate of the mean raw response on each dose.\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis.\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subject who count as complete for the purposes of timing interims – whether complete or opportunity to complete and at which visit is as defined on the Interims tab. If interims are by enrolment then CompleteInformation is the number of subjects that have completed – final endpoint data is available.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit.\n\n\nDR Param &lt;Param&gt;\n10\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 10 parameters, most less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the index “&lt;Param&gt;”.\n\n\nSd DR Param &lt;Param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod_ &lt;Model&gt; &lt;Parameter&gt; &lt;Visit&gt;\nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. For linear regression the parameters reported are: - Alpha – per visit – the mean estimate of the constant offset in the change in response from this visit to the final visit - Beta – per visit – the mean estimate of the coefficient of change in response from this visit to the final visit - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. For time course hierarchical the parameters reported are: - Alpha – per visit – the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit.. - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. - Tau – the mean estimate of the SD of the per subject random effect For ITP the parameters are: - K – per model – the mean estimate of the ITP shape parameter - Tau – per model - the mean estimate of the SD of the per subject random effect - Lambda – per model – the mean estimate of the Sd of the residual error. - Omega – per treatment arm – the mean estimate of the mean treatment arm effect. For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used. This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses.\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim\n\n\nMean resp &lt;Dose&gt;\nD\n✔\n✔\nThe estimated response of each treatment arm\n\n\nSD resp &lt;Dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm\n\n\nMean resp (lower CI) &lt;Dose&gt;\nD\n✔\n✔\nThe lower bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nMean resp (upper CI) &lt;Dose&gt;\nD\n✔\n✔\nThe upper bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nTrue Mean resp &lt;Dose&gt;\nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation\n\n\nMean Raw Response &lt;Dose&gt;\nD\n✔\n✔\nThe observed rate of response on each treatment arm (unadjusted by any modeling)\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects that have achieved the information criteria as specified on the Interims tab. May be the number enrolled, complete, or with the opportunity to complete. If complete or opportunity to complete, then the visit that should be complete is also specified.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit\n\n\nDR Param &lt;Param&gt;\n4\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the column their value appears in here\n\n\nSd DR Param &lt;Param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter\n\n\nLongmod_ &lt;Model&gt; &lt;Parameter&gt; &lt;Visit&gt;\nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. Each value is reported per visit unless otherwise stated.For Beta-Binomial the parameters reported are:• Alpha0 & Beta 0 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 0• Prob01 – the mean probability of the Beta(Alpha0, Beta0) – the probability of 1 being the final result if the result at the visit is 0• Alpha1 & Beta 1 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 1• Prob11 – the mean probability of the Beta(Alpha1, Beta1) – the probability of 1 being the final result if the result at the visit is 1For logistic regression the parameters reported are: • Prob11 – the probability of 1 being the final result if the result at the visit is 1and • Prob01 – the probability of 1 being the final result if the result at the visit is 0For restricted Markov the parameters reported are:• Alpha0 – Alpha for state 0• AlphaS – Alpha for stable state• Alpha1 – Alpha for state 1• Prob0 – Transition probability to state 0• ProbS – Probability of remaining stable• Prob1 – Transition probability to state 1(values are for the transition to the next visit, so thre are no values for the final visit)If using a dichotomized continuous response, these columns will be for the selected continuous longitudinal model:For linear regression the parameters reported are:• Alpha – the mean estimate of the constant offset in the change in response from this visit to the final visit• Beta – the mean estimate of the coefficient of change in response from this visit to the final visit• Lambda – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.For ITP the parameters are:• K – single value per model – the mean estimate of the ITP shape parameter• Tau – single value per model - the mean estimate of the SD of the per subject random effect• Lambda – single value per model – the mean estimate of the Sd of the residual error.• Omega – per treatment arm not visit – the mean estimate of the mean treatment arm effect.For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD * V\n✔\n✔\nThese are only calculated and written out if the ITP longitudinal model is being used for a dichotomized continuous endpoint.This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns are grouped in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv",
    "text": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv\nThese file contain the frequentist results after an interim analysis (weeks file) or final analysis (simulations file), apart for the first column, the columns are the same in the two types of file, and the same across the different methods for treating missing data.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation (simulations_freq_…csv only)\n\n\n# Weeks\n1\nThe week of the interim (weeks_freq_…csv only)\n\n\nTheta\nD\nUnadjusted estimate of response per treatment arm\n\n\nTheta SD\nD\nThe SD of the estimate of response per treatment arm\n\n\nTrt Effect\nD\nThe relative size of the treatment effect compared to control per arm (treatment effect for control is always 0). The direction of the comparison takes into account whether a response represents a subject’s condition improving or worsening, so that a positive treatment effect shows the response on the dose is better than on control.\n\n\nt-Stat\nD\nThe t-test statistic per treatment arm (for control this is always 0)\n\n\np-Value\nD\nThe unadjusted t-test p-value per arm (for control this is always 0)\n\n\nlower CI\nD\nThe unadjusted lower bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nupper CI\nD\nThe unadjusted upper bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nMin p-Value\n1\nThe smallest of the t-test p-values\n\n\np-Value Dunnett\nD\nThe Dunnett adjusted t-test p-value per arm (for control this is always 0).\n\n\nlower CI Dunnett\nD\nThe Dunnett adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper CI Dunnett\nD\nThe Dunnett adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Dunnett\n1\nThe smallest of the Dunnett adjusted p-values.\n\n\np-Value Bonf\nD\nThe Bonferroni adjusted t-test p-value per arm.\n\n\nlower-CI Bonf\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper-CI Bonf\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Bonf\n1\nThe smallest of the Bonferroni adjusted t-test value per arm.\n\n\nt-Stat trend\n1\nThe t statistic from the trend test.\n\n\np-Value trend\n1\nThe p-value from the trend test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-patientsnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nBaseline\n1\nSubject baseline response, if simulated. If not simulated, then fixed at -9999.\n\n\nVisit &lt;Visit&gt;\nV\nSubject’s response at each visit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nBaseline\n1\nAlways -9999 since there’s no baseline in dichotomous response trials.\n\n\nVisit &lt;Visit&gt;\nV\nSubject’s response at each visit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-mcmcnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nTheta &lt;Dose&gt;\nD\nThe estimate of the mean response for each dose, based on the dose response model fitted.\n\n\nSigma\n1\nThe estimate of the SD of the endpoint.\n\n\nBeta\n1\nIf baseline is included in the simulated data and using the “Baseline Adjusted Model”, this is the value of “Beta”, the baseline coefficient.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\n\n\nIf using a dichotomized continuous variable, then the parameters for the LM are the same as for the continuous model MCMC output listed in the Continuous tab.\nIf using a dichotomous longitudinal model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\nIf using a restricted markov model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nCurrently the values of Alpha0, Alpha1, AlphaS (the parameters to the Dirichlet distribution) are (unnecessarily – these values are not sampled) output, along with the values for P0, P1 and PS the transition probabilities of each model at each visit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html",
    "href": "documentation/v71/userguides/core/simulation/tte.html",
    "title": "Time to Event Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described in Section 16, below.\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select a row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nTo view graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot.\n\n\n\n\n\n\nFigure 1: Select the scenarios to include in Across Scenario graphs.\n\n\n\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks. (Not displayed if the ‘y’ value must lie in the interval 0-1.\nSpace doses evenly – if the x-value is ‘dose’, then data for each dose can either be spaced equally or proportionate to the doses’ effective dose strength.\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#allocation-box-and-whisker-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#allocation-box-and-whisker-plot",
    "title": "Time to Event Output",
    "section": "Allocation Box and Whisker Plot",
    "text": "Allocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 2: Distribution of Subject Allocation Across Simulations.\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\n\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\nThe mean number of events observed in each arm across the simulations shown as a red triangle.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#events-boxplot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#events-boxplot",
    "title": "Time to Event Output",
    "section": "Events Boxplot",
    "text": "Events Boxplot\n\n\n\n\n\n\nFigure 3: Distribution of Number of Events Across Simulations\n\n\n\nThis graph displays a box and whisker plot of the number of events observed in each arm. These plots show:\n\nThe distribution over all simulations of the number of events observed in each arm shown as a box and whisker plot.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-subject-allocation",
    "href": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-subject-allocation",
    "title": "Time to Event Output",
    "section": "Hazard Ratio and Subject Allocation",
    "text": "Hazard Ratio and Subject Allocation\n\n\n\n\n\n\nFigure 4: Hazard Ratio and Subject Allocation\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean number of events observed and the mean estimated hazard ratio. Specifically:\n\nThe blue bars show the mean number of subjects without events and the brown bars mean number of subjects who had events.\nThe black line shows the true Hazard Ratio being simulated (but without allowing for any effect of the predictor)\nThe green dashed line (drawn if a response model is fitted), shows the mean of the estimated hazard ratios across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated hazard ratios (if less than 20 simulations have been run it simply shows the full spread).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-target-selection-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-target-selection-graphs",
    "title": "Time to Event Output",
    "section": "Hazard Ratio and Target Selection graphs",
    "text": "Hazard Ratio and Target Selection graphs\n\n\n\n\n\n\nFigure 5: Hazard Ratio and \\(Pr\\)(MED relative to Control: Delta=-0.2) Selection\n\n\n\nThese plots show the true simulated hazard ratio (without allowance for the effect of the predictor) and the mean and 95% spread of the mean fitted hazard ratios along with bars showing the proportion of simulations the different treatment arms are the finally selected target. The target displayed is selected by a control on the graph, from any of the defined Target QOIs.:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#predictor-response-and-allocation",
    "href": "documentation/v71/userguides/core/simulation/tte.html#predictor-response-and-allocation",
    "title": "Time to Event Output",
    "section": "Predictor: Response and Allocation",
    "text": "Predictor: Response and Allocation\n\n\n\n\n\n\nFigure 6: Predictor Hazard Ratio and Subject Allocation\n\n\n\nThese plots show the true simulated predictor response and the mean and 95% spread of the mean fitted predictor response model, with the blue bars showing the mean number of subjects allocated to each arm across the simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#per-dose-qois",
    "href": "documentation/v71/userguides/core/simulation/tte.html#per-dose-qois",
    "title": "Time to Event Output",
    "section": "Per Dose: QOIs",
    "text": "Per Dose: QOIs\n\n\n\n\n\n\nFigure 7: Distribution of P(Succ. Future Trial: N=250; Sup. \\(\\alpha=0.025\\), \\(\\delta=0\\) Adjusted Significance) Across Simulations\n\n\n\nThese plots show a box plot showing the distribution of the values for any of the defined QOIs for each dose.\nWhich QOI is displayed can be selected by the user from a drop down list on the graph. Any of the Posterior Probability, Predictive Probability, P-value and Target QOI’s can be selected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#target-hazard-ratio-scatter-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#target-hazard-ratio-scatter-plot",
    "title": "Time to Event Output",
    "section": "Target Hazard Ratio Scatter Plot",
    "text": "Target Hazard Ratio Scatter Plot\n\n\n\n\n\n\nFigure 8: Posterior Mean Hazard Ratio at Pr(Max) vs Total Sample Size per Simulation Run\n\n\n\nThis graph shows a scatter plot of trial outcomes with the estimate hazard ratio as the y-axis and total number of subjects recruited as the x-axis. A drop down list on the graph allows the user to select which target QOI is used to supply the hazard ratio:\nThe trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as its possible?\nWhen and with what response rate trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\nThere are two further variants of this graph, these have alternative y-axes: the hazard ratio at the EDx or at the MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#cumulative-operating-characteristics-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#cumulative-operating-characteristics-plot",
    "title": "Time to Event Output",
    "section": "Cumulative Operating Characteristics Plot",
    "text": "Cumulative Operating Characteristics Plot\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 9\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#time-course-for-stopping",
    "href": "documentation/v71/userguides/core/simulation/tte.html#time-course-for-stopping",
    "title": "Time to Event Output",
    "section": "Time course for stopping",
    "text": "Time course for stopping\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 10\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial.\nThe x-axis is configurable, the user can select cumulative stopping to be plotted relative to time, sample size or number of events observed.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#arm-dropping-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#arm-dropping-graphs",
    "title": "Time to Event Output",
    "section": "Arm Dropping Graphs",
    "text": "Arm Dropping Graphs\nIf the design has used arm dropping, the following graphs are available.\n\nHazard ratio and Ppn Arms Dropped\n\n\n\n\n\n\nFigure 11: Hazard Ratio and Proportion Arm Dropped\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\nTime Course for Arm Dropping\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\n\n\n\n\n\nFigure 12: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\nTime Course for Arm retention\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\n\n\n\n\n\nFigure 13: Cumulative Proportion of Trials Where Arm Retained\n\n\n\n\n\nArm Retention Proportion\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\n\n\n\n\n\nFigure 14: Arm Retention Proportion",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#frequentist",
    "href": "documentation/v71/userguides/core/simulation/tte.html#frequentist",
    "title": "Time to Event Output",
    "section": "Frequentist",
    "text": "Frequentist\nThese graphs are available if frequentist analysis is enabled.\n\nFrequentist P(significance)\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\n\n\n\n\n\nFigure 15: Distribution of \\(P\\)(Log Rand Adjusted Significance) Across Simulations\n\n\n\n\n\nFrequentist: Response and Significance\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from the Unadjusted, Bonferroni or Log Rank, Log Rank Bonferroni, Wilcox and Wilcox Bonferroni p-values.\n\n\n\n\n\n\nFigure 16: Hazard Ratio and Ppn Log Rank Significance",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#per-sim-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#per-sim-graphs",
    "title": "Time to Event Output",
    "section": "Per Sim Graphs",
    "text": "Per Sim Graphs\nThis set of graphs includes a control that allows the user to select which simulation to graph the results from. Each graph shows the output from only 1 simulated trial.\n\n\n\n\n\n\nFigure 17: The plot window when selecting a Per Sim graph. Note the Simulation control in the bottom left allowing for the selection of which simulated trial the graph should be made for.\n\n\n\n\nHazard Ratio and Subject Allocation\nThis graph shows the final analysis at the end of individual simulations.\n\n\n\n\n\n\nFigure 18: Hazard Ratio and Subject Allocation\n\n\n\nOn each graph\n\nThe black line shows the ‘True’ Hazard Ratio being simulated, (not adjusted to include any predictor effects)\nThe blue bars show the mean number of subjects without events and the brown bars mean number of subjects who had events.\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\n\n\n\nPosterior Quantities\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\n\n\n\n\n\nFigure 19: Hazard Ratio and \\(Pr\\)(\\(HR_d\\) - 1 &lt; -0.2) Probability\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from and which QOI to plot. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot, including QOI’s based on the predictor.\n\n\nPredictor Response and Allocation\n\n\n\n\n\n\nFigure 20: Predictor Hazard Ratio and Subject Allocation (Week: 80)\n\n\n\nIf the simulations include a predictor this graph is available.\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ Predictor response being simulated\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\nBlue Bars showing number of subjects allocated to each treatment arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#per-interim-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#per-interim-graphs",
    "title": "Time to Event Output",
    "section": "Per Interim Graphs",
    "text": "Per Interim Graphs\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100). Note the control in the bottom left that allows for specification of Simulation number as well as Interim number.\n\n\n\n\n\n\nFigure 21: The plot window when selecting a Per Interim graph. Note the Simulation control in the bottom left allowing for the selection of which simulated trial and which interim analysis the graph should be made for.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#post-simulation-boundary-finding-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#post-simulation-boundary-finding-graphs",
    "title": "Time to Event Output",
    "section": "Post Simulation Boundary Finding Graphs",
    "text": "Post Simulation Boundary Finding Graphs\n\nExplore Success/Futility Eval Criteria\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: Max, EDx or MED, and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis).\nFor the given target the proportion of trials that would meet each of the criteria over the range of threshold values is plotted. As in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\nExplore Early Success/Futility Eval Criteria\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 22\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (as in the examples above where the shape of the “existing stopping rules” line indicates that no early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which stopping criteria is evaluated and from which interim stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\nExplore Arm Dropping Criteria\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\nSuccess/Futility Stopping Contours\n\n\n\n\n\n\n\n\n\n\nFigure 24\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the criterion to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are a success/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#selected-arms",
    "href": "documentation/v71/userguides/core/simulation/tte.html#selected-arms",
    "title": "Time to Event Output",
    "section": "Selected Arms",
    "text": "Selected Arms\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\n\n\n\n\n\nFigure 25: Across scenario graph for selected arms with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#qoi-box-plots",
    "href": "documentation/v71/userguides/core/simulation/tte.html#qoi-box-plots",
    "title": "Time to Event Output",
    "section": "QOI Box Plots",
    "text": "QOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\n\n\n\n\n\nFigure 26: Across scenario graph showing QOIs per arm with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#ppn-success",
    "href": "documentation/v71/userguides/core/simulation/tte.html#ppn-success",
    "title": "Time to Event Output",
    "section": "Ppn Success",
    "text": "Ppn Success\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\n\n\n\n\n\nFigure 27: Across scenario graph for showing the proportion of trials that result in success with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#response",
    "href": "documentation/v71/userguides/core/simulation/tte.html#response",
    "title": "Time to Event Output",
    "section": "Response",
    "text": "Response\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\n\n\n\n\n\nFigure 28: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#allocation",
    "href": "documentation/v71/userguides/core/simulation/tte.html#allocation",
    "title": "Time to Event Output",
    "section": "Allocation",
    "text": "Allocation\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\n\n\n\n\n\nFigure 29: Across scenario graph for showing the distribution of allocation per arm with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#sample-size",
    "href": "documentation/v71/userguides/core/simulation/tte.html#sample-size",
    "title": "Time to Event Output",
    "section": "Sample Size",
    "text": "Sample Size\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\n\n\n\n\n\nFigure 30: Across scenario graph for showing the mean sample size of the study with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#interim-vs-final-scatter-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#interim-vs-final-scatter-plot",
    "title": "Time to Event Output",
    "section": "Interim vs Final Scatter Plot",
    "text": "Interim vs Final Scatter Plot\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\n\n\n\n\n\nFigure 31: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#receiver-operating-characteristics",
    "href": "documentation/v71/userguides/core/simulation/tte.html#receiver-operating-characteristics",
    "title": "Time to Event Output",
    "section": "Receiver Operating Characteristics",
    "text": "Receiver Operating Characteristics\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\n\n\n\n\n\nFigure 32: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#summary-per-scenario",
    "href": "documentation/v71/userguides/core/simulation/tte.html#summary-per-scenario",
    "title": "Time to Event Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after time-to-event simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn S uc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn F ut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inco nclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt; Dose&gt;&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn I ncorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation tabs provided in FACTS output for time-to-event trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nResponse\nThe following columns provide summaries of the estimated hazard ratios based on the bayesian model incorporating the dose response model and the predictor model, if one is specified.\n\nResponse columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nSD Trt .: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the treatment hazard ratio for each arm. The SD of the treatment response for the control arm is always 0.\n\n\nTrue Resp: &lt;Dose&gt;\nOne per arm\nThis is the true Hazard Ratio from which the simulation data was sampled. When using VSR with Event Rate | Predictor, these rates will not be the same as the Dose Response HRs entered on the VSR &gt; Explicitly Defined (ER | P) &gt; Dose Response tab.\n\n\n\n\n\nObserved\nBy right clicking and selecting the Observed columns, a pop-out will appear that provides the following columns. These columns relate to the raw data observed in the trial.\n\nObserved columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Total Events\n1\nThe mean (over the simulations) of the number of events observed in each trial.\n\n\nMean Events &lt;Dose&gt;\nOne per arm\nThe mean (over the simulations) of the number of events observer in each arm in each trial.\n\n\nMean Events &lt;Dose&gt; &lt;S egment&gt;\nOne per arm per segment\nThe mean (over the simulations) of the number of events observer in each arm and segment in each trial.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Predictor Data, Opportunity to Complete Predictor, Events, Predictor Events) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\nOne per arm\nIf dropouts are simulated this is the mean (over the simulations) of the number of subjects in each arm that dropout before an event is observed.\n\n\nMean Exposure &lt;Dose&gt; &lt;S egment&gt;\nOne per arm\nThe mean (over the simulations), of the total exposure of the subjects observed on each arm and segment.\n\n\n\n\n\nProbabilities\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns. These columns provide summaries of the Quantities of Interest values.\n\nProbabilities columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\nHierarchical Prior Parameters\nBy right clicking and selecting the Hierarchical Prior Parameters columns, a pop-out will appear that provides the following columns.\n\nHierarchical Prior Parameters columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean BAC Mu\n1\nThe average (over the simulation) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nSD BAC Mu\n1\nThe average (over the simulations) of the standard deviation of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nMean BAC Tau\n1\nThe average (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nSD BAC Tau\n1\nThe standard deviation (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Control (hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nMean BAAC Mu\n1\nThe average (over the simulation) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nSD BAAC Mu\n1\nThe average (over the simulations) of the standard deviation of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nMean BAAC Tau\n1\nThe average (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nSD BAAC Tau\n1\nThe standard deviation (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Active Comparator (hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator, otherwise the column contains -9999.\n\n\n\n\n\nPredictor\nBy right clicking and selecting the Predictor columns, a pop-out will appear that provides the following columns. These colums provide summaries of the bayesian model estimates relating to the predictor endpoint.\n\nPredictor columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Pred. Resp.: &lt;Dose&gt;\nOne per arm\nThe mean (over the simulations) of the mean estimate of the predictor response for each arm. The response reported is:\nContinuous predictor: The mean change from baseline\nDichotomous predictor: The response rate\nTTE predictor: The hazard ratio\nNo predictor: 0\n\n\nSD Pred. Resp.: &lt;Dose&gt;\nOne per arm\nThe SD (over the simulations) of the mean estimate of the predictor response for each arm.\n\n\nMean Pred. Sigma\n1\nThis is the average (over the simulations) of the estimate of sigma (SD of response) of a continuous predictor, if one was being used. Otherwise the column contains -9999.\n\n\nSD Pred Sigma\n1\nThis is the standard deviation (over the simulations) of the estimate of the sigma (SD of the response) of a continuous predictor, if one was being used. Otherwise the column contains -9999.\n\n\nTrue P redictor Resp: &lt;Dose&gt;\nOne per arm\nThis is the true predictor response from which the simulated subject predictor responses were sampled. For a continuous predictor this is the mean of the response, for a dichotomous predictor it is the predictor response rate and for a time-to-event predictor it is the hazard ratio of the predictor, otherwise the column contain -9999.\n\n\nTrue P redictor Sigma: &lt;Dose&gt;\nOne per arm\nThis is the true standard deviation of the predictor response from which the simulated subject predictor responses were sampled when the predictor is a continuous measure, otherwise the column contains -9999\n\n\nMean Pred. Lambda: &lt;Dose&gt;\nOne per arm\nThis is the average (over the simulations) of the estimate of Lambda for each dose in the endpoint predictor model – the estimated mean time to the final event for each dose either where the predictor is zero for a continuous or dichotomous predictor, or for the time after observing the event of time-to-event predictor. Otherwise the column contains 0.\n\n\nSD Pred. Lambda: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of Lambda for each dose (see above). Otherwise the column contains 0.\n\n\nMean Pred. Beta\n1\nThis is the average (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor. Otherwise the column contains -9999.\n\n\nSD Pred. Beta\n1\nThis is the standard deviations (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor. Otherwise the column contains -9999.\n\n\n\n\n\nModel Parameters\nBy right clicking and selecting the Model Parameters columns, a pop-out will appear that provides the following columns.\n\nModel Parameters columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nTrue P redictor Baseline Hazard Lambda: &lt;s egment&gt;\nOne per VSR pr edictor hazard rate segment\nThe true hazard rate for the predictor (if the predictor is itself an event), in each of the predictor VSR time segments.\n\n\nLambda: &lt;s egment&gt;\nOne per Design, Hazard Model segment\nThe mean (over the simulations) of the mean estimate of lambda – the event rate (events per week) in each time segment of the hazard model.\n\n\nSE Lambda: &lt;s egment&gt;\nOne per Design, Hazard Model segment\nThe standard error (over the simulations) of the mean estimate of lambda for each time segment of the hazard model.\n\n\nTrue Lambda: &lt;s egment&gt;\nOne per VSR control hazard segment\nThe true hazard rate for each segment of the virtual subject response hazard rate profile.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#detailed-simulation-results",
    "href": "documentation/v71/userguides/core/simulation/tte.html#detailed-simulation-results",
    "title": "Time to Event Output",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 15‑1) displays the individual results for each simulation. This is the contents of the “simulations.csv” file, which is described below.\n\n\n\n\n\n\nFigure 33: The simulation results pop out viewer showing simulation highlights.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary.csv",
    "title": "Time to Event Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContents of the summary.csv file for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nT imestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTC that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.subj 80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nP(ES)\n\n1\n\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;dose&gt;\nD\nThe mean of the estimates of hazard ratio of each treatment arm. (This is always 1 for the control arm)\n\n\nSE Resp &lt;dose&gt;\nD\nThe standard error, over the simulations, of the estimate of hazard ratio of each treatment arm. (This is always 0 for the control arm).\n\n\nTrue Mean resp &lt;dose&gt;\nD\nThe true hazard ratio for each treatment arm used to derive the simulated subject event times (the HR for the control arm is always 1).\n\n\nMean p redictor resp &lt;dose&gt;\nD\nThe mean (over the simulations) of the estimated predictor response per arm – this is the estimated mean response for a continuous predictor, the estimated response rate for a dichotomous predictor and the estimated hazard ratio for a time to event predictor.\n\n\nSE p redictor resp &lt;dose&gt;\nD\nThe standard error (over the simulations) of the estimated predictor response per arm.\n\n\nMean P redictor Sigma\n1\nThe mean (over the simulation) of the estimated standard deviation in the predictor response when modeling a continuous predictor.\n\n\nSE P redictor Sigma\n1\nThe standard error (over the simulations) of the estimated standard deviation in the predictor response when modeling a continuous predictor.\n\n\nTrue Mean p redictor resp &lt;dose&gt;\nD\nThe true predictor response rate used to drive the simulation of the subjects’ predictor outcomes – this is the mean of the response for a continuous predictor, the response rate for a dichotomous predictor and the hazard ratio for a time to event predictor.\n\n\nTrue Sigma p redictor &lt;dose&gt;\nD\nThe true sigma used to drive the simulation of the subjects’ predictor outcomes when the predictor is a continuous measure.\n\n\nMean P redictor Lambda &lt;dose&gt;\nD\nThe mean (over the simulations) of the estimate of lambda – the estimated mean time to the final event in the endpoint predictor model – the estimated mean time to the final event for each dose when the predictor is zero (continuous or dichotomous) or for the time after observing the predictor event.\n\n\nSE P redictor Lambda &lt;dose.\nD\nThis is the standard error (over the simulations) of the estimate of lambda for each dose.\n\n\nMean P redictor Beta\n1\nThis is the mean (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor.\n\n\nSE p redictor Beta\n1\nThis is the standard error (over the simulations) of the estimate of Beta.\n\n\nMean TTE P redictor Baseline Hazard Lambda &lt;s egment&gt;\nS\nThis is the mean (over the simulations) of the estimate of the event rate on the control arm in each hazard model time segment.\n\n\nSE TTE P redictor Baseline Hazard Lambda &lt;s egment&gt;\nS\nThis is the standard error (over the simulations) of the estimate of the event rate on the control arm in each hazard model time segment.\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nEst. Lambda &lt;s egment&gt;\nS\nThe mean, over the simulations, of the estimate of the event rate on the control arm in each time segment of the hazard model.\n\n\nSE lambda &lt;s egment&gt;\nS\nThe mean, over the simulations, of the SD of the estimate of the event rate on the control arm in each time segment of the hazard model.\n\n\nMean Total Events\n1\nThe mean, over the simulations of the total number of events observed in the trials.\n\n\nSE Total Events\n\nThe standard error, over the simulations, of the total number of events observed in the trials.\n\n\nNo. Events &lt;dose&gt;\nD\nThe mean, over the simulations, of the number of events observed on each arm.\n\n\nSE No. Events &lt;dose&gt;\n\nThe standard error, over the simulations, of the number of events observed on each arm.\n\n\nMean Exposure &lt;dose&gt;\n\nThe mean, over the simulations, of the total exposure of the subjects observed on each arm.\n\n\nSE Exposure &lt;dose&gt;\n\nThe standard error, over the simulations, of the total exposure of the subjects observed on each arm.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nQOI Columns\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary_freq.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary_freq.csv",
    "title": "Time to Event Output",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContents of the summary_freq.csv file for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn LR Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Log Rank test) is less than the user specified one-sided alpha.\n\n\nPpn LR Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Log Rank test) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn LR Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Log Rank test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn LR Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Log Rank test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn Wilcox Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Wilcoxon test) is less than the user specified one-sided alpha.\n\n\nPpn Wilcox Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Wilcoxon test) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn Wilcox Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Wilcoxon test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn Wilcox Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Wilcoxon test) is less than the Bonferroni correct user specified one-sided alpha.\n\n\nMean HR &lt;dose&gt;\nD\nThe mean Hazard Ratio per dose.\n\n\nSE HR &lt;dose&gt;\nD\nThe standard error of the Hazard Ratio per dose\n\n\nPpn HR Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Cox model) is less than the user specified one-sided alpha.\n\n\nPpn HR Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Cox model) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn HR Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Cox model) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn HR Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Cox model) is less than the Bonferroni correct user specified one-sided alpha.\n\n\nBias &lt;dose&gt;\nD\nThe difference between the mean response and the true (simulated) response per dose\n\n\nCoverage &lt;dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true response rate used to simulate subject responses.\n\n\nMean KM med &lt;dose&gt;\nD\nThe mean Kaplan-Meier estimate of the median survival time per dose\n\n\nSE KM med &lt;dose&gt;\nD\nThe standard error of the Kaplan-Meier estimate of the median survival time per dose\n\n\nPredictor Cols\n\nThe appropriate output columns for the chosen type of predictor. See next tables for specifics of columns provided for each predictor type.\n\n\n\n\nContinuous Predictor:\n\nExtra columns on the summary_freq.csv file when using a continuous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox Mean HR &lt;dose&gt;\nD\nThe mean value of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Cox SE HR &lt;dose&gt;\nD\nThe standard error of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Avg. Min &lt;dose&gt;\nD\nThe average of the minimum value for the predictor per dose.\n\n\nPredictor Avg. 10-percentile &lt;dose&gt;\nD\nThe average of the 10th percentile values for the predictor per dose.\n\n\nPredictor Avg. 25-percentile &lt;dose&gt;\nD\nThe average of the 25th percentile values for the predictor per dose.\n\n\nPredictor Avg. Median &lt;dose&gt;\nD\nThe average of the median values for the predictor per dose.\n\n\nPredictor Avg. 75-percentile &lt;dose&gt;\nD\nThe average of the 75th percentile values for the predictor per dose.\n\n\nPredictor Avg. 90-percentile &lt;dose&gt;\nD\nThe average of the 90th percentile values for the predictor per dose.\n\n\nPredictor Avg. Max &lt;dose&gt;\nD\nThe average of the maximum value for the predictor per dose.\n\n\nPredictor Mean &lt;dose&gt;\nD\nThe mean of the mean value of the predictor per dose.\n\n\nPredictor SE &lt;dose&gt;\nD\nThe standard error of the mean value of the predictor per dose.\n\n\n\n\n\nDichotomous Predictor:\n\nExtra columns on the summary_freq.csv file when using a dichotomous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox Mean HR &lt;dose&gt;\nD\nThe mean value of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Cox SE HR &lt;dose&gt;\nD\nThe standard error of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Mean Response Rate &lt;dose&gt;\nD\nThe mean of the predictor response rate per dose.\n\n\nPredictor SE Response Rate &lt;dose&gt;\nD\nThe standard error of the predictor response rate per dose.\n\n\n\n\n\nTime-to-Event Predictor\n\nExtra columns on the summary_freq.csv file when using a time-to-event predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Mean KM Median &lt;dose&gt;\nD\nThe mean of the Kaplan-Meier estimate of the median time to the predictor event per dose.\n\n\nPredictor SE KM Median &lt;dose&gt;\nD\nThe standard error of the Kaplan-Meier estimate of the median time to the predictor event per dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nSimulations and weeks file column names for a time-to-event endpoint.\n\n\n\n\n\n\n\n\n\nColumn Title\nNu mber of col umns\nIn s imula tions file\nIn w ee ks fi le\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nLastInt erimNumber\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:\n\n= Early success\n= Late success\n= Late futility\n= Early futility\n= Success to futility flip-flop\n= Futility to success flip-flop\n= Inconclusive\n\n\n\nEarly Success\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nMean resp &lt;dose&gt;\nD\n✔\n✔\nThe estimated hazard ratio of each treatment arm.\n\n\nSD resp &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of the hazard ratio of each treatment arm.\n\n\nTrue Mean resp &lt;dose&gt;\nD\n✔\n✔\nThe true mean response (hazard ratio) of each treatment arm for this simulation.\n\n\nMean Predictor Resp &lt;dose&gt;\nD\n✔\n✔\nThe estimated response of the predictor at each treatment arm.\nCts: mean change from baseline\nDich: response rate\nTTE: hazard ratio with control\n\n\nSD Predictor Resp &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of the predictor response.\n\n\nMean Predictor Sigma\n1\n✔\n✔\nThe mean of the estimate of the sigma of the predictor response (the sd of the response of a continuous predictor)\n\n\nSD Predictor Sigma\n1\n✔\n✔\nThe SD of the estimate of the sigma of the predictor response (the sd of the response of a continuous predictor)\n\n\nTrue Predictor Mean resp &lt;dose&gt;\nD\n✔\n✔\nThe true mean predictor response being simulated for each treatment arm (mean of a continuous predictor, response rate of a dichotomous predictor and the hazard ratio of a time-to-event predictor)\n\n\nTrue Predictor Sigma &lt;dose&gt;\nD\n✔\n✔\nThe true sigma of the predictor response of each treatment arm, if the predictor is a continuous endpoint\n\n\nMean Predictor Lambda &lt;dose&gt;\nD\n✔\n✔\nThe mean of the estimate of Lambda for each dose in the endpoint predictor model – the estimated mean time to the final event for each dose where the predictor is zero for a continuous or dichotomous predictor or the time after observing the event of a time-to-event predictor.\n\n\nSD Predictor Lambda &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of Lambda for each dose in the endpoint predictor model (see above).\n\n\nMean Predictor Beta\n1\n✔\n✔\nThe mean of the estimate of the Beta coefficient in the endpoint predictor model.\n\n\nSD Predictor Beta\n1\n✔\n✔\nThe standard deviation of the estimate of the Beta coefficient in the endpoint predictor model.\n\n\nMean TTE Predictor Baseline Hazard Lambda &lt;seg&gt;\nS\n✔\n✔\nThe mean estimate of the hazard rate of the predictor event on the control arm, when using a time-to-event predictor, for each time segment of the predictor control hazard model.\n\n\nMean TTE Predictor Baseline Hazard Lambda &lt;seg&gt;\nS\n✔\n✔\nThe standard deviation of the estimate of the hazard rate of the predictor event on the control arm, when using a time-to-event predictor, for each time segment of the predictor control hazard model.\n\n\nNum Events &lt;dose&gt;\n&lt;segment&gt;\nS * D\n✔\n✔\nThe number of events observed on each arm in each control hazard model time segment.\n\n\nTotal Exposure &lt;dose&gt;\n&lt;segment&gt;\nS * D\n✔\n✔\nThe total exposure (in weeks) of the subjects on each arm in each control hazard model time segment.\n\n\nSeed1, Seed2\n2\n✔\n✔\nThe random number seeds at the start of the interim.\nDue to a change in the random number generator to one that uses seeds far larger than two 32-bit integers these values are not currently being written out.\n\n\nDR Param &lt;param&gt;\n4\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled α1, α2, .. the subscripts corresponding to the column their value appears in here.\n\n\nSd DR Param &lt;param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nMean Lambda &lt;seg&gt;\nS\n✔\n✔\nThe mean estimate of the hazard rate on the control arm, for each time segment of the analysis model.\n\n\nSd Lambda &lt;seg&gt;\nS\n✔\n✔\nThe standard deviation of the estimate of the hazard rate on the control arm, for each time segment of the analysis model.\n\n\nTrue Lambda\nS\n✔\n✔\n\n\n\nNum Events &lt;dose&gt;\nD\n✔\n✔\nThe number of events observed on each treatment arm.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns are grouped in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations_freq.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations_freq.csv",
    "title": "Time to Event Output",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\nContents of the simulations_freq.csv file for a time-to-event simulation.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nLR Stat &lt;dose&gt;\nD\nUnadjusted log-rank test statistic per arm\n\n\nLR pval &lt;dose&gt;\nD\nThe unadjusted log-rank p-value per arm\n\n\nLR adj_pval &lt;dose&gt;\nD\nThe Bonferroni adjusted log-rank p-value per arm\n\n\nWilcoxon stat &lt;dose&gt;\nD\nThe Wilcoxon test statistic per arm\n\n\nWilcoxon pval &lt;dose&gt;\nD\nThe unadjusted Wilcoxon p-value per arm\n\n\nWilcoxon adj_pval &lt;dose&gt;\nD\nThe Bonferroni adjusted Wilcoxon p-value per arm\n\n\nHR &lt;dose&gt;\nD\nThe Cox model hazard ratio per arm\n\n\nHR pval &lt;dose&gt;\nD\nThe unadjusted Cox model p-value per arm\n\n\nHR lower CI &lt;dose&gt;\nD\nThe unadjusted lower bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nHR upper CI &lt;dose&gt;\nD\nThe unadjusted upper bound of the alpha confideneeeddce interval of the hazard ratio estimate.\n\n\nHR adj pval &lt;dose&gt;\nD\nThe Bonferroni adjusted Cox model p-value per arm\n\n\nHR adj lower CI &lt;dose&gt;\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nHR adj upper CI &lt;dose&gt;\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nKM med &lt;dose&gt;\nD\nThe Kaplan Meier estimates of the median survival time per arm.\n\n\nPredictor Cols\nD\nThe appropriate output columns for the chosen type of predictor. See next tables for specifics of columns provided for each predictor type.\n\n\n\n\nContinuous predictor\n\nExtra columns on the simulations_freq.csv file when using a continuous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox HR &lt;dose&gt;\nD\nThe Cox model Hazard Ratio per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pred\n1\nThe Cox model predictor coefficient\n\n\nPredictor Cox HR pval &lt;dose&gt;\nD\nThe Cox model p-value per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pval pred\n1\nThe Cox model predictor p-value\n\n\nPredictor Cox HR lower CI &lt;dose&gt;\nD\nThe Cox model lower bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR lower CI pred\n1\nThe Cox model lower bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Cox HR upper CI &lt;dose&gt;\nD\nThe Cox model upper bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR upper CI pred\n1\nThe Cox model upper bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Min &lt;dose&gt;\nD\nThe minimum value of the predictor value for each treatment arm.\n\n\nPredictor 10-percentile &lt;dose&gt;\nD\nThe 10th percentile value of the predictor value for each treatment arm.\n\n\nPredictor 25-percentile &lt;dose&gt;\nD\nThe 25th percentile value of the predictor value for each treatment arm.\n\n\nPredictor Median &lt;dose&gt;\nD\nThe mediam value of the predictor value for each treatment arm.\n\n\nPredictor 75-percentile &lt;dose&gt;\nD\nThe 75th percentile value of the predictor value for each treatment arm.\n\n\nPredictor 90-percentile &lt;dose&gt;\nD\nThe 90th percentile value of the predictor value for each treatment arm.\n\n\nPredictor Max &lt;dose&gt;\nD\nThe maximum value of the predictor value for each treatment arm.\n\n\nPredictor Mean &lt;dose&gt;\nD\nThe mean of the predictor value for each treatment arm\n\n\nPredictor SD &lt;dose&gt;\nD\nThe SD of the mean of the predictor value for each arm\n\n\n\n\n\nDichotomous predictor\n\nExtra columns on the simulations_freq.csv file when using a dichotomous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox HR &lt;dose&gt;\nD\nThe Cox model Hazard Ratio per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pred\n1\nThe Cox model predictor coefficient\n\n\nPredictor Cox HR pval &lt;dose&gt;\nD\nThe Cox model p-value per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pval pred\n1\nThe Cox model predictor p-value\n\n\nPredictor Cox HR lower CI &lt;dose&gt;\nD\nThe Cox model lower bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR lower CI pred\n1\nThe Cox model lower bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Cox HR upper CI &lt;dose&gt;\nD\nThe Cox model upper bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR upper CI pred\n1\nThe Cox model upper bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Response Rate &lt;dose&gt;\nD\nThe predictor response rate per arm.\n\n\n\n\n\nTime-to-event Predictor\n\nExtra columns on the simulations_freq.csv file when using a time-to-event predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor KM Median &lt;dose&gt;\nD\nThe Kaplan Meier estimate of the median time to the predictor event per arm",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\nThe patients file output when simulating a time-to-event tria..\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nDuration\n1\nThe time of observation of the subject (in weeks)\n\n\nOutcome\n1\nWhether an event was observed (1) or not (0)\n\n\nPredictor\n1\nThe value of the observed predictor\n\n\nPred Outcome\n1\nA flag indicating whether the predictor was observed or not\n\n\nDropout\n1\nA flag indicating whether the subject dropped out (1) or not (0).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nMCMC file format for a time-to-event simulation.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nHR &lt;dose&gt;\nD\nThe estimate of the hazard ratio for each dose, based on the dose response model fitted.\n\n\nLambda &lt;seg&gt;\nS\nThe estimate of the control hazard rate in each segment of the control hazard rate model\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nPredictor …\n\nThe parameters of the predictor endpoint, these vary by the type of endpoint that the predictor has – they will be consistent with the parameters that would be output if the predictor was the only endpoint in a single endpoint design.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/dichotomous.html",
    "href": "documentation/v71/userguides/core/vsr/dichotomous.html",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "",
    "text": "In FACTS Core with a dichotomous endpoint there are 4 different ways to specify the virtual subject response:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/dichotomous.html#dose-response",
    "href": "documentation/v71/userguides/core/vsr/dichotomous.html#dose-response",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Dose Response",
    "text": "Dose Response\n\n\n\n\n\n\nFigure 1: The Explicitly Defined &gt; Dose Response model for a dichotomous endpoint.\n\n\n\nDose response profiles can be added and deleted, and for each profile the user specifies:\n\nThe response rate for each treatment arm.\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\n\nThe graph on the tab shows the mean response rate specified and the target.\nIf a 2D treatment arm model is being used, the doses are listed in “effective dose strength order” as was defined on the treatment arm tab.\n\nLoad Scenario Rates From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses. Each individual simulation uses one set of response rates from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from one ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 2: The Explicitly Defined &gt; Dose Response tab when loading scenario means from a file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual response rates and the mean response rate over all the VSRs.\nThere is a check box per dose that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\nThe format of the “.mvsr” file is a simple CSV text file. Lines starting with a ‘#’ character are ignored, so the file can include comment and header lines. There must be one column per treatment arm, and they must be in dose index order. Each value is the underlying response rate to simulate. E.g.:\n#Cntrl, D1, D2, D3, D4\n0.05, 0.1, 0.15, 0.25, 0.5\n0.05, 0.1, 0.15, 0.23, 0.45\n0.05, 0.1, 0.15, 0.21, 0.4\n0.05, 0.1, 0.15, 0.19, 0.35\n0.05, 0.1, 0.15, 0.17, 0.3\n0.05, 0.1, 0.15, 0.15, 0.25",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/dichotomous.html#longitudinal",
    "href": "documentation/v71/userguides/core/vsr/dichotomous.html#longitudinal",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Longitudinal",
    "text": "Longitudinal\nThe tab for defining dichotomous longitudinal response profiles allows the user to specify the overall transition probabilities between responder and non-responder.\nIf no special longitudinal options are selected on the Study Info page, then this default simulation method that uses transition probabilities to simulate sequential dichotomous endpoints is used.\nThe transition probabilities method for generating longitudinal responses simulates the response observed at each visit by using the probability that a subject becomes or remains a ‘1’ from one visit to the next – and all subjects start with a response of 0.\n\n\n\n\n\n\nFigure 3: The dichotomous endpoint Longitudinal VSR tab.\n\n\n\nThe user specifies for each visit,\n\nthe probability of a subject whose response was a ‘0’ at the previous visit having a response of ‘1’ at this visit\nthe probability of a subject whose response was a ‘1’ at the previous visit having a response of ‘1’ at this visit\n\nHowever, these probabilities imply a particular probability that a subject has a response of ‘1’ at the final visit, so they need to be modified for each arm in each dose response profile to give the desired final probability of response. This is done by numerically determining for each final response rate, a single value which when added to all the specified transition probabilities in the log-odds space yield the desired probability of final response.\nLet \\(Q_{td}\\) be the probability of transitioning from 0 to 1, and \\(R_{td}\\) be the probability of transitioning from 1 to 1 for each visit (t) and dose (d).\nFor the 1st visit:\n\\[P\\left( y_{1d} = 1 \\right) = Q_{1d}\\]\nwhich is like considering the imaginary 0th visit to have been a non-response. For subsequent visits:\n\\[P\\left( y_{td} = 1 \\middle| y_{t - 1,d} = 0 \\right) = Q_{td}\\]\n\\[P\\left( y_{td} = 1 \\middle| y_{t - 1,d} = 1 \\right) = R_{td}\\]\nThe dose response (\\(P_{d}\\)) is first specified in the VSR &gt; Explicitly Defined &gt; Dose Response tab, and then longitudinal components (\\(q_{t}\\) and \\(r_{t}\\)) are specified separately. FACTS then makes an adjustment to the longitudinal components \\(q_{t}\\) and \\(r_{t}\\) to calculate \\(Q_{td}\\) and \\(R_{td}\\) for each dose while maintaining the value for \\(P_{d}\\) specified in the Dose Response tab.\nThe matrices Q and R are calculated by applying offsets \\(f_{d}\\) in log odds space to the longitudinal values:\n\\[Q_{t,d} = \\frac{e^{q'_{td}}}{1 + e^{q'_{td}}}, \\ \\ q'_{td} = \\ln\\left( \\frac{q_{t}}{1 + q_{t}} \\right) + f_{d}\\]\n\\[R_{t,d} = \\frac{e^{r'_{td}}}{1 + e^{r'_{td}}}, \\ \\ r'_{td} = \\ln\\left( \\frac{r_{t}}{1 + r_{t}} \\right) + f_{d}\\]\nwhere \\(f_{d}\\) is calculated iteratively for each dose to ensure that Q and R give the correct final probability of response for each dose. As a result,\n\\[P_{d} = x_{Td}\\]\nwhere\n\\[x_{1d} = Q_{1d}\\]\nand\n\\[x_{t} = x_{t - 1}R_{td} + \\left( 1 - x_{t - 1} \\right)Q_{td}\\]\n\nExample\nWith 3 visits and probabilities of \\(0\\rightarrow 1\\) of \\(0.2\\) and of \\(1\\rightarrow 1\\) of \\(0.9\\) at each visit, the probability of a final response is 0.438. This 0.438 is fixed, and cannot be changed without changing the transition probabilities.\n\n\n\n\n\n\n\n\n\n\nIndex\nVisit\nProb 1 \\(\\rightarrow\\) 1\n(\\(r_t\\))\nProb 0 \\(\\rightarrow\\) 1\n(\\(q_t\\))\nCumulative Pr(Resp)\n\n\n\n\n1\nVisit 1\n\n\\(0.2\\)\n\\(0.2\\)\n\n\n2\nVisit 2\n\\(0.9\\)\n\\(0.2\\)\n\\(0.2*0.9\\) \\(+\\;(1-0.2)*0.2\\) \\(= 0.34\\)\n\n\n3\nVisit 3\n\\(0.9\\)\n\\(0.2\\)\n\\(0.34*0.9\\) \\(+\\;(1-0.34)*0.2\\) \\(= 0.438\\)\n\n\n\nIf a response profile calls for the probability of a final response to be simulated with a probability of 0.8, a fixed offset in log-odds is found which, when applied to all the transition probabilities, results in the desired final probability of a response. Replicating this by hand to an accuracy of 4 significant digits yields an offset of \\(f_d = 1.173\\), which gives:\n\\[\\text{logit}^{-1}(\\text{logit}(0.2) + 1.173) = 0.4469\\] \\[\\text{logit}^{-1}(\\text{logit}(0.9) + 1.173) = 0.9668\\]\nThen,\n\n\n\n\n\n\n\n\n\n\nIndex\nVisit\nProb 1 \\(\\rightarrow\\) 1\n(\\(r_t\\))\nProb 0 \\(\\rightarrow\\) 1\n(\\(q_t\\))\nCumulative Pr(Resp)\n\n\n\n\n1\nVisit 1\n\n\\(0.4469\\)\n\\(0.4469\\)\n\n\n2\nVisit 2\n\\(0.9668\\)\n\\(0.4469\\)\n\\(0.4469*0.9668\\) \\(+ (1-0.4469)*0.4469\\) \\(= 0.6792\\)\n\n\n3\nVisit 3\n\\(0.9668\\)\n\\(0.4469\\)\n\\(0.6792*0.9668\\) \\(+ (1-0.6792)*0.4469\\) \\(= 0.8000\\)",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/multendpt.html",
    "href": "documentation/v71/userguides/core/vsr/multendpt.html",
    "title": "Virtual Subject Response - Multiple Endpoint",
    "section": "",
    "text": "When simulating multiple endpoints, each endpoint will have its own set of explicitly defined VSR tabs for specifying their response profiles. See the continuous or dichotomous VSR pages for descriptions on these tabs.\n\n\nIn addition to the standard VSR tabs that all endpoints have, the multiple endpoint engine also has a Composite VSR tab that allows for the creation of a set of VSR scenarios that are made up of one response VSR for each endpoint and one longitudinal VSR for each endpoint that uses a longitudinal model.\nThe number of different profiles that need to be combined to fully define the responses to be simulated in FACTS Core Multiple Endpoint grows rapidly with the number of endpoints, and the number of different combinations quickly becomes far greater than in the single endpoint design engines. The Composite Response tab has been added to the FACTS GUI to help the user manage which combinations of profiles they would like to include in the simulations.\n\n\n\n\n\n\nFigure 1: The Composite Response tab for a multiple endpoint design with 2 endpoints.\n\n\n\nThere essentially are two approaches available to the user to specify the combinations of response profiles:\n\nClicking on the ‘Generate’ button to generate all possible combinations and then deleting the combinations that are not of interest\nBy adding composite profiles and manually selecting the individual response profiles that go together to comprise the composite profile.\n\nThe tab is divided into three areas:\n\nThe list of profiles on the left hand side, with controls at the top to add another profile or delete the currently selected profile, and controls at the bottom to delete the current profiles generate all possible profiles, or simple delete all current profile. The properties of the currently highlighted profile are shown on the right of the screen.\nOn the right, for each of the endpoints there are controls to show which baseline, dose response and longitudinal profile for that endpoint are used in the currently selected composite profile. The profile names are shown in ‘drop down’ lists that the user can use to show the list of all the profiles of that type defined for that endpoint and to select an alternative for the currently selected composite profile.\nOn the right below the controls, a table of the treatment arms with a check box per arm is displayed along with a graph that shows the individual and combined utilities of each endpoint for the currently selected dose response profiles. The check boxes allow the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were both successful and where a ‘good’ treatment arm selected. The arm section uses the target QOI specified on the Variants tab (whether or not Variants are being used).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/multendpt.html#composite-response",
    "href": "documentation/v71/userguides/core/vsr/multendpt.html#composite-response",
    "title": "Virtual Subject Response - Multiple Endpoint",
    "section": "",
    "text": "In addition to the standard VSR tabs that all endpoints have, the multiple endpoint engine also has a Composite VSR tab that allows for the creation of a set of VSR scenarios that are made up of one response VSR for each endpoint and one longitudinal VSR for each endpoint that uses a longitudinal model.\nThe number of different profiles that need to be combined to fully define the responses to be simulated in FACTS Core Multiple Endpoint grows rapidly with the number of endpoints, and the number of different combinations quickly becomes far greater than in the single endpoint design engines. The Composite Response tab has been added to the FACTS GUI to help the user manage which combinations of profiles they would like to include in the simulations.\n\n\n\n\n\n\nFigure 1: The Composite Response tab for a multiple endpoint design with 2 endpoints.\n\n\n\nThere essentially are two approaches available to the user to specify the combinations of response profiles:\n\nClicking on the ‘Generate’ button to generate all possible combinations and then deleting the combinations that are not of interest\nBy adding composite profiles and manually selecting the individual response profiles that go together to comprise the composite profile.\n\nThe tab is divided into three areas:\n\nThe list of profiles on the left hand side, with controls at the top to add another profile or delete the currently selected profile, and controls at the bottom to delete the current profiles generate all possible profiles, or simple delete all current profile. The properties of the currently highlighted profile are shown on the right of the screen.\nOn the right, for each of the endpoints there are controls to show which baseline, dose response and longitudinal profile for that endpoint are used in the currently selected composite profile. The profile names are shown in ‘drop down’ lists that the user can use to show the list of all the profiles of that type defined for that endpoint and to select an alternative for the currently selected composite profile.\nOn the right below the controls, a table of the treatment arms with a check box per arm is displayed along with a graph that shows the individual and combined utilities of each endpoint for the currently selected dose response profiles. The check boxes allow the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were both successful and where a ‘good’ treatment arm selected. The arm section uses the target QOI specified on the Variants tab (whether or not Variants are being used).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/multendpt.html#required-format-of-externally-simulated-file",
    "href": "documentation/v71/userguides/core/vsr/multendpt.html#required-format-of-externally-simulated-file",
    "title": "Virtual Subject Response - Multiple Endpoint",
    "section": "Required Format of Externally Simulated File",
    "text": "Required Format of Externally Simulated File\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nEndpoint Index (1 = primary, 2 = first auxiliary, …)\nVisit Index (1 = first visit, 2 = second visit, …), if baseline is included, then baseline is visit 0.\nResponse\n\nContinuous:\n\nIf no baseline is included, then the response provided must be change from baseline\nIf baseline is included, then the response provided must be the absolute response (and the design engine will compute change from baseline or analyze the absolute response as specified by the user on the Study tab).\n\nDichotomous: 0 for no response, 1 for response, -1 for stable (restricted Markov)\n\n\nSubjects need to have unique, positive integer IDs, all records for each subject should be contiguous and all records for a particular endpoint for each subject should be contiguous.\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file with a continuous primary endpoint and dichotomous secondary endpoint.\n#subj id, Arm ID, Endpoint ID, Visit ID, Response\n1,1,1,1,-0.4233\n1,1,1,2,-0.8466\n1,1,1,3,-1.6933\n1,1,1,4,-2.1166\n1,1,2,1,0\n1,1,2,2,0\n1,1,2,3,0\n1,1,2,4,0\n2,1,1,1,0.4710\n2,1,1,2,0.9421\n2,1,1,3,1.8843\n2,1,1,4,2.3554\n2,1,2,1,0\n2,1,2,2,1\n2,1,2,3,0\n2,1,2,4,1",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/continuous.html",
    "href": "documentation/v71/userguides/core/longitudinalmodels/continuous.html",
    "title": "Longitudinal Models for Continuous Endpoints",
    "section": "",
    "text": "LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {\\(y_{it}\\)} is the set of observed responses from early visits, and \\(y_{i t_m}\\) is the last observed value of \\(y_{i t}\\), then the LOCF model for the final endpoint \\(Y_i\\) is\n\\[Y_i\\mid \\{y_{it}\\} = y_{it_m}\\]\nIn the continuous engine \\(t_m\\) can be any earlier observed visit including the baseline value.\n\n\nLinear Regression\n\n\n\n\n\n\nShiny App\n\n\n\nThe following shiny application for a tool that helps visualize and set priors for the linear regression longitudinal model.\nSee here.\n\n\nThe linear regression model fits a simple linear model from the data at each visit with the final visit\n\\[Y_i \\mid y_{it} \\sim \\alpha_t + \\beta_t y_{it} + \\text{N}(0,\\lambda_t^2)\\]\nThe parameter \\(\\alpha_t\\) is the intercept of the model for visit t, and the parameter \\(\\beta_t\\) is a multiplicative modifier (slope) of the response observed longitudinal at visit \\(t\\) to adjust the prediction of the final endpoint.\nImputation of the final endpoint value for a subject using the linear regression longitudinal model is based on only the latest observed visit’s endpoint value.\nThe default setting of “Same priors across all model instances and visits,” implies that each parameter \\(\\alpha\\), β, and λ have the same prior for all t. Estimation of those parameters is still done independently for each model instance. The one prior across all model instance are formulated as:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_\\mu, \\alpha_\\sigma^2)\\] \\[\\beta_t \\sim \\text{N}(\\beta_\\mu, \\beta_\\sigma^2)\\] \\[\\lambda_{t}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nThe above prior formulation may not be desirable if specifying priors that are not extremely diffuse – especially on the \\(\\beta\\) parameters. Instead, selecting “Specify priors per visit across all model instances,” will share the prior specification across all instances of the model, but allows for different priors to be put on the parameters associated with each visit. The user inputted prior parameters are now subscripted with \\(t\\) to denote the visit they correspond to. These priors apply to all model instances:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_{\\mu_t}, \\alpha_{\\sigma_t}^2)\\] \\[\\beta_t \\sim \\text{N}(\\beta_{\\mu_t}, \\beta_{\\sigma_t}^2)\\] \\[\\lambda_{t}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n_t}}{2},\\frac{\\lambda_{\\mu_t}^{2}\\lambda_{n_t}}{2} \\right)\\]\nIt is also possible to specify priors “Per model instance and visit,” in which every visit has separate priors, and those differing priors vary across model instances. This is the most flexible prior specification method. The user inputted prior parameters are now subscripted by both t for visit and i for model instance.\n\\[\\alpha_{ti} \\sim \\text{N}(\\alpha_{\\mu_{ti}}, \\alpha_{\\sigma_{ti}}^2)\\] \\[\\beta_{ti} \\sim \\text{N}(\\beta_{\\mu_{ti}}, \\beta_{\\sigma_{ti}}^2)\\] \\[\\lambda_{ti}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n_{ti}}}{2},\\frac{\\lambda_{\\mu_{ti}}^{2}\\lambda_{n_{ti}}}{2} \\right)\\]\nA potential starting place for non-informative prior values would be\n\n\\(\\alpha\\)\n\nmean of 0, SD \\(\\ge\\) largest expected response\n\n\\(\\beta\\)\n\nmean of either 0 or \\(\\frac{\\text{final visit time}}{\\text{early visit time}}\\), SD \\(\\ge\\) largest expected ratio of final visit to first visit\n\n\\(\\lambda\\)\n\nmean of expected SD of the endpoint (‘sigma’), weight of 1. The variability of the prediction from the longitudinal model (based on an observed intermediate response) should be less than that based solely on the treatment allocation, thus this is a weakly pessimistic prior on the effectiveness of the longitudinal model that should be quickly overwhelmed by the data.\n\n\nThis model is easy to understand and can be used even if there is only one visit, but doesn’t get more powerful if there are more visits. Its power will depend on the degree of correlation between the intermediate visit response and the final response.\n\n\nTime Course Hierarchical\nThe Time Course Hierarchical models the relationship between subjects’ early responses and their final response. It incorporates a per-subject offset from the mean response and a scaling factor from each visit to the final endpoint, but with no model of the change from visit-to-visit.\nThe response at the \\(t_{th}\\) visit for the \\(i^{th}\\) subject, having been randomized to the \\(d^{th}\\) dose is modeled as:\n\\[y_{it} \\sim e^{\\alpha_t}(\\theta_d + \\delta_i) + \\text{N}(0, \\lambda_t^2)\\]\nThe imputed final response (visit \\(T\\)$) for the \\(i^{th}\\) subject, having been randomized to the \\(d^{th}\\) dose is modeled as:\n\\[Y_{iT} \\sim \\theta_d + \\delta_i + \\text{N}(0, \\lambda_T^2)\\]\n(i.e. \\(\\alpha_T\\) is 0).\nThe model parameters can be interpreted as follows:\n\n\\(\\theta_d\\)\n\nthe estimated mean response at the final visit in dose \\(d\\) from the dose response model.\n\n\\(\\delta_i\\)\n\nthe estimated patient level random effect around the mean final response (\\(\\theta_d\\)) for the dose \\(d\\) that patient \\(i\\) is randomized to.\n\n\\(\\alpha_t\\)\n\na scaling parameter that determines the proportion of the final response that is observable at visit \\(t\\). A value of \\(\\alpha_t=0\\) indicates that the expected value of early visit \\(t\\) is equal to the estimated final visit mean \\(\\theta_d\\). A value of \\(\\alpha_t= −0.69315\\) indicates that the expected value of early visit \\(t\\) is 50% of the estimated final visit mean \\(\\theta_d\\).\n\n\\(\\lambda_t^2\\)\n\nthe variance of the endpoint around the estimated mean response at visit \\(t\\).\n\n\nThe prior for \\(\\alpha_t\\) is a normal distribution with a user specified the mean and standard deviation:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_\\mu, \\alpha_\\sigma^2)\\]\nThe prior for the \\(\\delta_i\\) terms is a normal distribution with a mean of 0 and variance τ2.\n\\[\\delta_i \\sim \\text{N}(0, \\tau^2)\\]\n\\(\\tau^2\\) is estimated as part of the longitudinal model, and has an inverse gamma prior distribution with prior central value \\(\\tau_\\mu\\) and weight (in terms of “equivalent number of observations”) \\(\\tau_n\\):\n\\[\\tau^{2} \\sim \\text{IG}\\left( \\frac{\\tau_{n}}{2},\\\\\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nThe prior for the \\(\\lambda_t^2\\) terms is an inverse gamma distribution with prior central value \\(\\lambda_\\mu\\) and weight (in terms of “equivalent number of observations”) \\(\\lambda_n\\):\n\\[\\lambda_{t}^{2}\\sim\\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be\n\n\\(\\alpha_t\\)\n\nmean of -2, SD of 2, … so the prior ~70% interval for \\(\\alpha_t\\) is between -4 and 0 (+/1 1 SD) and thus the prior 70% interval for \\(e^{\\alpha_t}\\) to be between 0.02 and 1.\n\n\\(\\tau\\)\n\nmean set to the expected SD of the endpoint (‘sigma’), with a weight of 1.\n\n\\(\\lambda_t\\)\n\nmean set to the expected SD of the endpoint (‘sigma’), with weight of 1.\n\n\nWe would expect \\(\\tau^2 + \\lambda^2 \\approx \\sigma^2\\), thus to specify a prior mean of \\(\\sigma\\) for each with a weight of 1 is a weakly pessimistic prior that should be quickly overwhelmed by the data.\nThis model is useful if there is thought to be a significant per-subject component to the response that should be manifest at the intermediate visits, and sufficient visits for the per-subject component to be estimated.\n\n\nKernel Density\nThe Kernel Density Model longitudinal model is a non-parametric re-sampling approach that is ideal for circumstances where the relationship between the interim time and the final endpoint is not known or not canonical.\nThe procedure is as follows. Assume an interim value for patient \\(i\\) at time \\(t\\), \\(Y_{it}\\). Patient \\(i\\) does not have an observed final endpoint at time \\(T\\), so one is to be imputed. Let \\((X_{1t},X_{1T}), \\ldots, (X_{nt}, X_{nT})\\) be the set of values for the previous subjects for whom there exists an interim value \\(X_{*t}\\) and final value \\(X_{*T}\\).\nTo impute a value of \\(Y_{iT}\\) given \\(Y_{it}\\), a pair \\((X_{kt},X_{kT})\\) is selected with probability based on the pair’s time \\(t\\) visit response’s proximity to the observed \\(Y_{it}\\):\n\\[\\Pr\\left(\\text{Selecting}\\left( X_{kt},\\\\X_{kT} \\right) \\right) = \\frac{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}{\\sum_{k = 1}^{n}{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}}\\]\nThen, a value of \\(Y_{iT}\\) is imputed from the following distribution, which uses the selected pair’s final endpoint response \\(X_{kT}\\):\n\\[Y_{iT} \\sim \\text{N}(X_{kT}, h_{X_T}^2)\\]\nThe bandwidths \\(h_{X_t}\\) and \\(h_{X_T}\\) are selected based on the criterion given by Scott (1992). That is,\n\\[h_{X_{j}} = \\sigma_{X_{j}} \\left( 1 - \\rho^{2} \\right)^{\\frac{5}{12}} \\left( 1 + \\frac{\\rho^{2}}{2} \\right)^{- \\frac{1}{6}}{\\\\n}^{- \\frac{1}{6}}\\text{   for } j = t \\text{ and } T\\]\nwhere \\(\\sigma_{X_j}\\) is the standard deviation of the observed responses at time \\(j\\), \\(n\\) is the number of pairs \\((X_{*t},X_{*T})\\) that were chosen between, and \\(\\rho\\) is the correlation coefficient between \\(X_t\\) and \\(X_T\\) in the pairs \\((X_{1t},X_{1T}), \\ldots, (X_{nt}, X_{nT})\\).\nThe Kernel Density model does not take prior distributions as input. So long as each early visit has more subjects with an observed early response and final response than the value entered in “Minimum number of participants with an early visit and final visit needed to estimate kernel bandwidths for that early visit:” then this algorithm runs without regard for user input.\nIf any visit has fewer subjects with early data and final data than the specified minimum number of participants, then instead of calculating the values of \\(h_{X_t}\\) or \\(h_{X_T}\\) the input values of “Fixed kernel bandwidth \\(h_x\\):” and “Fixed kernel bandwidth \\(h_y\\):” are used.\nFor \\(h_x\\) and \\(h_y\\), possible starting values are the expected SD of the endpoint (‘sigma’). The default value for the minimum number of subjects with complete early and final visits is 6, but this value can be set to anything greater than 0 that the user desires.\nThe Kernel Density model is effective and flexible with no model assumptions, but its computational overhead is large. Simulations may take \\(\\sim 10\\) times longer to run, and having no prior model there has to be ‘in trial’ data before it can come into play.\n\n\nITP\nThe ITP (Integrated Two-component Prediction) model fits an observation for patient \\(i\\) on dose \\(d\\) at visit \\(t\\) as:\n\\[y_{idt} = \\left( \\theta_{d} + s_{id} + \\epsilon_{idt} \\right)\\left( \\frac{1 - \\text{exp}\\left( kx_{idt} \\right)}{1 - \\text{exp}(kX)} \\right)\\]\nwhere \\[\\epsilon_{idt} \\sim \\text{N}(0, \\lambda^2)\\] \\[s_{id} \\sim \\text{N}(0, \\tau^2)\\]\nand \\(\\theta_d\\) is the mean estimate of the final endpoint for dose d using all complete and partial data and assuming an independent dose response model on the doses. \\(s_{id}\\) is a subject specific random effect, \\(k\\) is a shape parameter, \\(x_{idt}\\) is the time \\(y_{idt}\\) is observed, \\(X\\) is the time to final endpoint, and each \\(\\epsilon_{idt}\\) is a residual error.\nThe ITP model is similar to the Time Course Hierarchical above in that both models estimate subject specific component of the response. The biggest difference between the two is that in the ITP models the response changes over time as a parametric function based on the parameter \\(k\\), rather than having a separately estimated \\(e^{\\alpha_t}\\) for each visit.\nThe shape parameter \\(k\\) determines the rate at which the final endpoint’s eventual effect is observed during a subject’s follow-up. A value of \\(k=0\\) indicates that the proportion of effect observed moves linearly with time. A value of \\(k&lt;0\\) means that the eventual final effect is observed earlier in follow-up and plateaus off as time moves towards the final endpoint. A value of \\(k&gt;0\\) indicates that less of the total final endpoint effect is observed early in follow up, but as time approaches the final endpoint time the proportion of the effect observed increases rapidly. Values of \\(k\\) less than 0 tend to be more common than values of \\(k\\) greater than 0. See the figure below for a visualization of the different response-over-time curves that can be estimated with the ITP model.\n\n\n\n\n\n\nFigure 1: Examples of a variety of ITP models with the shape parameter k ranging from -1 to 1.\n\n\n\nThe priors for the parameters in the ITP model are: \\[k \\sim \\text{N}(\\mu_k, \\sigma_k^2)\\] \\[\\theta_d \\sim \\text{N}(\\mu_{\\theta_d}, \\sigma_{\\theta_d}^2)\\] \\[\\tau^2 \\sim \\text{IG}\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\] \\[\\lambda^2 \\sim \\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be:\n\n\\(\\theta_d\\)\n\nmean of the expected mean improvement from baseline on the Control arm, with SD greater than or equal to the expected treatment effect size.\n\n\\(k\\)\n\na mean of 0 and SD of 2, if subject improvement is expected to be rapid and then diminishing, the prior mean might be -0.5 or -1, if subject improvement is expected to be initially slow or non-existent then increasing, the prior mean might be 0.5 or 1.\n\n\\(\\tau\\)\n\nmean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\n\n\\(\\lambda\\)\n\nmean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\n\n\nThe ITP model implies that the variance of the observations shrinks towards 0 with the mean (so early visits have reduced expected responses and variances). The ITP model may result in biased estimates of \\(\\theta_d\\) and/or the variance terms \\(\\tau^2\\) and \\(\\lambda^2\\) if the mean-variance relationship assumed by the ITP model is not present in the observed data. If the model assumptions are correct this can be a very effective longitudinal model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Longitudinal Models",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/index.html",
    "href": "documentation/v71/userguides/core/longitudinalmodels/index.html",
    "title": "Longitudinal Models",
    "section": "",
    "text": "The time-to-event endpoints also allow for using an early predictor endpoint to help in the estimation of the primary endpoint. The time-to-event predictor model works differently than the continuous or dichotomous longitudinal models. See here for a description of the time-to-event predictor models.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Longitudinal Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/index.html#how-many-longitudinal-models",
    "href": "documentation/v71/userguides/core/longitudinalmodels/index.html#how-many-longitudinal-models",
    "title": "Longitudinal Models",
    "section": "How many longitudinal models?",
    "text": "How many longitudinal models?\nWhen specifying a longitudinal model in FACTS, the user must clarify whether the longitudinal model is shared across all doses, or if different arms are to estimate different functions connecting early endpoints to the final endpoint.\nThe options that may be selected for the number of model instances are:\n\n“Single model for all arms” – just one set of longitudinal parameters is estimated using all the subject data regardless of their treatment.\n“Model control separately” – two sets of longitudinal parameters are estimated, one using data from just those subjects on the control arm, and the other using all the subject data from all the other arms.\n“Model comparator separately” (only available if there is an active comparator arm) - two sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, and the other using all the subject data from all the other arms (control and active treatments).\n“Model control and comparator separately” (only available if there is an active comparator arm) - three sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, one using data from just those subjects on the control arm, and the third using all the subject data from all the other arms.\n“Model all arms separately” – a set of longitudinal parameters is independently estimated separately for each arm, each set estimated just using the data available from the subjects on that arm.\n\nThe fewer models used, the more data is pooled, the more precisely the model parameters can be estimated, and the more informative the intermediate data can be. Pooling data assumes that subjects in different groups have the same longitudinal profile, although they may still have different magnitude or probability of response (estimated by Dose Response Model).\nIf the response profile could be different on different treatment arms – for example, if subjects on control recover constantly over time, but those on an effective dose of the study drug recover very quickly early on, and more gradually later in their follow-up - a longitudinal model based on pooled data would tend to over-estimate the final endpoint of subjects on an effective dose of the study drug when only their early data was available. In a case like that, it would be better to use separate models. If the rapid early response was likely on all study drug treatment arms, then possibly use “model control separately.” If, however, the rapid response will only be seen on some study drug arms then use “model all arms separately”.\nIn addition to declaring how many longitudinal models should be estimated, when there is more than one model instance the user must declare if the prior distributions should be specified differently across each model instance or not. The options that may be selected for prior specification (for all LMs but linear regression) are:\n\nSame priors across all model instances\n\n\nEach instance of the model has identical prior distributions on its parameters. Estimation is still performed independently on each model instance.\n\n\nSpecify priors per model instance\n\n\nEach instance of the model has its own priors that may vary across instances.\n\nThe linear regression longitudinal model has a different parameter set estimated for each visit, so its prior specification rules are slightly different. See the linear regression section below for specifics about prior specification specific to this model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Longitudinal Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html",
    "href": "documentation/v71/userguides/core/study/continuous.html",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include accrual style, visit schedule, whether interim analyses will be simulated, and more.\n\n\n\n\n\n\nFigure 1: The study tab for a continuous trial.\n\n\n\n\n\nIn the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nWhether to include simulation of subject’s baseline score, and if so, whether the response to be modeled is change from baseline or final endpoint value.\n\n\n\n\nThe study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a larger endpoint value indicates subjects improvement or if a lower endpoint value is better. The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and with the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html#design-options",
    "href": "documentation/v71/userguides/core/study/continuous.html#design-options",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "In the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nWhether to include simulation of subject’s baseline score, and if so, whether the response to be modeled is change from baseline or final endpoint value.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html#study-information",
    "href": "documentation/v71/userguides/core/study/continuous.html#study-information",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a larger endpoint value indicates subjects improvement or if a lower endpoint value is better. The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and with the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html#d-treatment-arm-model",
    "href": "documentation/v71/userguides/core/study/continuous.html#d-treatment-arm-model",
    "title": "Study Tab - Continuous Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens[^3].\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/multendpt.html",
    "href": "documentation/v71/userguides/core/study/multendpt.html",
    "title": "Study Tab - Multiple Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include accrual style, visit schedule, whether interim analyses will be simulated, and more.\n\n\n\n\n\n\nFigure 1: The Study &gt; Study Info tab for a multiple endpoint trial\n\n\n\nThe study tab is simpler for multiple endpoint than it is for other FACTS Core design engines (Continuous, Dichotomous, or Time-to-event). Some options that are endpoint specific have been moved to the Endpoints tab, which only exists in the multiple endpoint engine.\n\n\nIn the design options section of the Study tab the user gets a check box for whether they want to enable adaptive features or not. Endpoint specific choices about using longitudinal modelling or special longitudinal options are moved to the Endpoints tab.\n\n\nSpecify whether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\n\nThe study information section allows the user to specify how many subjects to accrue and how subject accrual should be simulated.\n\n\nSpecify the maximum number of subjects that can be enrolled in the trial. Adaptive designs may stop sooner than this value, but no simulation can ever go past it.\n\n\n\nIn multiple endpoint, subject accrual can only be done continuously or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nThe overall visit schedule is specified here. It’s called the “overall” visit schedule in the multiple endpoint engine because the visits entered here make up the set of all visits where any of the endpoints can be observed. When the details of the different endpoints are entered on the Endpoints tab, you will be able to specify which of these visits each endpoint is observed at and which visit will be the final observation for that endpoint.\n\n\n\n\n\n\nFigure 2: The Study Tab in the Multiple Endpoint engine.\n\n\n\nVisits can be specified one at a time by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/multendpt.html#design-options",
    "href": "documentation/v71/userguides/core/study/multendpt.html#design-options",
    "title": "Study Tab - Multiple Endpoint",
    "section": "",
    "text": "In the design options section of the Study tab the user gets a check box for whether they want to enable adaptive features or not. Endpoint specific choices about using longitudinal modelling or special longitudinal options are moved to the Endpoints tab.\n\n\nSpecify whether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/multendpt.html#study-information",
    "href": "documentation/v71/userguides/core/study/multendpt.html#study-information",
    "title": "Study Tab - Multiple Endpoint",
    "section": "",
    "text": "The study information section allows the user to specify how many subjects to accrue and how subject accrual should be simulated.\n\n\nSpecify the maximum number of subjects that can be enrolled in the trial. Adaptive designs may stop sooner than this value, but no simulation can ever go past it.\n\n\n\nIn multiple endpoint, subject accrual can only be done continuously or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nThe overall visit schedule is specified here. It’s called the “overall” visit schedule in the multiple endpoint engine because the visits entered here make up the set of all visits where any of the endpoints can be observed. When the details of the different endpoints are entered on the Endpoints tab, you will be able to specify which of these visits each endpoint is observed at and which visit will be the final observation for that endpoint.\n\n\n\n\n\n\nFigure 2: The Study Tab in the Multiple Endpoint engine.\n\n\n\nVisits can be specified one at a time by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/multendpt.html#d-treatment-arm-model",
    "href": "documentation/v71/userguides/core/study/multendpt.html#d-treatment-arm-model",
    "title": "Study Tab - Multiple Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens.\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/multendpt.html#create-endpoints",
    "href": "documentation/v71/userguides/core/study/multendpt.html#create-endpoints",
    "title": "Study Tab - Multiple Endpoint",
    "section": "Create Endpoints",
    "text": "Create Endpoints\nThe small table on the middle-left side of the screen allows for the creation of up to 4 endpoints. The name of each of the endpoints can be changed in the table by clicking on the entry. The order of the non-first endpoints can be changed by clicking on an endpoint and then clicking on the up or down arrows to the right of the table.\n\n\n\n\n\n\nFigure 6: The Treatments tab with the first endpoint (U1: Pain) selected.\n\n\n\nFor each endpoint, the Endpoint Properties section allows for the endpoint specific characteristics to be supplied.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/multendpt.html#endpoint-properties",
    "href": "documentation/v71/userguides/core/study/multendpt.html#endpoint-properties",
    "title": "Study Tab - Multiple Endpoint",
    "section": "Endpoint Properties",
    "text": "Endpoint Properties\n\nContinuous Endpoints\nFor continuous endpoints, the first value provided is whether a higher response is subject improvement, or if a lower response is subject improvement.\nThen, you select whether to simulate a baseline value for subjects. If yes, then specify whether the VSR (Virtual Subject Response) will be specified as a change from the baseline value or as a standalone final endpoint value.\nNext, specify whether to use longitudinal modeling for this endpoint. If the “Use longitudinal Modeling” option is checked, then select one or more visits that subjects will observe this endpoint at. If not using longitudinal modeling, then select which visit is considered the final observed value for this endpoint. Each endpoint can have its own visit schedule, as long as those visits are included as part of the overall visit schedule on the Study Info tab.\n\n\nDichotomous Endpoints\nFor dichotomous endpoints, the first value provided is whether an observed response is a positive outcome or a negative outcome.\nNext, specify whether to use longitudinal modeling for this endpoint. If the “Use longitudinal Modeling” option is checked, then the user can decide if they would like to use the restricted markov model to simulate longitudinal data or the standard transition matrix method. If using the restricted Markov model, specify whether subjects that reach the end of their follow-up without going to either absorbing states should be considered a success or a failure. Then, select one or more visits that subjects will observe this endpoint at.\nIf not using longitudinal modeling, select which visit is considered the final observed value for this endpoint. Each endpoint can have its own visit schedule, as long as those visits are included as part of the overall visit schedule on the Study Info tab.\n\n\nUtility Function\nThe Multiple Endpoint design engine allows clinical trials to be designed and simulated where the within-trial and end-of-trial decisions can be based on multiple endpoints by using a composite score, or utility, derived by combining the different endpoint estimates. The utility function approach is incredibly open-ended and flexible, able to cope with different types of endpoints, different endpoint scales and different endpoint interrelations.\nThe utility function approach has two stages:\n\nFirst, each endpoint is converted to its own utility score for each dose.\nThen, the endpoint specific utilities are combined into a single overall utility for each dose.\n\nEach endpoint has its own utility function, as described above. Utilities are flexible piecewise functions of the estimated response for the endpoint.\nFirst use the “Add” button to add the knots of the utility function – these are the segment boundaries in the range of the endpoint measure where different functions will be specified in each segment. For each segment created by adding a “knot” a row is created in the table, allowing the user to specify the coefficients of the utility function in that segment specifically.\nThe components of the utility that can be weighted based on their coefficients are: fixed, linear, quadratic, exponential, and log terms.\nThe coefficients that can be specified are:\n\nAlpha: the coefficient of the quadratic term\nBeta: the coefficient of the linear term\nGamma: the coefficient of the fixed term\nDelta: the coefficient of the exponential term\nEpsilon: the coefficient of x in the exponential term\nPhi: the coefficient of the log term\nPsi: the coefficient of x in the log term\n\nFinally, the user specified whether the \\(x\\) in the utility function is relative to control or not. If the “Parameterize response relative to control” is checked, then \\(x=\\theta_d - \\theta_0\\) for a continuous endpoint, and \\(x = P_d - P_0\\) for a dichotomous endpoint. If the “Parameterize response relative to control” is checked, then \\(x=\\theta_d\\) for a continuous endpoint and \\(x=P_d\\) for a dichtomous endpoint.\nFor a continuous endpoint, the utility function is defined on the range of \\(x \\in (-\\infty, \\infty)\\). For dichotomous endpoints, if \\(x\\) is relative to control, then the utility is on the range \\(x \\in (-1, 1)\\), and if \\(x\\) is not relative to control, then the utility is on the range \\(x \\in (0, 1)\\).\n\n\n\n\n\n\nNote on the coefficients of the utility function\n\n\n\nIt is not the intention that all, or even most, of the available coefficients are used in any one segment, typically only one or two are. The different terms are provided so that the required form can be selected for each segment – flat linear, quadratic, exponential or log. The default values of the coefficients are set so that only the linear component of the utility contributes.\n\n\n\n\nEstimation of Utility in FACTS\nAn important aspect of the way FACTS estimates utility is that it estimates a probability density for the utility within the MCMC sampling. That is, the utility is calculated for every parameter sample within the MCMC, and the final distribution of the utility is based on those samples just like the estimates for the model parameters.\nEach dose has a distribution of utility scores - not a single value. The utility is not estimated from the mean estimates of the dose response from the different endpoints.\nThis has some notable effects on the estimates of utility. If, for example, the utility function is a step function – for example if its value is 1 for response rates below a threshold and 0 above – as the estimate of the response rate will have some uncertainty then when the mean response estimates are around the threshold value, the estimate of utility will be between 0 and 1, based on the proportion of MCMC samples the fitted response rate was below the threshold. Indeed, the utility can be interpreted as the ‘probability the response rate is below the threshold’, and thus be useful or even exactly what is required.\nSimilarly, any utility function that has a floor or ceiling will result in a bias in the estimate of the utility to be above the floor or below the ceiling. – because in the distribution of the values for the utility the lowest the value can be is the floor and the final estimate of the utility would only be at the floor if all the values for the utility sampled in the MCMC were at the floor. Another effect of utilities with a floor or ceiling is that the estimate of the mean utility has a smaller standard error the further the value of the estimate of the mean of the underlying response is from an inflexion point (or “knot”).\nThese are not errors, nor does it mean utility functions with steps, ceilings or floors should be avoided, but these artefacts need to be understood – particularly when graphs of the estimated utility and the true utility are compared.\n\n\n\n\n\n\nNote about FACTS’s utilities\n\n\n\nFACTS’ utility is based on the estimates of the response on each endpoint in each treatment arm. It is not the utility of outcome for each individual that would be a composite score and an endpoint in itself. To use that kind of utility, simulate the external subjects and their scores outside of FACTS using a program or script and calculate the composite score for each individual and write the results to a file in FACTS external virtual subject response format, this can then be used to drive simulations in FACTS of trial designs using that composite score. This might be a single endpoint (FACTS Core Continuous) design, or a Multiple Endpoint design, with the Composite score as the primary endpoint and up to 3 of the component scores as auxiliary endpoints.\nIn this latter case, the design would probably be making decisions based solely on the composite score (so the utility based criteria are unused), and FACTS Multiple Endpoint’s result summarization and charting is used to understand how when different responses are simulated on the component scores this translates into the composite score and the likely trial results.\n\n\n\n\nCalculation of Arms’ probability of having the greatest utility\nLike other probabilities in FACTS this is calculated during the MCMC sampling – the probability that an arm has the greatest probability is based on the proportion of MCMC samples when that arm had the greatest utility. It is not estimated from the utility of the mean estimates of the dose response for the different endpoints.\nIn any given sample if several arms have the same maximum toxicity, the arm with the lowest dose strength is selected. This is particularly useful when using dose response models with plateau features (the Plateau and U-shaped models). It means that where the utility is driven by this model, the arm that will be ranked most likely to have the greatest utility will be the one that lies at the start of the flat maximum response section. However it has a less desirable effect when the overall utility is formed by multiplying the utilities of individual endpoints and one of the endpoint utility functions has a segment where the utility is 0.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/multendpt.html#component-utility-combination-method",
    "href": "documentation/v71/userguides/core/study/multendpt.html#component-utility-combination-method",
    "title": "Study Tab - Multiple Endpoint",
    "section": "Component Utility Combination Method",
    "text": "Component Utility Combination Method\nIn order to derive the overall utility score for a dose we first need to decide how the different utilities are to be used how they are to be combined.\nAt the top of the Endpoint tab is a control that is displayed regardless of which endpoint is being specified. It allows the formula to be selected that defines the values of the individual utility scores for each endpoint are to be combined to form the overall utility. The only operators supported are addition and multiplication, but all logically distinct combinations (given that the user can re-order the auxiliary endpoints) are provided:\n\n\n\n\n\n\nFigure 7: All combinations of endpoint specific utilities into the overall utility.\n\n\n\n\n\n\n\n\n\nGuide for Utility Combination\n\n\n\nUtilities can be combined by multiplying them together or adding together, so when there are just two endpoints there are just two methods of combination: U1 + U2 and U1 * U2. With more endpoints there are more possible combinations. These allow utilities to be formed for a number of circumstances:\n\nOne efficacy and one safety/tolerability endpoint, U1 * U2. Here the purpose of the combination is to scale back the efficacy score if there are safety or tolerability issues. At its simplest, the utility function of the safety / tolerability endpoint is defined so that the estimate of the probability of an adverse event or lack of tolerability is transformed to so the utility is 0 where the safety / tolerability is completely unacceptable, and 1 where it is completely acceptable, with possibly a transition region in between. The utility of the efficacy endpoint could be simply the value of the efficacy endpoint.\nThis is not intended to replace SAE monitoring and the withdrawal of treatments arms that are unsafe, the safety monitoring may be for indicators of potential safety problems when the drug is taken for a longer duration than can be studied, or it may be for tolerability issues that would give compliance problems, or acceptability problems given the other treatments available.\nSome possible variants are: where current treatments have a level of unpleasant side effects, the utility of the probability of a side effect for our drug may be &gt;1 for side effect rates below this. The utility of the efficacy outcome may be 0 below a certain floor efficacy and capped at a maximum above a ceiling efficacy, perhaps to stop undue weight being given to an outcome that this thought unfeasible or avoid a level of utility on the efficacy score that would yield an overall utility that would be judged viable at a poor (but not utility 0) level of safety / tolerability.\nTwo efficacy endpoints, U1 + U2. Here the purpose of the combination is to allow success if either the response on either endpoint is very good, or is quite good on both endpoints. Some care will be required to define the utility functions of the endpoints so that different combinations of efficacy correctly yield sufficient utility or insufficient utility.\nAn efficacy endpoint and a ‘necessary but not sufficient’ biomarker, U1 * U2. Here the purpose of the combination is to yield an overall utility of 0 if the biomarker is not observed at the necessary levels, otherwise the utility is driven by the primary endpoint that is observed much later. This allows early stopping for futility, arm dropping, or adaptive allocation away from arms with poor levels of biomarker, but for success to be determined only on the basis of the primary endpoint.\n\nUtilities with more than two endpoints are usually some form of combination of the above, for example a primary secondary and secondary endpoint and a safety/tolerability endpoint: (U1 + U2) * U3.\n\n\nDevising and agreeing upon utility functions with a clinical team is part art and part science (and possibly, part politics). Some have expressed the opinion that these methods could never be used in practice because it would be impossible to get agreement, but experience shows agreement is possible. Generally, the process followed is roughly as follows:\n\nThe team agrees how the endpoints will be combined and specifies some key utility points – at specific combinations of response at the different endpoints – for example:\n\nIf there were no observed side effects, what is the minimum level of efficacy that would be a useful drug?\nIf the maximum expected level of efficacy was observed, what is the maximum level of side effects that could be observed that would leave a useful drug\nSee “Dose-Finding Based On Efficacy-Toxicity Trade-Offs” by Thall and Cook (2004), for a description of such an elicitation process.\n\nThe statistician creates utility functions for the endpoints that yield the desired overall utility value at the specified points.\nUsing FACTS some simple trials are simulated and example simulated datasets and analyses are studied and reviewed with the team. The team is asked the question “Given the data that was simulated, (and the distributions they were simulated from) what do they think of the utility assigned to the treatment arms?”\nThe statistician adjusts the utility functions and iterates the process of simulating and reviewing with the team.\nOnce the team is happy with individual examples of how utility is assigned, a larger number of simulations can be run and the estimates of the operating characteristics considered and other aspects of the trials design considered.\n\n\nEstimation of Utility in FACTS\nAn important aspect of the way FACTS estimates utility is that it estimates a probability density for the utility within the MCMC sampling. That is, the utility is calculated for every parameter sample within the MCMC, and the final distribution of the utility is based on those samples just like the estimates for the model parameters.\nEach dose has a distribution of utility scores - not a single value. The utility is not estimated from the mean estimates of the dose response from the different endpoints.\nThis has some notable effects on the estimates of utility. If, for example, the utility function is a step function – for example if its value is 1 for response rates below a threshold and 0 above – as the estimate of the response rate will have some uncertainty then when the mean response estimates are around the threshold value, the estimate of utility will be between 0 and 1, based on the proportion of MCMC samples the fitted response rate was below the threshold. Indeed, the utility can be interpreted as the ‘probability the response rate is below the threshold’, and thus be useful or even exactly what is required.\nSimilarly, any utility function that has a floor or ceiling will result in a bias in the estimate of the utility to be above the floor or below the ceiling. – because in the distribution of the values for the utility the lowest the value can be is the floor and the final estimate of the utility would only be at the floor if all the values for the utility sampled in the MCMC were at the floor. Another effect of utilities with a floor or ceiling is that the estimate of the mean utility has a smaller standard error the further the value of the estimate of the mean of the underlying response is from an inflexion point (or “knot”).\nThese are not errors, nor does it mean utility functions with steps, ceilings or floors should be avoided, but these artefacts need to be understood – particularly when graphs of the estimated utility and the true utility are compared.\n\n\nCalculation of Arms’ probability of having the greatest utility\nLike other probabilities in FACTS this is calculated during the MCMC sampling – the probability that an arm has the greatest probability is based on the proportion of MCMC samples when that arm had the greatest utility. It is not estimated from the utility of the mean estimates of the dose response for the different endpoints.\nIn any given sample if several arms have the same maximum toxicity, the arm with the lowest dose strength is selected. This is particularly useful when using dose response models with plateau features (the Plateau and U-shaped models). It means that where the utility is driven by this model, the arm that will be ranked most likely to have the greatest utility will be the one that lies at the start of the flat maximum response section. However it has a less desirable effect when the overall utility is formed by multiplying the utilities of individual endpoints and one of the endpoint utility functions has a segment where the utility is 0.\n\n\n\n\n\n\nCaution when using 0 in your utilities\n\n\n\nHaving segments of utility 0 for an endpoint, and calculating the overall utility by multiplying the component utilities will lead to segments of 0 in the overall utility. If in only a proportion of the MCMC samples all arms have a utility of 0, this will result in that proportion of the probability of being the arm with the maximum utility being placed on the arm with the lowest dose strength, which might be odd given the utility in the other samples. It can lead to counter intuitive and sometimes undesired adaptations, such as in the adaptive allocation of subjects between the arms.\nThe solution is to not use 0 for the segments of low utility, but a small value such as 0.01, the multiplication with the utility of the other endpoints will then not flatten them all to exactly 0, but retain the utility profile at attenuated values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html",
    "title": "Hazard Model and Predictor Model for Time to Event Trials",
    "section": "",
    "text": "The two main differences in the design tabs available for a time-to-event endpoint, rather than continuous or dichotomous, are ways that the control arm hazard model is defined and the use of a predictor model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Hazard Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html#fixed-priors",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html#fixed-priors",
    "title": "Hazard Model and Predictor Model for Time to Event Trials",
    "section": "Fixed priors",
    "text": "Fixed priors\nIf “Enable hierarchical data modeling for the control arm” is not selected, then independent gamma distributions with parameters specified in Hazard Rate tab’s table are used. The gamma parameters have been reparameterized so that the mean hazard rate and a weight (in terms of number of events) are provided instead of the traditional \\(\\alpha\\) and \\(\\beta\\) parameters.\n\n\n\n\n\n\nFigure 2: Specifying a fixed prior for a piecewise exponential model with 2 time segments.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Hazard Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html#hierarchical-prior",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html#hierarchical-prior",
    "title": "Hazard Model and Predictor Model for Time to Event Trials",
    "section": "Hierarchical Prior",
    "text": "Hierarchical Prior\nIf “Enable hierarchical data modeling for the control arm” is selected, then the independent priors specified per segment are augmented by additional data specified on the “Hierarchical Priors” tab.\n\n\n\n\n\n\nFigure 3: Specifying the sufficient statistics and hierarchical models hyper-parameters for the bayesian augmented control model.\n\n\n\nThe additional data comes in the form of sufficient statistics from an outside data source. The sufficient statistics are, for each segment in each study, the number of events on the control arm and the control arm exposure time in subject weeks. The information from the prior study can be ‘down-weighted’ by reducing, pro-rata, the number of events and exposure time before entering them. By supplying the summary statistics from previous trials and specifying the parameters for prior distributions for the parameters of a hierarchical model, the gamma priors are updated and become the priors applied to the data collected on the virtual subjects.\nThe hyper-parameters are the mean and standard deviation of Normal distribution for the log hazard ratios of the event rates of the historical studies and the current study. The prior distribution for the mean hyper-parameter is a Normal distribution, for which the user specifies the mean and standard deviation. The prior distribution for the standard deviation hyper-parameter is an Inverse-Gamma distribution for which the user specifies either as a mean and a weight, or via Alpha and Beta parameters (depending on the user selection on Settings &gt; Options &gt; Gamma Distribution Parameters).\n\n\n\n\n\n\nSetting the priors for Hierarchical model hyper parameters\n\n\n\nUnless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.\nIn this case the following would be reasonable:\n\nSet the prior mean value for Mu as the mean of the log-hazard ratios of the event rates of the control arm and the historic studies (usually this will be 0)\nSet the prior SD for Mu equal to at least the largest log hazard ratio of the event rates for the historic studies.\nSet the mean for tau to the same value as the prior SD for Mu.\nSet the weight for tau to be &lt; 1.\n\nOne can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small relative to the number of studies, then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.\nTo give some prior preference towards pooling or separate analysis the weight for tau has to be large, relative to the number of historic studies. To have a design that is like pooling the historic studies, the mean for tau needs to be small – say 10% or less of the value suggested above. For there to be no borrowing from the historic studies the value for tau needs to be large – say 10x or more the value suggested above.\nThe best way to understand the impact of the priors is try different values and run simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Hazard Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/interims.html",
    "href": "documentation/v71/userguides/core/design/interims.html",
    "title": "Interims",
    "section": "",
    "text": "Interim analyses allow for decision making throughout the lifecycle of an adaptive trial in FACTS. Interim analyses can adjust allocation probabilities, drop arms, or allow for early success/futility of the trial. Interims can either be specified with calendar frequency – occurring every specified number of weeks or specified to occur after a specified amount of information has been collected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Interims"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/interims.html#continuous-and-dichotomous-endpoint",
    "href": "documentation/v71/userguides/core/design/interims.html#continuous-and-dichotomous-endpoint",
    "title": "Interims",
    "section": "Continuous and Dichotomous Endpoint",
    "text": "Continuous and Dichotomous Endpoint\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have actually completed a specified visit\nthe number of subjects who have had the opportunity to complete a specified visit (includes drop-outs)\n\nWhen specifying an interim analysis schedule, it can be done either based on time or based on one of the information categories above.\nIf specifying interims based on time the first interim analysis timing must be based on information, and each subsequent interim is triggered after the provided amount of time has elapsed. If accrual completes before the first interim threshold is reached, and the first interim was defined in terms of the number of subjects enrolled, then the interims by time start at full accrual. If the first interim is defined in any other terms (by events, subjects complete or subjects with opportunity to complete) then interims only start when this is reached (which might be never).\nIf specifying interims based solely on information, the table on the “Interims” tab determines when the analyses will be triggered. Each interim is defined individually by the number of patients/observations that have satisfied some criteria. If information is If information is defined as Subjects Enrolled, then interim are triggered immediately upon enrollment of the subject satisfying the criteria. If information is defined as completers or opportunity to complete, then interims are triggered immediately upon the visit being reached that satisfies the specified criteria. Successive interims must be in terms of the same or more observations at the same or later visit, and either Visit or Subject must increase. Different types of information cannot be mixed to trigger interim analyses except in using time to trigger interims after the first based on information.\n \nIf interims are governed by time, there is the option as to whether interims should continue after full accrual, or discontinue.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Interims"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/interims.html#time-to-event-endpoint",
    "href": "documentation/v71/userguides/core/design/interims.html#time-to-event-endpoint",
    "title": "Interims",
    "section": "Time-to-Event Endpoint",
    "text": "Time-to-Event Endpoint\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have observed their predictor endpoint\nthe number of subjects who have had the opportunity to observe their predictor endpoint (includes drop-outs)\nspecified numbers of events observed\nspecified number of predictor events observed\n\nOutside of the new types of information, the time-to-event triggers work in exactly the same way that continuous and dichotomous triggers do.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Interims"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html",
    "href": "documentation/v71/userguides/core/design/doseresponse.html",
    "title": "Dose Response Models",
    "section": "",
    "text": "Dose response models in FACTS may be more accurately called final endpoint models. They create and model a relationship across the doses specified in the Treatment Arms tab. Often, but not always, the dose strength, called “Effective Dose Strength” in the Study &gt; Treatment Arms tab of FACTS, is used in the dose response models to determine the order of doses, and which doses are more related to others.\nThe dose response models can be simple, and model the doses largely independently, as is done with the Independent Dose Model or the Independent Beta Binomial Model (dichotomous only). They can have logistic style models with interpretable parameters, like the 3-parameter logistic or the \\(E_{max}\\) model (called Sigmoidal in FACTS). The dose response model can also be a model that creates a smooth, spline like, model over the doses using a normal dynamic linear model (NDLM), a monotonic NDLM, or a 2nd order NDLM.\nFor all endpoints, we model the response at each dose, d, in terms of \\(\\theta_d\\) on a continuous scale, allowing a consistent and rich range of dose response models to be used for all endpoint types. Transformations (see below) of the dichotomous and time-to-event responses are used to achieve this.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#independent-dose-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#independent-dose-model",
    "title": "Dose Response Models",
    "section": "Independent Dose Model",
    "text": "Independent Dose Model\nThe “Independent Dose Model” providing a simple pair-wise comparison between the study drug arms and the control arm and/or the active comparator arm. The doses are modelled as independent and normally distributed with a prior of:\n\\[\\theta_d \\sim \\text{N}(\\mu_d, \\nu_d^2)\\]\nWhere \\(\\mu_d\\) and \\(\\nu_d^2\\) are specified in FACTS and can either be the same or vary across arms.\nThis model is useful:\n\nWhen there is only one or two experimental arms\nWhen the study drug arms don’t differ by dose but in other ways, so there is no ordered sequence of treatments – e.g. each arm is the study drug in combination with a different additional drug.\nFor simulating simple trial designs\nFor simulating a ‘conventional’ or ‘strawman’ design to compare more complex designs against\n\nOtherwise for trials with multiple related arms that differ by dose, it is usually more efficient to use a dose-response model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#independent-beta-binomial-model-dichotomous-only",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#independent-beta-binomial-model-dichotomous-only",
    "title": "Dose Response Models",
    "section": "Independent Beta-Binomial Model (Dichotomous Only)",
    "text": "Independent Beta-Binomial Model (Dichotomous Only)\nThis is a “no model” option similar to the above, but only available for the Dichotomous endpoint. Unlike any other dose response model, this model uses the beta distribution to model the probability of response directly rather than fitting a model to the log-odds of the probability.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(P_d)\\] where \\(P_d\\) is the probability that a patient is a response at the final endpoint for subjects randomized to dose \\(d\\). With posterior\n\\[P_d \\sim \\text{Beta}(\\alpha_d + \\text{responders}_d, \\beta_d + \\text{non-responders}_d)\\]\nWhere \\(\\alpha_d\\), \\(\\beta_d\\) are the priors for the arm \\(d\\), \\(\\text{responders}_d\\) is the number of responders on arm \\(d\\) and \\(\\text{non-responders}_d\\) is the number of non-responders on arm \\(d\\).\nThis model has the advantages of an easier to understand prior, and better estimation of \\(P_d\\) when the number of responders and non-responders is small (either is &lt; 5) compared to the log-odds model. As it’s a independent dose model, it is well suited to analyzing single arm, two arm or three arm trials, or trials where the experimental arms are unrelated, and so there is no basis on which to borrow information between them. With trials with multiple arms of the same treatment at different doses it is usually more advantageous to fit a dose response model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#simple-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#simple-ndlm",
    "title": "Dose Response Models",
    "section": "Simple NDLM",
    "text": "Simple NDLM\nThe Normal Dynamic Linear Model (NDLM) estimates the final endpoint as a smoothed curve across doses included in the model. Doses are tied directly to their nearest neighbor, and the prior expectation for a dose is that it is equal to its neighboring doses, tending to a constant dose response across doses in the absence of data. The model is defined as follows:\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately.\nThe dose response of the first dose, \\(d'\\), has a prior of:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nwhere \\(\\mu_{d'}\\) and \\(\\tau_{d'}^2\\) are specified directly in FACTS. Subsequent dose response estimates \\(\\theta_{d'+1}, \\ldots, \\theta_D\\) have priors centered at the previous dose response with variances based on the distance between the dose \\(d\\) strength and the dose \\(d-1\\) strength. Specifically,\n\\[\\theta_d \\sim N\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\text{ for } d=d'+1, \\ldots, D\\]\nwhere for dose strengths \\(\\nu_d\\) and \\(\\nu_{d-1}\\), \\(\\tau_{d-1}^2\\) is defined as \\[\\tau^2_{d-1}=\\tau^2\\left(\\nu_d-\\nu_{d-1}\\right)\\]\nThe prior distribution for the “drift” parameter, which controls the amount of smoothing is:\n\\[\\tau^{2}\\sim IG\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) and \\(\\tau_n\\) are specified in the Dose Response tab in FACTS under Model Parameters. See here for help with specifying an inverse gamma distribution with center and weight.\nIn the continuous case the residual error around the estimated dose response is\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nwhere \\(\\sigma_\\mu\\) and \\(\\sigma_n\\) are specified on the Dose Response tab in FACTS under Error Parameters.\nThe Simple NDLM is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Simple NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a null scenario where the response on all the doses is the same as control, the simple NDLM reduces type-1 error. The estimate of \\(\\tau^2\\) tends towards zero and the estimate of the dose response tends to a flat line. The smoothing of the estimates of the response reduces the random fluctuations in the estimates from dose to dose and is a very effective counter to the problem of type-1 error inflation due to the multiplicity of doses being tested. A small prior for the mean value of \\(\\tau\\) will accentuate this effect. The price to be paid for this benefit is a tendency to underestimate large changes in response or where there is a peak in the response. This over-smoothing can reduce power. A prior for the mean of \\(\\tau\\) centered at large values will minimize over-smoothing, and a very large prior would simply remove any smoothing effect at all. A standard prior for the mean of \\(\\tau\\) would be the expected average change in response from dose to dose, scaled to take into account the differences in the specified effective dose strengths.\nUsually, the choice of prior for \\(\\tau^2\\) is tuned based on simulation results, trading off reducing type-1 error in the null case against loss of power and reduced estimate of maximum response in the effective scenario with the largest jump in response or steepest peak.\n\n\n\n\n\n\nNote on the use of an NDLM with doses without subjects\n\n\n\n\n\nWhen using the NDLM model or any of its alternatives (2\\(^{nd}\\) order or Monotonic NDLM), and including doses in a design that receive no subject allocation (ie. Allocation ratio set to 0), some important characteristics of the model should be considered in the interpretation of results. First, the NDLM model “borrows” information from neighboring doses, and therefore including unallocated intermediate doses will increase uncertainty in the fitted response. Information must “pass through” these intermediate doses, and uncertainty should be expected to increase as the number of intermediate unallocated doses between two allocated doses increases. Secondly, the NDLM model estimates the response at unallocated doses, but without subject data at these doses the uncertainty in the estimate can be considerably larger than doses with subject data. This high degree of uncertainty can lead to a vestigial probability that an unallocated dose is the target dose (Pr(EDq), Pr(MED), Pr(Max)) even though the mean response estimate and probabilities at neighboring doses with subject data would not suggest this to be the case.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#monotonic-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#monotonic-ndlm",
    "title": "Dose Response Models",
    "section": "Monotonic NDLM",
    "text": "Monotonic NDLM\nThe Monotonic NDLM endpoint model is similar to the NDLM in the sense that it smooths dose response estimates using neighboring doses, but is constrained to be either monotonically increasing or monotonically decreasing across doses. The direction of monotonicity depends on the value specified in the “Monotonic:” selection in the Model Parameters section of the Monotonic NDLM Dose Response page.\nThe use of this model is generally limited to endpoints where a strong presumption of increasing/decreasing response with dose is reasonable – such as a biomarker or a toxicity endpoint.\nLet doses \\(d = d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately. The following model is the monotonically positive NDLM:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nand\n\\[\\theta_d \\sim N^+\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\] where \\(\\tau_{d-1}^2\\) is defined as in the NDLM, and \\(X \\sim \\text{N}^+(\\mu, \\sigma^2)\\) refers to a positive truncated normal distribution with density function:\n\\[f_{X}(x) = \\frac{1 - \\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&gt;0\\]\nThe result of this dose-response model is that the curve is monotonically increasing, in that \\(\\theta_d&gt;\\theta_{d-1}\\).\nThe monotonically decreasing NDLM is similar except: \\[\\theta_d \\sim N^-\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\]\nwhere \\(X \\sim \\text{N}^-(\\mu, \\sigma^2)\\) refers to a negative truncated normal distribution:\n\\[f_{X}(x) = \\frac{\\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&lt;0\\]\nThe result of this dose-response model is that the curve is monotonically decreasing, in that \\(\\theta_d&lt;\\theta_{d-1}\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#second-order-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#second-order-ndlm",
    "title": "Dose Response Models",
    "section": "Second Order NDLM",
    "text": "Second Order NDLM\nThe second order NDLM serves as a “smoother” similar to the NDLM, however it tends to smooth toward a linear fit with a slope (the NDLM itself tends to shrink each dose’s fit toward its neighbors, while the second order NDLM prefers any trend in the neighbors).\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control arm or control is included in the dose response model, and \\(d'=2\\) if the control arm is modelled separately. The initial dose \\(d'\\) is modeled:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{0},\\tau^2_{0}\\right)\\]\nwhere \\(\\mu_0\\) and \\(\\tau_0^2\\) are specified directly in FACTS.\nIn the case of a time-to-event endpoint, the initial dose \\(d'\\) is the control arm, and has a \\(\\theta_{d'}= 0\\) by definition, so no prior distribution is needed.\nThe prior distribution for the dose response for the dose directly after the initial dose is specified on the difference between the \\(d'\\) and \\(d'+1\\) level doses:\n\\[\\theta_{d'+1} - \\theta_{d'} \\sim N\\left(\\mu_{1},\\tau^2_{1}\\right)\\]\nSuccessive doses are then modeled based on differences in slope between the dose and the two doses below them. Let:\n\\[\\theta_{d} = \\theta_{d - 1} + \\Delta_{d}\\zeta_{d} + \\frac{\\Delta_{d}}{\\Delta_{d - 1}}\\left( \\theta_{d - 1} - \\theta_{d - 2} \\right)\\]\nfor doses \\(d=d'+2,\\ldots,D\\), where \\(\\Delta_d=\\nu_d-\\nu_{d-1}\\) and \\(\\Delta_{d-1}=\\nu_{d-1}-\\nu_{d-2}\\). The priors for the dose response smoothing terms \\(\\zeta_d\\) are:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau_2^2)\\] The smoothing is determined by the parameter \\(\\tau_2\\). Small values of \\(\\tau_2\\) lead to more smoothing, while large values of \\(\\tau_2\\) lead to little information sharing between doses (above dose 1). Note that “small” and “large” is dependent on the scale of the doses. The prior on the smoothing parameter is:\n\\[\\tau_{2}^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau_2\\), and \\(\\tau_n\\) is the prior weight. See here for help with specifying an inverse gamma distribution with center and weight.\nNote that that in this formulation, \\(\\tau_2^2\\) can be thought of as the deviation from linearity. The linear scaling of deviations thus leads to a conditional variance that scales as the square of the dose difference:\n\\[\\text{Var}[\\theta_d \\mid \\theta_{d-1}, \\theta_{d-2}]=\\tau_2^2\\cdot (\\nu_d-\\nu_{d-1})^2\\]\nThe second order NDLM, like the simple NDLM, is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Second Order NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a null scenario where the response on all the doses is the same as control the Second Order NDLM, like the Simple NDLM, tends to reduce type-1 error. As the estimate of \\(\\tau^2\\) tends to zero the estimate of the dose response tends to a line (with non-zero slope if appropriate).\nThe second order NDLM has more power than the Simple NDLM when the dose response is smooth, but will tend to shrink estimates to the control by more than the Simple NDLM if there are marked peaks or steps in the response. Because it borrows from two neighboring doses, it is better at estimating a response for a dose where there is no data than the simple NDLM can, and can be used with burn-in’s where not all doses have been allocated to, but doesn’t cope well if there are consecutive doses where there is no data.\nHowever, unlike the simple NDLMs where the variance structure is preserved if doses are added to or dropped from the design, this is not true for the 2nd order NLDM. Thus, if using the 2nd order NDLM and the doses that are available to the model are changed, then the parameters for the prior for \\(\\tau_2^2\\) may need to be re-visited.\nThe simple NDLM is likely to be preferred where there are fewer doses (4-5), and the second order NDLM is often preferred when there are 7 or more (it will depend on the expected smoothness of the response – the smoother it is the more useful the second order NDLM will be).\nAs with the simple NDLM, the choice of prior for \\(\\tau_2^2\\) can be tuned based on simulation results: balancing having a smaller prior that limits type-1 error in the null case, against having a larger prior that yields more power and better estimate of maximum response in the effective scenarios with the large jumps in response or pronounced peaks.\n\n\n\n\n\n\nNote on the Second Order NDLM before FACTS 4.0\n\n\n\n\n\nThe second order NDLM described in this section is the version utilized in FACTS version 4.0 and later. The model labelled “Second Order NDLM” in versions before 4.0 is was maintained as the model labelled “Legacy 2nd Order NDLM” until the release of FACTS 7.1, at which time it was removed.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#parameter-logistic",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#parameter-logistic",
    "title": "Dose Response Models",
    "section": "3-Parameter Logistic",
    "text": "3-Parameter Logistic\nThe 3-parameter logistic dose response model estimates efficacy as a smooth curve using an intuitive parameterization. The dose response for a dose \\(d\\) with effective dose strength \\(\\nu_d\\) is:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}}\\]\nWhere the \\(a\\) parameters have the following description:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated change in dose response for a dose with strength ∞ when compared to a dose of strength 0.\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum (\\(a_2\\))\n\n\nThe shape of the 3-parameter logistic curve can vary wildly based on the parameter values, but always starts at \\(a_1\\) at dose strength 0 and monotonically increases to \\(a_1+a_2\\) as the effective dose strength goes to infinity.\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nIn the continuous case, the prior distribution for the error term in the 3-Parameter Logistic is Inverse Gamma:\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nAn advantage of the 3-Parameter Logistic model is that it is has a small number of parameters that have defined, intuitive interpretations. Since there are few parameters it’s generally easy to estimate the parameters even on small amounts of data. Because of the limited flexibility of the logistic curve, the 3-parameter logistic model should only be used when there is evidence that it is appropriate. See the Hierarchical Logistic and Sigmoid (Emax) models for dose response models with a similar pattern, but slightly more flexibility in shape.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-logistic",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-logistic",
    "title": "Dose Response Models",
    "section": "Hierarchical Logistic",
    "text": "Hierarchical Logistic\nThe  hierarchical logistic model Scott Berry’s favorite dose response model.  is an extension of the 3-parameter logistic with the form:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}} + \\zeta_{d}\\]\nwhere \\(\\zeta_d\\) is a random intercept term that modifies \\(a_1\\) differently for each dose under the constraint that all \\(\\zeta_d\\) must sum to 0.\nThe additional term \\(\\zeta_d\\) is a per dose ‘off-curve’ effect. It allows for a secondary non-parametric dose effect over and above the logistic model. It has been used in circumstances in which there is actually some falling away of effect at the highest dose. This is an example mean fit from 100 simulations. The model fit and error bars are in green, and the true dose responses are shown by the black line.\n\n\n\n\n\n\nFigure 1: An example of a fit from the hierarchical logistic model averaged over 100 simulations. The black line shows the truth, and the green line with error bars shows the model fits.\n\n\n\n\\(\\zeta_d\\) is modelled as:\n\\[\\zeta_d \\sim \\text{N}(0, a_4^2)\\]\nconditioned that\n\\[\\sum_{d}^{}\\zeta_{d} = 0\\]\nAnd \\(a_4^2\\) has an inverse gamma prior:\n\\[a_{4}^{2}\\sim IG\\left( \\frac{\\Lambda_{n}}{2},\\frac{\\Lambda_{\\mu}^{2}\\Lambda_{n}}{2} \\right)\\]\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nA typical recommended value for the center of the prior distribution of \\(\\alpha_4\\) is the largest expected likely deviation in true response from the underlying logistic with a weight of 1. The effect of this prior is then checked through simulation of a range of plausible dose response profiles, with a range of designs to check the sensitivity of the final estimate of response to this prior. Additionally, see here for help with specifying an inverse gamma distribution with center and weight.\nIn this model, compared to the ‘plain’ 3-parameter logistic above, it is important that the true ED50 lies within the expected dose range, and that the prior for the ED50, \\(a_3\\), has the majority of its probability mass in the available dose range. For example, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be:\n\\[a_{3}\\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\nUsing a weaker prior, such as \\(\\text{N}^+(\\nu_D, \\nu_D^2)\\) leads to a more linear fit. With just this change to the prior for \\(a_3\\) the average of the estimated of the mean response changes from the graph above to:\n\n\n\n\n\n\nFigure 2: An example of hierarchical logistic model fits using a prior that results in a flatter dose response model fit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#sigmoid-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#sigmoid-model",
    "title": "Dose Response Models",
    "section": "Sigmoid Model",
    "text": "Sigmoid Model\nA sigmoid model (\\(\\text{E}_{\\text{max}}\\)) provides a coherent parameterization of the dose response trend similar to the 3-parameter logistic, but with one extra shape parameter, \\(a_4\\).\nThe model formula is:\n\\[\\theta_{d} = a_{1} + \\frac{(a_{2} - a_{1})v_{d}^{a_{4}}}{{a_{3}}^{a_{4}} + v_{d}^{a_{4}}}\\]\nThe interpretation of the four parameters is:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated dose response for a dose of strength \\(\\infty\\) (slight difference from Logistic models)\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum attainable effect (\\(a_2-a_1\\))\n\n\\(a_4\\)\n\ncontrols the slope of the dose response model at the ED50. A larger value of \\(a_4\\) corresponds to a steeper slope. A value of \\(a_4=1\\) makes the Sigmoid model equivalent to a Three Parameter Logistic model with \\(a_2\\) equal to \\(a_1 + a_2\\) from the Sigmoid model. A value of \\(a_4\\) approaching 0 corresponds to a dose response model that is nearly flat at \\(\\frac{a_{1} + a_{2}}{2}\\). By differentiation, it can be seen that the slope where the effective dose \\(\\nu_d=a_3\\) is \\((a_{2} - a_{1})\\frac{a_{4}}{4a_{3}}\\).\n\n\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_4, \\lambda_4^2)\\]\nThe advantage of this model is that it is considerably more flexible than the logistic, but still more efficient to estimate than an NDLM. This model (if applicable) is ideal if the target is an Effective Dose (ED) such as an ED90. Estimating an ED requires estimating the response on control, the maximum response, and the gradient which is requires a lot of data to do well if using no model or a smoothing model such as one of the NDLMs. With the 4-parameter Sigmoid model (or 3-parameter logistic – but that has a more limited applicability due to being significantly less flexible in shape), it’s usually not an issue to estimate the EDx well.\nThe caveats to using this model are:\n\nWhilst being much more flexible than the logistic, it still has model assumptions, in particular monotonicity, that must be met for this to be a reasonable model to be used.\nThe curve is only well estimated if the true ED50 lies within the doses tested.\nLike the hierarchical logistic model above, the prior for \\(a_3\\) should be weakly informative, with the majority of its probability mass in the available dose range. So similarly, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be: \\[a_3 \\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\n\n\n\n\n\n\n\nFigure 3: Example of a sigmoid model with \\(\\alpha_1=5\\), \\(\\alpha_2=10\\), \\(\\alpha_3=3\\), and \\(\\alpha_4=5\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#u-shaped-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#u-shaped-model",
    "title": "Dose Response Models",
    "section": "U-Shaped Model",
    "text": "U-Shaped Model\nThe U-shaped model when the allows for an initial increase (decrease) in the dose response with dose, followed by a leveling out of the mean, then a decrease (increase). Whether the U-Shaped model increases then decreases or decreases then increases is specified by the user on the Dose Response tab. An example curve for this model is given in the figure below.\nThe dose-response curve can be characterized in four different regions of doses. For the first region, which contains the lowest doses, \\(0&lt;\\nu_d&lt;p_{min}\\), the dose-response curve is increasing (decreasing):\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( \\frac{\\nu_{d}}{p_{\\min}} \\right)^{\\alpha}\\]\nThe next region is the plateau, where the dose-response curve is constant. For \\(p_{min} &lt; \\nu_d &lt; p_{min}+p_{width}\\): \\[\\theta_d=\\theta_0 + S\\cdot\\delta\\] For the third region, the dose-response curve is decreasing (increasing). For \\(p_{min}+p_{width} &lt; \\nu_d &lt; p_{min}+p_{width} + w_{width}\\),\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( 1 - \\frac{\\nu_{d} - \\left( p_{\\min} + p_{width} \\right)}{w_{width}} \\right)^{\\beta}\\]\nFor the final region, the dose-response curve is again constant, at the same level as the zero-dose. For \\(\\nu_d &gt; p_{min}+p_{width} + w_{width}\\), \\[\\theta_d = \\theta_0\\]\nThe parameters of the model are described below:\n\n\\(S\\) is \\(1\\) or \\(-1\\), as determined by the Model is increasing/decreasing radio buttons. \\(S=1\\) if Model is Increasing is selected, indicating that the model starts increasing at low doses.\n\\(\\theta_0\\) represents the zero-strength dose response. Its prior is: \\[\\theta_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\]\n\\(\\delta\\) represents the maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[\\delta \\sim \\text{N}^+(\\mu_\\delta, \\sigma_\\delta^2)\\]\n\\(p_{min}\\) represents the lowest dose at which the maximal change in response from the zero-strength is achieved. It is restricted to be positive, and has a prior of: \\[p_{min} \\sim \\text{N}^+(\\mu_{min}, \\sigma_{min}^2)\\]\n\\(p_{width}\\) represents the width of the plateau – the region where all doses achieve maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[p_{width} ~ \\sim \\text{N}^+(\\mu_{width}, \\sigma_{width}^2)\\]\n\\(w_{width}\\) represents the width of the region for doses beyond the plateau, where response is returning to the zero-strength dose level. It is restricted to be positive, and has a prior of: \\[w_{width} ~ \\sim \\text{N}^+(\\mu_{w}, \\sigma_{w}^2)\\]\n\\(\\alpha\\) determines the rate of change of the dose response curve for doses below the plateau. Values less than \\(1\\) indicate a rapid initial change from baseline, with slower change as the plateau is reached. Values greater than \\(1\\) indicate a slow initial change from baseline, with more rapid change as the plateau is reached. To help avoid identifiability issues, \\(\\alpha\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). \\(\\alpha\\)’s prior is: \\[\\alpha \\sim \\text{LN}^*(\\mu_\\alpha, \\sigma_\\alpha^2)\\] where \\(\\text{LN}^*()\\) represents the lognormal distribution with truncation constraints at \\(10^{-1}\\) and \\(10^{1}\\).\n\\(\\beta\\) determines the rate of change of the dose response curve for doses beyond the plateau. Values less than \\(1\\) indicate a slow initial change from the plateau level, with more rapid change subsequently. Values greater than \\(1\\) indicate a rapid initial change from the plateau level, with slower change subsequently. To help avoid identifiability issues, \\(\\beta\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). The prior on \\(\\beta\\) is: \\[\\beta \\sim \\text{LN}^*(\\mu_\\beta, \\sigma_\\beta^2)\\]\n\nThe U-shaped model has seven free parameters, and thus it is inadvisable to use such a model for dose-finding trial designs that contain only a few dose levels. When a small number of doses are available, it may help to constrain the values of \\(\\alpha\\) and \\(\\beta\\) by utilizing small standard deviations in the priors.\n\n\n\n\n\n\nFigure 4: An example dose response curve for the U-shaped model. In this example, \\(\\alpha&lt;1\\) and \\(\\beta &gt; 1\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#plateau-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#plateau-model",
    "title": "Dose Response Models",
    "section": "Plateau Model",
    "text": "Plateau Model\nThe plateau model is a special case of the U-shaped model, in which \\(p_{width}=\\infty\\). That is, there is no return to baseline for high doses. This model eliminates three parameters from the U-Shaped model, since \\(p_{width}\\), \\(w_{width}\\), and \\(\\beta\\) are not used. See the figure below to see an example dose response model for the plateau model that mimics the beginning of the U-Shaped model above.\n\n\n\n\n\n\nFigure 5: An example dose response curve for the Plateau model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#parameter-exponential-logistic-dichotomous-only",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#parameter-exponential-logistic-dichotomous-only",
    "title": "Dose Response Models",
    "section": "3 Parameter Exponential Logistic (Dichotomous Only)",
    "text": "3 Parameter Exponential Logistic (Dichotomous Only)\nThe 3-parameter exponential logistic model has the following structure:\n\\[\\theta_d = a_1 + a_2 \\nu_d^{a_3}\\]\nWhere \\(\\nu_d\\) is the effective dose strength of dose \\(d\\). This is a logistic model for the dichotomous endpoint because \\(\\theta_d\\) is the log odds ratio of the probability of the response, \\(P_d\\) at dose \\(d\\).\nThe exponent parameter \\(\\alpha_3\\) allows the change in slope at the lower end of the curve to be asymmetric in comparison to the change in curve at the upper end of the curve.\nThe priors for the parameters are:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] The interpretations of the parameters defining this model are:\n\n\\(a_1\\)\n\nthe dose response for a dose with strength 0\n\n\\(a_2\\)\n\nthe slope associated with the exponentiated dose strength\n\n\\(a_3\\)\n\na shape parameter modifying the effective dose strength through exponentiation.\n\n\nThe figure below shows an example of two different 3-parameter exponential logistic model fits. Notably, the fit shown in green has an \\(a_3\\) parameter greater than \\(1\\), which leads to faster increases of the response rate model as the effective dose strength increases.\n\n\n\n\n\n\nFigure 6: Two different examples of 3-parameter exponential logistic model fits. The green model has an \\(\\alpha_3\\) parameter greater than 1, which leads to faster increases of the sigmoid model as the effective dose strength increases.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-model",
    "title": "Dose Response Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nLike the Independent Dose Model, the hierarchical model treats the arms as if they are an unordered set of treatments, but unlike the Independent Dose Model the Hierarchical Model shares information between doses included in the model. The hierarchical model is a way of encouraging the estimates of arm responses to be similar, and the degree of this encouragement can be controlled using parameters of the prior distribution. The hierarchical model assumes that the arm parameters are drawn from the same normal distribution:\n\\[\\theta_d \\sim \\text{N}(\\mu, \\tau^2)\\]\nWhere \\(d\\) is the set of doses included in the model. The prior distributions for \\(\\mu\\) and \\(\\tau^2\\) are\n\\(\\mu \\sim \\text{N}(\\Lambda_\\mu, \\lambda_\\mu^2)\\)\nand\n\\[\\tau^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau\\), and \\(\\tau_n\\) is the prior weight. \\(\\tau^2\\) governs the amount of information shared between doses in the model. The estimates of the dose parameters can be encouraged to be close together by choosing a small value for \\(\\tau_\\mu\\) and a large value for \\(\\tau_n\\). See here for a tool to help understand the inverse gamma distribution specified by center and weight parameters.\nThe control arm can be included in the hierarchical model if it is desired to encourage the experimental arms to be similar to the control arm, or excluded from it if the experimental arms are encouraged to be similar to each other but not necessarily to the control arm. This is not true with time-to-event data, when the control arm can only be excluded from the hierarchical model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#linear-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#linear-model",
    "title": "Dose Response Models",
    "section": "Linear Model",
    "text": "Linear Model\nThe linear model uses a strong assumption of exact linearity between the responses and the dose strengths. (For dichotomous models, the linearity is on the log-odds scale, and the linearity is on the log hazard ratio scale for TTE models.) The model is\n\\[\\theta_d=\\alpha+\\beta\\nu_d\\] for all doses \\(d\\) in the model. Both \\(\\alpha\\) and \\(\\beta\\) are given normal prior distributions:\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\]\nThe linear model is appropriate when there is strong scientific knowledge that the shape of the response curve should be linear as a function of dose strength. Note that if an arm has a dose strength between two other arms in the model, its response will always be estimated to be between the responses of those two arms. As a result, the linear model may not be appropriate in combination with certain decision quantities such as Pr(Max), because some doses are guaranteed to have probability zero of being the best arm. If the control arm is included in the linear model and it has the smallest dose value, then all experimental arms will always have exactly the same probability of being better than control.\nWe recommend creating scenarios where the true parameters deviate from linearity and simulating them to test the robustness of a design based on the linear model.\n\n\n\n\n\n\nNote\n\n\n\nFor dichotomous models, the linear model replaces the 2-parameter logistic model available in FACTS versions before 6.4. The two models are identical.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-linear-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-linear-model",
    "title": "Dose Response Models",
    "section": "Hierarchical Linear Model",
    "text": "Hierarchical Linear Model\nA more flexible version of the linear model is the hierarchical linear model. This model encourages the arm responses to have a linear-like shape, but also includes a term allowing parameters to deviate from linearity when appropriate. The arm parameters satisfy the relationship\n\\[\\theta_d = \\alpha + \\beta \\nu_d + \\zeta_d\\] where the \\(\\alpha\\) and \\(\\beta\\) parameters are as in the linear model, with prior distributions\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\] and the \\(\\zeta_d\\) parameters are the deviations from linearity. These deviations are assumed to have a normal distribution constrained so that their average value is zero:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau^2) \\text{ with } \\sum_d\\zeta_d=0.\\]\nThe prior distribution for \\(\\tau^2\\) is\n\\[\\tau^{2}\\sim\\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nIf \\(\\tau^2\\) is small, which can be encouraged by choosing \\(\\tau_\\mu\\) to be small and \\(\\tau_n\\) to be large, then the dose parameter estimates will lie close to a line. See here for help understanding FACTS’s parameterization of the inverse gamma distribution.\nThe hierarchical linear model is a good choice when, for example, one expects a linear relationship between the dose strengths and the response but one is not prepared to assume exact linearity. It can also be a good way of encouraging, but not requiring, monotonicity in the response.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#d-continuous-factorial-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#d-continuous-factorial-model",
    "title": "Dose Response Models",
    "section": "2D Continuous Factorial Model",
    "text": "2D Continuous Factorial Model\nThe 2D Continuous Factorial Model fits a regression on the dose response that uses the numeric dose strength as a continuous valued covariate. The dose response (with \\(\\eta_r\\) and \\(\\zeta_c\\) denoting dose strength of the row level and column level, respectively) is modeled as:\n\\[\\theta_{rc} = \\alpha_0 + \\alpha_1 \\zeta_c + \\alpha_2 \\eta_r + \\alpha_3\\zeta_c \\eta_r\\] With priors\n\\[\\alpha_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\] \\[\\alpha_1 \\sim \\text{N}(\\mu_1, \\sigma_1^2)\\] \\[\\alpha_2 \\sim \\text{N}(\\mu_2, \\sigma_2^2)\\] \\[\\alpha_3 \\sim \\text{N}(\\mu_3, \\sigma_3^2)\\]\nThen, \\(\\alpha_0\\) is the response at the control combination, \\(\\alpha_1\\) is the linear coefficient of the response to the column factor strengths \\(\\zeta_c\\), and \\(\\alpha_2\\) is the linear coefficient of the response to the row factor strengths \\(\\eta_r\\).\nThe user has the option to simplify the model and exclude the interaction term \\(\\alpha_3\\), which is the coefficient of the product of the two factor strengths.\nNote that despite being called “continuous” this dose response model applies to all endpoints. It’s called continuous because the coefficients act on the dose strengths as continuous measures of strength.\n\n\n\n\n\n\nFigure 7: 2D Continuous Factorial Dose Response model specification page.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#d-discrete-factorial-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#d-discrete-factorial-model",
    "title": "Dose Response Models",
    "section": "2D Discrete Factorial Model",
    "text": "2D Discrete Factorial Model\nThe 2D Discrete Factorial Model fits a regression on the dose response that uses the dose strength as a factor covariate rather than a numeric dose strength. Each dose is an independent level within each row or column factor, so any dose is equidistant from all other doses\n\\[\\theta_{rc} = \\alpha + \\gamma_r + \\beta_c\\]\nWith priors\n\\[\\alpha \\sim \\text{N}(\\mu_\\alpha, \\sigma_\\alpha^2)\\]\n\\[\\beta_c \\sim \\text{N}(\\mu_{\\beta_c}, \\sigma_{\\beta_c}^2)\\] \\[\\gamma_r \\sim \\text{N}(\\mu_{\\gamma_r}, \\sigma_{\\gamma_r}^2)\\]\nThe parameters associated with lowest level of each factor, \\(\\gamma_0\\) and \\(\\beta_0\\), are constrained to be \\(0\\).\n\n\n\n\n\n\nFigure 8: 2D Discrete Factorial Dose Response model specification page.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#d-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#d-ndlm",
    "title": "Dose Response Models",
    "section": "2D NDLM",
    "text": "2D NDLM\nThe 2-D NDLM Model takes the idea of smoothing across doses that was used in the standard NDLM model, but smooths across row factors and column factors separately.\n\nThe Base Model, with Control Included\nThe treatment effect for the combination of level \\(r\\) in the row factor and level \\(c\\) in the column factor is denoted as \\(\\theta_{rc}\\), and \\(Y_{rc}\\) is the observed data in that cell. The borrowing parameters are denoted as \\(\\phi\\) for the row factor smoothing, and \\(\\tau\\) for the column factor smoothing. The dose strengths are denoted as \\(\\nu_r\\) for the row factors, and \\(\\omega_c\\) for the column factors. Let \\(\\Delta \\nu_r = \\nu_r - \\nu_{r-1}\\) and \\(\\Delta \\omega_c = \\omega_c - \\omega_{c-1}\\) (for \\(r&gt;0\\) and \\(c&gt;0\\)). For notational convenience at the grid edge, let \\(\\theta_{-1, c} = 0\\), \\(\\theta_{r,-1}\\), \\(\\Delta\\nu_0\\equiv\\infty\\), and \\(\\Delta\\omega_0\\equiv\\infty\\).\nThe 2-D NDLM Model with control included in the model can then be specified as:\n\\[\\theta_{0,0} \\sim \\text{N}(\\mu_0, \\tau_0^2)\\] \\[\\theta_{rc} \\sim \\text{N}(\\mu_{rc}, \\tau_{rc}^2)\\] where\n\\[\\tau_{rc}^{2} = \\left( \\frac{1}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{1}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)^{- 1}\\]\n\\[\\mu_{rc} = \\tau_{rc}^{2}\\left( \\frac{\\theta_{r - 1,c}}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{\\theta_{r,c - 1}}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)\\]\nwith priors\n\\[\\tau^{2}\\sim\\text{IG}\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\n\\[\\phi^{2}\\sim\\text{IG}\\left( \\frac{\\phi_{n}}{2},\\frac{\\phi_{\\mu}^{2}\\phi_{n}}{2} \\right)\\]\nNote: that not all combinations of \\(r\\) and \\(c\\) will have data associated with them. However, FACTS will still evaluate the prior at all combinations. Thus, in the situation depicted below where the grayed-out point indicates a treatment combination with no allocation, \\(\\theta_{1,2}\\) is not modeled conditioned only on \\(\\theta_{1,1}\\). \\(\\theta_{0,1}\\) also informs on \\(\\theta_{1,2}\\) via \\(\\theta_{0,2}\\).\n\n\n\n\n\n\nFigure 9: 2D dosing grid example with no data for (0,2).\n\n\n\n\n\nFix smoothing ratio for row factor and column factor\nOptionally, the user might want to specify the row factor borrowing as a function of the column factor borrowing:\n\\[\\phi\\equiv k \\cdot \\tau\\] where \\(k\\) is a user-specified constant. This can be performed by selecting “Fixed smoothing ratio” in the \\(\\phi^2\\) prior specification area.\n\n\nControl not in model, no zero-level doses\nIf neither treatment arm allows zero-level doses (e.g. like one would expect if the column factor represented doses and the row factor represented frequency), then a prior for the lowest dose would instead be needed as input from the GUI:\n\\[\\theta_{1,1} \\sim \\text{N}(\\mu_1, \\tau_1^2)\\]\n\n\n\n\n\n\nFigure 10: 2D NDLM Dose Response model specification page.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-priors",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-priors",
    "title": "Dose Response Models",
    "section": "Hierarchical Priors",
    "text": "Hierarchical Priors\nIf a hierarchical prior is used for either the Control or Active Comparator arm, then FACTS will estimate a prior for that arm using external data that the user provides. This prior is estimated through a hierarchical model, and then the in-study data collected on subjects randomized to that arm are used to estimate the posterior distribution for the arm.\nTo specify a hierarchical prior, the user specifies the sufficient statistics from each historical study. These are:\n\nContinuous: the mean response and the SD of the response of the control or active comparator arm and the number of subjects observed.\nDichotomous: the observed number of responders and the number of subjects observed in the study.\nTime-to-Event: the number of events and the amount of exposure within each bin in the piecewise model.\n\nThe information from the historical studies can be ‘down-weighted’ by decreasing the effective information in the sample size. For continuous, this can be done by decreasing the sample size by a percentage. For dichotomous, both the number of responders and the number of subjects would be decreased by a percentage. For time-to-event, multiplying the number of events and the exposure by the same fraction will reduce the information in the study without changing the reported hazard rate.\n\n\n\n\n\n\nFigure 11: The Hierarchical Priors tab for a continuous study when both the Control and Active Comparator are given hierarchical priors.\n\n\n\nThe hierarchical models for the control or active comparator rates are very similar across the endpoints. They are briefly described below.\n\nContinuousDichotomousTime-to-Event\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\theta_{0t} \\sim \\text{N}(\\mu_0, \\tau_0^2) \\text{ for } \\tau=0,1,2,\\ldots,T\\] where \\(\\theta_{0t}\\) is the mean for the control arm in trial \\(t\\) (\\(t=0\\) for the current trial and \\(t=1,2,\\ldots,T\\) for previous trials). A user needs to specify appropriate priors for the hyper-parameters:\n\\[\\mu_0 \\sim \\text{N}(m_0, \\sigma_0^2)\\] \\[\\tau_0^2 \\sim \\text{IG}(a_0, b_0)\\]\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\theta_{0t} \\sim \\text{N}(\\mu_0, \\tau_0^2) \\text{ for } \\tau=0,1,2,\\ldots,T\\] where \\(\\theta_{0t}\\) is the log-odds for the control arm in trial \\(t\\) (\\(t=0\\) for the current trial; \\(t=1,2,\\ldots,T\\) for previous trials). A user needs to specify appropriate priors for the hyper-parameters:\n\\[\\mu_0 \\sim \\text{N}(m_0, \\sigma_0^2)\\] \\[\\tau_0^2 \\sim \\text{IG}(a_0, b_0)\\]\nThe prior distributions for the hyper-parameters of the hierarchical model. The hyper-parameters are the mean and standard deviation of Normal distribution for the log hazard ratios of the event rates of the historical studies and the current study.\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\lambda_{st}=\\lambda_s \\exp(\\gamma_t) \\text{ for } t=0,1,2,\\ldots,T\\] where \\(\\lambda_{st}\\) is the hazard rate for the control arm in segment \\(s\\) (\\(s=1,2,\\ldots,S\\)) for previous trial \\(t\\) (\\(t=1,2,\\ldots,T\\)) and \\(\\lambda_{s0}\\) is the hazard rate for the current control arm in segment \\(s\\); \\(\\lambda_s\\) is a base hazard for segment \\(s\\); and \\(\\gamma_t\\) is the log hazard ratio between that base rate and the \\(\\lambda_{st}\\) values.\nThe following hierarchical model is used\n\\[\\gamma_t \\sim \\text{N}(\\mu_\\gamma, \\tau_\\gamma^2) \\text{ for } t=0,1,2,\\ldots,T\\] Users specify priors for the hyper-parameters:\n\\[\\mu_\\gamma \\sim \\text{N}(m_\\gamma, t_\\gamma^2)\\] \\[\\tau^2 \\sim \\text{IG}(a_\\gamma, b_\\gamma)\\]\nThe formulation above is not identifiable as changes in \\(\\lambda_s\\) can be compensated for by changes in the \\(\\gamma_t\\) values (thus, one can use different combination of \\(\\lambda_s\\) and \\(\\gamma_t\\), but acquire the same set of values \\(\\lambda_{st}\\) and thus the same likelihood). To avoid this difficulty, we use the above formulation but fix \\(\\gamma_0 = 0\\). In addition to preserving the identifiability of the structure, this constraint allows \\(\\lambda_s\\) to have the interpretation of being the hazard rate for the current control arm, and thus the prior on \\(\\lambda_s\\) from the main dose response may be used as the prior for \\(\\lambda_s\\).\n\n\n\n\nSetting Priors for Hierarchical Model Hyper Parameters\nUnless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.\nIn this case the following would be reasonable:\n\nSet the prior mean value for Mu as the mean of the mean responses on the control arm in the historic studies\nSet the prior SD for Mu equal to the average SD of response divided by the square root of the number of studies.\nSet the center for tau to the same value as the prior SD for Mu.\nSet the weight for tau to be &lt; 1.\n\nOne can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small (relative to the number of studies), then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.\nTo give some prior preference towards pooling or separate analysis the weight for tau has to be large (relative to the number of historic studies) – to have a design that is like pooling all the historic studies the mean for tau needs to be small (say 10% or less of the value suggested above). For there to be no borrowing from the historic studies the value for tau needs to be large (say 10x or more the value suggested above).\nThe best way to understand the impact of the priors is try different values and run simulations.\n\n\nBayesian Augmented Control (BAC) Example:\nIt is easiest to study the impact of BAC in a simplified version of the trial being designed, for example use a 2 arm fixed design with ‘no model’, no adaptation, no longitudinal modeling, etc.\nFor instance, in a continuous setting with an expected sample size of 50 on control, a mean response of 5 and an SD in the response of 2, looking at the effect of having 4 prior, similarly sized studies:\n\n\n\nExternal Study Sufficient Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of subjects\n\n\nMean Response\n\n\nSD of Response\n\n\n\n\n\n\nStudy 1\n\n\n50\n\n\n4.76\n\n\n2\n\n\n\n\nStudy 2\n\n\n50\n\n\n4.93\n\n\n2\n\n\n\n\nStudy 3\n\n\n50\n\n\n5.07\n\n\n2\n\n\n\n\nStudy 4\n\n\n50\n\n\n5.24\n\n\n2\n\n\n\n\nFor simplicity we have just varied the means of the studies – they are all consistent with a normally distributed estimate, with mean of 5 and a standard error of \\(\\frac{2}{\\sqrt{50}}\\).\nBy simulating a fixed trial with a sample size of 100 and fixed 1:1 allocation between control and a study arm we can easily observe the effects of a BAC on the control arm. Creating 6 VSR profiles with sigma = 2, and the same mean response for control and the study arm in each profile of: 4.53, 4.76, 5.07, 5.24 and 5.47 we can look at the bias of the final estimate and the shrinkage in the SD of the estimate that could be brought about by using BAC:\n\n\n\nQuick simulation study of how the hierarchical model for BAC effects estimates of the control rate under different true control rate scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Mean\n\n\nRaw mean\n\n\nRaw SD\n\n\nEstimate inc BAC\n\n\nSD inc BAC\n\n\nBias\n\n\nEffective additional Subjects\n\n\n\n\n\n\n4.53\n\n\n4.55\n\n\n0.28\n\n\n4.64\n\n\n0.255\n\n\n2.1%\n\n\n11.1\n\n\n\n\n4.76\n\n\n4.78\n\n\n0.28\n\n\n4.83\n\n\n0.250\n\n\n1.0%\n\n\n13.5\n\n\n\n\n4.93\n\n\n4.95\n\n\n0.28\n\n\n4.96\n\n\n0.248\n\n\n0.2%\n\n\n14.3\n\n\n\n\n5.07\n\n\n5.09\n\n\n0.28\n\n\n5.07\n\n\n0.248\n\n\n-0.4%\n\n\n14.2\n\n\n\n\n5.24\n\n\n5.26\n\n\n0.28\n\n\n5.20\n\n\n0.250\n\n\n-1.1%\n\n\n13.2\n\n\n\n\n5.46\n\n\n5.49\n\n\n0.28\n\n\n5.38\n\n\n0.255\n\n\n-1.9%\n\n\n10.7\n\n\n\n\nNote it is important to use the column “SD Mean resp 1” from the simulations file to see the SD of the estimate and not use the SD of the estimates in the summary table.\nThe small shrinkage in the SD is more impressive when viewed in terms of how many additional subjects would have to be recruited to achieve the same shrinkage in the raw estimate. These are useful savings for small additional bias.\nThe “effective additional subjects” was calculated: \\[\\left( \\frac{\\text{True sigma}}{\\text{AVG(SD Mean resp)}} \\right)^{2} - \\left( \\frac{\\text{True sigma}}{\\text{AVG(SE Mean Raw Response)}} \\right)^{2}\\] where in this example \\(\\text{True sigma}\\) was 2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#time-to-event-missingness",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#time-to-event-missingness",
    "title": "Dose Response Models",
    "section": "Time-to-Event Missingness",
    "text": "Time-to-Event Missingness\nFor a time-to-event endpoint, as is the convention in this endpoint, subjects are observed up to the time they have an event (or the end of follow up), so no subject drops out after having had an event. Subjects that do drop-out are included in the analysis as not having had an event up to the last observation of the subject.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/frequentistanalysis.html",
    "href": "documentation/v71/userguides/core/design/frequentistanalysis.html",
    "title": "Frequentist Analysis",
    "section": "",
    "text": "On the Frequentist Analysis tab the user can specify that some standard frequentist analyses be performed at trial analyses. The frequentist analysis tab is completely separate from, and independent of, any p-value QOIs that have been defined. The analyses specified on this tab cannot be used for simulated trial decisions - they are for storing in output only.\nEach specified analysis can be conducted using a variety of ways of handling missingness. Select all ways of handling missingness that are desired:\n\nMissing data replaced by last observation carried forward (LOCF)\nMissing data replaced by baseline observation carried forward (BOCF). This is only available if the endpoint is continuous and Baseline is being simulated.\nMissing data is ignored (a “per-protocol” analysis).\nMissing data is treated as a failure. This is only available if the endpoint is dichotomous.\n\nIf the trial has interim analyses, then for the simulations for which frequentist weeks files are to be output (specified on the Simulation tab) the standard frequentist analyses will be performed. If the trial has p-values QOIs, those QOIs are calculated every interim in all simulations.\nHaving the frequentist analysis include Dunnett’s adjusted p-values is a separate option (that applies to all the analysis type requested) because of the significant run-time overhead this can entail. Dunnett’s adjustment is available for continuous and dichotomous frequentist analyses.\nThe frequentist analysis tabs for the continuous and dichotomous engines also have trend tests, and allow the user to specify contrast coefficients to conduct those tests.\nNote that the reported frequentist estimates of the treatment effect take the specified direction of response on the Study tab (whether a response indicates subject improving or worsening) into account. They are adjusted so that a treatment that is estimated to be better than the control always has a positive treatment effect.\n\n\n\n\n\n\nFigure 1: The frequentist analysis tab for a continuous endpoint.\n\n\n\n\nContinuous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing unadjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\n(p-value)[## “1-sided p-value is reported in all the frequentist results. This is done in order to be consistent with comparisons with 1-sided α-values elsewhere.”],\nconfidence interval for the mean difference,\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nIf selected, using Dunnett-adjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\np-value,\nconfidence interval for the mean difference\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nIf neither placebo nor an active comparator are simulated, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out.\nIf high values of the endpoint are good, P-values are calculated using a one-sided t-test testing \\[H_0: \\mu_T &lt; \\mu_C\\] against \\[H_1: \\mu_T \\ge \\mu_C\\] with \\(\\mu_T\\) being the true treatment response mean and \\(\\mu_C\\) being the true control response mean. If low values of the endpoint are good, then the signs of the hypotheses are flipped.\n\n\nDichotomous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing the methodology described by Agresti, Mee and Nurminem for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nIf checked, using Dunnett-adjusted dose-placebo comparisons for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nP-values are calculated by user choice either using a normal approximation (see previous section on p-value calculations for continuous endpoints) or a Fisher’s exact test, which is a conservative test to test the null hypothesis that the outcome is independent of the treatment assignment. For the normality approximation to have good asymptotic coverage, the expected number of successes and failures under the null hypothesis should be greater or equal to 5 for both groups. In cases where this is not guaranteed, Fisher’s exact test is recommended when strict type 1 error control is required.\nIf neither placebo nor active comparator are specified, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out.\n\n\nTime-to-Event Frequentist Analysis\nFor each simulated trial, the following frequentist analyses will be performed:\n\nDose-placebo comparisons based on log-rank test and the Cox proportional hazards model. Summaries include:\n\nThe log-rank and Wilcoxon test statistics and the corresponding p-values,\nEstimated hazard ratio and its confidence interval from Cox model,\nFor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were, based on the hazard ratio inference from the Cox model).\n\nMedian survival times based on the Kaplan-Meier method.\nFor the predictor, descriptive statistics are computes – a 7 number summary for a continuous predictor, percentages for the dichotomous predictor, and median time, average hazard, and a cox model with predictor as covariate are computed for a time to event predictor.\n\nThe following variations for adjusting the alpha level for multiple comparisons should be available using the user-specified 1-sided alpha: unadjusted, and Bonferonni adjusted.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Frequentist Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/execution.html",
    "href": "documentation/v71/userguides/core/execution.html",
    "title": "Execution Tab",
    "section": "",
    "text": "The Accrual sub-tab provides an interface for specifying accrual profiles. Accrual profiles define the mean recruitment rate week by week during the course of the trial. Virtual subjects are simulated from a Poisson process in which the expected number of subjects per week is allowed to change week by week.\nAccrual profiles are shown as a list on the left of the screen, as depicted below. These accrual profiles may be renamed by double-clicking on them and typing a new profile name. After creating a profile, the user must create at least one recruitment region. Early in the trial design process, detailed simulation of the expected accrual pattern is typically not necessary and a single region with a simple mean accrual rate is sufficient.\nTo model the expected accrual rates more precisely over the course of the trial, the user may specify multiple regions for each accrual profile and separately parameterize them. Regions are added via the table in the center of the screen Figure 1. Within this table, the user may modify:\n\nthe peak mean weekly recruitment rate,\nthe start date (in weeks from the start of the trial) for this recruitment region,\nwhether the region will have a ramp up phase and if so when the ramp up will be complete (in weeks from the start of the trial).\nWhether the region will have a ramp down, and if so when the ramp down start and when the ramp down will complete (in weeks from the start of the trial).\n\nRamp up/ramp down define simple linear increase/decreases in mean recruitment rate from the start to the end of the ramp. Note that simulation of accrual is probabilistic, but ramp downs are defined in terms of time, so even if ramp downs are planned so that at the average accrual rate they will occur as the trial reaches cap, there is a risk in simulations when accrual has been slower than average, that ramp downs occur before the full sample size is reached. It is advisable to have at least one region that doesn’t ramp down to prevent simulations being unable to complete.\nA graph of the recruitment rate of the highlighted region is shown as well. As the recruitment parameters are changed, the graph will update to show the time at which full accrual is reached. An accrual profile that does not reach full accrual is invalid and cannot be used to run simulations.\n\n\n\n\n\n\nFigure 1: Execution &gt; Accrual tab.\n\n\n\nIn the screenshot above you can see the two step ramp up in accrual from two regions – each starting at different offsets into the trial.\nNote that the accrual profile graph is only the mean expectation; actual accrual is simulated using exponential distributions for the intervals between subjects, derived from the mean accrual profile specified here. Thus some simulated trials will recruit more quickly than this and some more slowly.\nThere are commands to import and export region details from/to simple external XML files. When importing, the regions defined in the external file are added to the regions already defined, they don’t replace them.\nThis is an example of a very simple region file defining just one region:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;regions&gt;\n&lt;region&gt;\n&lt;name&gt;Region 1&lt;/name&gt;\n&lt;rate&gt;5&lt;/rate&gt;\n&lt;start&gt;0&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;/regions&gt;\n\n\nIf “Deterministic” accrual has been specified on the Study &gt; Study Info tab, then on the Accrual tab, rather that specifying an accrual profile from which subject recruitment times are simulated, the user loads a file of specific accrual dates for every subject.\n\n\n\n\n\n\nFigure 2: Execution &gt; Deterministic Accrual tab.\n\n\n\nThe user specifies a “.dat” file to load that contains the  subject accrual dates in weeks This value is in weeks from FACTS 7.0 onwards, previously it was in days.  from the start of the trial.\nThe required file format is a text file with comma separate values. One row per subject, with 3 fields on each row:\n\nthe subject ID, (an integer)\nthe ID of the region where the subject was recruited (an integer)\nand the subjects randomization date (in weeks from the start of the trial – this is a ‘real’ number allowing fractions of a week to be specified)\n\nThe file must contain sufficient entries to allow the maximum number of subjects specified on the Study &gt; Study Info tab to be recruited.\nAfter successfully loading a file, the FACTS GUI shows a plot of the resulting weekly accrual rate",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/execution.html#deterministic-accrual",
    "href": "documentation/v71/userguides/core/execution.html#deterministic-accrual",
    "title": "Execution Tab",
    "section": "",
    "text": "If “Deterministic” accrual has been specified on the Study &gt; Study Info tab, then on the Accrual tab, rather that specifying an accrual profile from which subject recruitment times are simulated, the user loads a file of specific accrual dates for every subject.\n\n\n\n\n\n\nFigure 2: Execution &gt; Deterministic Accrual tab.\n\n\n\nThe user specifies a “.dat” file to load that contains the  subject accrual dates in weeks This value is in weeks from FACTS 7.0 onwards, previously it was in days.  from the start of the trial.\nThe required file format is a text file with comma separate values. One row per subject, with 3 fields on each row:\n\nthe subject ID, (an integer)\nthe ID of the region where the subject was recruited (an integer)\nand the subjects randomization date (in weeks from the start of the trial – this is a ‘real’ number allowing fractions of a week to be specified)\n\nThe file must contain sufficient entries to allow the maximum number of subjects specified on the Study &gt; Study Info tab to be recruited.\nAfter successfully loading a file, the FACTS GUI shows a plot of the resulting weekly accrual rate",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/successfutility.html",
    "href": "documentation/v71/userguides/core/design/successfutility.html",
    "title": "Success/Futility Criteria",
    "section": "",
    "text": "The Success/Futility Criteria tab is where users specify the decision rules for determining study success or failure. The Final Analysis criteria always exist, and should, in general, be specified for every simulated trial. If simulating an adaptive trial, then interim analysis decision rules are also specified here.\n\nDecisions in FACTS Core\nThere are \\(7\\) possible decisions that can be made in a FACTS Core design, each with a numeric identifier that FACTS uses in the .csv output to denote decisions. The Outcome column contains the decision made, and the number 1-7 map to decisions as follows:\n\n1. Early Success\n\nEarly Success is achieved if and only if the trial meets the success condition at an interim analysis, and does not meet the futility criteria at the final analysis. The final analysis criteria must not be met whether or not subjects were selected to follow-up after an early success decision.\n\n2. Late Success\n\nLate Success is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then meets the final analysis success criteria.\n\n3. Late Futility\n\nLate Futility is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then meets the final analysis futility criteria. Late futility is not automatically the complement of late success; the futility rule must be specified as the complement of the success rule to make it true.\n\n4. Early Futility\n\nEarly Futility is achieved if and only if the trial meets the futility condition at an interim analysis, and does not meet the success criteria at the final analysis. The final analysis criteria must not be met whether or not subjects were selected to follow-up after an early success decision.\n\n5. Success to Futility Flip-Flop\n\nSuccess to Futility Flip-Flop is achieved if and only if the trial meets the success condition at an interim analysis, but meets the futility condition at the final analysis. Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision.\n\n6. Futility to Success Flip-Flop\n\nFutility to success flip-flop is achieved if and only if the trial meets the futility condition at an interim analysis, but meets the success condition at the final analysis. Futility to Fuccess Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision.\n\n7. Inconclusive\n\nInconclusive is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then does not meet the final success or final futility criteria.\n\n\nEvery simulated trial will result in exactly one of these decisions. Non-adaptive trials will result in either Late Success, Late Futility, or Inconclusive. An adaptive trial that does not stop at an interim analysis will result in Late Success, Late Futility, or Inconclusive. An adaptive trial that stops enrolling for early success at an interim analysis will end up as an Early Success or a Success to Futility flip-flop. An adaptive trial that stops enrolling for early futility will result in Early Futility or Futility to Success flip-flop.\n\n\nFinal Evaluation\nOn the Final Evaluation tab, the user can specify rules for judging the study for final futility or final success at its end.\nThe left column of the Final Evaluation tab contains the specification of the trial final futility rule, and the right column contains the specification of the final success rule.\nTo add a decision rule, click the “Add…” button within the appropriate column, select a decision quantity QOI, a comparison inequality sign, and a threshold. Final success and final futility criteria can each have multiple components to them, and the selection at the bottom of the column called “Combine criteria using:” dictates if success or futility should be declared if every single criteria is met (AND) or if any criteria is met (OR).\nThe success and futility rules need not be complementary - there can be trials that do not meet either criteria at the final analysis. These trials would be considered inconclusive. It is allowable, although generally not recommended, to specify overlapping success and futility rules. If a trial were to satisfy both the success and futility criteria at the final analysis it would be considered a final futility.\nThe Final Evaluation Criteria are always applied at the time of the final analysis, even if the study stops early for success or futility, and whether or not “Continue follow-up if study stopped for success” or “Continue follow-up if study stopped for futility” are selected. As a result, setting final analysis criteria that are easier to satisfy than interim analysis criteria may result in the labelling of trials that meet the interim analysis criteria but do not satisfy the final analysis criteria as flip-flops.\n\n\n\n\n\n\nFigure 1: The final evaluation tab within the success futility tab.\n\n\n\n\n\nInterim Analysis Criteria\nOn the success/futility criteria of a design with “Enable adaptive features” checked on the Study &gt; Study Info page, the user can specify rules for judging the study for futility or success at an interim and at the final evaluation. If the trial has no interims there will be just a tab for the Final Evaluation criteria. If the trial has interim analyses, then there can be tabs that define different early success/futility criteria at the different interims.\nAt the top of the main tab is a control to allow tabs to be created for different interims. In a newly created adaptive design, FACTS will create a tab for interim 1.\nIf early success/futility criteria are specified for an interim, then they will be taken to apply to all subsequent interims until the next one for which criteria are supplied, then those criteria will apply until the next interim for which criteria are applied and so on. There will be no stopping for success or futility until the first interim for which early stopping criteria have been defined.\nIt is possible to specify overlapping early success and early futility criteria at an interim, but it is not possible to stop for both success and futility at the same time. The FACTS design engines will stop the trial but the user should not rely on one or other outcome taking precedence. It is not considered good practice to have success and futility rules that could both be true, so FACTS does not  guarantee a “tie break” rule As of FACTS 7.1 early futility will be the result if both early success and early futility criteria are met. .\nIn the output files there are columns labeled “Success ” and “Futile ” indicating whether any decision criteria became true, and “Success Combined” and “Futile Combined” indicating if all the criteria for a success or futility determination have been met. If both sets of criteria are met simultaneously, FACTS will only flag one of “Success Combined” and “Futile Combined” as being met, corresponding to how the outcome of the trial has been flagged.\n\n\n\n\n\n\nFigure 2: An interim evaluation tab within the success futility tab. The selected interim evaluation tab controls the interim analysis decisions at the 3rd interim analysis and all later interims.\n\n\n\nHaving created a tab to define the early success/futility criteria at an interim, the user has options to delete the tab or to copy criteria that have already been entered on another tab before editing them.\nThe user specifies:\n\nWhether early stopping for futility or success is to be allowed by selecting the “Futility criteria” or “Success criteria”\nThe stopping rules are specified, by selecting the QOI to be tested, the direction of the comparison and the threshold value for the criteria to be met, as in the Final Evaluation tab above.\nThe user can specify whether, if multiple criteria have been specified, they all need to be meet (Criteria combined using “AND”) or only any one of them needs to be met (Criteria combined using “OR”). These are specified independently for stopping for success and for futility.\nIf stopping is allowed, the user specifies what the minimum amount of information (using the information type specified on the Interims tab). This can be specified both in overall terms and information on specific doses. If multiple minimum-information criteria are specified they must all be met for the QOI criteria to be evaluated.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Success/Futility Criteria"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/index.html",
    "href": "documentation/v71/userguides/core/design/index.html",
    "title": "Design Overview",
    "section": "",
    "text": "The FACTS core engine allows for the design and simulation of fixed and adaptive clinical trials, especially focused on, but not limited to, Bayesian designs with multiple active arms.\n\nSub-tabs of the Design Tab\nTrials designed in the core engines are comprised of a number of elements:\n\nThe dose response model: the user must specify how the doses are related to each other in the primary analysis, though there is a simple ‘no model’ option that estimates the mean treatment effect of each arm independently. A fixed trial uses the dose response model for the final Bayesian analysis of the data; an adaptive trial uses the same model both for the final analysis and at the interim updates. For time-to-event endpoints, there is an additional tab for estimating the control arm hazard rate called hazard model.\nThe longitudinal model or predictor model: whether the trial is adaptive or fixed, the user may select whether to use a longitudinal model for continuous or dichotomous primary endpoints or a predictor model when using a time-to-event primary endpoint. In a fixed trial the longitudinal model can be used to multiply impute final values for subjects that have dropped out. In an adaptive trial it is also used at the interim updates to multiply impute final values for subjects who have been recruited but do not yet have final values. In a fixed trial with no subject dropouts using a longitudinal model would have no effect on the outcome, analysis, or conduct of the trial.\nAllocation rules: in a fixed trial the user just specifies the proportion of subjects to be recruited to each arm, and the same can be done in an adaptive trial (i.e. an adaptive trial does not have to adapt the allocation), but an adaptive trial has a range of options that the user can use to adapt how subjects are allocated to the different treatment arms as the trial progresses.\nEarly stopping rules: in an adaptive trial the user can select the criteria and specify the thresholds at which trial should be stopped at any interim where the conditions are satisfied. Early stopping is optional, and even in an adaptive design the user can opt to always recruit the maximum permitted number of subjects. In a fixed trial there are no interim analyses and hence no opportunity to stop early.\nFinal evaluation criteria: the same Bayesian evaluation criteria are available whether the trial is fixed or adaptive. The user selects which criteria to use and what thresholds will constitute success or failure. The success and failure criteria do not have to be complements of eachother, and any analysis that doesn’t completely satisfy either the success or futility criteria is called, “inconclusive.”\nFrequentist analysis: frequentist p-values can be calculated comparing each dose to the control arm (or a fixed value if there is no control). P-values can be used as decision making quantities at interim updates or final analyses. p-value cannot benefit from the dose reponse models or longitudinal models, which are specific to the Bayesian model in FACTS.\n\n\n\n\n\n\n\nDesign tabs for multiple endpoint designs\n\n\n\nThe tab layout for multiple endpoint designs is slightly different when compared to the single endpoint engines. The multiple endpoint engine must allow for separate Dose Response, Frequentist Analysis, and Longitudinal specifications for each endpoint.\nTo allow for this, there is a “Response Models” tab as a first level sub-tab of the Design tab. There is 1 sub-tab below Response Models for each endpoint. Within the endpoint tab there will be a Dose Response, Frequentist Analysis, and Longitudinal tab (if applicable).\n\n\n\n\nEvaluation of Bayesian Posterior Estimates\nAt every interim and final analysis there is a Bayesian model fit to the data observed up to that point in the trial. The Bayesian model contains a dose response model and, often, a longitudinal model. In the absence of a longitudinal model, the posterior is calculated as:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)}\\]\nwhere \\(\\phi\\) is the set of parameters of the selected response model, \\(p(\\phi)\\) is the prior for those parameters, \\(y_i\\) is the final response for each subject and \\(n\\) is the number of subjects with complete data.\nWith a longitudinal model, this becomes:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)\\prod_{i = 1}^{n}{\\prod_{j = 1}^{L}{p(y_{ij}|\\psi)p(\\psi)}}}\\]\nwhere \\(\\psi\\) is the set of parameters of the selected longitudinal model, \\(p(\\psi)\\) is the prior for those parameters, \\(y_{ij}\\) is the response for each subject \\(i\\) at each visit \\(j\\) and \\(L\\) is the number of visits.\nThe posterior is evaluated using  MCMC Markov Chain Monte Carlo  with individual parameters updated by Metropolis Hastings (or Gibbs sampling where possible), using only the \\(y_i\\) and \\(y_{ij}\\) data available at the time of the update.\nThe likelihood and priors for each of the dose response models are provided in the description of the Dose Response tab, and the likelihood and priors for the multiple imputation models are provided in the description of the Longitudinal Models tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html",
    "href": "documentation/v71/userguides/core/design/allocation.html",
    "title": "Allocation",
    "section": "",
    "text": "The options on allocation tab depend on whether an adaptive or non-adaptive design has been selected on the ‘Study &gt; Study Info’ tab, and if adaptive whether subjects are recruited sequentially or in cohorts.\nIf the design is non-adaptive then the only allocation option is blocked with fixed allocation ratios.\nIf the design is adaptive with Continuous or Deterministic recruitment, then there are 4 allocation options.\nIf the design is adaptive with cohort recruitment then there are 3 allocation options, all of which can be combined with early stopping:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#randomization-ratio-and-blocking",
    "href": "documentation/v71/userguides/core/design/allocation.html#randomization-ratio-and-blocking",
    "title": "Allocation",
    "section": "Randomization Ratio and Blocking",
    "text": "Randomization Ratio and Blocking\nIn the Randomization Ratio and Blocking section of the Allocation tab the user inputs the components of randomization blocks that enroll from the onset of the study and until any arm is dropped. These blocks work like the Fixed Allocation tab blocks.\nOnce an arm is dropped the “Upon arm drop:” option in the Setup section of the allocation tab will determine how randomization proceeds.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#arm-dropping-criteria",
    "href": "documentation/v71/userguides/core/design/allocation.html#arm-dropping-criteria",
    "title": "Allocation",
    "section": "Arm Dropping Criteria",
    "text": "Arm Dropping Criteria\nThe user may select any combination of Bayesian “Per Dose” QOIs: Posterior Probabilities or Predictive Probabilities, or a Target Dose QOI. For each criteria selected (at least one must be selected), the user supplies a direction of comparison and probability threshold. If at any interim, any arm meets the specified arm dropping criteria, then that arm becomes a candidate for dropping. After all candidates are identified for dropping, the Setup rules determine which, if any, of the candidates will be dropped.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#setup",
    "href": "documentation/v71/userguides/core/design/allocation.html#setup",
    "title": "Allocation",
    "section": "Setup",
    "text": "Setup\nA variety of rules are specified in the Setup section of the Allocation tab.\n\nMax number of arms that can be dropped during the study\nThe maximum number of arms that can be dropped – this can be set to from 0 to the number of active arms, the Control and Active Comparator are never dropped. This figure is the ‘maximum number of arms that can be dropped over the duration of the trial’. At any interim any number between 0 and the ’maximum number of arms can be dropped as long as this limit is not exceeded on the overall number of arms that would have been dropped.\nIf the maximum number of arms that can be dropped is set to the number of study drug arms, then if all arms are dropped this is recorded as an “early termination for futility”.\n\n\nPrune from lowest/highest dose\nArm dropping can be restricted to ‘pruning from the lowest dose’ and/or ‘pruning from the highest dose’. If ‘prune from the lowest dose’ is selected, then until the lowest dose meets the arm dropping criteria the second lowest dose cannot be dropped and so on. At an interim when the lowest dose does meet the arm dropping criteria, and the second dose does too, then the second dose can be dropped as well (and third and fourth …), it does not have to wait until the next interim. Pruning from the highest dose does the same thing, except that no dose can be dropped unless every larger dose will also be dropped.\nIf no pruning is selected or pruning from both ends is selected, then the user must select to prioritize the higher or lower dose to cover the instance when more doses are candidates for dropping than are allowed to drop by the maximum threshold. If “Prioritize dropping the lowest dose” is selected then when forced to choose between two or more doses to drop, the one with the lowest effective dose strength will be dropped.\n\n\nUpon arm drop\nFinally, specify what is to be done with the unused subjects that would have been allocated to an arm that has now been dropped. There are three options:\n\nMaintain study size, maintain combined block size of treatments\n\nSubjects that would have been allocated to arms that have been dropped are now randomized between all the remaining active treatment arms (excluding control and active comparator) using their original allocation ratios. For example, if the allocation ratio was 2:1:1:1 between Control and D1, D2 and D3, then after dropping D1 the block size remains size 5 (2 to Control and 1 to each of D2 and D3) with the \\(5^{th}\\) slot being allocated 1:1 between the remaining two study arms D2 & D3. Thus, the overall proportion of subjects on Control (and Active Comparator) is maintained.\n\nMaintain study size, reduce combined block size of treatments\n\nSubjects that would have been allocated to any arms that have been dropped are randomized between all remaining arms (including control) using a reduced total block size. Thus, if the allocation ratio was 2:1:1 between Control and D1 and D2, then after dropping D1 the block size drops to total size three: 2 to Control and 1 to D2.\n\nDecrease the study size, reduce combined block size of treatments\n\nSubjects that would have been allocated to any arms that have been dropped are no longer recruited, and the total sample size of the study is reduced; the remaining subjects are recruited using a reduced block. If interims are specified by number of subjects recruited, these interim sizes are reduced proportionally. As an example of this, if an arm that gets 20% of the randomization is dropped when there are 100 subjects enrolled, then if the next interim is scheduled at 200 subjects enrolled, it will actually be performed at 180 subjects enrolled.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#early-stopping-only",
    "href": "documentation/v71/userguides/core/design/allocation.html#early-stopping-only",
    "title": "Allocation",
    "section": "Early Stopping Only",
    "text": "Early Stopping Only\nIf allocation is to be fixed the “Early Stopping Only” option should be selected. Then on this tab the fixed allocation ratios and block size are specified – for the first cohort and then for all subsequent cohorts. In each case the user specifies the number of each arm to allocate in each cohort and the sum must equal the size of the cohort.\n\n\n\n\n\n\nFigure 7: Cohort allocation tab with early stopping, but no adaptive allocation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#adaptive-allocation-1",
    "href": "documentation/v71/userguides/core/design/allocation.html#adaptive-allocation-1",
    "title": "Allocation",
    "section": "Adaptive Allocation",
    "text": "Adaptive Allocation\nIf allocation is to be adaptive the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort by sampling from allocation vector” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation ratios for the remaining subjects in the cohort as for adaptive allocation (see Adaptive Allocation above).\n\n\n\n\n\n\n\n\nFigure 8: Cohort allocation tab adaptive allocation.\n\n\n\n\nAdaptively allocate to best dose\nIf allocation is to be adaptive, but at each cohort just Control and the “best dose” are to be allocated to, the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort to single treatment (plus control)” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation weighting to calculate for the non-Control arms, (see Adaptive Allocation above) and all the subjects not allocated to control will be allocated to this arm.\n\n\n\n\n\n\n\n\nFigure 9: Cohort allocation tab with adaptive allocation allocating to only the best dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/tte.html",
    "href": "documentation/v71/userguides/core/study/tte.html",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include whether interim analyses will be simulated, if a predictor is used to impute subject event times, sample size, maximum number of events, follow-up times, and more.\n\n\n\n\n\n\nFigure 1: The study tab for a time-to-event trial.\n\n\n\n\n\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether an early predictor is to be included in the design. An early predictor can be a continuous or dichotomous outcome, or a precursor event. The user specifies whether higher score, a response, or precursor event is a positive or negative outcome for the subject (this is used in the comparison with the CSD – not in the relation between the predictor and the final event, that is solely down to the parameters entered on the virtual subject response tabs). For continuous or dichotomous predictors, the user specifies after what time (weeks after randomization) the predictor is observed. For all predictors a clinically significant difference can be specified which can be used to specify early stopping or final evaluation conditions on the predictor. If the predictor is continuous, the value is delta on the mean difference between a study arm and control, if dichotomous, the value is the difference in rate and if the predictor is a precursor event then the value is a clinically significant hazard ratio.\nIf an early predictor is included, then a ‘Predictor’ tab is added to the Virtual Subject Response tabs, and a Predictor tab is added to the Design tabs, with a predictor Dose Response sub-tab and Relationship to Endpoint sub-tab.\n\n\n\n\n\n\nThe maximum number of subjects that can be recruited into the study and the maximum number of events to be observed (if there is a time-to-event predictor, the maximum events to be observed can be specified to be the predictor events).\n\n\n\nDetermine what counts as an event when determining when the trial should be stopped due to reaching the number of events specified in the “Maximum number of events” box. The options are “Final events” and “Predictor events.”\n\n\n\nThe maximum number of events (type of event specified above) allowed in the trial. When this value is reached the trial moves on to the final analysis stage.\n\n\n\nThe maximum follow-up time per subject. No subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event. With this option all subjects have the same maximum follow-up.\n\n\n\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event. With this option subjects recruited earlier may be follow-up for longer than subjects recruited later in the trial and the overall amount of exposure is maximized.\n\n\n\nWhether an event indicates a success (time to recovery) or a failure (death or progression) for a subject. The value specified here also dictates the direction of frequentist tests and predictive probabilities.\n\n\n\nDictate whether subjects are recruited sequentially or deterministically.\nIf recruited sequentially, the user recruitment will be simulated stochastically using a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/tte.html#design-options",
    "href": "documentation/v71/userguides/core/study/tte.html#design-options",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Whether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether an early predictor is to be included in the design. An early predictor can be a continuous or dichotomous outcome, or a precursor event. The user specifies whether higher score, a response, or precursor event is a positive or negative outcome for the subject (this is used in the comparison with the CSD – not in the relation between the predictor and the final event, that is solely down to the parameters entered on the virtual subject response tabs). For continuous or dichotomous predictors, the user specifies after what time (weeks after randomization) the predictor is observed. For all predictors a clinically significant difference can be specified which can be used to specify early stopping or final evaluation conditions on the predictor. If the predictor is continuous, the value is delta on the mean difference between a study arm and control, if dichotomous, the value is the difference in rate and if the predictor is a precursor event then the value is a clinically significant hazard ratio.\nIf an early predictor is included, then a ‘Predictor’ tab is added to the Virtual Subject Response tabs, and a Predictor tab is added to the Design tabs, with a predictor Dose Response sub-tab and Relationship to Endpoint sub-tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/tte.html#study-information",
    "href": "documentation/v71/userguides/core/study/tte.html#study-information",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The maximum number of subjects that can be recruited into the study and the maximum number of events to be observed (if there is a time-to-event predictor, the maximum events to be observed can be specified to be the predictor events).\n\n\n\nDetermine what counts as an event when determining when the trial should be stopped due to reaching the number of events specified in the “Maximum number of events” box. The options are “Final events” and “Predictor events.”\n\n\n\nThe maximum number of events (type of event specified above) allowed in the trial. When this value is reached the trial moves on to the final analysis stage.\n\n\n\nThe maximum follow-up time per subject. No subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event. With this option all subjects have the same maximum follow-up.\n\n\n\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event. With this option subjects recruited earlier may be follow-up for longer than subjects recruited later in the trial and the overall amount of exposure is maximized.\n\n\n\nWhether an event indicates a success (time to recovery) or a failure (death or progression) for a subject. The value specified here also dictates the direction of frequentist tests and predictive probabilities.\n\n\n\nDictate whether subjects are recruited sequentially or deterministically.\nIf recruited sequentially, the user recruitment will be simulated stochastically using a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html",
    "href": "documentation/v71/userguides/core/study/dichotomous.html",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include accrual style, visit schedule, whether interim analyses will be simulated, and more.\n\n\n\n\n\n\nFigure 1: The study tab for a dichotomous trial.\n\n\n\n\n\nIn the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nIf “Use Longitudinal Modeling” is selected, then special longitudinal options can be turned on.\n\n\nIf this option is selected, then the endpoint is actually simulated as a dichotomized continuous response – e.g. a “responder analysis” where a subject with a change from baseline above (or below) a certain threshold is a ‘responder’ and otherwise is not.\nWhen using a dichotomized response, the longitudinal analysis is on the underlying continuous response rather than the dichotomous responder endpoint. After imputing subject values based on intermediate subject data they are dichotomized.\n\n\n\nWhen using the restricted Markov model a subject can be in one of 3 states “Success”, “Failure” or “Stable.” Once the subject has become a “Success” or “Failure” they are  fixed in that state Success and Failure are called “absorbing states. , and it will be their final outcome.\nAll subjects are initially in the “Stable” state and may or may not transition to another state during follow-up. If a subject finishes follow-up in the “Stable” state, they must be classified as failure or success.\n\n\n\n\n\nThe study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a response indicates the subject improving or worsening (and thus whether a higher or lower response rate is a good thing). The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html#design-options",
    "href": "documentation/v71/userguides/core/study/dichotomous.html#design-options",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "In the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nIf “Use Longitudinal Modeling” is selected, then special longitudinal options can be turned on.\n\n\nIf this option is selected, then the endpoint is actually simulated as a dichotomized continuous response – e.g. a “responder analysis” where a subject with a change from baseline above (or below) a certain threshold is a ‘responder’ and otherwise is not.\nWhen using a dichotomized response, the longitudinal analysis is on the underlying continuous response rather than the dichotomous responder endpoint. After imputing subject values based on intermediate subject data they are dichotomized.\n\n\n\nWhen using the restricted Markov model a subject can be in one of 3 states “Success”, “Failure” or “Stable.” Once the subject has become a “Success” or “Failure” they are  fixed in that state Success and Failure are called “absorbing states. , and it will be their final outcome.\nAll subjects are initially in the “Stable” state and may or may not transition to another state during follow-up. If a subject finishes follow-up in the “Stable” state, they must be classified as failure or success.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html#study-information",
    "href": "documentation/v71/userguides/core/study/dichotomous.html#study-information",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a response indicates the subject improving or worsening (and thus whether a higher or lower response rate is a good thing). The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html#d-treatment-arm-model",
    "href": "documentation/v71/userguides/core/study/dichotomous.html#d-treatment-arm-model",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens.\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/tte.html",
    "href": "documentation/v71/userguides/core/longitudinalmodels/tte.html",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "",
    "text": "In FACTS Core TTE, ‘predictor endpoints’ can be used in a similar way to early observations of the clinical endpoint measure in FACTS Core Continuous and Dichotomous. That is, they are used to inform the analysis about what the final outcome is likely to be for subjects for whom the final outcome has not  yet been observed If a subject drop’s out, their final endpoint will never be observed, but their predictors are still used to impute the subject’s possibly final time to event if the predictor is observed before the dropout. . The estimates of the predictor response can also be used in QOIs, and thus somewhat like an additional endpoint and used for decision making.\nThe dose-predictor model is used to impute subjects’ predictor responses based on their treatment allocation when their predictor response has not yet been observed. The predictor-endpoint model is used to impute their time to final endpoint when that has not yet been observed.\nThe differences between predictor endpoints in FACTS Core TTE and early endpoints with longitudinal modeling in FACTS Core Continuous and Dichotomous are that:\nFor all predictor responses (\\(Z\\)) for time-to-event endpoints, the engine estimates both a marginal predictor distribution (normal mean and variance for continuous, probability of response for dichotomous, or a piecewise exponential hazard model for time to event predictors), and a working model relating the predictor to the final endpoint. The marginal predictor distribution is used to impute predictors for subjects lacking an observed predictor value, and may also be used for stopping (see section on stopping). The working model is used to impute final event times for subjects lacking a final endpoint. For subject missing both a predictor and final endpoint, the predictor is imputed first from its marginal distribution, and then the final endpoint is imputed conditionally on the predictor \\(Z\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Predictor Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/tte.html#continuous-dichotomous-predictors",
    "href": "documentation/v71/userguides/core/longitudinalmodels/tte.html#continuous-dichotomous-predictors",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Continuous & Dichotomous Predictors",
    "text": "Continuous & Dichotomous Predictors\nFor the predictor model with a continuous or dichotomous predictor, there are two tabs within the Design &gt; Predictor Model tab – the Dose Response and the Relationship to Endpoint tabs.\nThe dose response model for the predictor is the model used to estimate the marginal distribution of the predictor response at each dose as a normal mean and variance for a continuous endpoint or mean and variance of the log-odds for a dichotomous endpoint. The dose response options are as per the continuous and dichotomous endpoint standard dose response options (except that the use of BAC for the predictor is not supported). The predictor dose-response model can be selected and specified completely independently of the dose response model for the primary time-to-event endpoint (the model of the log hazard ratio).\n\n\n\n\n\n\nFigure 1: The dichotomous predictor model showing a Simple NDLM dose response. The NDLM model for the predictor response works exactly like the NDLM dose response model.\n\n\n\n\nContinuous Predictor\nWithin each dose (including control and active comparator), the marginal distribution of the predictor \\(Z\\) is a normal distribution with mean \\(\\theta_{Zd}\\) and standard deviation \\(\\sigma_Z\\). The standard deviation is common across the doses, but the means \\(\\theta_{Zd}\\) are allowed to vary across the same range of doses based on the predictor’s dose response model. The dose response for the predictor does not need to match the dose response for the final endpoint, and the two models are estimated independently of eachother.\n\n\nDichotomous Predictor\nA dichotomous predictor is handled similarly to a continuous predictor, with the predictor’s marginal distribution having its own dose response model. Like in the typical dose response model on a dichotomous endpoint when it is the primary endpoint, the dose response model estimates the log-odds of the response rate for each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Predictor Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor",
    "href": "documentation/v71/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Time-to-Event Predictor",
    "text": "Time-to-Event Predictor\nFor the predictor model with a time-to-event predictor, there are three tabs within the Design &gt; Predictor Model tab – the Hazard Model, Dose Response, and the Relationship to Endpoint tabs.\nAs in the response model for the primary event, the response model for a time to event predictor comprises a hazard model of the predictor for the event rate on the control arm, and a dose response model used to estimate the marginal distribution of the non-control dose predictor response for each dose as a normal log-hazard ratio.\nThe hazard model and dose response options for the predictor endpoint are similar to the hazard model and dose response options for the primary time-to-event endpoint, except that the use of the cox model or BAC for the predictor hazard model is not supported. The predictor dose-response model is selected, specified, and estimated completely independently of the dose response model for the time-to-event endpoint.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Predictor Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/tte.html#continuous-and-dichotomous-predictors",
    "href": "documentation/v71/userguides/core/longitudinalmodels/tte.html#continuous-and-dichotomous-predictors",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Continuous and Dichotomous Predictors",
    "text": "Continuous and Dichotomous Predictors\nThe relationship to endpoint model is:\n\n\\(T_{i} \\sim \\text{Exp}(\\lambda_{d}\\ e^{\\beta Z_{i}})\\) for a continuous predictor where \\(Z_i\\), is the observed value of the predictor for the \\(i\\)th subject\n\\(T_{i} \\sim \\text{Exp}(\\lambda_{d}\\ e^{\\beta Z_{i}})\\) for a dichotomous predictor where \\(Z_i=0\\) or \\(1\\) is the observed dichotomous predictor for the ith subject\n\nThat is, for the ith subject, the time to their event is taken to follow an exponential distribution with mean \\(\\lambda_d\\ e^{\\beta Z_i}\\).\n\n\n\n\n\n\nFigure 2: The Relationship to Endpoint tab for a continuous or dichotomous predictor.\n\n\n\nFor both continuous and dichotomous predictors, the priors on the \\(\\beta\\) and \\(\\lambda_d\\) parameters are:\n\\[ \\lambda_d \\sim \\text{Gamma}(\\alpha_d, \\beta_d) \\]\n\\[  $\\beta \\sim \\text{N}(m, s)\\]\n\\(\\lambda_d\\) is esimated independently for all doses. The coefficient \\(\\beta\\) does not have a subscript, because it is constant across all doses.\nFor the dichotomous predictor, we can simplify the notation of the relationship to endpoint model since \\(Z\\) can only be two different values (0 or 1).\n\\[T\\mid (Z=0) \\sim \\text{Exp}(\\lambda_d)\\] and \\[T \\mid (Z=1) \\sim Exp(\\lambda_d e^{\\beta})\\].\n\n\n\n\n\n\nNotes on priors for continuous/dichotomous relationshipt to endpoint models\n\n\n\nFor a continuous predictor, \\(\\lambda_d\\) is the dose dependent hazard rate when the predictor is 0. For a dichotomous predictor \\(\\lambda_d\\) is the dose dependent hazard rate when the predictor is 0. The \\(\\lambda\\)s have independent gamma priors for each dose, specified by their expected mean and a weight in equivalent number of events seen. Thus, a weight of 1 would be very weakly informative with any normal number of events. A weight of 0.1 would be uninformative with even a small number of events. A way to think about the use of a weight of \\(&gt;1\\) is, if weight of \\(n&gt;1\\) is used, then after \\(n\\) actual events are observed the posterior mean estimate of \\(\\lambda\\) will be the average of the observed mean and the prior mean.\nThe \\(\\beta\\) parameter has a normal prior, specified by its prior mean and standard deviation. For a continuous predictor, this is the log scaling factor of the hazard rate for the correlation of the event rate to the predictor. For a dichotomous predictor, it is the log scaling factor of the hazard rate for subjects who have had response on the predictor. For a continuous predictor with the center and scale values set so that the value of \\(Z\\) will vary approximately between \\(-1\\) and \\(1\\), and a dichotomous predictor where the predictor value is \\(0\\) or \\(1\\), a prior distribution of \\(N(0,5)\\) for \\(\\beta\\) would mean that \\(e^{\\beta Z}\\) will take values between \\(22,000^{-1}\\) to 22,000 and the prior could be deemed to be essentially uninformative. Large values (&gt;&gt;5) for the SD of the prior for \\(\\beta\\) can lead to numeric overflow in the simulator. If the values of Z do not largely fall in the range \\([-1, 1]\\), it is suggested that the prior for \\(\\beta\\) be constructed to limit the coefficient \\(\\exp{(Z\\beta)}\\) to lie within its plausible range. If the prior for \\(\\beta\\) is left uninformative and \\(Z\\) can take values \\(&gt;&gt; 1\\) it can result in inflation in the uncertainty in the estimates of the hazard ratios of subject’s time to final event from a few extreme imputed time-to-event values for subjects whose times-to-event are imputed, particularly if their predictor value is imputed too.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Predictor Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor-1",
    "href": "documentation/v71/userguides/core/longitudinalmodels/tte.html#time-to-event-predictor-1",
    "title": "Predictor Models for Time-to-event Endpoints",
    "section": "Time-to-Event Predictor",
    "text": "Time-to-Event Predictor\nFor the time-to-event predictor, the relationship to endpoint model is simply a per-dose hazard rate for the time from the predictor event to the final event. For each dose, the user specifies the parameters of a gamma prior distribution for this ‘post-predictor event’ hazard rate. This model is a simple exponential, not piecewise.\n\n\n\n\n\n\nFigure 3: The Relationship to Endpoint tab for the time-to-event engine with a time-to-event predictor.\n\n\n\nThe time-to-event predictor is qualitatively different than the continuous or dichotomous predictors. Instead of the predictor adjusting the hazard, the time-to-event predictor is viewed as an offset. The final endpoint is viewed as a sum of a predictor time \\(Z_1\\) and a post-predictor time \\(Z_2\\), where \\(Z_1\\) and \\(Z_2\\) are independent random variables and the final endpoint is thus \\(Z_1 + Z_2\\).\nFor the relationship to endpoint model, \\(Z_1 \\sim PWExp(\\lambda_{1s}*\\theta_{1d})\\) and \\(Z_2 \\sim Exp(\\lambda_{2d})\\), with priors \\(\\theta_{1s} \\sim Gamma(\\alpha_{1s}, \\beta_{1s})\\), \\(\\theta_{2d} \\sim Gamma(\\alpha_{2d}, \\beta_{2d})\\) (with \\(Z_1\\)’s control hazard model potentially being piecewise exponential).\nFor imputation of the primary endpoint event time, a subject missing both the biomarker and final endpoint times has both \\(Z_1\\) and \\(Z_2\\) imputed, with the final endpoint imputed as the sum of the two. For a subject with a predictor time but no final endpoint, \\(Z_2\\) is imputed and added to the observed predictor time to impute the final endpoint.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Modelling for Time-to-Event Endpoints",
      "Predictor Model"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/dichotomous.html",
    "href": "documentation/v71/userguides/core/longitudinalmodels/dichotomous.html",
    "title": "Longitudinal Models for Dichotomous Endpoints",
    "section": "",
    "text": "LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {\\(y_{it}\\)} is the set of observed responses from early visits, and \\(y_{i t_m}\\) is the last observed value of \\(y_{it}\\), then the LOCF model for the final endpoint \\(Y_i\\) is\n\\[Y_i \\mid \\{y_{it}\\} = y_{i t_m}\\]\n\n\nBeta Binomial\nThe Beta Binomial longitudinal model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(\\pi_{t y_{it}})\\]\nwhere \\(\\pi_{t y_{it}}\\) is the probability that a patient is a response at the final endpoint given its early observed endpoint at time \\(t\\) is \\(y_{it}\\),\n\\[\\pi_{t y_{it}} = \\Pr(Y_i = 1 \\mid y_{it}) \\sim \\text{Beta}(\\alpha_{t {y_it}}, \\beta_{t y_{it}})\\]\nWe use the set cardinality operator \\(\\mid \\ldots \\mid\\) to obtain the posterior distributions of \\(\\alpha_t\\) and \\(\\beta_t\\) as:\n\\[\\alpha_{t0} = \\alpha_{\\mu 0} + \\left| Y_i = 1, y_{it} = 0 \\right| \\] \\[\\alpha_{t1} = \\alpha_{\\mu 1} + \\left| Y_i = 0, y_{it} = 0 \\right| \\] \\[\\beta_{t0} = \\beta_{\\mu 0} + \\left| Y_i = 1, y_{it} = 1 \\right| \\] \\[\\beta_{t1} = \\beta_{\\mu 1} + \\left| Y_i = 0, y_{it} = 1 \\right| \\]\ni.e. a prior value \\((\\alpha_{\\mu 0}, \\alpha_{\\mu 1}, \\beta_{\\mu 0}, \\beta_{\\mu 1})\\) plus the number of subjects for which the final response is known to be 1 for \\(\\alpha_{tx}\\) (or 0 for \\(\\beta_{tx}\\)) and the response at time \\(t\\) is \\(x\\).\nThe \\(\\alpha_{tx}\\) and \\(\\beta_{tx}\\) parameters are independently estimated using only patients in their model instance, and may or not have identical priors \\(\\alpha_{\\mu *}\\) and \\(\\beta_{\\mu *}\\) depending on the Model Priors selection in FACTS. A common non-informative prior for the \\(\\pi_{t0}\\) and \\(\\pi_{t1}\\) parameters is \\(\\text{Beta}(1,1)\\).\n\n\nLogistic regression\nThe Logistic regression longitudinal model works similarly to the Beta Binomial imputation model. The difference is in the method of modeling the probability of a transition from an interim visit to the final visit \\(\\Pr(Y_i = 1 \\mid y_{it})\\). Like the Beta Binomial model, the logistic regression model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(\\pi_{t y_{it}})\\]\nwhere \\(\\pi_{t y_{it}}\\) is the probability of a response at the final endpoint time given that its early observed endpoint at time \\(t\\)$ is \\(y_{it}\\). Then, we define the parameter\n\\[\\theta_{ty_{it}} = \\text{logit}\\left( \\pi_{ty_{it}} \\right) = \\log\\left( \\frac{\\pi_{ty_{it}}}{1 - \\pi_{ty_{it}}} \\right)\\].\nThe priors on \\(\\theta_{t0}\\) and \\(\\theta{t1}\\) are:\n\\[\\theta_{t0} \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\] \\[\\theta_{t1} \\sim \\text{N}(\\mu_1, \\sigma_1^2)\\]\nThe model computes the posterior distribution of \\(\\theta_{t0}\\) and \\(\\theta_{t1}\\) using all patients on arms belonging to the model instance that have observed endpoint values at time \\(t\\) and the final endpoint time \\(T\\).\nThe priors on \\(\\theta_{t0}\\) and \\(\\theta_{t1}\\) may be shared across model instances and/or visits depending on the selection made in the Model Priors section of the FACTS Logistic regression Longitudinal model page.\nA possible starting place for non-informative priors in this model would be: \\(\\mu=0\\), \\(\\sigma=2\\). A weakly informative set of priors that an early response makes a final response more likely could be \\[\\theta_{t0} \\sim \\text{N}(-.75, 1.25^2)\\] and \\[\\theta_{t1} \\sim \\text{N}(0.75, 1.25^2)\\]\n\n\nRestricted Markov Model (Absorbing Markov Chain)\n\n\n\n\n\n\nUsing the Restricted Markov Model\n\n\n\nThe restricted markov model is special in the sense that it can be used if and only if the “Use longitudinal modeling” check box is checked, the “Enable Special Longitudinal Options” check box is checked, and “Use restriced Markov model” is selected. When these conditions are met the Virtual Subject Response tab changes and the Design &gt; Longitudinal tab only has the Restricted Markov option.\n\n\nThe Absorbing Markov Chain model assumes that at each visit a subject is in one of three states – responder (1), stable (S), or failure (0). The responder and failure states are absorbing, meaning that once a patient has entered one of those states they must remain in that state for the remainder of the visits. Patients in the stable state may move to a responder or a failure in subsequent visits.\nUnlike most other longitudinal models in FACTS, the Restricted Markov Model estimates the probability of a result at a visit based on the visit right before it, rather than predicting directly to the final endpoint from the early visit.\n\\[\\Pr(y_{it} = n \\mid y_{i, t-1} = S) \\sim \\text{Dirichlet}(\\{\\alpha_{0,t}, \\alpha_{1,t}\\, \\alpha_{S,t}\\}) \\text{ for } t\\ge 2\\]\nWhere n can be 0, 1, or S, denoting the probability of going to the Fail state, the Responder state, or the Stable state at visit \\(t\\) from the Stable state at visit \\(t-1\\). \\(t\\) must be greater than or equal to \\(2\\), because we do not impute the first visit – a subject missing visit 1 and all visits after does not contribute to the longitudinal model or dose response model.\nThe priors for the \\(\\alpha\\) parameters are specified in terms of the prior number of transitions from Stable at \\(t-1\\) to each different state at time \\(t\\). For example, if the prior value for the parameter \\(\\alpha_{1,3}\\) is \\(2\\), we are putting apriori information into the Dirichlet distribution suggesting that \\(2\\) patients transitioned from Stable at visit 2 to Responders at visit 3. Specified priors can either be common or different across model instances based on user specification.\nThe parameters defining the posterior distribution of the state probabilities are available in closed form as:\n\\[\\alpha_{0,t} = \\gamma_{0,t} + \\left|y_{it}=0, y_{i, t-1} = S\\right|\\] \\[\\alpha_{S,t} = \\gamma_{S,t} + \\left|y_{it}=S, y_{i, t-1} = S\\right|\\] \\[\\alpha_{1,t} = \\gamma_{1,t} + \\left|y_{it}=1, y_{i, t-1} = S\\right|\\]\nTo create a dichotomous endpoint, the user specifies in the Study &gt; Study Info &gt; Design Options section whether patients remaining in a stable state at the final visit should be dichotomized as responders or failures.\n\n\nDichotomous Endpoint: Dichotomized Continuous Longitudinal Model\nThe user may select (on the Study tab) to assume that the dichotomous final endpoint is generated by observing continuous longitudinal data and then dichotomizing the final endpoint based on whether it is greater than or less than a provided threshold. If the user selects this option, then they may select any of the continuous longitudinal models specified in the Continuous Longitudinal Models section. The engine will impute incomplete subjects according to the continuous model, resulting in a continuous imputed final endpoint. The imputed dichotomous final endpoint is then simply the dichotomized version of the continuous imputation.\nAll priors and methods are identical to the continuous longitudinal models mentioned above.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Design",
      "Longitudinal Models",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html",
    "href": "documentation/v71/userguides/core/vsr/tte.html",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual externally simulated patient responses. When simulations are executed, they will be executed for a specific scenario – where a scenario is a combination of one each of predictor VSR, control hazard rate, dose response VSR, accrual rate, and dropout rate. If an external file is used to specify the subject responses to be simulated, this effectively replaces the predictor, control hazard and dose response profiles in a scenario.\nUnlike other endpoints where just a response (mean change from base line or rate) is specified, specification of subject responses for a time-to-event endpoint is done by first specifying a piecewise exponential event rate for the control population and then hazard ratios for the treatment arms. For simplicity, this means of specifying the simulated event rates is also used when no control arm is present and the comparison is with historic control rates.\nIn FACTS Core TTE, there is also the ability to include the simulation of a ‘predictor’ endpoint. Predictor endpoints can be a continuous measure, dichotomous outcome, or a precursor event. The interface for specifying how the predictor endpoint data is to be simulated is different in each case.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined",
    "href": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nWith no predictor, there are two screens for defining virtual subject responses; the first is used to define the control hazard rate and the second to define the hazard ratio of the events on each arm to control.\n\nControl Hazard Rates\nThe user may create a number of different control hazard rate profiles to simulate from. The different profiles will be used to define different simulation scenarios in combination with other profiles defining the other properties that have to be simulated.\nThe hazard rate to simulate in a profile is specified as a piecewise exponential. The follow-up time can be divided into different time segments and a different event rate simulated in each segment.\nSegment boundaries and hazard rates are always entered using “weeks” as the time unit. While this might not always be the most convenient, it allows FACTS to use the same time unit everywhere.\nDifferent segments in the follow-up period are specified by adding segment ends. Adding a ‘segment end’ adds a segment interval to the list to allow the event rate for that interval to be specified. To delete an interval, select the interval starting with the breakpoint to be deleted and click the “Delete” button.\nThe graph can show the hazard rate, the cumulative probability of not having an event or the probability of not having seen an event over time, using the event rates specified. This is useful for checking that the segments and event rates have been entered correctly.\n\n\n\n\n\n\nFigure 1: The tab for specifying the control arm hazard rate for an explicitly defined VSR.\n\n\n\n\n\nDose Response\nThe user may create a number of different dose response profiles to simulate from. The different profiles will be used to define different simulation scenarios in combination with other profiles defining the other properties that have to be simulated.\nWithin each profile the user specifies:\n\nThe hazard ratio compared to control for each treatment arm (except control itself, where of course the ratio is 1).\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\n\nOn the graph the different hazard ratios for the profile are plotted, along with the ‘target’ – this is the default CSHRD or NIHRD offset from the QOI tab, the direction of the offset is dependent on whether events are ‘good’ or ‘bad’ and whether the aim of the trial is to show superiority on non-inferiority.\n\n\n\n\n\n\nFigure 2: Specify the hazard ratio per dose\n\n\n\n\n\nLoading Scenario Control Hazard Rates and Scenario Hazard Ratios from file\nIf the “Load scenario hazard rates from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses across the simulations.\nEach individual simulation uses one set of responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\nFor a TTE endpoint the user must supply 2 files – one for the Control Hazard Rates and one for the Hazard Ratios in each group. MVSR hazard rates are only combined with MVSR control hazard rates. The lines from each file are paired up for each simulation, so the first control hazard rate is used with the first dose response hazard ratio, the second control hazard rate is used with the second dose response hazard ratio and so on. There must be the same number of lines in each file.\n\n\n\n\n\n\nFigure 3: Load a scenario’s control hazard rates from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the different control hazard rates over time.\n\n\n\n\n\n\nFigure 4: Load a scenario’s treatment hazard ratios from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the different individual hazard ratios for each dose.\nThe VSR parameters are provided in two separate files, (the number of lines in the files should be the same for the two files). The formats are:\n\nControl Hazard Rate File: Each line should contain columns [L1, L2, … , LS] giving the true control hazard rates (\\(\\lambda\\)) for each of the S segments. (Note: FACTS will treat the hazard rate as per week).:\nHazard Ratio File: Each line should contain columns [HR1, HR2, … , HRD] giving the true Mean Hazard Ratios for each of the D dose arms. (Note: HR1 = 1 by definition, but the column of 1’s to be included here for completeness.)\n\nThe use of MVSR files has not been extended to the case where a predictor is being used.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#external-files",
    "href": "documentation/v71/userguides/core/vsr/tte.html#external-files",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External Files",
    "text": "External Files\nAs well as simulating subject responses within FACTS they can be simulated externally and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data can be done from the External Files sub-tab depicted in Figure 5 below.\nTo import an external file, the user must first add a profile to the table. After adding the profile, the user must click “Browse” to locate the file of externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 5: The external files sub-tab for uploading patient data to be sampled from.\n\n\n\n\nRequired Format of Externally Simulated Data\nThe supplied data should have the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nUncensored time to event in weeks\nThis column is a placeholder for predictor data. It may be filled out or empty when there is no predictor. It will be ignored.\n\nThe GUI requires that the file name has a “.dat” suffix. The file need not have column headers, but if it does the first column name must start with a pound sign (#) which tells FACTS to ignore that row.\nThe following shows values from an example file with a dichotomous predictor. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n\n\n\n\n\n\n\n\n\n#Patient ID\nDose Index\nUncensored Time to Event (weeks)\nOptional Predictor value\n\n\n\n\n1\n1\n8.87\n0\n\n\n2\n1\n9.34\n0\n\n\n3\n2\n6.78\n-9999\n\n\n4\n2\n10.23\n0\n\n\n5\n2\n9.96\n0\n\n\n6\n2\n5.6\n1\n\n\n7\n1\n37.01\n20.36\n\n\n8\n1\n28.67\n0\n\n\n9\n1\n39.70\n0\n\n\n\nIn the above, column headers have been included to make it clearer to read but they are not required.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-event-rate-predictor",
    "href": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-event-rate-predictor",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined (Event Rate | Predictor)",
    "text": "Explicitly Defined (Event Rate | Predictor)\nThe first method (described in this section) may in some circumstances be the more natural way to think of the data (we might want to simulate for instance that subjects with a reduction in tumor size have a longer survival time), however it gives rise to data which does not have an exponential distribution. To retain an exponential distribution in the simulated event rate it is necessary to use the second method where the control rate and treatment arm hazard ratios are specified first and then the probability of the observed predictor derived from that (described in the next section).\n\nContinuous Predictor\nIf a predictor with a continuous endpoint is being used then there is a new tab in the VSR section to allow the specification of profiles that define how the predictor endpoint is to be simulated.\n\nPredictor\nFirstly, the predictor endpoint to be simulated is specified in the same way as a FACTS Core continuous endpoint. A number of profiles can be specified, in each one the mean change from baseline of the predictor for each treatment arm is specified, along with the variability in the change. The variability is specified as the SD of a Normal error in the observed change, a single value can be specified for all arms, or separate values for each arm.\nThe mean hazard rate for the simulated subjects now depends on the value of the predictor \\(Z\\), the baseline hazard rate for the time segment \\(\\mu_{s}\\), the log hazard ratio for the dose \\(\\theta_{d}\\), and with parameters \\(b_{Zd}\\), center and scale for the predictor:\n\\[h_{sdZ} = \\mu_{s}exp\\left( \\theta_{d} \\right)\\exp\\left( b_{Zd}\\frac{Z - center}{scale} \\right)\\]\nNote that in this form of the predictor, the observed event rate is affected by the predictor, it will not be the same as specified in the dose-response profile, also the observed times to event will not be exponentially distributed. The impact of this is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a control hazard rate of 0.01, and dose response hazard ratio of 1, when a continuous predictor is added with SD 2, and mean response of 1 on control and 3 on the treatment arm, with center specified at 2, scale at 3 and \\(b_{z}\\) at 0.2, simulations of 1000 subjects’ times-to-events on each of control and treatment arm give (ignoring censoring) a HR of between 1.05 and 1.21.\nNote that if events are bad and higher predictor scores indicate subject improvement (lower hazard rate) then the \\(b_{z}\\) coefficients need to be negative. The same is true if events are good and lower predictor scores indicate subject improvement.\n\n\n\n\n\n\nFigure 6: Specifying the continuous predictor’s VSR and how it effects the hazard rate for an arm.\n\n\n\nOn the graph showing the predictor response to be simulated, the ‘target’ line shows the offset of the predictor CSD from the predictor response on the Control arm.\n\n\nControl Hazard Rates\nWith a continuous predictor, the control hazard rate tab does not change. It’s the same as the non-predictor tab.\n\n\nDose Response\nThe manner of specifying the dose response does not change – multiple profiles may be specified and in each profile the hazard ratio for each treatment arm is specified. However, the overall hazard ratio simulated will depend on the combination of the control hazard rate, the dose response hazard ratio and the effect of the predictor on the final event rate.\nThis leads to two different ways for the predictor to be brought into the simulation. First, the hazard ratios \\(\\theta_{d}\\) may all be 1, while the mean of the predictor may change across doses. This would indicate that dose makes no difference given a fixed value of the predictor, but that the different doses achieve their effect by changing the distribution of the predictor values themselves. If the \\(\\theta_{d}\\) values differ from 1, then this indicates that there is a dose effect even conditional on a fixed value of the predictor (thus, for example, a control subject with Z=1 has a different hazard than a treatment subject with Z=1).\nThe resulting hazard ratio is plotted in the graph at the bottom of the tab. The predictor profile and control hazard rate profile to use in generating the graph can be selected in the controls to the right of the graph.\n\n\n\n\n\n\nFigure 7: Specify the hazard ratio per arm. The true simulated hazard ratio will be a combination of this value and the values provided in the predictor tab.\n\n\n\n\n\n\nDichotomous Predictor\nWhen simulating the event rate conditional on the predictor, the predictor endpoint to be simulated is specified in the same way as a for a simple dichotomous endpoint, the control hazard rate is specified separately for each dichotomous predictor value, and the Dose Response for the time-to-event endpoint has a hazard ratio that depends on the dichotomous predictor’s value.\nSo, the effect of the predictor on the background event rate is seen on the control hazard rate tab, and on the treatment arm hazard ratios on the dose response tab.\nOn those tabs the user specifies\n\nseparate control hazard rates for subjects who have a predictor response and those who do not\nseparate hazard ratios for each dose for subjects who have a predictor response and those who do not.\n\nThe hazard rates and ratios apply from the moment a subject is recruited (they do not change after the dichotomous predictor is assessed) and do not depend on the predictor being observed (which could be prevented if the event happens first).\n\n\n\n\n\n\nFigure 8: Specifying the dichotomous predictor’s probability of response for each arm.\n\n\n\nThe graph shows the specified response rate for each treatment arm and the target rate on control plus CSD.\n\nControl Hazard Rate with Dichotomous Predictor\n\n\n\n\n\n\nFigure 9: Specify the control arm hazard rate for each potential value of the dichotomous predictor.\n\n\n\nAs usual, multiple control hazard rate profiles can be created and the hazard rate on the control arm specified over different time segments. What differs from the case where there is no predictor is that, if a dichotomous predictor (with event rate simulated dependent on the predictor) is being used, separate hazard rates are specified for subjects depending on whether or not they will have the dichotomous predictor response or not.\n\n\nDose Response with a Dichotomous Predictor\n\n\n\n\n\n\nFigure 10: Specify the hazard ratio for active arms given the value of the dichotomous predictor.\n\n\n\nAs usual, multiple dose response profiles can be created, and in each profile the hazard ratio to simulate for each treatment arm compared to the control arm is specified. What differs from the case where there is no predictor, is that, if a dichotomous predictor is being used, separate hazard ratios are specified for subjects depending on whether or not they will have the dichotomous predictor response or not.\nAs with the continuous predictor, the observed event rate is affected by the predictor (it will not be the same as specified in the dose-response profile). Similarly, the observed times to event will not be exponentially distributed. The graph shows the effective combined hazard ratio for a given combination of control hazard rate, predictor rates and dose response. The controls for selecting which control hazard rate and which predictor rates to use are to the right of the graph.\nThe impact is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a predictor response rate on control of 0.1 and control hazard rates of 0.01 (with a predictor response) and 0.02 (with no predictor response), and treatment arm with a predictor response rate of 0.2 and hazard ratios of 0.9 (with a predictor response) and 0.8 (with no predictor response), simulations of 1000 subjects times to events on each of control and treatment arm give (ignoring censoring) a HR of between 0.75 and 0.90.\n\n\n\nTime-to-Event Predictor\n\nPredictor\nThe predictor endpoint to be simulated is specified in a similar manner to specifying the simulation of a time to event endpoint. A number of profiles can be specified; in each one the hazard rate on the control arm is specified over one, or more, time segments, and the overall hazard ratio of the time to the predictor event of each treatment arm to the control arm is specified.\nThe simulation of the time to the final event is in terms of the event rate (over one, or more, time segments) on the control arm after the predictor event, and then the hazard ratio of the time to the final event after the predictor event of each treatment arm to the control arm.\nOn those tabs the user specifies\n\nseparate control hazard rates for subjects post predictor event\nhazard ratios for each treatment arm the time to final event, after the predictor event has occurred.\n\n\n\n\n\n\n\nFigure 11: Specifying the dichotomous predictor’s control hazard rate and active arm hazard ratios for each arm.\n\n\n\nMultiple predictor profiles can be created.\nFor each profile the control hazard rate can be specified over an arbitrary set of time segments (i.e the time segments can vary from profile to profile, can be different from the observation times (if any) and different from the time segments used in the analysis model.\nThe hazard ratio of the control arm to itself has to be 1 and cannot be modified. For the other treatment arms, the time to predictor event is specified by specifying the hazard ratio on that treatment arm, to the control arm.\nThe graph can be used to show the hazard rate, probability of event or probability of not having the event on each arm.\n\n\nControl Hazard Rates\nSpecifying the simulation of the control hazard rate with a TTE predictor is the same as specifying the control hazard rate without a predictor. The difference is that with a TTE predictor, the hazard rate specified here is only simulated after the predictor has been seen.\n\n\n\n\n\n\nFigure 12: Specify the control arm’s event rate after the predictor endpoint event has occured.\n\n\n\n\n\nDose Response\n\n\n\n\n\n\nFigure 13: Specify the hazard ratio for the active arms on the hazard rate after the predictor event is observed. The primary endpoint event time can also be set to equal the predictor event time with some probability.\n\n\n\nAs usual, multiple hazard ratio profiles can be created, and in each profile the hazard ratio to simulate each treatment arm compared to the control arm specified. What differs from the case where there is no predictor is that if a TTE predictor is being used,\n\nthe hazard ratios are specified for the occurrence of the final event having observed the predictor event\na probability can be specified that the post predictor event to endpoint event time is 0.\n\nAs with the continuous predictor, the observed event rate is affected by the predictor, it will not be the same as the hazard ratio for endpoint post event predictor. The observed times to event will not be exponentially distributed. The impact is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a predictor hazard rate on control of 0.1 and post predictor hazard rates of 0.01 and probability that the post predictor event time is 0 of 0.1, and treatment arm with a predictor hazard ratio of 0.9 and post predictor hazard ratio of 0.8 and probability that the post predictor event time is 0 of 0.1, simulations of 1000 subjects times to events on each of control and treatment arm give (ignoring censoring) a HR of between 0.74 and 0.88.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-predictor-event-rate",
    "href": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-predictor-event-rate",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined (Predictor | Event Rate)",
    "text": "Explicitly Defined (Predictor | Event Rate)\nTo simulate event times and an associated predictor with a possible correlation between the two in a way that preserves the exponential distribution of the event times, use this tab to specify the simulation of subjects’ time to event first and then the probability of the observed predictor derived from that. This is only supported for Dichotomous and TTE predictors.\nThe “Explicitly Defined (Predictor | Event Rate)” tab allows the specification of profiles that define how the predictor endpoint is to be simulated in relation to the control hazard rate and hazard ratios for the final events. This allows the simulated final observed final events to still have an exponential distribution.\nOnce the time-to-event for a subjects has been simulated, a simple user specified transformation of the time-to-final-event provides the expected value of the predictor’s distributions.\n\nDichotomous\nWhen using a dichotomous predictor, control hazard rates and dose response hazard ratios are specified as when there is no predictor.\n\nPredictor\n\n\n\n\n\n\nFigure 14: Specify the distribution of the predictor per arm given the uncensored endpoint value for a patient.\n\n\n\nAs usual, multiple profiles can be defined. To simulate the dichotomous endpoint given the event rate, the predictor values are simulated by drawing from the Bernoulli distribution with probability given by the inverse logit(\\(\\alpha + \\beta Y\\)), where \\(\\alpha\\) and \\(\\beta\\) are specified here and \\(Y\\) is the subject’s final time to event (in weeks). A single set of values for \\(\\alpha\\) and \\(\\beta\\) can be specified, or separate values per treatment arm can be specified. The expected response rate is shown in the plot at the bottom of the predictor tab.\n\n\n\nTime-to-Event\nWhen simulating a time-to-event predictor, control hazard rates and dose response hazard ratios are specified as when there is no predictor.\n\nPredictor\n\n\n\n\n\n\nFigure 15: Specify the distribution of the predictor event per arm given the uncensored endpoint value for a patient.\n\n\n\nMultiple profiles can be defined. To simulate predictor event endpoint given the event rate of the primary event, the predictor values are simulated by drawing from the Exponential distribution with rate given by (\\(\\lambda_{z}\\exp(\\ \\beta Y)\\)), where \\(\\lambda_z\\) and \\(\\beta\\) are specified here and \\(Y\\) is the subject’s time to event (in weeks). A single set of values for \\(\\lambda_z\\) and \\(\\beta\\) can be specified, or separate values per treatment arm can be specified.\nThe arm specific hazard ratios of the predictor given the final endpoint event rate of a specified scenario is shown in the plot at the bottom of the predictor tab. The final endpoint scenario can be changed using the dropdown boxes the right of the figure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#external",
    "href": "documentation/v71/userguides/core/vsr/tte.html#external",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subject responses with predictors within FACTS they can be simulated externally and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below (Figure 6‑13).\nTo import an external file, the user must first add a profile to the table. After adding the profile, the user must click “Browse” to locate the file of externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 16: The external files sub-tab for uploading patient data to be sampled from.\n\n\n\n\nRequired Format of Externally Simulated Detail\nThe supplied data should have the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nUncensored time to event in weeks\nUncensored Predictor value, one of the following depending on the predictor type:\n\nContinuous: “NN.NN” change from baseline\nDichotomous: 0 for no response, 1 for response\nTTE: uncensored time to event in weeks\n\n\nThe GUI requires that the file name has a “.dat” suffix. The file need not have column headers, but if it does the first column name must start with a pound sign (#).\nThe following shows values from an example file with a dichotomous predictor. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n\n\n\n\n\n\n\n\n\n#Patient ID\nDose Index\nUncensored Time to Event (weeks)\nUncensored Predictor value\n\n\n\n\n1\n1\n8.87\n0\n\n\n2\n1\n9.34\n0\n\n\n3\n2\n6.78\n1\n\n\n4\n2\n10.23\n0\n\n\n5\n2\n9.96\n0\n\n\n6\n2\n5.6\n1\n\n\n7\n1\n37.01\n0\n\n\n8\n1\n28.67\n0\n\n\n9\n1\n39.70\n0\n\n\n\nIn the above, column headers have been included to make it clearer to read but they are not required.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/index.html",
    "href": "documentation/v71/userguides/core/vsr/index.html",
    "title": "Virtual Subject Response",
    "section": "",
    "text": "The Virtual Subject Response (VSR) tab allows the user to explicitly define virtual subject response profiles that dictate the distribution that patients’ endpoints should be simulated from. VSRs can be specified explicitly by specifying the control distribution and treatment effects, or by importing externally simulated patient responses.\nIf specifying, the VSR in the “Explicitly Defined” tab you will always have the dose response VSR to specify, and if subjects have multiple visits, then a Longitudinal VSR must be specified as well. If using a continuous endpoint and simulating a baseline, then a baseline simulation VSR must be specified as well.\nA dose response VSR specifies how all arms in the study should have their final endpoint value simulated. For continuous endpoints this is the mean and standard deviation of the normal distribution. For dichotomous endpoints it is the probability of response, and for time-to-event endpoints it is the hazard rate for each arm.\nThe longitudinal VSR dictates how subject visits are correlated with the final endpoint value. Each endpoint type has different methods of simulating longitudinal correlation, which are explained in detail in the following sections.\nIt is common, and advisable, to create a variety of VSR scenarios. Each scenario is a combination of a dose response VSR, a longitudinal VSR (if it exists), and a baseline VSR (if it exists). Generally, at least one null VSR, and a set of alternative scenarios are created. In a null scenario the treatment arms have the  same efficacy For a superiority study. In a non-inferiority study the treatment arms will have an efficacy profile that lies on the control minus the non-inferiority margin.  profile as the control arm.  Alternative This term (and the term ‘null scenario’) is borrowed from hypothesis testing. It indicates that the assumed scenario belongs to the alternative space of a traditional hypothesis test.  VSRs have treatments with a variety of treatment effects, usually with treatment arms simulated to be better than control.\nIf an external file is used to specify the subject responses to be simulated, it replaces the dose response, longitudinal, and baseline profiles specified in an explicitly defined VSR tab. An entire sequence of visit responses for a subjects is drawn from the uploaded patient responses file. This is elaborated on more in each endpoint’s VSR description.\nEach endpoint type (Continuous, Dichotomous, and Time-to-event) has its own method for specifying the dose response VSR and the visit to visit correlation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html",
    "href": "documentation/v71/userguides/core/vsr/continuous.html",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "",
    "text": "In FACTS Core with a continuous endpoint there are 2 different ways to specify the virtual subject response:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html#dose-response",
    "href": "documentation/v71/userguides/core/vsr/continuous.html#dose-response",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Dose Response",
    "text": "Dose Response\n\n\n\n\n\n\nFigure 1: The Explicitly Defined &gt; Dose Response model for a continuous endpoint.\n\n\n\nDose response profiles can be added and deleted, and for each profile the user specifies:\n\nThe mean response for each treatment arm. If baseline is being simulated the user can select (on the Study tab) whether the response to be analyzed is  the change from baseline or absolute response The mean response to be analyzed is always as specified on this tab, if the analysis is on change from baseline then the response specified here is change from baseline, if the response to be analyzed is absolute, then the response specified here is the absolute final score. , and depending on that selection the response specified on this tab is either change from baseline or absolute response.\nThe standard deviation of the response – either through a common SD of response for all treatment arms, or by specifying the standard deviation for the response on each treatment arm separately.\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario.FACTS uses this value to report on the proportion of simulations that were successful and selected a ‘good’ treatment arm. The check box values do not effect the simulation code, just how output is reported.\n\nThe graph on the Explicitly Defined &gt; Dose Response tab shows the mean response specified for each treatment arm +/- 1.96 SD, and the control response + the default  CSD Clinically significant difference  level specified on the QOI tab.\nIf a 2D treatment arm model is being used, the doses are listed in “effective dose strength order” as was defined on the treatment arm tab.\n\n\n\n\n\n\nFigure 2: The Explicitly Defined &gt; Dose Response tab when using a 2D treatment arm model.\n\n\n\n\nLoad Scenario Means From File\nIf the “Load scenario means from a file” option is selected, then in scenarios using this profile the simulations will use a range of dose responses instead of what is specified in the table.\nEach individual simulation uses one set of mean responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstance. Note that, to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 3: The Explicitly Defined &gt; Dose Response tab when loading scenario means from a file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual mean responses and the overall mean response over all the VSRs.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. There must be two columns per treatment arm giving the mean and SD of the change from baseline on each arm, the columns must be grouped first means then SDs and within each group they must be in dose index order. E.g.:\n#Cntrl, D1, D2, D3, Cntrl, D1, D2, D3\n-1, -1, -1, -1, 5, 5, 5, 5\n-1, -1.2, -1.4, -1.6, 5, 5, 5, 5\n-1, -1.8, -2.5, -2.5, 5, 5, 5, 5\n-1, -2, -2.5, -3, 5, 5, 5, 5\n-1, -2.5, -3.25, -3.25, 5, 5, 5, 5\n-1, -2.5, -3.25, -2.5, 5, 5, 5, 5",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html#longitudinal-vsr",
    "href": "documentation/v71/userguides/core/vsr/continuous.html#longitudinal-vsr",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Longitudinal VSR",
    "text": "Longitudinal VSR\nThe explicitly defined virtual subject longitudinal responses can be specified with any of 3 methods. No matter which longitudinal VSR simulation method is selected, it can be combined with any Dose Response VSR, and the dose response VSR is  guaranteed to have the marginal distribution Unless Use Baseline Adjustment for Subject Response is specified in the Baseline VSR tab.  specified in the dose response VSR section. The longitudinal VSR determines how to correlate early endpoint values with the final endpoint value.\nThe three longitudinal VSRs available for continuous endpoints are:\n\nCorrelated\nHierarchical, which comes in two ‘flavors’:\n\nHierarchical (as in versions of FACTS prior to 6.1): the per subject random element ‘delta’ is scaled at visit ‘t’ by the response fraction ft at that visit.\nHierarchical MMRM (first available in FACTS 6.1): the per subject random element ‘delta’ is the same at all visits.\n\nITP\n\n\nCorrelated Simulation of Longitudinal Data\nThe Correlated method for generating longitudinal responses simulates the response observed at each visit by summing 3 elements – a fraction of the final response, a fraction of the ‘noise’ from the previous observation, and additional element of noise at the current visit.\n\n\n\n\n\n\nFigure 4: The Longitudinal VSR tab when specifying the Correlated continuous longitudinal VSR.\n\n\n\nThe user specifies:\n\n\\(\\rho_{t}\\)\n\nthe correlation in the observation at visit \\(t\\) with visit \\(t - 1\\). Values should be in the range 0-1. The closer to 1, the greater the correlation between visit \\(t - 1\\) and visit \\(t\\). The \\(\\rho\\) at Visit 1 is not enterable, because there is no previous visit to correlate the first visit value with.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\varphi_{t}^{2}\\)\n\nthe fraction of the final standard deviation (SD) that will be observed at this visit. Values must be \\(&gt;0\\). An additional element of noise at visit \\(t\\) is sampled so that in combination with the noise carried forward from visit \\(t - 1\\), the overall variance in observations at visit \\(t\\) will have this fraction of the final variance.\n\n\nFor the first visit of patient \\(i\\), the response \\(y_{i,1}\\) assuming patient \\(i\\) was randomized to dose \\(j\\) is simulated as:\n\\[y_{i,1}\\ \\sim\\ N\\left( \\mu_{j,1},\\sigma_{j,1}^{2} \\right)\\]\nFor time points after the first visit, observed data is simulated as:\n\\[y_{i,t}\\ \\sim\\ \\mu_{j,t} + \\sqrt{1 - \\rho_{t}^{2}}N\\left( 0,\\sigma_{j,t}^{2} \\right) + \\rho_{t}\\left( y_{i,t - 1} - \\mu_{j,t - 1} \\right)\\frac{\\sigma_{j,t}}{\\sigma_{j,t - 1}} \\text{   for } t \\geq 2.\\]\nThere are 3 components of this equation. First, is the marginal mean of the response at the particular visit \\(t\\). This parameter is called \\(\\mu_{j,t}\\). \\(\\mu_{j,t}\\) is simply the dose response at the final endpoint times the proportion of the final effect that should be observed at visit \\(t\\).\n\\[\\mu_{j,t} = f_{t}\\mu_{j}\\]\nwhere \\(f_{t}\\) is input as the response fraction for each visit, and \\(\\mu_{j}\\) is the dose response for dose \\(j\\), which is input as the Response for each dose on the Dose Response tab.\nThe second component controls the adjustment of the variance based on the correlation of the endpoint to be sampled with the previous visit’s response. A strong correlation (\\(\\rho_{t}\\) close to \\(\\pm 1\\)) results in a variance reduction term \\(\\sqrt{1 - \\rho_{t}^{2}}\\) close to 0, which guarantees that the time \\(t\\) response is very close to the time \\(t - 1\\) response. The variance reduction term modifies a dose’s marginal visit variance \\(\\sigma_{j,t}^{2}\\). This dose by visit variance is calculated by squaring the marginal standard deviation for a dose’s final endpoint response \\(\\sigma_{j}\\) times the proportion of the total variance that is observed at visit \\(t\\), \\(\\phi_{t}\\), which is a user input on the Longitudinal VSR tab.\nSo, \\(\\sigma_{j,t}^{2} = \\phi_{t}^{2}\\sigma_{j}^{2}\\) where \\(\\phi_{t}\\) is the fraction of final SD specified in the Longitudinal VSR tab in FACTS, and \\(\\sigma_{j}\\) is the SD of the response specified in the Dose Response VSR tab in FACTS.\nThe final component of the model adjusts the mean of the visit \\(t\\) response that is to be simulated based on the residual of the time \\(t - 1\\) response. This component is:\n\\[\\rho_{t}\\left( y_{i,t - 1} - \\mu_{j,t - 1} \\right)\\frac{\\sigma_{j,t}}{\\sigma_{j,t - 1}}\\]\nValues of \\(\\rho_{t}\\) close to 1 lead to visit \\(t\\) responses with Z-scores similar to the visit \\(t - 1\\) responses, values of \\(\\rho_{t}\\) close to 0 lead to simulation of visit \\(t\\) responses with no regard to the previous visit’s residual value, and values of \\(\\rho_{t}\\) close to -1 lead to visit \\(t\\) responses with Z-scores that are -1 times the previous visit’s Z-score.\nThe specification of the visit level means \\(\\mu_{j,t}\\ \\)and variances \\(\\sigma_{j,t}^{2}\\) as response fractions and fractions of final SD, respectively, allows for each created longitudinal VSR to work with every created Dose Response VSR.\n\n\nHierarchical model\nThe hierarchical method for generating longitudinal responses simulates the response observed by sampling responses from a Normal distribution, where the mean is a combination of the mean final response of the treatment and a per-subject difference, scaled by a visit dependent coefficient, and the variance is a fraction of the variance of the final treatment effect.\nThis Hierarchical form of this model has a per-subject random effect parameter \\(\\delta_{i}\\) that is scaled down by the response scaling parameter \\(f_{t}\\). In the very similar Hierarchical (MMRM) model (described below), the random effect \\(\\delta_{i}\\) is not scaled by \\(f_{t}\\), so it provides a constant adjustment to all visits.\n\n\n\n\n\n\nFigure 5: The Longitudinal VSR tab when specifying the Hierarchical (MMRM) continuous longitudinal VSR.\n\n\n\nIn the Hierarchical longitudinal subject data simulation model, the response variance is decomposed into two components. One is the within-subject variability (called intra-subject variability), and the other is across-subject variability (Called inter-subject variability). The parameter \\(\\omega\\) determines how much of the total variability is simulated in the inter- and intra-subject variabilities. More variability in the inter-subject simulation means that there is less variability in the intra-subject simulation, so the early visit endpoints will be more correlated with the final visit endpoint, and vice versa.\nThe parameters of the Hierarchical longitudinal subject data simulation model are:\n\n\\(\\omega\\)\n\nthe fraction of the variance of the final response (\\(\\sigma_{j}^{2}\\)) on the treatment arm that will be simulated in the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\phi_{jt}^{2}\\)\n\nthe fraction of the final endpoint variance that will be observed at this visit. Values must be \\(&gt;0\\) and must be such that \\(\\phi_{jt}^{2} - f_{t}^{2}\\omega\\) is \\(&gt;0\\).\n\n\nObservation \\(y_{it}\\) is the visit response at visit \\(t\\) for a subject \\(i\\) that was randomized from dose \\(j\\). The observation is generated from the distribution described by:\n\\[y_{i,t} = f_{t}\\left( \\mu_{j} + \\delta_{i} \\right) + N\\left( {0,\\ \\left( \\phi_{jt}^{2} - f_{t}^{2}\\omega \\right)\\ \\sigma}_{j}^{2} \\right)\\]\nwhere \\(\\sigma_{j}^{2}\\) is the variance of the final endpoint response for dose \\(j\\), \\(\\phi_{jt}^{2}\\) is the fraction of total variance observed at visit \\(t\\) on dose \\(j\\), \\(\\mu_{j}\\) is the final endpoint response mean for dose \\(j\\), \\(\\delta_{i}\\) is a subject level random effect, \\(f_{t}\\) is the fraction of the final endpoint response that is observed at visit \\(t\\), and \\(\\omega\\) is the proportion of overall variance due to intersubject (across subject) variability.\nThe prior for the patient random effect term of patient \\(i\\) who was randomized to dose \\(j\\) is\n\\[\\delta_{ij}\\ \\sim\\ N\\left( 0,\\ \\omega\\sigma_{j}^{2} \\right)\\]\nIf \\(\\omega\\) is close to 1, then most of the variability in the responses comes from differences in participants and the visit-to-visit correlation within a participant’s follow-up is high. If \\(\\omega\\) is close to 0, then there is little correlation between visits within a participant’s follow-up, and most of the overall variance comes from noise in the within patient responses, rather than differences across patients. In other words, large values of \\(\\omega\\) lead to early data that is more predictive of the final endpoint.\nNote that, in this model the response fraction \\(f_{t}\\) is multiplied by both the final endpoint mean and the patient level random effect. This results in the variance of the dose \\(j\\) response at visit \\(t\\) being \\[{\\left( \\phi_{jt}^{2} - f_{t}^{2}\\omega \\right)\\sigma}_{j}^{2} + f_{t}^{2}\\omega\\sigma_{j}^{2} = \\phi_{jt}^{2}\\sigma_{j}^{2}.\\]\nAdditionally, since \\(f_{t}\\) changes the proportion of the overall variance that comes from the random effect, the simulated correlation between visits decreases when values of \\(f_{t}\\) less than 1 are provided. See the MMRM version of the Hierarchical simulation model if this is undesirable.\nIf all \\(\\phi_{jt}^{2} = 1\\) and \\(f_{t} = 1\\), then the visits will have pairwise correlations equal to \\(\\omega\\). If the \\(\\phi_{jt}^{2}\\) or \\(f_{t}\\) are less than 1, then the visit pairwise correlations will depend on the input variance fractions \\(\\phi_{jt}^{2}\\) and \\(f_{t}\\).\n\n\nHierarchical (MMRM) model\nThe Hierarchical (MMRM) longitudinal patient simulation model is very similar to the Hierarchical simulation method, except that in the MMRM version the response fraction for a visit does not modify the patient level random effect. The user inputs for the Hierarchical (MMRM) model are nearly identical to the plain Hierarchical model.\n\n\\(\\omega\\)\n\nthe fraction of the variance of the final response on the treatment arm (\\(\\sigma_{j}^{2}\\)) that will be simulated in the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\phi_{jt}^{2}\\)\n\nthe fraction of the final endpoint variance that will be observed at this visit for dose \\(j\\), values must be &gt;0 and must be such that \\(\\phi_{jt}^{2} - \\omega\\) &gt; 0.\n\n\nThe Hierarchical MMRM method simulates responses at visit \\(t\\) for a subject \\(i\\) that was randomized to dose \\(j\\) from the distribution:\n\\[y_{i,t} = f_{t}\\mu_{j} + \\delta_{i} + N\\left(0,\\ \\left( \\phi_{jt}^{2} - \\omega \\right)\\ \\sigma_{j}^{2} \\right)\\].\nwhere \\(\\sigma_{j}^{2}\\) is the variance of the final endpoint response for dose \\(j\\), \\(\\phi_{jt}^{2}\\) is the fraction of total variance observed at visit \\(t\\) on dose \\(j\\), \\(\\mu_{j}\\) is the final endpoint response mean for dose \\(j\\), \\(\\delta_{i}\\) is a subject level random effect, \\(f_{t}\\) is the fraction of the final endpoint response that is observed at visit \\(t\\), and \\(\\omega\\) is the proportion of overall variance due to intersubject (across subject) variability.\nThe prior for the patient random effect term of patient \\(i\\) who was randomized to dose \\(j\\) is\n\\[\\delta_{ij}\\ \\sim\\ N\\left( 0,\\ \\omega\\sigma_{j}^{2} \\right)\\]\nIf \\(\\omega\\) is close to 1, then most of the variability in the responses comes from differences in participants and the visit-to-visit correlation within a participant’s follow-up is high. If \\(\\omega\\) is close to 0, then there is little correlation between visits within a participant’s follow-up, and most of the overall variance comes from noise in the within patient responses rather than differences across patients. In other words, large values of \\(\\omega\\) lead to early data that is more predictive of the final endpoint.\nNote that, in this model the response fraction \\(f_{t}\\) is multiplied by only the final endpoint mean and not the patient level random effect. This results in the variance of the dose \\(j\\) response at visit \\(t\\) being \\[{\\left( \\phi_{jt}^{2} - \\omega \\right)\\sigma}_{j}^{2} + \\omega\\sigma_{j}^{2} = \\phi_{jt}^{2}\\sigma_{j}^{2}.\\]\nThis total variance is the same as the non MMRM Hierarchical method, but only the \\(\\phi_{jt}^{2}\\) parameter effects the variance of early endpoint responses. If all \\(\\phi_{jt}^{2} = 1\\), then the visits will have pairwise correlations equal to \\(\\omega\\). If the \\(\\phi_{jt}^{2}\\) are less than 1, then the visit pairwise correlations will be larger than \\(\\omega\\), with the exact value depending on the input variance fractions \\(\\phi_{jt}^{2}\\).\n\n\nIntegrated Two Component Prediction (ITP) Simulation of Longitudinal Data\nThe ITP method for generating longitudinal responses simulates the response observed at each visit by summing 3 elements and scaling them by an exponential function. The 3 elements are: the mean final response, an element of inter-subject variability, and a residual variability at the current visit.\n\n\n\n\n\n\nFigure 6: The Longitudinal VSR tab when specifying the ITP continuous longitudinal VSR.\n\n\n\nThe user specifies\n\n\\(\\omega_{j}\\)\n\nfraction (for each dose) of the variance of the final response on the treatment arm (\\(\\sigma_{j}^{2}\\)) used for the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(k_{j}\\)\n\nthe shape parameter (for each dose) of the exponential component governing the increase in the observed response. The values of \\(k\\) should be scaled to take into account the length of time (in weeks) to the intermediate and final visits. See below for more intuition on sensible values of \\(k\\).\n\n\nFor subject \\(i\\) at visit \\(t\\), who was randomized to dose \\(j\\), the response \\(y_{it}\\) is simulated as:\n\\[y_{it} = \\left( \\mu_{j} + s_{i} + \\epsilon_{it} \\right)\\left( \\frac{1 - \\text{exp}\\left( k_{j}x_{t} \\right)}{1 - \\text{exp}\\left( k_{j}x_{T} \\right)} \\right)\\]\nwhere \\(\\mu_{j}\\) is the mean of the final endpoint on dose \\(j\\), \\(s_{i}\\sim N\\left( 0,\\ \\omega_{j}\\sigma_{j}^{2} \\right)\\) is a subject specific random effect, each \\(\\epsilon_{it}\\sim N\\left( 0,{\\ \\sigma}_{j}^{2}\\left( 1 - \\omega_{j} \\right) \\right)\\) is a residual error, \\(k_{d}\\) is a shape parameter, \\(x_{t}\\) are the visit times that the \\(y_{it}\\) are observed, and \\(x_{T}\\) is the time of the final endpoint.\nThe ITP model implies that the variance of the observations shrinks towards 0 with the mean (so early visits have reduced expected responses and variances).\nThe shape parameter \\(k\\) determines the rate at which the final endpoint’s eventual effect is observed during a subject’s follow-up. A value of \\(k = 0\\) indicates that the proportion of effect observed moves linearly with time. A value of \\(k &lt; 0\\) means that the eventual final effect is observed earlier in follow-up and plateaus off as time moves towards the final endpoint. A value of \\(k &gt; 0\\) indicates that less of the total final endpoint effect is observed early in follow up, but as time approaches the final endpoint time the proportion of the effect observed increases rapidly. Values of \\(k\\) less than 0 tend to be more common than values of \\(k\\) greater than 0. See the figure below for a collection of possible shapes of the change in response using different values of \\(k\\).\nAdditionally, unlike the Correlated or Hierarchical simulation methods, the ITP method uses the actual visit time to simulate subject values.\n\n\n\n\n\n\nFigure 7: The relationship across intermediate visit means for different shape parameters \\(k\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html#baseline-vsr",
    "href": "documentation/v71/userguides/core/vsr/continuous.html#baseline-vsr",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Baseline VSR",
    "text": "Baseline VSR\nIf simulation of baseline has been included on the Study &gt; Study Info tab, a new virtual subject response tab is available for specifying the baseline score.\n\n\n\n\n\n\nFigure 8: The Baseline VSR tab with adjustment based on the baseline response turned on.\n\n\n\nThe simulation of distribution of baseline scores is specified using a normal distribution with user specified mean and standard deviation, and optionally applied upper and lower bounds to reflect limitations on the score range or screening criteria. If the simulated baseline score is truncated, then the true mean and SD of the baseline are likely to be different from these values of the mean and SD which are before truncation.\nIf the response is chosen to be change from baseline on the Study Info tab, then the dose response VSR is specified as change from baseline, and the raw endpoint for a subject will actually be their baseline value plus their simulated change from baseline value. If the response is chosen to be Final endpoint value on the Study Info tab, then the dose response VSR specifies the direct distribution that the final endpoint will be sampled from, and changing the baseline distribution does not effect the final endpoint distribution.\nWhether the final endpoint is change from baseline or final endpoint value, it is possible to adjust the final response based on the simulated baseline value. To do so, the user selects “Use Baseline Adjustment for Subject Response” and supplies 3 parameters:\n\n\\(\\beta\\)\n\na coefficient that reflects the degree of influence of baseline on final score and the degree of variability in the final score due to baseline.\n\nc\n\na centering offset, typically the expected mean of the observed baseline scores\n\ns\n\na scaling element, typically set to the expected SD of the baseline.\n\n\nBe aware that performing a baseline adjustment for subject response can change the marginal distribution of the dose response VSR.\n\nExample\nIn the above screenshot a baseline of mean 25 and SD 10 has been specified for the distribution of the baseline values, so a centering of \\(c=25\\) and scaling of \\(s=10\\) is used. Wishing to simulate an overall SD of 5 in the final change from baseline and apportion two-thirds the variance to baseline, \\(\\beta\\) has been set as follows:\n\nThe desired final variance is 25 (\\(5^2\\)), divided into 1/3 dose response and 2/3 baseline effects.\nThe SD of the simulated response is set to \\(\\sqrt{\\left( 25*\\frac{1}{3} \\right)} = 2.89\\)\nThe SD of the scaled baseline score is 1, so to contribute half the final variance of 25, Beta is set to \\(\\sqrt{\\left( 25*\\frac{2}{3} \\right)} = 4.08\\)\nNote that when simulating a baseline effect in this way, limiting the range of baseline by specifying upper and lower cut-offs – which might be natural limits of the endpoint, or due to inclusion / exclusion criteria in the protocol – can significantly reduce the variance in the final endpoint due to the baseline effect.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html",
    "title": "Multiple Endpoint Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘.csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described below.\nThe graphs in the multiple endpoint engine are largely the same as the graphs in the single endpoint engine, except that the graphs from the single endpoint core engines that show responses for an endpoint now have a control allowing for the selection of which endpoint the plot should be made for.\nThis page will attempt to highlight differences between the single endpoint and the multiple endpoint engines, largely focusing on plots that do not exist in the single endpoint output. See Continuous/Dichotomous Simulation Output for full details about the single endpoint graphs in FACTS.\n\n\nMany of the multiple endpoint graphs are repeated from single endpoint, potentially with the ability to view for any of the simulated endpoints. When the endpoint to use can be selected in the controls for a plot, it is common that the utility can also be selected instead of an endpoint. This will allow for assessment of the distribution of the combined utility in addition to the response estimates for the individual endpoints. The repeated graphs are listed here:\n\nAllocation Box and Whisker Plot\n&lt;Endpoint&gt; Response and Subject Allocation\n&lt;Endpoint&gt; Response and Target Selection\nPer Dose: QOI Box and Whisker Plots\nTarget Response by Sample Size Scatter Plot\nCumulative Operating Characteristics Plot\nTime Course for Stopping\nTime Course for Arm Dropping/Retention\nArm Retention Proportion\nFrequentist P(significance)\nFrequentist Response and Significance\nPer Sim: &lt;Endpoint&gt; Response Alloc\nPer Sim: &lt;Endpoint&gt; Response Dist\nPer Interim: &lt;Endpoint&gt; Response Alloc\nPer Interim: &lt;Endpoint&gt; Response Dist\nExplore Final Success/Futility Criteria\nExplore Early Success/Futility Criteria\nExplore Arm Dropping Criteria\nExplore Success/Futility Contours\n\n\n\n\n\n\n\n\n\nFigure 1: Utility and Subject Allocation.\n\n\n\nThis graph displays a histogram of the mean subject allocation and a line graph of the mean estimated utility. These plots show:\n\nThe blue bars show the mean number of subjects allocated to each arm over the simulations.\nThe green dashed line is the average of the mean estimate of utility over the simulations\nThe black line is the true utility of the combination of the underlying response profiles being simulated for all the endpoints.\nThe green vertical lines around the green dashed line shows the range of the central 95% of mean estimates of utility over the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Utility and Ppn Greatest Pr(UMax)\n\n\n\n\n\n\n\n\n\n\n\n(b) Utility and Ppn Greatest Pr(UMin absolute: Delta=0.5)\n\n\n\n\n\n\n\nFigure 2\n\n\n\nThese plots show the utility of the true simulated response profiles and the mean and 95% spread of the mean estimate of utility ratios along with bars showing the proportion of simulations the different treatment arms are the finally selected target:\n\nThe dose with the maximum utility (left)\nThe Minimum Acceptable Utility Dose (Umin), the dose most likely to be the minimum dose that exceeds the Clinically Significant Minimum Utility (CSMU) of 0.5 (right)\n\n\n\n\nThis box and whisker plot shows the distribution of the estimate of utility for each dose:\n\n\n\n\n\n\nFigure 3: Utility Boxplot\n\n\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations.\n\n\n\n\n\n\nFigure 4: Estimated Utility for all endpoints and overall, as well as the number of subjects allocated to each arm.\n\n\n\nThese graphs includes a control that allows the user to select which simulation to graph the results from.\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the number of subjects finally allocated to each arm.\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations. This is the graph that’s shown when the Endpoint control is chosen to be “Utility” in the “Per Sim: Utility/Response Dist” plotting controls.\n\n\n\n\n\n\nFigure 5: Estimated Utility for all endpoints and overall, as well as the estimate of a chosen QOI\n\n\n\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the final values of a selected Utility QOI.\n\n\n\n\nThis is an identical set of graphs to the Per Sim: Utility and Subject Allocation and Per Sim: Utility and QOI plots, except that in addition to the control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100). number of files to be read and subsequent processing of the data.\n\n\n\n\nThe Across Scenario graphs for the multiple endpoint engine are the same as the graphs available in the individual continuous and dichotomous engines.\nFor reference, these are:\n\nSelected Arms\nQOI Box Plots\nPpn Success\nResponse/Utility\nAllocation\nInterim vs Final Scatter Plot\nReceiver Operating Characteristics\n\nThe only differences from the single endpoint across scenario graphs are:\n\nQOI Box Plots graph now has Utility as a selectable endpoint and the utility QOIs are available to be plotted\nRespnnse/Utility is the same as the single endpoint Response graph, except that Utility is a selectable endpoint\nInterim vs Final Scatter Plot now has utility QOIs\nReceiver Operating Characteristics plots now have utility QOIs.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#per-scenario-graphs",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#per-scenario-graphs",
    "title": "Multiple Endpoint Output",
    "section": "",
    "text": "Many of the multiple endpoint graphs are repeated from single endpoint, potentially with the ability to view for any of the simulated endpoints. When the endpoint to use can be selected in the controls for a plot, it is common that the utility can also be selected instead of an endpoint. This will allow for assessment of the distribution of the combined utility in addition to the response estimates for the individual endpoints. The repeated graphs are listed here:\n\nAllocation Box and Whisker Plot\n&lt;Endpoint&gt; Response and Subject Allocation\n&lt;Endpoint&gt; Response and Target Selection\nPer Dose: QOI Box and Whisker Plots\nTarget Response by Sample Size Scatter Plot\nCumulative Operating Characteristics Plot\nTime Course for Stopping\nTime Course for Arm Dropping/Retention\nArm Retention Proportion\nFrequentist P(significance)\nFrequentist Response and Significance\nPer Sim: &lt;Endpoint&gt; Response Alloc\nPer Sim: &lt;Endpoint&gt; Response Dist\nPer Interim: &lt;Endpoint&gt; Response Alloc\nPer Interim: &lt;Endpoint&gt; Response Dist\nExplore Final Success/Futility Criteria\nExplore Early Success/Futility Criteria\nExplore Arm Dropping Criteria\nExplore Success/Futility Contours\n\n\n\n\n\n\n\n\n\nFigure 1: Utility and Subject Allocation.\n\n\n\nThis graph displays a histogram of the mean subject allocation and a line graph of the mean estimated utility. These plots show:\n\nThe blue bars show the mean number of subjects allocated to each arm over the simulations.\nThe green dashed line is the average of the mean estimate of utility over the simulations\nThe black line is the true utility of the combination of the underlying response profiles being simulated for all the endpoints.\nThe green vertical lines around the green dashed line shows the range of the central 95% of mean estimates of utility over the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Utility and Ppn Greatest Pr(UMax)\n\n\n\n\n\n\n\n\n\n\n\n(b) Utility and Ppn Greatest Pr(UMin absolute: Delta=0.5)\n\n\n\n\n\n\n\nFigure 2\n\n\n\nThese plots show the utility of the true simulated response profiles and the mean and 95% spread of the mean estimate of utility ratios along with bars showing the proportion of simulations the different treatment arms are the finally selected target:\n\nThe dose with the maximum utility (left)\nThe Minimum Acceptable Utility Dose (Umin), the dose most likely to be the minimum dose that exceeds the Clinically Significant Minimum Utility (CSMU) of 0.5 (right)\n\n\n\n\nThis box and whisker plot shows the distribution of the estimate of utility for each dose:\n\n\n\n\n\n\nFigure 3: Utility Boxplot\n\n\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations.\n\n\n\n\n\n\nFigure 4: Estimated Utility for all endpoints and overall, as well as the number of subjects allocated to each arm.\n\n\n\nThese graphs includes a control that allows the user to select which simulation to graph the results from.\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the number of subjects finally allocated to each arm.\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations. This is the graph that’s shown when the Endpoint control is chosen to be “Utility” in the “Per Sim: Utility/Response Dist” plotting controls.\n\n\n\n\n\n\nFigure 5: Estimated Utility for all endpoints and overall, as well as the estimate of a chosen QOI\n\n\n\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the final values of a selected Utility QOI.\n\n\n\n\nThis is an identical set of graphs to the Per Sim: Utility and Subject Allocation and Per Sim: Utility and QOI plots, except that in addition to the control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100). number of files to be read and subsequent processing of the data.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#across-scenario-graphs",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#across-scenario-graphs",
    "title": "Multiple Endpoint Output",
    "section": "",
    "text": "The Across Scenario graphs for the multiple endpoint engine are the same as the graphs available in the individual continuous and dichotomous engines.\nFor reference, these are:\n\nSelected Arms\nQOI Box Plots\nPpn Success\nResponse/Utility\nAllocation\nInterim vs Final Scatter Plot\nReceiver Operating Characteristics\n\nThe only differences from the single endpoint across scenario graphs are:\n\nQOI Box Plots graph now has Utility as a selectable endpoint and the utility QOIs are available to be plotted\nRespnnse/Utility is the same as the single endpoint Response graph, except that Utility is a selectable endpoint\nInterim vs Final Scatter Plot now has utility QOIs\nReceiver Operating Characteristics plots now have utility QOIs.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#summary-per-scenario",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#summary-per-scenario",
    "title": "Multiple Endpoint Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after multiple endpoint simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ -&gt; Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut -&gt; Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt;Dose&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn Incorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation columns provided in FACTS output for multiple endpoint trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nUtility\n\nThe utility columns provided in FACTS output for multiple endpoint trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Utility: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the posterior estimate of the overall utility of this dose, this is the utility of each endpoint at the dose, combined using the specified utility combination method.\n\n\nSD Utility: &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the posterior estimate of the overall utility of this dose.\n\n\n\n\n\nProbabilities\n\nThe Utility QOIs, which are visible when Probabilities columns are selected in simulation output for multiple endpoint trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per Utility QOI\nFor each Utility Posterior Probability QOI this is the mean over the simulations of the estimate of the posterior probability of the QOI for each dose.\nFor each Utility Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The Max Utility target is always identifiable, so the Ppn(target) should sum to 1 across the doses.\nA Umin target is not guaranteed to be identifiable, if no dose meets the CSMU criteria in any MCMC sample so all doses have a 0 probability of having greater utility than Control then no dose is the Umin. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\n\n\n\n\n\n&lt;Endpoint&gt; Response\nThe columns that appear when selecting &lt;Endpoint&gt; Response vary depending on whether the selected endpoint is continuous or dichotomous.\n\nContinuousDichotomous\n\n\n\nThe columns provided if selecting endpoint specific response for a continuous endpoint.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nSD Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nMean Sigma (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint.\n\n\nSD Mean Sigma (&lt;Endpoint&gt;)\n1\nThis is the standard deviation (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint\n\n\nTrue Mean Resp (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the true change from baseline / response rate from which the simulation data was sampled.\n\n\nTrue SD Resp.: &lt;Dose&gt;\n1\nIf the endpoint is continuous these columns are included and report the true standard deviation of the simulated response. If baseline and “baseline adjustment of the simulated subject response” is used, this reported SD will include that impact of that and be different from the value(s) for sigma entered on the VSR &gt; Dose Response tab.\n\n\nMean Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the posterior estimate of the utility of this endpoint on this dose.\n\n\nSD Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the posterior estimate of the overall utility of this dose.\n\n\n\n\n\n\nThe columns provided if selecting endpoint specific response for a dichotomous endpoint.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nSD Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nTrue Mean Resp (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the true change from baseline / response rate from which the simulation data was sampled.\n\n\nMean Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the posterior estimate of the utility of this endpoint on this dose.\n\n\nSD Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the posterior estimate of the overall utility of this dose.\n\n\n\n\n\n\n\n\n&lt;Endpoint&gt; Probabilities\n\nThe endpoint specific QOI columns for simulation output in the multiple endpoint engine.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI defined on this endpoint\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\n&lt;Endpoint&gt; Baseline – Continuous Endpoints Only\n\nThe baseline columns shown when &lt;Endpoint&gt; Baseline is selected. Only available for continuous endpoints.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Baseline Beta (&lt;Endpoint&gt;)\n1\nIf a baseline adjusted dose response model is being used then this is the mean (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nSD Baseline Beta (&lt;Endpoint&gt;)\n1\nIf a baseline adjusted dose response model is being used then this is the standard deviation (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nMean Baseline (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the mean observed baseline score.\n\n\nSD Baseline (&lt;Endpoint&gt;)\n1\nThis is the standard deviation (over the simulations) of the mean observed baseline score.\n\n\nTrue Mean Baseline (&lt;Endpoint&gt;)\n1\nThis is the true mean of the baseline distribution used to simulate the baseline scores.\n\n\nTrue SD Baseline (&lt;Endpoint&gt;)\n1\nThis is the true standard deviation of the distribution used to simulate the baseline scores.\n\n\n\n\n\n&lt;Endpoint&gt; Observed\n\nThe columns shown when &lt;Endpoint&gt; Observed is selected.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the mean (over the simulations) of dropouts per arm per visit in this scenario.\n\n\n\n\n\n&lt;Endpoint&gt; Model Params\n\nThe columns shown when &lt;Endpoint&gt; Model Params is selected for a specific endpoint.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nBAC Mean (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Tau (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Tau (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\n\n\n\nFrequentist Results (&lt;Missingness&gt; &lt;Endpoint&gt;)\n\nThe columns available when selecting Frequentist results for a specific endpoint and missingness handling method.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n\nMean Theta (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\n\nThe mean response per dose.\n\n\n\nSD Mean Theta (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\n\nThe standard error of the response per dose\n\n\n\nPpn Signif (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of | simulations where the unadjusted p-value is | less than the user specified one-sided alpha. | | The marginal probability of significance per | dose (using unadjusted p-values).\n\n\n\nBias (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\n\nCoverage (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the treatment difference compared to control contains the true mean treatment difference compared to control used to simulate subject responses.\n\n\n\nPpn Signif Dunnett (&lt;Missingness&gt; &lt;Endpoint&gt;)\n1\nFor each treatment arm, the proportion of simulations where at least one of the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Dunnett (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Dunnett (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval for the treatment difference compared to control contains the true mean treatment difference compared to control used to simulate subjects responses.\n\n\n\nPpn Signif Bonf (&lt;Missingness&gt; &lt;Endpoint&gt;)\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Bonf (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Bonf (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Trend (&lt;Missingness&gt; &lt;Endpoint&gt;)\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#detailed-per-simulation-results",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#detailed-per-simulation-results",
    "title": "Multiple Endpoint Output",
    "section": "Detailed Per Simulation Results",
    "text": "Detailed Per Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulations output in the table by double-clicking on the appropriate row. A separate window pops up and displays the results for each individual simulation. This is a “prettier” view of the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 6: The simulation results pop out viewer showing simulation highlights.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-summary.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first two lines are header lines, starting with a ‘#’, containing the column headings. The first line contains the lengthy “human readable” form of the QOI names, the second line contains the shorter “computer readable” alternate QOI names. For columns that are not QOI values, the column names are the same in the two rows.\n\nContents of the summary.csv file for the multiple endpoint engine.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the “.facts” file – not including the file extension.\n\n\nScenario\n1\nThe scenario name as used in FACTS – made by concatenating the names of the various profiles that make up the scenario\n\n\nTimestamp\n1\nThe time the simulations started\n\n\nVersion\n1\nThe overall version of FACTS that ran the simulations (not the version of the specific design engine)\n\n\nNSim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.subj 80%-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nNo. Dropouts &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of dropouts for each treatment arm.\n\n\nMean Util &lt;Dose&gt;\nD\nThis is the mean (over the simulations) of the posterior estimate of the utility of this dose.\n\n\nSE Util &lt;Dose&gt;\nD\nThis is the SD (over the simulations) of the posterior estimate of the utility of this dose.\n\n\nMean Resp &lt;Endpoint&gt;&lt;Dose&gt;\nD per endpoint\nThis is the mean (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nSE Resp &lt;Dose&gt;\nD per endpoint\nThis is the standard deviation (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nMean Sigma\n1 per continuous endpoint\nThis is the mean (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint.\n\n\nSE Mean Sigma\n1 per continuous endpoint\nThis is the standard deviation (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint\n\n\nTrue Mean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nThis is the true change from baseline / response rate from which the simulation data was sampled.\n\n\nTrue SD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nIf the endpoint is continuous these columns are included and report the true standard deviation of the simulated response. If baseline and “baseline adjustment of the simulated subject response” is used, this reported SD will include that impact of that and be different from the value(s) for sigma entered on the VSR &gt; Dose Response tab.\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit per endpoint\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit, based on the independent estimate of the dose response (omega_d), scaled by the longitudinal model’s estimate of the proportion of the final effect observed at the visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit per endpoint\nThis is the SD ( over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Util &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nThis is the mean (over the simulations) of the mean estimate of the utility of this endpoint on this dose.\n\n\nSE Util &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nThis is the standard error (over the simulations) of the mean estimate of the utility of this endpoint on this dose.\n\n\nMean Beta &lt;Endpoint&gt;\n1 per continuous endpoint\nIf a baseline adjusted dose response model is being used then this is the mean (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nSE Beta &lt;Endpoint&gt;\n1 per continuous endpoint\nIf a baseline adjusted dose response model is being used then this is the standard error (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nMean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the mean (over the simulations) of the mean observed baseline score.\n\n\nSE Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the standard error (over the simulations) of the mean observed baseline score.\n\n\nSD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the mean (over the simulations) of the SD of the observed baseline score.\n\n\nSE SD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the standard error (over the simulations) of the mean of the observed baseline score.\n\n\nTrue Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the true mean of the baseline distribution used to simulate the baseline scores.\n\n\nTrue SD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the true SD of the baseline distribution used to simulate the baseline score.\n\n\nBAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first sit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive location.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first sit, to last person first visit (LPFV).\n\n\nQOI Columns\n\n\n\nThe QOI Columns depend on the QOIs that have been defined for this design. The columns are grouped, so that first all of the Utility QOIs are listed, then all of the Endpoint 1 QOIs are listed, and so on. Within the utility or an endpoint group, the QOIs are arranged in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-summary_freq_missing_endpoint.csv",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-summary_freq_missing_endpoint.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of Summary_freq_<missing>_<Endpoint>.csv",
    "text": "Contents of Summary_freq_&lt;missing&gt;_&lt;Endpoint&gt;.csv\nThere is a frequentist summary file for each type of treatment of missing values for each endpoint.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nThe contents of the Summary_freq_&lt;missing&gt;_&lt;Endpoint&gt;.csv file.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\nSE Mean Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\nMean Beta (baseline)\n1 if endpoint continuous\nThe mean of the estimate of the baseline coefficient.\n\n\nSE Mean Beta (baseline)\n1 if endpoint continuous\nThe standard error of the estimate of the baseline coefficient\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the mean response and the true (simulated) response per dose\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true response rate used to simulate subject responses.\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true response rate used to simulate subjects responses.\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true response rate used to simulate subjects responses.\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nThe contents of the simulations.csv file and the weeksNNNNN.csv file for the multiple endpoint engine.\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis.\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index 999). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:\n\n= Early success\n= Late success\n= Late futility\n= Early futility\n= Success to futility flip-flop\n= Futility to success flip-flop\n= Inconclusive\n\n\n\nMean Utility &lt;Dose&gt;\nD\n✔\n✔\nThe mean estimate of utility per dose\n\n\nSD Utility &lt;Dose&gt;\nD\n✔\n✔\nThe SD of the estimate of utility\n\n\nEarly Success\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects on each dose that have completed – final endpoint data is available.\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subject who count as complete for the purposes of timing interims – whether complete or opportunity to complete and at which visit is as defined on the Interims tab.\nIf interims are by enrolment then CompleteInformation is the number of subjects that have completed – final endpoint data is available.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\n✔\n✔\nThe number of subjects that have dropped out on each arm at each visit\n\n\nMean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe true mean response of each treatment arm for this simulation.\n\n\nTrue SD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe true SD of the response of each treatment arm for this simulation\n\n\nSigma &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe pooled estimate of the SD of the response across all the treatment arms\n\n\nSD Sigma &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe SD of the pooled estimate of the SD of the response across all the treatment arms\n\n\nBeta &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe estimate of Beta the baseline adjustment coefficient, if baseline adjustment is being used.\n\n\nSD Beta &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe SD of the estimate of Beta the baseline adjustment coefficient.\n\n\nMean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe estimate of the mean baseline score.\n\n\nSE Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe standard error of the estimate of the mean baseline score.\n\n\nSD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe estimate of the SD of the baseline score.\n\n\nTrue Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe true mean of the simulated baseline score\n\n\nTrue SD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe true SD of the simulated baseline score\n\n\nMean Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe mean estimate of the utility of this endpoint\n\n\nSD Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe SD of the estimate of utility of this endpoint\n\n\nMean Raw Response &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe mean raw response based solely on the observed data on each dose.\n\n\nSE mean Raw Response &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe SE of the estimate of the mean raw response on each dose.\n\n\nComplete &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe number of subjects with final data on this endpoint on each arm at the end of the trial / time of the interim.\n\n\nDR Param &lt;Endpoint&gt; &lt;Param&gt;\n10 per continuous endpoint\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 10 parameters, most less. The model parameters are labeled α1, α2, .. the subscripts corresponding to the column index &lt;Param&gt;.\n\n\nSd DR Param &lt;Endpoint&gt; &lt;Param&gt;\n10 per continuous endpoint\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod &lt;Endpoint&gt; &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nLM*LMP*V per continuous endpoint\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of model instances the user has specified.\nFor linear regression the parameters reported are:\n\nAlpha – per visit – the mean estimate of the constant offset in the change in response from this visit to the final visit\nBeta – per visit – the mean estimate of the coefficient of change in response from this visit to the final visit\nLambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\n\nFor time course hierarchical the parameters reported are:\n\nAlpha – per visit – the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit..\nLambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\nTau – the mean estimate of the SD of the per subject random effect\n\nFor ITP the parameters are:\n\nK – per model – the mean estimate of the ITP shape parameter\nTau – per model - the mean estimate of the SD of the per subject random effect\nLambda – per model – the mean estimate of the Sd of the residual error.\nOmega – per treatment arm – the mean estimate of the mean treatment arm effect.\n\nFor LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp &lt;Endpoint&gt; &lt;Dose&gt; &lt;Visit&gt;\nD*V per continuous endpoint\n✔\n✔\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean response for a particular dose at a particular visit.\n\n\nMean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nMean resp (loser CI) &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe lower bound of the 95% Credible Interval of the estimate of response for each treatment arm.\n\n\nMean resp (upper CI) &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe upper bound of the 95% Credible Interval of the estimate of response for each treatment arm.\n\n\nTrue Mean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe true response rate of each treatment arm for this simulation.\n\n\nMean Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe mean estimate of the utility of this endpoint\n\n\nSD Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe SD of the estimate of utility of this endpoint\n\n\nMean Raw Response &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe mean raw response based solely on the observed data on each dose.\n\n\nComplete &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe number of subjects with final data on each arm at the end of the trial / time of the interim.\n\n\nDR Param &lt;Endpoint&gt; &lt;Param&gt;\n4 per dichotomous endpoint\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled α1, α2, .. the subscripts corresponding to the column their value appears in here.\n\n\nSd DR Param &lt;Endpoint&gt; &lt;Param&gt;\n4 per dichotomous endpoint\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod &lt;Endpoint&gt; &lt;Model&gt; &lt;Visit&gt;\nLM*LMP*V per dichotomous endpoint\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of model instances the user has specified.\nFor Beta-Binomial the parameters reported are:\n\nAlpha0 & Beta 0 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 0\nProb01 – the mean probability of the Beta(Alpha0, Beta0) – the probability of 1 being the final result if the result at the visit is 0\nAlpha1 & Beta 1 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 1\nProb11 – the mean probability of the Beta(Alpha1, Beta1) – the probability of 1 being the final result if the result at the visit is 1\n\nFor logistic regression the parameters reported are:\n\nProb11 – the probability of 1 being the final result if the result at the visit is 1and\nProb01 – the probability of 1 being the final result if the result at the visit is 0\n\nFor restricted Markov the parameters reported are:\n\nAlpha0 – Alpha for state 0\nAlphaS – Alpha for stable state\nAlpha1 – Alpha for state 1\nProb0 – Transition probability to state 0\nProbS – Probability of remaining stable\nProb1 – Transition probability to state 1\n\n(values are for the transition to the next visit, so there are no values for the final visit)\n\n\nBAC Mean &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Tau &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n\n\n\n\n\n\nThe QOI Columns depend on the QOIs that have been defined for this design. The columns are grouped, so that first all of the Utility QOIs are listed, then all of the Endpoint 1 QOIs are listed, and so on. Within the utility or an endpoint group, the QOIs are arranged in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-simulations_freq_missingness_endpoint.csv-weeks_freq_missingness_endpoint.csv",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-simulations_freq_missingness_endpoint.csv-weeks_freq_missingness_endpoint.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of Simulations_freq_<missingness>_<Endpoint>.csv, Weeks_freq_<missingness>_<Endpoint>.csv",
    "text": "Contents of Simulations_freq_&lt;missingness&gt;_&lt;Endpoint&gt;.csv, Weeks_freq_&lt;missingness&gt;_&lt;Endpoint&gt;.csv\nThese file contain the frequentist results after an interim analysis (weeks file) or final analysis (simulations file), apart for the first column, the columns are the same in the two types of file, and the same across the different methods for treating missing data and different endpoints.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation (simulations_freq_…csv only)\n\n\n# Weeks\n1\nThe week of the interim (weeks_freq_ only)\n\n\nTheta\nD\nUnadjusted estimate of response per treatment arm\n\n\nTheta SD\nD\nThe SD of the estimate of response per treatment arm\n\n\nTrt Effect\nD\nThe relative size of the treatment effect compared to control per arm (treatment effect for control is always 0). The direction of the comparison takes into account whether a response represents a subject’s condition improving or worsening, so that a positive treatment effect shows the response on the dose is better than on control.\n\n\nt-Stat\nD\nThe t-test statistic per treatment arm (for control this is always 0)\n\n\np-Value\nD\nThe unadjusted t-test p-value per arm (for control this is always 0)\n\n\nlower CI\nD\nThe unadjusted lower bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nupper CI\nD\nThe unadjusted upper bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nMin p-Value\n1\nThe smallest of the t-test p-values\n\n\np-Value Dunnett\nD\nThe Dunnett adjusted t-test p-value per arm (for control this is always 0).\n\n\nlower CI Dunnett\nD\nThe Dunnett adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper CI Dunnett\nD\nThe Dunnett adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Dunnett\n1\nThe smallest of the Dunnett adjusted p-values.\n\n\np-Value Bonf\nD\nThe Bonferroni adjusted t-test p-value per arm.\n\n\nlower-CI Bonf\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper-CI Bonf\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Bonf\n1\nThe smallest of the Bonferroni adjusted t-test value per arm.\n\n\nt-Stat trend\n1\nThe t statistic from the trend test.\n\n\np-Value trend\n1\nThe p-value from the trend test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-patientsnnnnn.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in\n\n\nDateInWeeks\n1\nThe date, in weeks, from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit Overall\n1\nThe time of the last observation of the subject – the index of the visit of the observation from the overall visit list. NOTE this is the index of the last visit using the overall visit index (index of all visits). E.g. for a patient with 6 visits (index: 1, 2, …6), and an endpoint observed on 3 visits: visits 2, 4 and 6 of the overall schedule, if that patients last observed data was the second time this endpoint was observed (last visit for that endpoint will be “2”), the last visit overall will be “4”.\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nLastVisit &lt;Endpoint&gt;\n1\nThe time of the last observation of this endpoint – the index of the visit of the observation from the visit list for the endpoint\n\n\nBaseline &lt;Endpoint&gt;\n1\nThe baseline score for the subject, if baseline is simulated. If no baseline, or the endpoint is dichotomous, then this column is -9999.\n\n\nResponse &lt;Endpoint&gt; &lt;Visit&gt;\nV\nOne column per visit from the visit list from the endpoint, recording the observed endpoint value at that visit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/multendpt.html#contents-of-mcmcnnnnn.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file, if requested for output by the user, contains all the MCMC samples for the estimated responses for each arm on each endpoint. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The next set of columns are the MCMC samples for the first endpoint. This includes response rate estimates and, if the endpoint is continuous, a sigma. Then, various longitudinal model parameters are reported if used for that endpoint.\nThen second endpoint’s MCMC columns begin, again providing response estimates with or without sigma and longitudinal model parameters (if used). The other endpoints proceed in kind.\nThe first two columns are:\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\n\nThen, the endpoint specific output is provided for each endpoint. For a particular endpoint index, &lt;Endpoint&gt;, the columns output are:\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nTheta_&lt;Endpoint&gt; &lt;Dose&gt;\nD\nThe estimate of the mean response for each dose, based on the dose response model fitted.\n\n\nSigma_&lt;Endpoint&gt;\n1\nThe estimate of the SD of the endpoint.\n\n\nBeta_&lt;Endpoint&gt;\n1\nIf baseline is included in the simulated data and using the “Baseline Adjusted Model”, this is the value of “Beta”, the baseline coefficient.\n\n\nA&lt;1-8&gt;_&lt;Endpoint&gt; / Tau_&lt;Endpoint&gt;\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod_&lt;Endpoint&gt; &lt;model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\n\n\nIf using a dichotomized continuous variable, then the parameters for the LM are the same as for the continuous model MCMC output listed in the Continuous tab.\nIf using a dichotomous longitudinal model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nPi_&lt;Endpoint&gt; &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA&lt;1-8&gt;_&lt;Endpoint&gt; / Tau_&lt;Endpoint&gt;\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod_&lt;Endpoint&gt; &lt;model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\nIf using a restricted markov model for the endpoint, then the following columns are output in the MCMC file for that endpoint.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nPi_&lt;Endpoint&gt; &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA&lt;1-8&gt;&lt;Endpoint&gt; / Tau&lt;Endpoint&gt;\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod_&lt;Endpoint&gt; &lt;model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nCurrently the values of Alpha0, Alpha1, AlphaS (the parameters to the Dirichlet distribution) are (unnecessarily – these values are not sampled) output, along with the values for P0, P1 and PS the transition probabilities of each model at each visit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation",
      "Multiple Endpoint"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html",
    "href": "documentation/v71/userguides/core/simulation/index.html",
    "title": "Simulation",
    "section": "",
    "text": "The Simulation tab allows the user to execute simulations for each of the scenarios specified for the study. The user may choose the number of simulations, whether to execute locally or on the Grid, and modify the random number seeds.\nIn the Simulation tab the user can provide simulation configuration parameters like the number of simulations to run, whether the simulations can be run on the Grid, the parallelization strategy, the random number seed used in the simulations, and the number of certain output files that should be kept during the simulation execution.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#number-of-simulations",
    "href": "documentation/v71/userguides/core/simulation/index.html#number-of-simulations",
    "title": "Simulation",
    "section": "Number of simulations",
    "text": "Number of simulations\nThis box allows the user to enter the number of simulations that they would like FACTS to run for each scenario listed in the table at the bottom of the simulation tab. There is no set number of simulations that is always appropriate.\n\n10 simulations\n\nYou might want to run 10 simulations if you just want to look at a few simulated trials and assess how the decision rules work and if FACTS is simulating what you expected based on what you input on the previous tabs. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\n\n100 simulations\n\nYou might want to run 100 simulations if you want to look at many individual trials to make sure that what you want to happen is nearly always happening. You can also start to get a very loose idea about operating characteristics like power based on 100 sims. 100 simulations is also usually sufficient to spot big problems with the data analysis such as poor model fits or significant bias in the posterior estimates.\n\n1,000 simulations\n\nYou might want to run 1,000 simulations if you want estimates of operating characteristics like power, sample size, and Type I error for internal use or while iterating the design. This generally isn’t considered enough simulations for something like a regulatory submission. With 1,000 simulations the standard error for a typical type I error calculation is on the order of \\(0.005\\).\n\n10,000 simulations\n\nYou might want to run 10,000 simulations per scenario if you are finalizing a design and are preparing a report. This is generally enough simulations for a regulatory submission, especially in non-null simulation scenarios. The standard error for a typical type I error calculation using 10,000 simulations is on the order of \\(0.0015\\).\n\n&gt; 10,000\n\nYou might want to run more than 10,000 simulations if you want to be very certain of an operating characteristic’s value - like Type I error. And plan to use the measurement of the quantity for something important like a regulatory submission. The standard error of a Type I error calculation with 100,000 simulations, e.g., is on the order of \\(0.0005\\).\n\n&gt; 100,000\n\nYou probably don’t want to run more than 100,000 simulations per scenario. Maybe your finger slipped an hit an extra 0, or you thought there were 5 zeroes in that number when there were actually 6. If the simulated trial is adaptive, this is going to take a while.\n\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. Not all scenarios must be run with the same number of simulations. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#start-at-simulation",
    "href": "documentation/v71/userguides/core/simulation/index.html#start-at-simulation",
    "title": "Simulation",
    "section": "Start at Simulation",
    "text": "Start at Simulation\nThe “Start at simulation” option allows for the simulation of a particular trial seen in a previous set of simulations without having to simulate all of the previous trials in that previous set to get to it.\nThe initial random seed for FACTS simulations is set in the simulation tab. The first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. Thus, it is possible to re-run a specific simulation out of a large set without re-running all of them. For example, say the 999th simulation out of a set displayed some unusual behavior, in order to understand why, one might want to see the individual interim analyses for that simulation (the “weeks” file), the sampled subject results for that simulation (the “Subjects” files) and possibly even the MCMC samples from the analyses in that simulation. You can save the .facts file with a slightly different name (to preserve the existing simulation results), then run 1 simulation of the specific scenario, specifying that the simulations start at simulation 999 and that at least 1 weeks file, 1 subjects file and the MCMC samples file (see the “MCMC settings” dialog) are output.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#parallelization-packet-size",
    "href": "documentation/v71/userguides/core/simulation/index.html#parallelization-packet-size",
    "title": "Simulation",
    "section": "Parallelization Packet Size",
    "text": "Parallelization Packet Size\nThe parallelization packet size option allows simulation jobs to be split into runs of no-more than the specified number of trials that are run in parallel. If more simulations of a scenario are requested than can be done in one packet, the simulations are broken into the requisite number of packets, run, and combined and summarized when they are all complete. The final results files will look just as though all the simulations were run as one job or packet.\nThe packet size must be a perfect divisor of the number of simulations. This is usually easy since common numbers of simulations are multiples of 100, but don’t try to use a prime number for Number of Simulations or you’re stuck with only 2 packet size options.\nBy default (if the check box with Choose Parallel Packet Size is not checked) the number of simulations per packet depends on the number of simulations per scenario. If the number of simulations is less than 1000, then each scenario is packaged as a single packed and simulated. If the number of simulations per scenario is greater than or equal to 1000, the default packet size is 10 and all simulations are decomposed into packets of size 10.\n\n\n\n\n\n\nPacketization with VSRs using .mvsr files\n\n\n\nIf an external file is used to create explicit VSRs (a .mvsr file), then the packet size should be a multiple of the number of rows in that MVSR file. Each packet will get passed the entire .mvsr file to run. If there are multiple .mvsr files with differing numbers of lines then only the VSR scenarios that have a .mvsr file that has a number of rows that is a divisor of the packet size will be run. The rest will error. The packet size can then be modified to get each of the .mvsr specified VSR files to be run.\nCare should be taken when packetizing a scenario that includes an external data file to supply the virtual subject responses; in this situation, a of copy of the external file is included in each packet which can cause the packetisation process to run out of memory as the packets are being created. In this case, use a smaller number of larger packets, such as packets that are 1/10th of the total number of simulations.\n\n\nWhen running simulations, FACTS will create and run as many packets in parallel as there are execution threads on the local machine. In general, the overhead of packetization is quite low, so a packet size of 10 to 100 can help speed up the overall simulation process. Threads used to simulate scenarios that finish quickly can pick up packets for scenarios that take longer. The progress bar updates as simulation packets complete, so the smaller the packet size, the more accurately FACTS can report the overall progress of the simulation execution.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#random-seed",
    "href": "documentation/v71/userguides/core/simulation/index.html#random-seed",
    "title": "Simulation",
    "section": "Random Seed",
    "text": "Random Seed\nRandom number generation plays a huge role in FACTS’s virtual patient generation and statistical analyses. In order to exactly reproduce a statistical set of results, it is necessary to start the random number generation process from an identical “Random Seed”. Using the same random seed in the same version of FACTS guarantees that simulated trials will always be reproducible. Changing the design parameters or the version of FACTS may or may not remove this reproducibility depending on the change.\nEven a small change in the random seed will produce very different simulation results.\nIn addition to setting the seed, the user can choose whether they want the “Same seed for all scenarios” or “Different seed” for different scenarios. If “Same seed for all scenarios” is selected, the subjects generated for each simulated trial will match for the different scenarios. This induces a correlation among the simulation output for different scenarios. This can be good if you’re trying to compare operating characteristics for different scenarios, but it can also be misleading. To disable this option select the “Different Seed” option. If “Different seed” is selected, then each scenario has its own seed that samples a different set of subjects than any other scenario. This uncorrelates the simulation output across scenarios, which can be advantageous if the absolute value of the operating characteristics are more valuable to you than the comparison of operating characteristics across scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#mcmc-settings",
    "href": "documentation/v71/userguides/core/simulation/index.html#mcmc-settings",
    "title": "Simulation",
    "section": "MCMC Settings",
    "text": "MCMC Settings\nTo set advanced settings for simulation, the user may click the “MCMC Settings” button, which will display a number of additional specifiable parameters for simulation in a separate window.\n\n\n\n\n\n\nFigure 2: Advanced MCMC settings.\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the MCMC chain to reach its stationary distribution. Burn-in samples are output in MCMC files if the files are output.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest.\n\nThe third parameter controls the number of MCMC samples taken after each imputation of missing data using the longitudinal model. The default value is 1. This parameter only has an effect if Bayesian imputation is being used to impute missing or partially observed data. Increasing the value of this parameter allows the parameter estimates to converge somewhat to a potentially new stationary distribution for each new set of imputed data. If the imputed data is only a small percentage of the overall data this is likely unnecessary. As a rough guide, if it at some early interims &gt; 5% of the data being analyzed will be imputed, a value in the range 2 to 10 is recommended to avoid underestimating the uncertainty. A higher number should be used the greater the proportion of imputed data.\nThe next parameter concerns the output of the MCMC samples to a file. It is possible to have the design engine output the sampled values from the MCMC in all of the interims of the first N simulated trials of each scenario by specifying the “Number of MCMC files to output” to be greater than 0. The resulting files, ‘mcmcNNNN.csv’, will be in the results directory with all the other results files for that scenario. These files include the burn-in samples from the MCMC chains.\nThe final parameter in MCMC Settings is the thinning parameter. This parameter will only keep every \\(N^{th}\\) sample taken during MCMC where \\(N\\) is the thinning parameter. Thinning MCMC samples can reduce the autocorrelation of consecutive MCMC iterations, which increases the effective samples per retained sample, but also results in needing many more MCMC iterations to reach the same number of retained samples. Generally, we do not recommend thinning for standard simulation runs.\n\n\n\n\n\n\nWarning about thinning\n\n\n\nUnlike other software that performs MCMC, when you choose to thin by a value, FACTS does not increase the number of MCMC iterations it performs in order to retain the value specified in “Number of Samples”. So if you leave “Number of Samples” at its default value, \\(2500\\), and thin by \\(10\\), you will be left with \\(250\\) retained samples. You should adjust for this by increasing the “Number of Samples” if you choose to thin.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#results-output",
    "href": "documentation/v71/userguides/core/simulation/index.html#results-output",
    "title": "Simulation",
    "section": "Results Output",
    "text": "Results Output\nThe results output section of the Simulation tab allows for the specification of how many output files should be generated for files that are individually created for each simulation. Summary files (summary.csv) that have 1 line per scenario are always created. Simulations files (simulations.csv) that have 1 line per simulation are always created. Weeks files (weeksXXXXX.csv), patients files (patientsXXXXX.csv), and frequentist weeks files (weeks_freq_{missingness}_XXXXX.csv) are not created for every single simulation. Instead, the number of simulation specific output files can be set per type. This limits the amount of output files that FACTS will save.\nSee the endpoint specific descriptions of the output files for descriptions of what the previously mentioned output files report. See here for core continuous or dichotomous output files, and see here for core time-to-event output files.\nSome plots in FACTS that are created based on weeks files, and if very few weeks files are saved, the plots will not be as accurate or descriptive.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/core/simulation/index.html#facts-grid-simulation-settings",
    "title": "Simulation",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nA user with access to a computational grid may choose to run simulations on the grid instead of running them locally. This frees the user’s computer from the computationally intensive task of simulating so that they can continue other work or even shutdown their PC or laptop. In order to run simulations on the grid, it must first be configured. This is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#right-click-menu",
    "href": "documentation/v71/userguides/core/simulation/index.html#right-click-menu",
    "title": "Simulation",
    "section": "Right Click Menu",
    "text": "Right Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 4: The menu that appears when you right click on the table within the simulation tab.\n\n\n\nThese will respectively:\n\nOpen a new Windows directory browser window showing the contents of the simulation results for that scenario.\nOpen a window that displays the individual simulation results for that scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results (see below) the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of the Simulation Results highlights window.\nOpen a window that displays the frequentist analysis summary results. This option is only available if one or more frequentist analyses have been selected on the Design &gt; Frequentist Analysis tab. (If more than one analysis has been requested – using different treatments of missing data there will be separate options in the menu to display each summary).\nOpen R loading in the result files for that scenario as separate dataframes.\nOpens the FACTS graph control displaying the graphs for that scenario.\nOpens the FACTS graph control that displays the trellis plot of graphs of selected scenarios for selected design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#open-in-r",
    "href": "documentation/v71/userguides/core/simulation/index.html#open-in-r",
    "title": "Simulation",
    "section": "Open in R",
    "text": "Open in R\nThe “Open in R” button allows for the creation of an R script that has pre-populated code for loading in output files created by the FACTS simulations.\nBy default, any/all of the simulation output files can be included in the created script. If “Aggregation” (see below) has been performed, then only the aggregated files will be available for being loaded in R.\nWhen the button is clicked, FACTS will create an R script with the correct file paths to load in the data, as well as creating a function that will read the files in correctly. The file is then opened in the default R editor for the user. If there is no default program for opening a .R file, your operating system should ask how you want to open the file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#aggregation",
    "href": "documentation/v71/userguides/core/simulation/index.html#aggregation",
    "title": "Simulation",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 5: Window that appears when aggregating simulation results.\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nValues in columns that are independent of dose will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. WeeksNNNNN.csv files are aggregated into a single agg_[pivot_]weeks.csv file. PatientsNNNNN.csv files are aggregated into a single agg_patients.csv file, but they are never pivoted because each row already refers to a single dose. Similarly the various frequentist results at the summary, simulation and weeks level are aggregated (if they’ve been output).\nRegionIndex.csv is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nRecruitment Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g. External Subjects Profile if there are no external scenarios)\n\n\nDropouts Profile\n\n\n\nLongitudinal Rates Profile\n\n\n\nDose Response Profile\n\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nP(TS)\nProportion of trial success (early success + late success)\n\n\nP(TF)\nProportion of trial futility (early futility + late futility)\n\n\nSim\nSimulation number. Only present in weeks and patients files.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#design-report",
    "href": "documentation/v71/userguides/core/simulation/index.html#design-report",
    "title": "Simulation",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/analysis.html",
    "href": "documentation/v71/userguides/core/analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "The analysis tab allows the user to supply a specific data set for analysis by the design specified in the Design tab of the “.facts” file.\nClicking on the “Use Design to Analyze Data” button, will create an empty “subject.csv” file in the main simulation results directory and an ‘Analysis’ sub-directory there for running the analysis and saving the outputs.\nAlternatively, clicking on the “Import Data to Analyze” launches a file browser, allowing the user to select a ‘.csv’ file to load as the data to analyze. This is a shortcut for first clicking on the “Use Design to Analyze Data” button, and then clicking on the “Select File to Create New Analysis” button on the subject data tab.\nAfter enabling data analysis, the analysis screen is shown with no data loaded. By clicking on the “Subject Data” tab the user is now able to enter data values directly, or to load a ‘.csv’ file already containing data:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/analysis.html#the-subject.csv-file-format",
    "href": "documentation/v71/userguides/core/analysis.html#the-subject.csv-file-format",
    "title": "Analysis",
    "section": "The subject.csv file format",
    "text": "The subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nThe format of the file is the same as the ‘patientsNNNN.csv’ output file (continuous/dichotomous, time-to-event), or multiple endpoint and the column values described above.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "href": "documentation/v71/userguides/core/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "title": "Analysis",
    "section": "Converting arrival date value from days to weeks",
    "text": "Converting arrival date value from days to weeks\nFrom FACTS 7.0 the value in the Date field is interpreted as being in weeks (rather than days as in previous versions). If you have existing data, a simple conversion tool is provided “Convert Date from Days to Weeks” that simply divides all of thete values by 7. Having run the conversion, you then need to save the modified data. The values of the Date field will only make a difference to TTE analyses.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: The data as provided to the analysis tab with dates in days.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: The same dates, but converted to weeks.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/index.html",
    "href": "documentation/v71/userguides/core/qois/index.html",
    "title": "Quantities of Interest",
    "section": "",
    "text": "The QOI (Quantities Of Interest) tab allows the user to specify the Bayesian posterior quantities and frequentist p-values that are eligible to be used to make decisions in the trial as well as being output to the simulations results files.\nThere are 3 classes of QOI:\nTrial decisions at interim analyses or the final analysis are made based on decision quantities, but adaptations like adaptive allocation can be based on posterior probabilities, predictive probabilities, and target probabilities.\nNote that to creating a QOI for early stopping or final evaluation decisions will involve using 2 or 3 QOIs:\nThere are a number of pre-defined, default QOIs which simplifies the specification of the most commonly used decision quantities, and the importation of past FACTS designs. These are:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/index.html#posterior-probabilities",
    "href": "documentation/v71/userguides/core/qois/index.html#posterior-probabilities",
    "title": "Quantities of Interest",
    "section": "Posterior Probabilities",
    "text": "Posterior Probabilities\nThese are Bayesian quantities to be calculated at each interim and at the final analysis.\n\n\n\n\n\n\nFigure 2: Add dichotomous posterior probabilities page.\n\n\n\nA Posterior Probability is specified by providing a value for each of the following:\n\nCompare:\n\nContinuous: Means\nDichotomous: Rates or Log-odds\nTime-to-Event: Hazard Ratio or Hazard Rates.\n\nCondition: “&gt;” or “&lt;” a comparison value.\nRelative to an absolute value or relative to the response on a specific dose.\nThe comparison can include a delta, which is the absolute value to be compared against if t the difference relative to the comparison arm is compared to. he comparison is absolute, or a value that\n\nThe QOI will be given a “name” derived from these details and a short or Alternative name that will be used in the output files to allow easier access from other software, e.g. from within R.\nIf the endpoint is TTE and the design includes a predictor endpoint, then the definition of the QOI includes the specification of which endpoint it applies to.\n\n\n\n\n\n\nSpecial Qualities of Posterior Probabilities in Time-to-Event\n\n\n\n\n\nThere are a couple of aspects of posterior probabilities that are slightly different for posterior probability QOIs.\nFirst, in FACTS Core TTE a Posterior Probability QOI (a Bayesian comparison of estimates of response) can be used to compare Hazard Ratios or Hazard Rates. In their descriptions Hazard Ratio QOIs use “HR” (e.g. “HRd &lt; 1”) and Hazard Rate QOIs use \\(\\lambda\\) (e.g. “\\(\\lambda_d\\) &lt; \\(\\lambda_{control}\\)”). If there are hazard rate QOIs defined, then FACTS restricts the hazard model (defined on the Design &gt; Hazard Model tab) to use just a single segment (so that there is just one lambda to compare). This is set in the “Com are” box of the QOI definition window:\n\n\n\n\n\n\nFigure 3: Specifying a posterior probability QOI based on hazard rate.\n\n\n\nAdditionally, in FACTS Core TTE when defining a Posterior Probability QOI (a Bayesian comparison of estimates of response) or a Target QOI (a Bayesian assessment of which treatment arm is most likely to fulfill a ‘target’ criteria) – the user can select whether the evaluation uses the Final Event endpoint, or the Predictor.\n\n\n\n\n\n\nFigure 4: Specifying a posterior probability QOI based on the predictor endpoint.\n\n\n\n\n\n\n\nNotes on setting Deltas\nFor the three endpoint types, delta’s are defined as:\n\nContinuous\n\nA CSD (Clinically Significant Difference) in the estimates of the mean response.\n\nDichotomous\n\nA CSD in the estimate of the response rates if Rates is selected in the QOI, and of log-odds of the response rate if Log-odds is selected in the QOI.\n\nTime-to-Event\n\nA CSHRD (Clinically Significant Hazard Ratio Difference) in the estimate of the Hazard Ratio\n\n\nA standard hypothesis test for demonstrating superiority to control uses a delta of 0. Testing for superiority with a non-zero delta is different, and the implications need to be  carefully understood Simulating its use in FACTS is a good way to achieve that understanding! . Where the term CSD is used in this document, it should be taken to refer to CSHRD as well unless it is specifically stated otherwise.\nWhen setting a target treatment difference from control for the study to beat, it is also common to require a degree of confidence that the target has been beaten. To achieve posterior probabilities of &gt;50% that the target has been beaten, the estimated mean difference will have to be greater than the target difference.\nThus, when setting a delta, we are setting up two hurdles the study drug must beat, first the delta and then on top of that an additional margin to give a &gt;50% confidence that the margin has been beaten. Thus, it is necessary to avoid setting the target delta too large. A common mistake is to set the delta to the ‘expected difference’ (the value that might have been used in a conventional sample size calculation). In scenarios where the simulated response is equal to the ‘expected difference’ and hence the CSD this will give probabilities of being better than control by the delta of 50% on average, regardless of the sample size.\nIt is inadvisable to require a posterior probability of 50% that the response is better than the Control by the delta margin as this turns the test into one that simply depends on whether the point estimate of the response is better.\nIt is inadvisable to require posterior probability of less than 50% for success, as such criteria have the undesirable characteristic that they can be met in circumstances where it can be seen that if further data was gathered consistent with what has already been seen, it would lead the threshold no longer being met! The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a delta that is less than the expected difference that it is hoped to achieve, somewhat like having a non-inferiority margin around the target. So, when the design is simulated with a response at the expected difference the target will be clearly exceeded. A useful default value to use for a delta is half the ‘expected difference’. This usually yields “comprehensible” probability thresholds for both success and futility.\nUsing a delta has benefits however – simply being better than control but for very small difference can be established simply by having large sample sizes. Being better than control by a delta establishes some confidence that there is some patient benefit. Use of a delta is also useful if otherwise success thresholds “well out in the tail” (e.g. &gt; 0.99) are required such as the equivalent of an early look in a conservative Group Sequential design. Though the use of a delta does mean that there is a less direct equivalence to a p-value.\n\n\nP-value Delta’s\nSeparately from the CSD/CSHRD delta, with a continuous or dichotomous endpoint (but not time-to-event), a frequentist super-superiority/non-inferiority delta can be specified.\nThese use the same selection of super-superiority/non-inferiority as the CSD\n\n\n\n\n\n\nFigure 5: The “Standard Evaluation Variables” options at the bottom of the QOI page.\n\n\n\nCurrently, unlike the CSD delta that is only applied to the default posterior probability QOI and MED target QOI, the p-value delta is applied to all the p-value QOIs and it cannot be overridden.\nThe value of the p-value delta is constrained to be 0 or positive. The ‘direction’ of the delta depends then on the direction of the endpoint (whether “higher/lower is better” or “a response is a positive/negative outcome”).\n\n\n\nSuper superiority and non-inferiority margin directions.\n\n\n\n\n\n\n\n\n\n\n\n\nHigher is better / Response is positive\n\n\nLower is better / Response is negative\n\n\n\n\n\n\nSuper-Superiority\n\n\nTrt – Control &gt; delta\n\n\nTrt – Control &lt; -delta\n\n\n\n\nNon-inferiority\n\n\nTrt – Control &gt; -delta\n\n\nTrt – Control &lt; delta\n\n\n\n\n\n\nP-value Comparisons with No Control Arm\nIf no control arm is included p-values, with a continuous or dichotomous endpoint, the treatment response is compared to a fixed value. The fixed value is specified as “Frequentist response/rate to compare to for p-value QOIs:” in the Standard Evaluation Variables section at the bottom of the QOIs tab. It is not currently possible to compare different p-value QOIs to different fixed responses or rates.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/index.html#predictive-probabilities",
    "href": "documentation/v71/userguides/core/qois/index.html#predictive-probabilities",
    "title": "Quantities of Interest",
    "section": "Predictive Probabilities",
    "text": "Predictive Probabilities\nThere are two types of predictive probabilities –\n\nBayesian predictive probabilities, which are Bayesian predictions of frequentist outcomes and take into account the uncertainty around a set of parameters, and\nConditional Power, which aims to calculate the frequentist probability of success assuming a set of parameters to be true.\n\nThe primary difference between the Bayesian predictive probabilities and the conditional power calculations is that the Bayesian predictive probabilities are calculated taking the variability around the estimated treatment effect into account, while the conditional power calculations assume that the observed test statistic of past data is the exact effect that future data will be generated from.\nFor both Bayesian predictive probabilities and conditional power, we differentiate two types, predicting the outcome in the current trial and predicting the outcome in a future trial.\n\nBayesian predictive probabilities\n\nCurrent Trial Bayesian Predictive Probabilities\nIn the current trial, the outcome can be predicted under one of two assumptions:\n\nThat no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nThat the trial continues recruiting using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until complete.\n\nPredictive probabilities currently can only predict outcomes based on p-values. Note that since p-values are only calculated as comparisons against a control arm, predictive probabilities of success in the current trial are only available if the current trial includes a control arm.\n\n\n\n\n\n\nFigure 6: Add dichotomous predictive probability of current trial.\n\n\n\nThe user specifies how missingness is handled in the final analysis, the p-value test type – unadjusted, Bonferroni or Dunnett’s (Dunnett 1955) and the (one sided) alpha level for the significance test. The QOI assumes that the specified default p-value delta is used in the final test.\nThe predictive probability of the current trial at the maximum sample size is only available:\n\nIf the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation.\n\nThe predictive probability is calculated by simulating the remaining subjects, assuming their allocated at the current probabilities of allocation and simulating their final response based on the rate of response or (Normal) distribution of responses observed so far for each arm.\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\nThere is one simulation of the remaining data per MCMC loop, potentially significantly increasing the time to perform the overall simulation of the trial.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still perform the simulation of future subject outcomes based on posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\nCurrent Trial Bayesian Predictive Probabilities – Time-to-Event\nUnlike the Continuous and Dichotomous endpoints, to predict the probability of success at full enrollment with a Time-to-Event endpoint, it is also necessary to simulate the accrual rate (and hence how many events might be observed).\n\n\n\n\n\n\nFigure 7: Add time-to-event predictive probability of current trial.\n\n\n\nFor TTE, for a Predictive Probability of Success at Full Enrollment, there are new parameters to determine how accrual is modeled. There are 3 models for accrual\n\nFixed Rate, the parameters for this are:\n\nThe fixed (mean) accrual rate per week to simulate.\n\nEstimated from the Last ‘W’ Weeks accrual data, using a Poisson distribution, with a Gamma prior. The parameters for this are\n\nThe number of past weeks W to use the accrual data from.\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\nEstimated from all the accrual data from the start of the trial, using a Poisson distribution, with a Gamma prior, the parameters for this are:\n\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\n\n\n\nFuture Trial Bayesian Predictive Probabilities\nFor predictive probabilities for a future trial, a predictive probability of success in a subsequent phase 3 trial is estimated during the current trial. The probability of success in phase 3 is calculated for each study arm, based on the specification of the phase 3 trial given here of:\n\nwhether the aim is to show superiority or non-inferiority,\nthe sample size per arm,\nthe required one-sided alpha,\nand the super-superiority margin or non-inferiority margin (if any).\n\nGiven these criteria, FACTS calculates the predicted probability of success of the subsequent trial for that endpoint assuming the estimate of the response on that endpoint, integrated over the uncertainty in those estimates. The conventional expected power of the specified future trial is calculated for the treatment effect in each MCMC sample and then averaged. This predicted probability of success can then be used in the stopping criteria and final evaluation criteria for the current trial.\nThis QOI is an extension of the “probability of success in phase 3” in earlier versions of FACTS – the difference now is that multiple different possible future trials can be specified in different QOIs and used for different decisions.\n\n\n\n\n\n\nFigure 8: Add predictive probability of future trial non-inferiority.\n\n\n\nThis predictive probability has the following parameters that must be specified:\n\nWhether the future trial will be for Superiority or Non-inferiority.\nThe size of the future trial in terms of the number of subjects on each arm.\nThe (one sided) alpha level that will be used to determine the significance of the trial.\nThe Super-superiority margin (if any) or the non-inferiority margin. The default p-value delta is not used for this QOI it is specified as part of the QOI and can be different from the default.\n\nAs with all QOIs, the future trial predictive probability QOI will be given an alternative shorter name that can be used when accessing the output files from other software such as R.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still calculate the predictive probability of the future trial being a success based on the estimated posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\n\nConditional Power\nConditional power is currently available for Continuous and Dichotomous endpoints in Core and Staged Designs.\nWhen creating a Conditional Power QOI, the first specification that must be made is whether the conditional power should be calculated for the current trial or for a hypothetical future trial. The two selections are fundamentally different.\nThe Current Trial conditional power takes the information collected in the trial so far, and assumes that for the remainder of the current trial the data collected has an effect equal to the point estimate of the already collected data. The previously collected trial data is then combined with hypothetical future data and weighted based on the amount of information that has been and will be observed by the end of the trial. The result is the probability that the current trial will reject the null hypothesis at a provided alpha level under the assumption that the future patients are generated from a population having a treatment effect exactly equal to the past data’s test statistic.\nThe Future Trial conditional power also uses the information accrued in the trial so far to calculate the assumed treatment effect moving forward, but assumes that a new study would be started that does not use the data from the current trial in its analysis. The new study is assumed to be a 1:1 randomized study if there is a control arm, and a single arm study if no control, with a sample size per arm, objective, and alpha level specified at the time of QOI creation.\nIf a conditional power is ever calculated for an arm with no final response data or against a control arm with no final response data, the conditional power will be called missing (-9999 in FACTS).\n\nCurrent Trial Conditional Power\nWhen creating a current trial conditional power, the missingness strategy, multiplicity adjustment, final sample size, and alpha level for the significance test must be specified.\n\n\n\n\n\n\nFigure 9: Add a conditional power of the current trial QOI.\n\n\n\n\nHandle missingness using:\nMissingness handling for a continuous endpoint can be specified as:\n\nIgnore: subjects that are known dropouts are not included in the current or final sample size, and are not used to estimate the treatment effect.\nLast Observation Carried Forward (LOCF): subjects that are known dropouts have their last observed endpoint value assumed to be their final endpoint. Subjects with no early observed data are ignored. Subjects that have data to carry forward are considered complete, and are used to estimate the current treatment effect.\nBaseline Observation Carried Forward (BOCF): subjects that are known dropouts have their baseline value assumed to be their final endpoint. They are included in the current information as complete subjects and are used to estimate the current treatment effect. BOCF is only available for the continuous endpoint and when simulating baseline for subjects.\nFailure: subjects that are known dropouts are assumed to have a negative outcome at their final visit. If responses are good, the subject is a non-response, and if responses are bad, the subject is a responder. Failure is only available for dichotomous endpoints.\n\n\n\nTest Type\nThe test type can either be Unadjusted or Bonferroni. An unadjusted test will always use the specified alpha level in the significance test. The Bonferroni adjustment will divide the specified alpha value by the number of non-control arms enrolling in the study in the enrolment period leading up to the current analysis time.\n\n\nSample Size:\nThe current trial conditional power can be calculated at two different future time points.\n\nCurrent Enrollment: That no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nTrial Maximum: That the trial would continue recruiting subjects using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until they have the opportunity to complete.\n\n\n\nOne-sided Alpha\nThe threshold that the current trial would have to be less than in order to be considered a success. The success/futility values specified in the design tab are not used in the conditional power calculation. This value may be adjusted by the “Test type:” input.\n\n\nSuper-Superiority (Non-inferiority) margin for p-value:\nThis value cannot be changed on the QOI pop-up. It’s instead modified at the bottom of the QOI page. The objective of the current trial cannot be different for each individual current trial conditional power QOIs.\n\n\nAdditional Notes\nCurrently, conditional power will always assume that there is no correction applied to combine the different p-values from the analysis timepoints, i.e. no combination test is used.\nThe conditional power of the current trial at the maximum sample size is only available if the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation or deterministic allocation.\nConditional power for the current trial is calculated\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\n\n\n\nFuture Trial Conditional Power\nConditional power of a future trial is reduced to a power calculation for a future trial, given the current frequentist estimates of treatment effect and standard deviation and a set of assumptions for the future trial, such as sample sizes, hypothesis to be tested, and significance levels to be used.\nThe test type of the future trial can be set to Superiority or Non-inferiority. If the superiority or non-inferiority margin is set to 0, then both types of tests simplify to traditional tests of superiority with no margin. If a non-zero margin is set, then the future trial must meet the success criteria indicated by the test type and the margin. The margin cannot be negative, but a super superiority test with a margin of -0.5 is equivalent to a non-inferiority test with a margin of 0.5.\nThe subjects per arm dictates how many subjects would be enrolled on each arm in the future study. If there is a control arm in the current trial, then the future trial is assumed to be randomized 1:1 between the active arm and the control. If there is no control arm in the current study, then the future trial is a single arm trial testing against the performance goal specified in the Freq. Comparison Response box in the QOI creation pop-up.\nThe One-sided Alpha is the significance level of the final analysis test of the future trial. The future trial is assumed to have 1 active arm, so there are no adjustments to the alpha level available.\nThe superiority margin or non-inferiority margin indicate the amount that the active arm must be better/not worse than the control arm by. If there is no control arm then this is the Freq Comparison response, which is the value that the active arms must be significantly better than in order to be declared a success. The frequentist margin for p-value QOIs on the QOI tab is not used for Future conditional power calculations – the future trial can have a different objective than the current trial.\n\n\n\n\n\n\nFigure 10: Add a conditional power of a future trial QOI.\n\n\n\n\n\nTechnical Aspects of Conditional Power Calculations\nThe conditional power calculations in FACTS are all calculated similarly to Jennison and Turnbull (2000).\nFor continuous endpoint conditional power calculations, the conditional power of the future trial is calculated using a pooled standard deviation from all arms. For dichotomous conditional power calculations, each arm has its own standard deviation based on the MLE estimate of its proportion. These decision result in the conditional power tests matching how standard p-value QOIs are calculated for continuous and dichotomous endpoints.\nThe following sections will provide formulae to calculate the conditional power in the case where there is a control arm. Simplifications for when the arm is being compared to a fixed value rather than a control arm are trivial: information fractions are only a function of the active arm standard deviation and the test statistic does not have a second sample mean subtracted from the active arm.\nThe value of \\(\\delta\\), which can be a non-inferiority margin or a super superiority margin, is always positive coming out of FACTS. To make the math more concise we can define a couple of coefficients for the \\(\\delta\\) term that allow it to be used without re-writing formulas for different hypothesis test setups. If high values of the endpoint are good, then \\(s_1 = 1\\), and if low values of the endpoint are good, then \\(s_1=−1\\). If the specified \\(\\delta\\) is a non-inferiority margin, then \\(s_2 = 1\\), and if it’s a super superiority margin then \\(s_2=-1\\).\n\nContinuous Conditional Power for the Current Trial\nLet t be the interim index that the conditional power is being computed at, and T be the time of the analysis that the conditional power is being computed for. Then \\(Z_k\\) is the test statistic of the data collected up to the current interim analysis in the study, \\(I_k\\) is the information level at the time of the interim analysis, and \\(I_K\\) is the information level at the end of the study that the conditional power is being calculated for.\nLet arm 1 be the control and arm 2 be the active arm, \\(\\bar{x_{it}}\\) be the sample mean of arm \\(i\\) at time \\(t\\), \\(\\widehat{\\sigma_{i}^{2}}\\) be the sample variance of arm \\(i\\) at time \\(t\\), \\(n_{it}\\) be the number of subjects with complete known final data on arm \\(i\\) at interim analysis \\(t\\), and \\(n_{iT}\\) be the number of subjects with complete known final data on arm \\(i\\) at the time that conditional power is being calculated for. The pooled variance estimate is \\(\\widehat{\\sigma^{2}} = \\sum_{d = 1}^{D}\\widehat{\\frac{\\sigma_{d}^{2}}{n_{dt}}}\\) where D is the total number of arms in the study.\nThen,\n\\[I_{t} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1t}} + \\widehat{\\frac{\\sigma^{2}}{n_{2t}}} \\right)^{-1}\\]\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1T}} + \\widehat{\\frac{\\sigma^{2}}{n_{2T}}} \\right)^{-1}\\]\n\\[Z_{t} = \\left( {\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{1}s_{2}\\delta \\right)\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the non-inferiority or super superiority margin.\nThen for a one-sided alpha level of \\(\\alpha\\), let \\(Z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Continuous Conditional Power for a Future Trial\nMost of the future trial conditional power calculation is the same as the above current trial conditional power. There are some simplifications.\n\\({\\overline{x}}_{it}\\) and \\(\\widehat{\\sigma_{i}^{2}}\\) are the same as in the current conditional power calculation. \\(I_t\\), the weight of the current trial Z-score, is set to 0. \\(I_T\\) is now the information at the end of the future trial, and is calculated as:\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{T}} + \\widehat{\\frac{\\sigma^{2}}{n_{T}}} \\right)^{- 1}\\]\nwhere \\(n_T\\) is the sample size per arm in the future trial and again \\(\\widehat{\\sigma^{2}}\\) is the pooled variance.\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for the Current Trial\nThe dichotomous conditional power calculations are similar in spirit to the continuous conditional power calculations. One substantial difference is in how the dichotomous conditional power tests handle a non-inferiority or super superiority margin, \\(\\delta\\). The method of calculating an appropriate test statistic for a frequentist hypothesis test is not obvious when the null scenario has a non-zero \\(\\delta\\).\nWhen there is no margin, the estimate for each treatment is simply based on the observed response proportion \\(\\widehat{p_{i}}\\) for arm \\(i\\), and the test statistic for a comparison of the control arm, \\(c\\), with dose \\(d\\) is the usual Wald test\n\\[Z_{d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}}}{\\sqrt{\\frac{\\widehat{p_{d}}(1 - \\widehat{p_{d}})}{n_{d}} + \\frac{\\widehat{p_{c}}(1 - \\widehat{p_{c}})}{n_{c}}}}\\]\nWhen there is a margin, FACTS uses the Farrington-Manning Likelihood Score test statistic to estimate quantities \\(\\widetilde{p_{d}}\\) and \\(\\widetilde{p_{c}}\\) based on the MLEs of the arm proportions governed by the constraint that \\(\\widetilde{p_{d}} - \\widetilde{p_{c}} = - s_{1}s_{2}\\delta\\). These constrained MLE estimates are used in the standard error of the test statistic. The FM test statistic is then,\n\\[Z_{FM,d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}} + s_{1}s_{2}\\delta}{\\sqrt{\\frac{\\widetilde{p_{d}}(1 - \\widetilde{p_{d}})}{n_{d}} + \\frac{\\widetilde{p_{c}}(1 - \\widetilde{p_{c}})}{n_{c}}}}\\]\nSee the PASS documentation (NCSS 2025) or the SAS documentation (SAS Institute 2016) for a complete description of the calculations that go into the FM test. In the SAS documentation, note that the FM test is the same as the Miettinen-Nurminen test without including the \\(\\frac{n}{n - 1}\\) variance correction. The FM test was used rather than the MN test because as \\(\\delta \\rightarrow 0\\), the FM test converges to the simple Wald test.\nOnce the test statistic has been resolved, the dichotomous conditional power for the current trial is calculated as follows. For calculating the conditional power for arm 2 compared to the control arm, called arm 1 without loss of generality, let \\(I_t\\) be the current information amount and \\(I_T\\) be the amount of information that the conditional power is being calculated for. Then,\n\\[I_{t} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1t}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2t}} \\right)^{- 1}\\]\n\\[I_{T} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1T}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2T}} \\right)^{- 1}\\]\n\\[Z_{t} = \\left( \\widehat{p_{2}} - \\widehat{p_{1}} + s_{1}s_{2}\\delta \\right)*\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the super superiority or non-inferiority margin, and \\(n_{1t}\\) and \\(n_{2t}\\) are current number of completers on the control and active arm, and \\(n_{1T}\\) and \\(n_{2T}\\) are the number of completers that will be on the control and active arm at time that the conditional power is being calculated for. Additionally, if there is no non-inferiority or super superiority margin \\(\\delta\\), then all \\(\\widetilde{p_{*}}\\) values are equal to their corresponding \\(\\widehat{p_{*}}\\) values.\nFor a one-sided alpha level of \\(\\alpha\\), let \\(z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for a Future Trial\nMost of the calculation quantities for the future trial conditional power are the same as the current trial conditional power. The only difference is that for the future trial conditional power we discard the influence of the current trial test statistic on the final analysis, so \\(I_t=0\\). Then the conditional power calculations become:\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/index.html#p-values",
    "href": "documentation/v71/userguides/core/qois/index.html#p-values",
    "title": "Quantities of Interest",
    "section": "P-values",
    "text": "P-values\nA p-value QOI simply allows the user to specify which test type to be used (unadjusted, Bonferroni, Dunnett’s (Dunnett 1955), or Trend Test), and how missing data is to be handled (ignored, LOCF,  BOCF Only if continuous endpoint and baseline is being simulated. , and  missing is failure Dichotomous endpoint only. ). If a control arm is present, p-values are comparisons against the control arm. If there is no control arm present, p-value QOIs are comparisons against a fixed response or fixed rate that is specified at the bottom of the QOI tab (it cannot be set differently for separate QOIs).\nNote that for dichotomous endpoints p-value are calculated using the Test of Proportions, this test statistic asymptotically approaches a normal distribution, and at least  5 success and 5 failures Sometimes this is said to be 10 and 10, or 15 and 15. Your rule of thumb can be based on your comfort level for allowing CLT to kick in.  should be observed for this to be reasonable.\nThe p-values are calculated including the default frequentist delta that has been specified at the bottom of the screen. The delta margin cannot be modified as part of the QOI, the value of the delta and nature of the test is the same for all the p-value QOIs.\nIn a TTE design with a predictor, the p-values are only calculated for the final event endpoint, not the predictors.\n\n\n\n\n\n\nFigure 11: Add a p-value QOI.\n\n\n\n\nP-values when there is no control arm\nIf there is no control arm (not currently an option in Time-to-Event), the p-value delta is instead interpreted as the objective response or response rate to compare to. The option to set the trial type is also disabled (“Superiority” is selected and the “non-inferiority option is greyed out). For a non-inferiority trial comparing to an objective response or response rate simply use the objective rate minus the non-inferiority delta (if a higher response is better) or the objective rate plus the non-inferiority delta (if a lower response is better).\nIt is currently only possible to have one objective rate to compare against.\nThe same objective rate will be used for the target p-value test in the predictive probabilities.\n\n\n\n\n\n\nFigure 12: The Core Dichotomous engine QOI tab when no control arm is included.\n\n\n\n\n\nFisher-Exact Test\nWhen specifying the QOIs for a dichotomous endpoint in a trial with a control arm, the bottom of the QOI tab allows the user to specify what statistical test to use when calculating p-values. Options are “Normal approximation” (default) and “Fisher exact test”. “Normal approximation” uses a t-test and should yield similar results to a Chi-Square test without continuity correction, while “Fisher exact test” uses an exact Fisher’s exact test for 2x2 tables.\nIf “Fisher exact test” is chosen as the test type, all p-value QOIs will use a Fisher’s exact test, except for future trial predictive probabilities, which will still use a t-test.\nIf “Fisher exact test” is chosen as the test type, only “Bonferroni” and “Unadjusted” are available as multiplicity corrections and any QOIs previously created using different multiplicity corrections will be deleted.\n“Fisher exact test” is not available for non-inferiority comparisons.\n\n\n\n\n\n\nFigure 13: The Core Dichotomous engine QOI tab showing the Fisher exact test selected on the bottom.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/index.html#target-doses",
    "href": "documentation/v71/userguides/core/qois/index.html#target-doses",
    "title": "Quantities of Interest",
    "section": "Target Doses",
    "text": "Target Doses\nThe target dose QOIs are Bayesian posterior probabilities based on MCMC sampling of how often each dose meets the specified target criteria. There are 3 types of target specifiable\n\nMax – the dose with the maximum response, this has no parameters to specify, so is available as a pre-defined default QOI.\nMED – a Minimum Effective Dose, the lowest dose that has a response better than an absolute or relative target.\nEDq – an effective dose, the dose that achieves a specified proportion (quantile \\(q\\)) of the maximum improvement in response relative to an absolute value or the performance of a specific treatment arm.\n\n\n\n\n\n\n\nFigure 14: Add a probability of being target QOI.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/index.html#decision-quantities",
    "href": "documentation/v71/userguides/core/qois/index.html#decision-quantities",
    "title": "Quantities of Interest",
    "section": "Decision Quantities",
    "text": "Decision Quantities\nThe QOIs described so far have defined values to be calculated across all doses. For a Success/Futility decision to be made it is necessary to specify the treatment arm whose QOI value is to be used in comparison to the success and/or futility criteria. This selection can be done by specifying a specific arm, but normally it is specified by using a “Probability of being Target” selection criteria. A simple and common example is “The probability of the response being better than that of the Control of the Treatment arm with the maximum response”:\n\n\n\n\n\n\nFigure 15: Add a decision quantity QOI for the posterior probability of benefit over the control arm.\n\n\n\n\n\n\n\n\n\nFigure 16: Add a decision quantity QOI for the minimum effective dose.\n\n\n\nA decision QOI consists of (1) a QOI that has been calculated for each dose, and (2) a method of choosing a dose to use the QOI value of. Choosing the dose can be done either by using a target Dose QOI like Pr(Max), Pr(EDq…), etc, by choosing the dose with the highest or lowest value of a QOI, or by explicitly choosing a dose level in advance.\nAs an example using a target QOI, you can imagine evaluating a decision QOI that is specified to choose the probability of being better than Control by 2 units Pr(\\(\\theta_d - \\theta_0 &gt; 2\\)) based on the arm with the highest ED90 EDq relative to control; Quantile 0.9.\nInstead of a Target Dose QOI, it is also possible to specify a specific dose, or to use the terms “Max probability over all doses” and “Min probability over all doses” which mean that the dose with the minimum or maximum value of the per dose QOI is used. This enables:\n\nDecisions QOIs testing specific doses, for example by defining a Posterior probability QOI that compares all doses against the lowest dose, and then a Decision QOI using that posterior probability with the highest dose selected as the target dose it is possible to based decisions on the posterior probability that the highest dose is better than the lowest dose.\nA Decision QOI using “Max probability over all doses” allows a decision to be based on whether a probability or p-value is greater than a threshold at any dose, or less than a threshold at all doses.\nA Decision QOI using “Min probability over all doses” allows a decision to be based on whether a probability or p-value is less than a threshold for any dose or greater than a threshold at all doses.\n\nThere is one special case: if the QOI chosen is the Trend Test p-value, then because this is not actually a per-dose value, only one “Target dose” can be selected: “Overall Significance”.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois/index.html#standard-evaluation-variables",
    "href": "documentation/v71/userguides/core/qois/index.html#standard-evaluation-variables",
    "title": "Quantities of Interest",
    "section": "Standard Evaluation Variables",
    "text": "Standard Evaluation Variables\nThese 2 parameters are used across some of the default QOIs and hence specified outside the normal QOI dialog boxes.\n\nThe CSD value\nand whether absolute or relative to the Control arm\n\nthese are used in both the default “Pr(CSD)” and the “MED relative to Control” QOIs.\n\n\n\n\n\n\nFigure 17: Standard Evaluation Variables for specifying the clinically significant difference (CSD) and the default QOI comparison type.\n\n\n\nNote that the CSD value here is designed to be usually entered as a positive value, as in earlier versions of FACTS. Its sign is automatically adjusted if “lower score means subject improvement” or if the trial is a non-inferiority trial.\nThese adjustments are not made for other user entered QOIs. The directions of comparison both for the definition of the probability to be calculated and the comparison of the resulting probability with a threshold are under the user’s control, as are whether delta’s are negative or positive. This allows the user to define QOIs in whatever fashion is natural to them and their team.\n\nThe direction of comparison for default QOIs\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the NIM must always be a positive value. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD will need to be subtracted from the control score before comparing with the estimate of response on a treatment arm).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Phase II & III Designs",
      "Quantities of Interest"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html",
    "href": "documentation/v71/userguides/flfll.html",
    "title": "FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems.\n\n\n\nFLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#purpose-and-scope-of-this-document",
    "href": "documentation/v71/userguides/flfll.html#purpose-and-scope-of-this-document",
    "title": "FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#overview",
    "href": "documentation/v71/userguides/flfll.html#overview",
    "title": "FLFLL",
    "section": "",
    "text": "FLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#linux",
    "href": "documentation/v71/userguides/flfll.html#linux",
    "title": "FLFLL",
    "section": "Linux",
    "text": "Linux\n\nInstall Mono version 6.12 or later from https://www.mono-project.com/docs/about-mono/releases onto the target machine/server running FLFLL and ensure that all users of FLFLL can run Mono. A simple test would be to ask FLFLL users to run “mono –version”.\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” (via Mono) present in the application folder.\nRetrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server.\nWithin the FLFLL application folder, go to the “bin” folder (which contains the Linux engines used by FLFLL) and elevate the Linux engine permissions to being executable by running “chmod +x [name of linux engine executable here]” for each of the Linux engines. If you do not have permission to do so, please ask your IT administrator to run this command.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#windows",
    "href": "documentation/v71/userguides/flfll.html#windows",
    "title": "FLFLL",
    "section": "Windows",
    "text": "Windows\n\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL. We recommend using 7Zip (https://www.7-zip.org/) to perform the unzipping rather than the Windows in-built unzipping tool: the latter can result in the corruption of the FLFLL application as a security precaution.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” present in the application folder.\nIf the machine/server running FLFLL already has a licensed version of FACTS installed on it, the following step can be skipped. Otherwise, retrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#linux-1",
    "href": "documentation/v71/userguides/flfll.html#linux-1",
    "title": "FLFLL",
    "section": "Linux",
    "text": "Linux\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\nmono \"FLFLL.exe\" -file \"home/mono/FLFLL/Input/myfile.facts\" -nSim 1\n-seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n\"home/mono/FLFLL/Output\" -logPath “/home/mono/Log”\nRun FLFLL to generate parameter files only for multiple FACTS project files in a single directory:\nmono \"FLFLL.exe\" -file \"home/mono/FLFLL/Input \" -nSim 1 -seed 3500\n-nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n\"home/mono/FLFLL/Output\" -logPath “/home/mono/Log”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS files older than FACTS 6.2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#windows-1",
    "href": "documentation/v71/userguides/flfll.html#windows-1",
    "title": "FLFLL",
    "section": "Windows",
    "text": "Windows\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\nFLFLL.exe -file “C:\\MyDocuments\\FLFLL\\Input\\myfile.facts” -nSim 1\n-seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n“C:\\MyDocuments\\FLFLL\\Output\" -logPath “C:\\MyDocuments\\Log”\nRun FLFLL to generate parameter files only for multiple FACTS project files:\nFLFLL.exe -file “C:\\MyDocuments\\FLFLL\\Input” -nSim 1 -seed 3500\n-nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n“C:\\MyDocuments\\FLFLL\\Output\" -logPath “C:\\MyDocuments\\Log”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS file older than FACTS 6.2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#overview-1",
    "href": "documentation/v71/userguides/flfll.html#overview-1",
    "title": "FLFLL",
    "section": "Overview",
    "text": "Overview\nUsage: FLFLL.exe [options] where [options] are:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n-[h|help]\nDisplay the help menu. Default (False).\n\n\n-[f|file]\nSpecifies the file or top-level directory to open.\n\n\n-[n|nSim]\nNumber of simulations to run. Default (5).\n\n\n-[p|packet]\nPacket size for parallelization. Default (1000).\n\n\n-[g|grid]\nFlag indicating sims to run on grid. Default (False).\n\n\n-[a|agg]\nAggregation mode. Default (None).\n\n\n-[aggPrefix]\nPrefix for aggregated files. Default (agg).\n\n\n-[nBurn]\nNumber of MCMC burn-in iterations. Default (1000).\n\n\n-[nMCMC]\nNumber of MCMC sample iterations. Default (2500).\n\n\n-[nWeeksFiles]\nNumber of weeks files to generate. Default (100).\n\n\n-[nSubjectFiles]\nNumber of subjects files to generate. Default (1).\n\n\n-[nMCMCFiles]\nNumber of MCMC output files to generate. Default (0).\n\n\n-[nMCMCThin]\nMCMC thinning parameter. Default (0).\n\n\n-[nMCMCImpute]\nMCMC length per imputation parameter. Default (1).\n\n\n-[seed]\nSet the random number seed. Default (3500).\n\n\n-[logPath]\nIf provided, specifies a directory where a log file is generated.\n\n\n-[outputPath]\nSpecifies the directory where output will be generated. Default (“out”).\n\n\n-[endToEndRun]\nFlag indicating if simulations should be run.\n\n\n-[skipMissingParamsCheck]\nFlag indicating to skip checking for missing parameters.\n\n\n-[scenarios]\nFlag indicating which scenarios should be processed.\n\n\n-[useDifferentSeedPerScenario]\nFlag indicating whether to use a different seed for each simulated scenario. Default (False).\n\n\n-[useDifferentSeedPerDesign]\nFlag indicating whether to use a different seed for each simulated design. Default (False).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#arguments",
    "href": "documentation/v71/userguides/flfll.html#arguments",
    "title": "FLFLL",
    "section": "Arguments",
    "text": "Arguments\n\n–[h | help] (Help)\nThe –h command line option displays the command line help options in the terminal. No simulations will be performed when the –h option is specified.\n\n\n–[v | version] (Version)\nThe –v command line option displays the FLFLL version in the terminal. No simulations will be performed when the –v option is specified.\n\n\n–[f | file] FILE (Run a specific .facts file)\nThe –f command line option is used to specify the “.facts” file to process. The –f option must be followed by a valid path to an existing “.facts” file or directory containing one or more “.facts” files. Hint: Remember to use quotes around the path if it includes spaces.\nIf the supplied file name is a directory, then FACTS will process each “.facts” file in the directory in turn. As this only starts FACTS once, this can be quite a bit quicker than using a batch file or script that loops and starts FACTS separately for each “.facts” file.\nFurthermore, if the supplied file name is a directory then FACTS also automatically recurses through every sub-directory processing every “.facts” file it finds.\n\n\n–[n | nSim] N (Run N simulations for each scenario)\nThe –n command line option is used to specify the number of simulations to run. The –n option must be followed by an integer value greater than 0. For each scenario in the FACTS project file, the application will run N simulations. Defaults to 5 if unspecified.\n\n\n–[p | packet] N (Set the packet size for simulations)\nThe –p command line option is used to specify the packet size for parallelization of simulations. The –p option must be followed by an integer value greater than 0. When using the –g option to run on a grid, or when running on a multicore machine it is often beneficial to parallelize simulations using the packetization process (see grid documentation for more information on packetization). The packet size must be greater than zero, but as in the GUI, there is no restriction that it be less than the number of simulations. If it is greater than the number of simulations, the simulations will not be packetized. Defaults to 1000 if unspecified.\n\n\n–[g | grid] (Run on grid)\nThe –g command line option instructs the application to send simulations to the grid (assumes that the grid is correctly configured). When running on the grid, the action is still performed synchronously (i.e. FACTS will wait while the simulations run and collect the results before exiting). This option is useful to parallelize long running simulations more than they can be parallelized locally. Defaults to run locally if unspecified.\n\n\n–[a | agg] Mode (Aggregation Mode)\nThe –a command line option specifies the aggregation action to take for completed simulation results. The available modes for this option are:\n\nNone – no aggregation will be performed\nNoPivot – Only standard aggregation will be performed\nPivot – Both standard and pivoted aggregation will be performed.\nDefault, if unspecified, is None.\n\n\n\n–aggPrefix prefix (Prefix for aggregation files)\nThe –aggPrefix command line option specifies the prefix to use when naming aggregated files and must be followed by a valid file prefix. This option is only used when –a is set to NoPivot or Pivot. The aggregation files produced by aggregating across all scenarios will be named using the prefix&lt;_pivot&gt;_(filename).csv pattern, where &lt;_pivot&gt; is included for pivoted files only, and (filename) is replaced by the name of the file being aggregated. Defaults to “agg” if unspecified.\n\n\n–nBurn N (Number of MCMC burn-in iteractions)\nThe –nBurn command line option specifies the number of burn-in MCMC iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 1000 if unspecified.\n\n\n–nMCMC N (Number of MCMC sample iterations)\nThe –nMCMC command line option specifies the number of MCMC sampling iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 2500 if unspecified.\n\n\n–nWeeksFiles N (Number of weeks files to output)\nThe –nWeeksFiles command line option specifies the number of weeks files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 100 if unspecified.\n\n\n–nSubjectFiles N (Number of subjects files to output)\nThe –nSubjectFiles command line option specifies the number of subject files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 1 if unspecified.\n\n\n–nMCMCFiles N (Number of MCMC files to output)\nThe –nMCMCFiles command line option specifies the number of MCMC sample files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 0 if unspecified.\nNote: This option potentially produces a very large amount of output data and may fail if sufficient disk space is not available.\n\n\n–nMCMCThin N (MCMC output thinning value)\nThe –nMCMCThin command line option specifies the MCMC thinning value to apply to the MCMC output and must be followed by a valid integer value at least &lt;x&gt;. The thinning parameter applies only to the MCMC output, all MCMC samples are used for analysis. Defaults to &lt;x&gt; if unspecified.\n\n\n–nMCMCImpute N (MCMC length per imputation value)\nThe –nMCMCImpute command line option specifies the number of MCMC sampling iterations to use in the simulation for each imputation. Defaults to 1 if unspecified.\n\n\n–seed (Random number seed)\nThe –seed command line option sets the random number generator seed value. The default value (3500) is the same as that used in the GUI. It can be set to any positive integer.\n\n\n-logPath Path (Path where the optional log file is placed)\nThe –logPath command line option (if given) is used to specify the directory where a log file is generated. If not specified, a log file will not be generated. The –logPath option must be followed by a valid path. If the path does not exist, it will be created.\nNote: Remember to use quotes around the path if it includes spaces.\n\n\n-outputPath Path (Path where output is generated)\nThe –outputPath command line option (if given) is used to specify the directory where parameter files and optional simulation files are placed. If not specified, the output path will be the directory in which the .facts file is present. The –outputPath option must be followed by a valid path. If the path does not exist, it will be created.\n\n\n-endToEndRun (Flag to indicate if simulations should be run also)\nThe –endToEndRun command line instructs FLFLL to run full simulations. If unspecified, only parameter files will be generated.\n\n\n-skipMissingParamsCheck (Flag to indicate to skip checking of missing parameters)\nThe –skipMissingParamsCheck command line instructs FLFLL to skip the process of checking for missing parameters in legacy FACTS project files (.facts). If not specified, when running FACTS projects files prior to version 6.2, errors will prevent FLFLL from completing.\n\n\n-scenarios (Flag indicating which scenarios should be processed)\nThe –scenarios command line instructs FLFLL to run the specified scenarios by name. The names of the scenarios to run should be provided as a comma separated list. If not specified, all scenarios will be processed.\n\n\n-useDifferentSeedPerScenario (Flag indicating whether to use a different seed for each simulated scenario)\nThe –useDifferentSeedPerScenario command line option provides FLFLL with an option to set a different random number seed for each of the scenarios that are being simulated. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated scenario and then adds 1111 to the random number seed of any additional scenarios being simulated. For example, if three scenarios are being simulated and the -seed flag is set to 3500, the first scenario will have a random number seed of 3500, the second scenario a random number of 4611 and the third scenario a random number seed of 5722. This deterministic way of setting different random number seeds allows for reproducible simulation results.\n\n\n-useDifferentSeedPerDesign (Flag indicating whether to use a different seed for each simulated design)\nThe –useDifferentSeedPerDesign command line option provides FLFLL with an option to set a different base random number seed for each of the designs that are being simulated. This option is only used when a directory containing multiple FACTS designs is passed as an argument to the FLFLL command line, rather than a single design. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated design and then adds 1234 to the random number seed of any additional designs being simulated. For example, if three designs are being simulated and the -seed flag is set to 3500, the first design will have a base random number seed of 3500, the second scenario a random number of 4734 and the third design a base random number seed of 5968. This deterministic way of setting different random number seeds allows for reproducible simulation results.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html",
    "href": "documentation/v71/userguides/staged.html",
    "title": "Seamless Trial User Guide",
    "section": "",
    "text": "FACTS seamless designs, simulated using the Staged Design Engine, is for simulating trials where there are two stages. The first stage can be a full adaptive trial, the second stage can be a full adaptive trial, and there is a transition that allows for specifying how to transfer from the first stage to the second.\nFACTS Staged Design can thus be used to simulate:\nFACTS Staged Design can simulate these successive “stages” as two separate and distinct trials, as a single trial with a significant adaptation at some point, or somewhere on the spectrum between these two where two stages of clinical development are linked sequentially.\nThe structure of a Staged Design trial is very similar to that of a FACTS Core trial. If you are not familiar with using FACTS Core you should read the FACTS Core User Guide and acquaint yourself with some FACTS Core tutorials. In fact it is recommended you first design the first stage in FACTS Core, then import it into FACTS Staged Design.\nThe principal difference from a simple Core design is that a Staged Design has, or can have:\nAs a result, Staged Designs have more output than a Core Design – there are outputs for both the first and second stage. There can be two sets of outputs for the first stage – one at the time of the arm selection for Stage 2 (or the decision not to graduate), and one at the time of the complete data for the first stage. This is necessary because Staged Designs encompass a range of settings: from simulating what are essentially two separate trials, where the second depends on the first; to simulating a single trial with an interim where there is a trimming of the number of arms being studied.\nIf the patient data files are written out for the simulation, then there are patient data files for the first stage, second stage and overall.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#study-info",
    "href": "documentation/v71/userguides/staged.html#study-info",
    "title": "Seamless Trial User Guide",
    "section": "Study Info",
    "text": "Study Info\n\nContinuous/DichotomousMultiple EndpointTime-to-Event\n\n\n\n\n\n\n\n\nFigure 2: The Staged design Study &gt; Study Info tab for a continuous endpoint.\n\n\n\n\nDesign Options\nIn a staged design the “Enable adaptive features” option can be specified separately for Stage 1 and Stage 2. If either stage is not adaptive, then tabs such as interims, early stopping criteria, and adaptive allocation options are not available in that stage.\nThe “Use Longitudinal Modeling” option works the same as in core.\nThe “Include simulation of baseline” and “Special longitudinal models” are the same as in FACTS core.\n\n\nStudy Information\n\nSimulate Stage 1 Accrual\nIn the Staged design engine, subject recruitment for Stage 1 can be simulated continuously or deterministically. If recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab. Specifying deterministically allows the user to specify a file with subject recruitment dates that will be used for all simulations. Stage 2 accrual can only be continuous.\n\n\nMaximum Sample Sizes\nIn staged design, the maximum number of subjects is more complicated than in core designs. Rather than specifying a single maximum, the maximum number of subjects can be specified by stage.\nYou must either specify the maximum number of subjects combined across both stages, or specify the maximum number of subjects allowed in both Stage 1 and Stage 2. If the maximum number of subjects for a stage is left blank, then there is no maximum for that stage and it is allowed to use all remaining subjects up to the overall maximum.\nIf the “Maximum number of subjects” is ever reached in a simulation at any stage, then accrual stops. Interim analyses may still be triggered. Existing subjects will be followed up and a final analysis will be performed.\nIf the “Maximum number of subjects Stage 1” is reached during stage 1, then accrual stops, interim analyses may still be triggered, subjects are followed up, and a decision based on the “Transition” to stage 2 is made.\nThe “Maximum number of subjects Stage 2” may either be specified as a fixed value, or can “depend on number of treatment arms in Stage 2.” If the “Maximum number of subjects Stage 2” is set, then if the number of subjects accrued in Stage 2 gets to the specified maximum accrual will stop, subjects will be followed up, and the final analysis will be performed. If the maximum number of subjects in Stage 2 is not specified explicitly, but is chosen to depend on the number of arms in stage 2, then the Stage 2 sample size maximum is specified in the Stage 2 Design &gt; Allocation tab.\n\n\n\n\n\n\nNotes on Staged Design Sample Sizes\n\n\n\nIt is possible to specify rules that allow Stage 1 to use all the allowed subjects. If Stage 2 cannot accrue any subjects and Stage 2 analysis does not include Stage 1 data then it is considered a ‘Null Stage 2’. If some or all Stage 1 data is included in the Stage 2 analysis then Stage 2 immediately completes (after any specified operational delay) with a final analysis just using the Stage 1 data included in Stage2.\nThe fact that the Stage 2 analysis will be carried out even if Stage 2 has no sample size (if Stage 1 data has been included in Stage 2) creates a useful FACTS trick using staged design. Giving Stage 2 a size of 0 and carrying all Stage 1 data into Stage 2 allows FACTS Staged Design to be used to simulate a FACTS Core design where a different analysis model is to be used for the final analysis (e.g. with a different prior or not using longitudinal modeling).\n\n\n\n\nMaximum time for Stage 1 accrual\nThe “Maximum time for Stage 1 accrual” can be set instead of or in addition to a Stage 1 maximum sample size. If this option is specified and if, at the specified time Stage 1 has not stopped accruing, then accrual stops, any specified interims continue, and, if specified, subjects are followed to completion.\nIf the user specifies both a “Maximum number of subjects in Stage 1” and a “Maximum time for Stage 1 accrual” then whichever occurs first triggers the stopping of Stage 1 accrual. If Stage 1 does not stop or graduate at an interim, then after stopping accrual, Stage 1 subjects are followed up and only when all follow-up is complete does the Stage 1 final analysis occur.\n\n\nResponse\nThe response option for continuous, dichotomous, time-to-event, and multiple endpoint are the same as in FACTS Core.\n\n\n\nSchedule of Post-Baseline Visits\nIf “Use longitudinal modeling” is selected in the Design Options section, then the visit schedule for a patient should be specified in the “Schedule of Post-Baseline Visits” section in the same way as in FACTS Core.\n\n\nStage 2 Delay\nThe value provided in this box creates an operational delay between when arms are selected for inclusion in Stage 2 and the start of accrual in Stage 2.\n\n\n\nAs in the Core engines, many of the options available for a single endpoint on the Study &gt; Study Info tab have been moved to the Study &gt; Endpoints tab in the multiple endpoints engine.\n\nDesign Options\nIn a staged design the “Enable adaptive features” option can be specified separately for Stage 1 and Stage 2. If either stage is not adaptive, then tabs such as interims, early stopping criteria, and adaptive allocation options are not available in that stage.\nThe special endpoint options are located in the Study &gt; Endpoints tab.\n\n\nStudy Information\n\nSimulate Stage 1 Accrual\nIn the Staged design engine, subject recruitment for Stage 1 can be simulated continuously or deterministically. If recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab. Specifying deterministically allows the user to specify a file with subject recruitment dates that will be used for all simulations. Stage 2 accrual can only be continuous.\n\n\nMaximum Sample Sizes\nIn staged design, the maximum number of subjects is more complicated than in core designs. Rather than specifying a single maximum, the maximum number of subjects can be specified by stage.\nYou must either specify the maximum number of subjects combined across both stages, or specify the maximum number of subjects allowed in both Stage 1 and Stage 2. If the maximum number of subjects for a stage is left blank, then there is no maximum for that stage and it is allowed to use all remaining subjects up to the overall maximum.\nIf the “Maximum number of subjects” is ever reached in a simulation at any stage, then accrual stops. Interim analyses may still be triggered. Existing subjects will be followed up and a final analysis will be performed.\nIf the “Maximum number of subjects Stage 1” is reached during stage 1, then accrual stops, interim analyses may still be triggered, subjects are followed up, and a decision based on the “Transition” to stage 2 is made.\nThe “Maximum number of subjects Stage 2” may either be specified as a fixed value, or can “depend on number of treatment arms in Stage 2.” If the “Maximum number of subjects Stage 2” is set, then if the number of subjects accrued in Stage 2 gets to the specified maximum accrual will stop, subjects will be followed up, and the final analysis will be performed. If the maximum number of subjects in Stage 2 is not specified explicitly, but is chosen to depend on the number of arms in stage 2, then the Stage 2 sample size maximum is specified in the Stage 2 Design &gt; Allocation tab.\n\n\n\n\n\n\nNotes on Staged Design Sample Sizes\n\n\n\nIt is possible to specify rules that allow Stage 1 to use all the allowed subjects. If Stage 2 cannot accrue any subjects and Stage 2 analysis does not include Stage 1 data then it is considered a ‘Null Stage 2’. If some or all Stage 1 data is included in the Stage 2 analysis then Stage 2 immediately completes (after any specified operational delay) with a final analysis just using the Stage 1 data included in Stage2.\nThe fact that the Stage 2 analysis will be carried out even if Stage 2 has no sample size (if Stage 1 data has been included in Stage 2) creates a useful FACTS trick using staged design. Giving Stage 2 a size of 0 and carrying all Stage 1 data into Stage 2 allows FACTS Staged Design to be used to simulate a FACTS Core design where a different analysis model is to be used for the final analysis (e.g. with a different prior or not using longitudinal modeling).\n\n\n\n\nMaximum time for Stage 1 accrual\nThe “Maximum time for Stage 1 accrual” can be set instead of or in addition to a Stage 1 maximum sample size. If this option is specified and if, at the specified time Stage 1 has not stopped accruing, then accrual stops, any specified interims continue, and, if specified, subjects are followed to completion.\nIf the user specifies both a “Maximum number of subjects in Stage 1” and a “Maximum time for Stage 1 accrual” then whichever occurs first triggers the stopping of Stage 1 accrual. If Stage 1 does not stop or graduate at an interim, then after stopping accrual, Stage 1 subjects are followed up and only when all follow-up is complete does the Stage 1 final analysis occur.\n\n\n\nOverall Schedule of Post-Baseline Visits\nAs in the FACTS Core Multiple Endpoint Study Info tab, the “Overall Schedule of Post-Baseline Visits” tab creates the set of possible visits for observation of any of the endpoints. Each endpoint can be observed only at specific visits based on inputs in the Study &gt; Endpoints tab.\n\n\nStage 2 Delay\nThe value provided in this box creates an operational delay between when arms are selected for inclusion in Stage 2 and the start of accrual in Stage 2.\n\n\n\n\n\n\n\n\n\nFigure 3: The Staged design Study &gt; Study Info tab for a time-to-event endpoint.\n\n\n\n\nDesign Options\nThe staged design the “Enable adaptive features” option can be specified separately for Stage 1 and Stage 2. If either stage is not adaptive, then tabs such as interims, early stopping criteria, and adaptive allocation options are not available in that stage.\nThe “Enable predictor modeling” option in Staged TTE trials works the same as the Core predictor specification.\n\n\nStudy Information\n\nStudy and Events\n\nMaximum Sample Sizes\nIn staged design, the maximum number of subjects is more complicated than in core designs. Rather than specifying a single maximum, the maximum number of subjects can be specified by stage.\nYou must either specify the maximum number of subjects combined across both stages, or specify the maximum number of subjects allowed in both Stage 1 and Stage 2. If the maximum number of subjects for a stage is left blank, then there is no maximum for that stage and it is allowed to use all remaining subjects up to the overall maximum.\nIf the “Maximum number of subjects” is ever reached in a simulation at any stage, then accrual stops. Interim analyses may still be triggered. Existing subjects will be followed up and a final analysis will be performed.\nIf the “Maximum number of subjects Stage 1” is reached during stage 1, then accrual stops, interim analyses may still be triggered, subjects are followed up, and a decision based on the “Transition” to stage 2 is made.\nThe “Maximum number of subjects Stage 2” may either be specified as a fixed value, or can “depend on number of treatment arms in Stage 2.” If the “Maximum number of subjects Stage 2” is set, then if the number of subjects accrued in Stage 2 gets to the specified maximum accrual will stop, subjects will be followed up, and the final analysis will be performed. If the maximum number of subjects in Stage 2 is not specified explicitly, but is chosen to depend on the number of arms in stage 2, then the Stage 2 sample size maximum is specified in the Stage 2 Design &gt; Allocation tab.\n\n\n\n\n\n\nNotes on Staged Design Sample Sizes\n\n\n\nIt is possible to specify rules that allow Stage 1 to use all the allowed subjects. If Stage 2 cannot accrue any subjects and Stage 2 analysis does not include Stage 1 data then it is considered a ‘Null Stage 2’. If some or all Stage 1 data is included in the Stage 2 analysis then Stage 2 immediately completes (after any specified operational delay) with a final analysis just using the Stage 1 data included in Stage2.\nThe fact that the Stage 2 analysis will be carried out even if Stage 2 has no sample size (if Stage 1 data has been included in Stage 2) creates a useful FACTS trick using staged design. Giving Stage 2 a size of 0 and carrying all Stage 1 data into Stage 2 allows FACTS Staged Design to be used to simulate a FACTS Core design where a different analysis model is to be used for the final analysis (e.g. with a different prior or not using longitudinal modeling).\n\n\n\n\nMaximum Number of Events\nA Staged Design using a Time-to-Event endpoint can also define study size and stage lengths in terms of the number of events. This can be specified either by overall events, or events by stage. If there is a time-to-event predictor, then the maximum number of events can apply to the Final events or the Predictor events.\nThe event limits are specified, and work, in largely the same way as the sample size caps specified above.\nAgain, either the overall max must be set, or both a Stage 1 and a Stage 2 max must be set. If an overall max is set, Stage 1 and/or Stage 2 maxes may also be set.\nIf a cap is on events in S1, the events cap is respected for Stage 1 and the s1-patients files, but the cap on S1 events can be exceeded on S1 subjects during Stage 2. This will be reflected in the Stage 2 results, the master-patients and s2-patients data files.\nIf the cap is on overall events and there is no cap on S1, then S1 may complete when the overall event cap is reached, so there is no further data to collect in Stage 2.\n\nIf Stage 2 does not include Stage 1 data (specified on the Transition &gt; Data Inclusion tab) and there is no data to accrue in Stage 2 and it is a null stage.\nIf Stage 2 does include Stage 1 data then a Stage 2 analysis is performed immediately (after any operational delay) on the Stage 1 data. If there is an operational delay and not all S1 subjects are complete then they are followed up until the Stage 2 analysis. This is the one circumstance in which the overall event cap can be exceeded. This will be reflected in the Stage 2 results, the master-patients, and s2-patients data files.\n\n\n\n\nTiming\n\nMaximum time for Stage 1 accrual\nThe “Maximum time for Stage 1 accrual” can be set instead of or in addition to a Stage 1 maximum sample size. If this option is specified and if, at the specified time Stage 1 has not stopped accruing, then accrual stops, any specified interims continue, and, if specified, subjects are followed to completion.\nIf the user specifies both a “Maximum number of subjects in Stage 1”/“Maximum number of events Stage 1” and a “Maximum time for Stage 1 accrual” then whichever occurs first triggers the stopping of Stage 1 accrual. If Stage 1 does not stop or graduate at an interim, then after stopping accrual, Stage 1 subjects are followed up and only when all follow-up is complete does the Stage 1 final analysis occur.\n\n\nStage 1 follow-up\nMaximum subject follow-up is specified in terms of either the maximum follow-up for each subject, or the maximum follow-up beyond the full accrual of the stage.\nIf the Stage 1 “Max follow-up per subject” is selected and specified, then no subject accrued during S1 will be followed for longer than this. If Stage 1 does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event. With this option all Stage 1 subjects have the same maximum follow-up.\nIf “Follow-up after Stage 1 full accrual” is selected and specified, then if Stage 1 does not stop early or transition to stage 2 early, then it will stop the specified time after full enrolment or when every subject has had an event. With this option subjects recruited earlier may be follow-up for longer than subjects recruited later in the trial and the overall amount of exposure is maximized.\n\n\nStage 2 follow-up\nThe stage 2 follow-up rules work in the same way as the stage 1 rules. Max follow-up per subjects constrains follow-up so that every subjects has a maximum of the specified amount of exposure. The follow-up after Stage 2 full accrual option allows earlier subjects to be followed for longer, but also guarantees that ever subjects has at least a certain amount of follow-up if Stage 2 does not stop early.\nIf the maximum follow-up is specified as maximum follow-up beyond full accrual in both stages then it is also possible to specify that Stage 1 subjects are further followed into Stage 2:\n\nif this is not checked then Stage 1 subjects are not followed up after Stage 1;\nif this is checked then Stage 1 subjects included in Stage 2 are followed up using Stage 2 follow-up rules. Note that if Stage 1 completes and graduates to stage 2 at the final evaluation (after all S1 subjects have been fully followed up) then all Stage 1 subjects are complete and are not be further followed up in Stage 2, even if they have not yet had an event.\n\nMaximum follow-up is specified separately for each stage and can use different rules and durations.\n\n\n\n\n\n\n\nVisits (Time-To-Event Only)\nThe details of specifying a visit schedule for a staged time-to-event trial are identical to the same tab in the FACTS Core User Guide. The visit schedule specified here is shared across both stages.\n\n\nTreatment Arms\nTreatment arms in staged designs are specified in the same way as in the FACTS Core engine.\nIn the stage design engine it is common to make decisions leading to not randomizing subjects to particular arms in Stage 2, so it is worth noting that all treatment arms are included in analysis models in both stages of the trial regardless of whether they have been allocated to or not.\nThis results in treatment arms that are not accruing subjects having estimated QOI values. It can even lead to doses with no subjects allocated to them have a high probability of being the dose with the maximum response or being a minimally efficacious dose.\nIn Stage 2 there is an option (on the Transition &gt; Data Inclusion tab) to restrict the estimation of Target QOIs (such as Pr(Max) and Pr(MED)) to those arms that were selected for inclusion in Stage 2.\n\n\n\n\n\n\nEstimation of Dose Response models on doses without subjects\n\n\n\nNote that doses that have not been allocated to will have an estimated response based on the prior and the dose response model being used.\n\nFor dose response models with parametric relationships on the dose strength, like Hierarchical Logistic or Sigmoid/E-max models, the estimates may still be relatively precise depending on how well the curve can be estimated from the doses for which data has been gathered.\nFor dose response models that are not parameterized based on the effective doses strength, like the independent dose model, the estimate of response are likely to have high degrees of uncertainty.\nFor the complex shape models like “Plateau” and “Inverted U”, even though these are parameterized with respect to the effective dose strength, different parameters estimate the curve for different dose zones, and little information is shared across the dose zones. So, certain parameters may exibit high uncertainty depending on the doses that have not been allocated to. If only a small proportion of the doses have not been allocated to, then these models may function like the parametric response models.\n\n\n\nIf only a few doses are being allocated to in Stage 1, or there is a need to estimate the Target QOIs over the whole dose range in Stage 2. It is worth considering creating decision QOI’s based on specific doses that the design guarantees subjects have been allocated to. (E.g. if in Stage 1 subjects are only allocated to Control and the Top Dose, then create a decision QOI where the evaluation dose is explicitly the top dose for making end of Stage 1 decisions).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#variants",
    "href": "documentation/v71/userguides/staged.html#variants",
    "title": "Seamless Trial User Guide",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Each variant is a slightly different design that can be simulated on all created simulation assumption scenarios.\nFor Staged Continuous, Dichotomous, or Multiple Endpoint trials, the only design feature that can be changed is the sample size (maximum number of subjects). The overall maximum, maximum for stage 1, and maximum for stage 2 can be varied, and the same rules as on the Study &gt; Study Info tab apply in the variants.\nIn the Staged Time-to-Event engine, the maximum umber of subjects can be set, but the maximum number of events can be set as well. Again, the maximum number of events can be set overall or for one of/both stages of the design. The same rules as on the Study &gt; Study Info tab apply in the variants.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Maximum Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\n\n\n\n\n\n\nFigure 4: The Variants tab for a Continuous, Dichotomous, or Multiple Endpoint staged design.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#endpoints",
    "href": "documentation/v71/userguides/staged.html#endpoints",
    "title": "Seamless Trial User Guide",
    "section": "Endpoints",
    "text": "Endpoints\nAs with the FACTS Core Multiple Endpoint design Endpoints tab, the staged design Study &gt; Endpoints tab allows for specification of the number, type, and individual properties of the different endpoints. The details of this tab are identical to the Core design, except it is now possible to define the utility function per stage.\nThe Component utility combination method now has a drop down for each stage to specify how the endpoint utilities are to be combined to create the component utility. The Utility Function section has a Stage 1 tab and a Stage 2 tab with identical tables to enter the parameter coefficients for the endpoint utility calculation. Clicking the “Mirror Stage 1 data in Stage 2” option deactivates the Stage 2 utility table for the endpoint and will use whatever is entered on the Stage 1 utility tab for Stage 2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#accrual",
    "href": "documentation/v71/userguides/staged.html#accrual",
    "title": "Seamless Trial User Guide",
    "section": "Accrual",
    "text": "Accrual\nThe simulation of “Continuous” accrual and “Deterministic” accrual are done in the same way as in FACTS Core, but there is a little bit more to specify in the Staged design engine.\nAccrual can be specified in three ways:\n\nWith a single, common accrual profile that applies across both stages. In this situation, there is no break in recruitment between stages and regions do not ramp up a second time in the second stage. Regions’ start dates can be delayed so that they are likely to only participate in the second stage but, given the stochastic simulation of the recruitment and possibly uncertain timing of the end of the first stage if it is adaptive, under this option there is no certain linkage between the accrual rate and which stage the trial is in. This option is best used when the accrual is independent of the stage.\n\n\n\n\n\n\n\nFigure 5: Specify accrual rates with a common profile between stages.\n\n\n\n\nWith two separate profiles - one for each stage. If the profiles are similar, there is a control that can be used to copy the details of Stage 1 to Stage 2. Unlike the “mirroring” option on some of the design tabs, this does not link the Stage 2 profile to the Stage 1 profile, it simply replaces the current Stage 2 profile with a copy of the curernt Stage 1 profile.\nWith this option, the accrual in Stage 2 starts again from the beginning of the Stage 2 profile as if the start of Stage 2 is time 0 (for ramp-up, for example). All dates in the Stage 2 profile are relative to the start of Stage 2.\n\n\n\n\n\n\n\nFigure 6: Specify different accrual profiles between stages.\n\n\n\n\nIf “Determinisically” was selected for “Simulate Stage 1 Accrual:” on the Study &gt; Study Info tab, then the Stage 1 accrual gets its own tab that is exactly like the Core Design Deterministic Accrual tab for the Stage 1 accrual rate. The Stage 2 accrual profile is always Continuous, and is specified in the normal manner.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#dropout",
    "href": "documentation/v71/userguides/staged.html#dropout",
    "title": "Seamless Trial User Guide",
    "section": "Dropout",
    "text": "Dropout\nDropouts are specified in Staged Design in exactly the same manner, and with the same options as in FACTS Core Design for the same endpoint.\nNote that dropouts can either be simulated identically for both stages, or separate drop out patterns can be specified for each stage:\n\n\n\n\n\n\nFigure 7: The Execution &gt; Dropout Rate tab for a continuous endpoint, showing different dropout profiles by stage.\n\n\n\nA subject’s drop out probabilities are determined by the Stage they are recruited in, thus a subject recruited in Stage 1 but followed up in Stage 2 will continue to have the Stage 1 dropout probabilities even though Stage 2 has now started.\nIn case a complex dropout pattern has been specified (i.e. with per arm per visit probabilities) there is an option to copy the Stage 1 dropout parameters to Stage 2. This does not link the inputs, just copies the Stage 1 values into the Stage 2 entries.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#mirroring-functionality",
    "href": "documentation/v71/userguides/staged.html#mirroring-functionality",
    "title": "Seamless Trial User Guide",
    "section": "Mirroring functionality",
    "text": "Mirroring functionality\nNew checkboxes are available to the analysis model tabs: “Dose Response” and “Longitudinal Model” - to allow Stage 1 design data to be ‘mirrored’ in Stage 2. When mirrored, the Stage 2 data is ‘locked’ to that of Stage 1 – i.e. it is disabled and is synched to be an exact copy of Stage 1 data - updating automatically when this data changes. If the mirror is then switched off, the Stage 2 data becomes a copy of the Stage 1 data, but can now be edited to change it specifically for Stage 2.\nIn Staged Design with a Time-to-Event endpoint, the option to mirror the Stage 1 design in Stage 2 is also present on the TTE specific tabs: “Predictor Model &gt; Dose Response”, “Predictor Model &gt; Relationship to Endpoint” and “Hazard Model”.\nNote, mirroring of Dose Response and Hazard Model data which has hierarchical priors defined, will also lead to the related hierarchical priors data being mirrored (i.e. on the separate “Hierarchical Priors” tab).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#stage-1-interims",
    "href": "documentation/v71/userguides/staged.html#stage-1-interims",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 Interims",
    "text": "Stage 1 Interims\nThe Stage 1 interims have the same facilities (in “Interim Analysis Frequency”) for defining interims as FACTS Core.\n\n\n\n\n\n\nFigure 8: The Interims tab for the Stage 1 design.\n\n\n\nThe “Subject Followup Options” has been renamed to “Subject Follow-up Options After Early Decision” and has 1 new option. Otherwise it is the same as FACTS Core. The new option is whether or not to “continue follow-up after interim graduation”. If selected, this option says that if Stage 1 “graduates” (passes the decision criteria to go to Stage 2) at an interim (not the final evaluation when every subject would be complete), then we continue to follow-up the Stage 1 subjects who are not complete. Checking this option enables three further options:\n\nSpecify a delay between graduating at an interim and making the arm selection for Stage 2 and starting accrual. The options are:\n\nWait until all Stage 1 subjects have completed.\nWait a fixed additional time (or until all Stage 1 subjects have completed, whichever occurs first).\nNo delay\n\nRequest an additional analysis of the Stage 1 data after all Stage 1 subjects are complete. This is referred to as a Complete Data Analysis (CDA). This CDA would be assessed using the final evaluation criteria.\n\nIf a CDA is requested, by default the output of the Stage 1 patients data will be as at the Stage 1 CDA. If instead you want the patients data to be output as it was at the Stage 2 arm selection analysis, select the “Censor s1-patientsXXXX data as at arm selection” option.\n\nOn the “Transitions &gt; Data Inclusion” tab it will be possible to select from various options to use Stage 1 data in the Stage 1 analysis. If “continue follow-up after interim graduation” is not checked only the “not used” option will be available.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#stage-1-allocation",
    "href": "documentation/v71/userguides/staged.html#stage-1-allocation",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 Allocation",
    "text": "Stage 1 Allocation\nSpecification of Stage 1 allocation is identical to FACTS Core, except that using Arm Dropping in Stage 1 has some consequences that are unique to Staged Design:\n\nAt Stage 2 Arm selection, regardless of the arm selection rules, an arm dropped in Stage 1 cannot be selected for Stage 2.\nAs a dropped arm cannot be selected for Stage 2, if the data inclusion option only includes subjects on arms kept in Stage 2 then the subjects on a dropped arm will not be included in Stage 2... However if the Stage 1 data is included in full, or included in full and pooled then data from completed subjects on a dropped arm are included in Stage 2.\nIf a Subject is on an arm that is dropped, and then the Study stops for Success/Futility the subject only continues to be followed up if both “Continue follow up if arm dropped” has been selected and the appropriate “Continue follow-up if study stopped for success/futility” has been checked.\n\nAs noted in the Stage 2 section on Allocation below, Arm Dropping is not available as an allocation adaptation in Stage 2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#stage-1-successfutilitygraduation-criteria",
    "href": "documentation/v71/userguides/staged.html#stage-1-successfutilitygraduation-criteria",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 Success/Futility/Graduation Criteria",
    "text": "Stage 1 Success/Futility/Graduation Criteria\nThe Success/Futility/Graduation tab works in the same way as the Success/Futility tab in FACTS Core. For adaptive designs, the user can specify criteria for stopping in Stage 1 at an interim, and can define different criteria for different interims. Interim’s are indexed 1, 2, 3 etc. If stopping criteria are specified for an interim, they apply to all subsequent interims until there is an interim with new criteria specified. For example if stopping criteria are specified for Interim 1 and Interim 4, then at Interims 2 & 3, the stopping conditions for Interim 1 are used. The decision criteria for the final evaluation are always specified separately.\nIn a Staged Design, in additional to stopping Stage 1 for Success or Futility, it is possible to decide to stop Stage 1 in order to graduate to Stage 2. The decision criteria are checked in the order: ‘Futility’, ‘Success’, and then ‘Graduation’. The first criteria that are met are acted on. Note that deciding that Stage 1 is a Success or is Futile prevents Stage 2 from running, Success or Futility is a trial level decision.\n\nStage 1 Decisions\nThe possible outcomes for Stage 1, called “Outcome” in the Stage 1 output files, can be divided into 13 possible distinct decisions. Each simulated staged design makes a stage 1 decision that falls into exactly one of these categories. The outcomes reported for stage 1 do not depend on the decision made in Stage 2. They are fixed an known at the time of initiation of the second stage of the trial. The possible decisions are as follows:\n\nTrials that stopped in Stage 1\n\n1. Stage 1 Early Success\n\nStage 1 Early Success is achieved if and only if the trial meets the success condition at an interim analysis during Stage 1, and does not meet the futility criteria at the Stage 1 final analysis. The final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 1.\n\n2. Stage 1 Late Success\n\nStage 1 Late Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis success criteria.\n\n3. Stage 1 Late Futility\n\nStage 1 Late Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis futility criteria. Stage 1 Late Futility is not the complement of Stage 1 Late Success since the decision made if neither Late Success nor Late Futility are achieved at the Stage 1 final analysis is to graduate to Stage 2.\n\n4. Stage 1 Early Futility\n\nStage 1 Early Futility is achieved if and only if the trial meets the futility condition at an interim analysis during Stage 1, and does not meet the success criteria at the Stage 1 final analysis. The final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 1.\n\n5. Stage 1 Success to Futility Flip-Flop\n\nStage 1 Success to Futility Flip-Flop is achieved if and only if the trial meets the success condition at an interim analysis in Stage 1, but meets the futility condition at the Stage 1 final analysis. Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 1.\n\n6. Stage 1 Futility to Success Flip-Flop\n\nStage 1 Futility to Success Flip-Flop is achieved if and only if the trial meets the futility condition at an interim analysis in Stage 1, but meets the success condition at the Stage 1 final analysis. Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 1.\n\n\n\n\nTrials that graduated in Stage 1\n\n8. Early Graduation\n\nEarly Graduation is achieved if and only if the trial graduates at an interim analysis in Stage 1 and successfully chooses at least 1 active arm to carry forward into Stage 2.\n\n9. Late Graduation\n\nLate Graduation is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, and successfully chooses at least 1 active arm to carry forward into Stage 2.\n\n10. Early Graduation, None Selected\n\nEarly Graduation, None Selected is achieved if and only if the trial graduates at an interim analysis in Stage 1, but no active arms meet the criteria for moving to Stage 2. In this case, Stage 2 is considered “Null” and is not performed.\n\n11. Late Graduation, None Selected\n\nEarly Graduation, None Selected is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, but no active arms meet the criteria for moving to Stage 2. In this case, Stage 2 is considered “Null” and is not performed.\n\n\n\n\nOutcomes Specific to the Complete Data Analysis\nIf the option to “Perform Stage 1 complete data analysis” is checked in the Stage 1 Design &gt; Interims tab, then after an early graduation decision is made in Stage 1, the complete data analysis is conducted after full information is collected on all subjects enrolled during Stage 1. The complete data analysis row is recorded in the Stage 1 weeks files in the row with Interim Number equal to 1000. This row is distinct from the rest of the trial operation, and is not truly an interim analysis at which a trial decision can be made.\nThe special decisions, beyond the first 6 decisions that can be made for a trial that stops early in Stage 1, are:\n\n15. Early Graduation, Complete Data Analysis Futility\n\nEarly Graduation, Complete Data Analysis Futility is achieved if and only if the trial graduates at an interim analysis in Stage 1, and meets Stage 1 final futility criteria at the complete data analysis after full follow-up on subjects accrued during Stage 1 is collected.\n\n16. Early Graduation, Complete Data Analysis Success\n\nEarly Graduation, Complete Data Analysis Success is achieved if and only if the trial graduates at an interim analysis in Stage 1, and meets Stage 1 final futility criteria at the complete data analysis after full follow-up on subjects accrued during Stage 1 is collected.\n\n17. Early Graduation, Complete Data Analysis Inconclusive\n\nEarly Graduation, Complete Data Analysis Inconclusive is achieved if and only if the trial graduates at an interim analysis in Stage 1, and does not meet the Stage 1 final futility or Stage 1 final success criteria at the complete data analysis after full follow-up on subjects accrued during Stage 1 is collected.\n\n\nDecisions 15, 16, and 17 are unique to the complete data analysis, and cannot occur at a regular interim analysis or a final analysis.\n\n\n\nCreating Decision Criteria\nAs in FACTS Core, each decision can be specified in the following manner:\n\nFirst the decision is enabled / disabled if that outcome can / cannot be determined at that analysis.\nSecondly the “Decision QoI” to base the decision on, the direction of comparison, and the threshold to compare against are specified.\nMultiple “Decision QoI”s with their own comparisons may be specified as well as whether the test is for them all to be met (the criteria combined by AND) or for any one of them to be met (the criteria combined by OR).\nFinally, a minimum amount of information can be specified – either overall or on a specific arm. The nature of the information – Subjects Enrolled, Subjects Complete, or Subjects with the Opportunity to Complete is the same as has been selected on the Interim tab to define interim timings.\n\n\n\n\n\n\n\nFigure 9: The Success/Futility/Graduation Criteria tab at an interim analysis. Both early futility and early graduation can be achieved at this interim.\n\n\n\nSpecifying the Stage 1 Design’s Final Evaluation criteria is slightly different in staged than the Final Evaluation tab in Core:\n\nThere are no minimum information criteria, because it’s the final evaluation no further information will be gathered.\nThere are no graduation criteria. Graduation is the automatic decision if neither te Final Success Criteria or Final Futility Criteria are met.\n\n\n\n\n\n\n\nFigure 10: The Success/Futility/Graduation Criteria tab at the Stage 1 Final Evaluation. No Success Criteria has been provided, so any trial that does not meet the Futility Criteria will graduate to Stage 2.\n\n\n\n\n\n\n\n\n\nDesigning a Seamless Phase II/III Trial\n\n\n\nThere is a pit-fall that catches some users off guard when they are trying to design an inferentially seamless trial in the Staged design engine. A common staged design is one in which you would like to enroll a set number of subjects in Stage 1, and then transition to Stage 2 and seamlessly start enrolling Stage 2 with no pause. It seems logical that you could just not specify any interim graduation rules, and let the trial graduate at the final evaluation and start Stage 2. This would even seem to work on first glance.\nThe commonly unexpected issue is that the Final Evaluation Success/Futility/Graduation Criteria are not assessed until all accrued Stage 1 subjects have had the chance to observe their full follow-up. This means that if there is a long time to endpoint, that the trial will sit still and not accrue any new subjects for the length of the endpoint time. Additionally, when the study does transition to stage 2, it will have complete data on all Stage 1 subjects, so an early adaptation in Stage 2 may have more data to work with than intended.\nTo design an inferentially seamless trial, you must conduct an interim analysis at the time that the final Stage 1 subject is accrued, and you should set up the graduation rule at that interim analysis so that it is always met.\nEarly graduation at an interim analysis always transition seamlessly to the next stage as long as “Stage 2 Delay” is 0 on the Study &gt; Study Info tab, and the wait time before Stage 2 dose selection is set to “No Delay” on the Stage 1 Design &gt; Interims tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#dose-selection",
    "href": "documentation/v71/userguides/staged.html#dose-selection",
    "title": "Seamless Trial User Guide",
    "section": "Dose selection",
    "text": "Dose selection\nThere are two different forms of arm selection available: “Standard Selection Logic” and “Representative Arm Logic”. Independently of these, there are simple selections as to whether the Control and Active Comparator (if present) arms are to be included in Stage 2.\nThe same set of treatment arms are available in Stage 1 and Stage 2: these are the arms defined on the Study &gt; Treatment Arms tab. Usually all arms are available in Stage 1, but, if using fixed allocation in Stage 1, it is possible to assign zero subjects to an arm. This arm will still be included in analysis, however.\nIt is quite possible that, after a successful graduation decision (either at an interim in Stage 1 or at Stage 1 final analysis), the defined arm selection rules select no arms. This leads to a ’Null” Stage 2, i.e. Stage 2 does not run and overall the result of the combined stages is futility.\nIn outline, the two arm selection methods are as follows:\n\nStandard Selection Logic: This provides three levels of decision logic that are applied in turn to select:\n\nindividual specific arms to be kept or dropped;\narms identified as specific “Target” arms are kept;\narms that have scored highest or lowest on specified QoIs to be kept or dropped, with a specified minimum or maximum number of arms to be kept or dropped.\n\nRepresentative Arm Logic: This provides the ability to place the different treatment arms in different groups and then apply one of three forms of decision logic:\n\nKeep one from each group\nKeep groups based on the performance of each of the ‘representative arms’ from each group\nKeep just one group based on the performance of the ‘representative arm’ from each group.\n\n\n\nStandard Selection Logic\nWhen using “Standard Selection Logic” for the arm selection criteria, there are 3 levels of rules that are applied in order. It is not necessary to specify rules at each level; indeed if the selection you require can be achieved by a single rule at one particular level, that is usually the best way to specify it.\n\nIndividual Dose Decisions\nThe Individual Dose Decisions level is applied first. At this level the rule can be very simple: select a specific treatment arm, and specify a rule that decides if it should be kept or dropped. The rule can also be made dependent on a QoI achieving a particular threshold. Use this level of rules when you want to keep or drop specific arms.\n\n\n\n\n\n\nFigure 11: The Transition &gt; Dose Selection &gt; Individual Dose Decisions tab with rules specified for Dose 6 and Dose 3.\n\n\n\nWhen adding an Individual Dose Decision, the rule is made up of:\n\nThe specific arm the decision applies to.\nWhether the decision is to Keep or Drop the arm.\nThe decision criterion to use. Note this is optional and used to build more complex rules. Multiple criteria can be specified for each arm by adding new rules targeting the same arm. Each criterion is made of:\n\nThe “Decision Quantity” QoI to be tested. Often this will require a decision QoI to be created that is the evaluation of a QoI at the arm in question (but it could be a QoI at a different arm– e.g. keep the top dose if the middle dose has not met its criteria to be kept).\nThe direction of comparison.\nThe threshold.\nWhether, if additional criteria are specified, they are combined with a logical “AND” or a logical “OR”. This has to be specified on the first criteria for a particular arm, even if no other criteria are specified. When subsequent criteria are specified for the arm the join condition is displayed, but cannot be changed. To change the criteria, edit the first criteria for the arm.\n\n\n\n\n\n\n\n\nFigure 12: Add an Individual Dose Decision to drop Dose 1.\n\n\n\n\n\nTarget Dose Decisions\nThe second level of rules to be applied is “Target Dose Decisions”. The rules defined at this level are a simple selection from the list of available “Probability of being Target” QoIs (such as Pr(Max), Pr(MED), etc), and the arm that has the highest probability of meeting that target criteria is kept.\n\n\n\n\n\n\nFigure 13: The Transition &gt; Dose Selection &gt; Target Dose Decisions tab that keeps the dose with the highest Pr(Max).\n\n\n\n\n\nAll Dose Decisions\nThe final level of rules to be applied is “All Dose Decisions”, so called because the rule is applied to all the arms.\n\n\n\n\n\n\nFigure 14: The Transition &gt; Dose Selection &gt; All Dose Decisions tab that keeps 1 or 2 doses depending on how many have \\(\\Pr(\\theta_d - \\theta_{Control} &gt; 1)\\).\n\n\n\nThese rules comprise:\n\nA selection of whether these are rules to Keep or Drop arms.\nThe Minimum and the Maximum number of arms to Keep or Drop. Note that this minimum and maximum takes into account the number of arms kept or dropped by the rules at the earlier levels, and does not overrule them. For example: if the level 1 & 2 rules have caused 3 arms to be kept and the maximum is specified here to be 2, the number of arms kept is not reduced to 2, but no further arms, no matter how many meet this rule will be kept. Conversely if the previous levels have not caused any arms to be kept and the minimum is specified here to be 1, then 1 arm will be selected for keeping even if it does not fully meet the criteria.\nThe QoI to be used for the sort criterion is specified and the sort priority. The arms will be sorted by their value of this QoI when being tested against the additional criteria below.\nThere is no way to not specify a sort criterion, if the desire is that this rule should not be used to keep or drop doses, then the “minimum to keep” and “maximum to keep” should be set to 0.\nA filtering criteria can be specified using an “All Dose QOI”, condition and threshold. All arms will be judged using these criteria, and only those meeting the criteria will be selected as candidates. Multiple criteria may be specified, and the user selects whether these are combined with a logical ’AND” or a logical ’OR”. If no filtering criteria are specified then all the undecided arms are candidates.\nIf the already decided arms plus the candidates sum to more than the maximum arms being selected, then the candidate arms will be selected in order using the sort criteria up to the maximum.\nIf the already decided plus the candidate sum to less than the minimum required arms being selected, then additional arms will be selected from the remaining non-candidate arms in order, using the sort criteria, up to the specified minimum.\nIf the sum of the already decided arms plus the candidates lies between the specified minimum and maximum (inclusive) then no additional arms are selected, and the sort order is not required.\nIf no criteria are specified, then arms are simply selected in sort order.\n\n\n\nDecision Summary Tab\nTo simply the task of checking the rules that have been specified, there is a summary tab which describes the Selection Logic that has been specified.\n\n\n\n\n\n\nFigure 15: The Summary tab for the complicated set of dose selection rules provided in the screenshots above.\n\n\n\n\n\n\nRepresentative Arm Logic\nThe Representative Arm Logic method of specifying the arms to be selected for Stage 2 can be used when the selection needs to treat the arms as belonging to different groups, for example high and low doses or the treatment given in isolation or given in combination.\nThe tab allows the user to specify sub-groupings of the arms:\n\nThe number of groups is specified, initially with all the arms placed in “Group A”. Incrementing the “Group Count” parameter will create some additional empty groups – up to a maximum of four groups:\n\n\n\n\n\n\n\nFigure 16: The Group Builder’s default state when the Group Count is incremented to 2. All treatments are in the first group.\n\n\n\n\nThe treatment arms can then be moved between groups by simply dragging them, and the arms can be ordered within groups again by dragging them:\n\n\n\n\n\n\n\nFigure 17: The Group Builder with Dose 6, 5, and 4 dragged to the second group.\n\n\n\n\nThe top arm within each group is the default “representative arm” for that group for decision purposes (referred to below as the “Pre-selected Top Dose”). The arms can be dragged and reordered within groups to change the default representative.\n\n\n\n\n\n\n\nBreaking of Ties when Selecting Representative Arms\n\n\n\nWhen selecting doses between doses that have the same value for the QOI (which is highly unlikely except in circumstances where the value is ‘1’) in the Representative Arm logic, as elsewhere in FACTS, the lowest dose ‘wins’. Thus, if “keep one from each group” is being used and all the doses in a group are tied on the QOI criterion, the lowest dose in the group will be selected. If “keep one group” is being used and the representative doses of the groups are tied on the QOI criterion, then the group with the lowest representative dose will be selected.\n\n\nThere are three different “Representative Arm Logic” decision methods that can be used – as described in the following subsections.\n\nKeep one from each group\nWhen using Keep One From Each Group logic, the sub-groups are arranged with the intention to keep one arm from each group – selected according to specified criteria.\nAt least one criterion, comprising of an “All Dose” QoI with a condition and a threshold, is specified; this is taken as the primary decision to filter on and is used to sort the arms in each group. The direction of the condition determines the priority order of the QoI. Note the threshold can be set to 0 or 1 if no criterion is required. Additional criteria can be defined to refine the selection filter – and these are combined using a logical ‘AND’ or an ‘OR’. For each group, the arms that meet the criteria are selected, and (if there are multiple) the arm with the greatest (if the Condition is ‘&gt;’) or lowest (if the Condition is ‘&lt;’) value from the primary (first) criteria is selected as the arm from that group.\nIf no arms meet the criteria, then the arm with greatest/lowest value from the primary QoI is selected.\n\n\n\n\n\n\nFigure 18: The transition tab when using Representative Arm Decisions and keeping one dose from each group.\n\n\n\n\n\nKeep group based on representative\nWhen using Keep Group Based on Representative Arm logic, the user specifies how a representative arm in each group is selected and criteria to filter these representative arms; then all the arms in groups whose representative arm satisfies this filter are selected.\nThe user first specifies how the representative arm in each group is selected, this can either be:\n\nThe Pre-selected Top Dose: the arm that the user has moved to the top of the list, per group, in the Group Builder panel.\nAny Target QoI (such as Pr(Max)), in which case the arm in the group that has the greatest probability of being that target arm is the representative for that group.\n\nCriteria are then specified in the same manner as for the “Keep One From Each Group” logic. These criteria are applied to the representative arm of each group. If the criteria are met, all arms for that group are retained.\n\n\n\n\n\n\nFigure 19: The transition tab when using Representative Arm Decisions and keeping a group based on the representative.\n\n\n\n\n\nKeep one group\nWhen using Keep One Group logic, the user first specifies how the representative arm in each group is selected, this can either be:\n\nThe Pre-selected Top Dose: the arm that the user has moved to the top of the list, per group, in the Group Builder panel.\nThe Sum: the greatest summed value of an “All Dose” QoI for all the specific arms in the group (e.g. pick the group with the overall greatest probability of having the maximum response, rather than the arm with the probability of having the maximum response).\nAny Target QoI (such as Pr(Max)): the arm in the group that has the greatest probability of being that target arm is the representative for that group.\n\nThe user then specifies the selection QoI, which can be any of the “All Dose” QoIs (excluding the target QoIs – unless using Sum). The user then specifies the Priority Order as “Least” or “Greatest”. The group selected will be the group where the representative arm has the Greatest / Least value for the specified QoI.\n\n\n\n\n\n\nFigure 20: The transition tab when using Representative Arm Decisions and keeping one entire group.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#data-inclusion",
    "href": "documentation/v71/userguides/staged.html#data-inclusion",
    "title": "Seamless Trial User Guide",
    "section": "Data Inclusion",
    "text": "Data Inclusion\nOn the Data inclusion tab, the user specifies how much Stage 1 data is used in the Stage 2 analysis. The options are:\n\nnot used: Stage 1 data is not used at all in Stage 2. The two stages are effectively separate trials.\nincluded in full: All Stage 1 data is used in Stage 2. The two stages are effectively 2 parts of a single trial.\nincluded where the subjects are on arms that are kept in Stage 2: Stage 1 data is used in Stage 2, but just on the arms selected for Stage 2. This could be for example for an inferentially seamless phase 2/3 trial.\nincluded in full and pooled with the one Stage 2 treatment arm: Stage 1 data is used in full, but pooled onto the one treatment arm selected for Stage 2. This option is only available when the selection criteria can clearly only take one arm into Stage 2.\nrestricted to only those subjects that did not complete in Stage 1 and are on arms that are kept in Stage 2: Only incomplete Stage 1 subjects on arms that are selected for Stage 2 are used in the Stage 2 analysis. The user specifies what is the latest visit the user can have been observed at and still count as “incomplete”. Whether the visit was observed in stage 1 is judged at the Stage 1 Dose Selection analysis. This option is not available for designs using a TTE endpoint.\n\nThe user also specifies whether in Stage 2 “Target QoIs” are evaluated over all the arms in the trial or just over the arms selected for Stage 2. “Target QoIs” are QoIs such as Pr(Max) that calculate for each arm the probability that it meets some target criteria.\n\n\n\n\n\n\nFigure 21: The Transition &gt; Data Inclusion tab for a trial that doesn’t use the Stage 1 data in Stage 2.\n\n\n\nRegardless of whether and how Stage 1 data is used in Stage 2, all Stage 1 data is always used in the Stage 1 analysis.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#dose-response-modeling",
    "href": "documentation/v71/userguides/staged.html#dose-response-modeling",
    "title": "Seamless Trial User Guide",
    "section": "Dose Response Modeling",
    "text": "Dose Response Modeling\nThe dose response modeling in Stage 2 is applied to all doses, regardless of whether they have been selected for inclusion in Stage 2 or not. Unless all Stage 1 data is included in Stage 2, if only a small portion of all doses are included in Stage 2, then the Independent Dose Model should be considered for the dose response analysis, along with the option on the Data Inclusion tab to restrict the estimate of Target QOIs to only the arms selected for use in Stage 2.\nFor more on this discussion, see above.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#allocation",
    "href": "documentation/v71/userguides/staged.html#allocation",
    "title": "Seamless Trial User Guide",
    "section": "Allocation",
    "text": "Allocation\nIn Stage 2 of a staged design, there are only 2 allocation options: fixed and adaptive. There is no “arm dropping” option, because of the difficulty of specifying arm dropping rules when the number of arms starting out can vary.\n\nFixed Allocation\nFixed allocation in Stage 2 is specified in the same way as FACTS Core and Stage 1: allocation is blocked, and the user specifies the number of times each treatment occurs in the block (and hence the block size). The difference in Stage 2 is that the exact number of treatment arms may be uncertain, depending on the arm selection rules and the data available at the time of arm selection.\nThus, in Stage 2, rather than a single set of randomization weightings, fixed allocation is defined for each of the possible numbers of treatment arms taken to Stage 2 (but not on exactly which arms are taken, only the number taken).\nFACTS displays a table with a row for each potential number of arms in Stage 2 and the user can assign weightings to each one. Arms are identified in terms of their relative “Effective Dose Strength”. Trt 1 refers to the treatment arm selected for Stage 2 with the lowest effective strength, Trt 2 the next lowest, etc. Effective dose strength for each arm can be entered on the “Study &gt; Treatment Arms” tab. Hence if the 2nd, 3rd and 5th arms are selected they will receive the allocation of “Trt 1”, “Trt2” and “Trt 3”, on the “possible # treatments = 3” row.\n\n\n\n\n\n\nFigure 22: Stage 2 Fixed allocation when there could be 1, 2, or 3 treatments in Stage 2.\n\n\n\nIn the example above, the assignment to control has been changed from the default of ‘1’ to maintain a 50% allocation to Control in each of the possible settings, of whether 1, 2, or 3 treatment arms were selected for Stage 2.\nNote, the number of arms available to allocate to (i.e. the number of rows in the table) is dictated by the settings on the “Arm selection” tab – e.g. if using Standard Selection Logic with a min and max number of arms to keep of ‘3’ (and more than three arms are defined), there will be three rows in this table.\n\n\nAdaptive Allocation\nIf using adaptive allocation in Stage 2, there are two sets of allocation data to define:\n\nThe lower table in the tab defines the fixed allocation that occurs prior to the first Stage 2 interim analysis. This is specified in the same way as for fixed allocation (see the section above).\nThe upper table in the tab specifies which arms are adaptively allocated to after the first Stage 2 interim, how many slots there are in the randomization block, which arms are allocated with a fixed number of slots in the block, and hence how many slots are left over to be allocated adaptively. To allocate to an arm adaptively, no value should be entered in the cell, and the cell will turn orange. Note, the first row must have all values defined, whereas other rows, if they are to be specified adaptively, must have at least two empty cells. The table is laid out in the same way as the table for fixed allocation, with a row for each number of treatments that might be selected for Stage 2, and then the columns for the arms in Stage 2 ordered by their effective strength.\n\n\n\n\n\n\n\nFigure 23: The Stage 2 adaptive allocation tab when 1 to 3 arms could be carried forwards into Stage 2. Response adaptive randomization is configured to be based on the probability that each dose is better than control.\n\n\n\nThe adaptive allocation targets section is completed in the same way as in the FACTS Core design Adaptive Allocation tab. If static weights are assigned to arms, then the weighting is normalised across just those arms that have been selected for Stage 2 and are being adaptively allocated. To put it another way, arms that have not been selected for Stage 2 or are selected but have been given a fixed allocation are excluded from the static weighting scheme, the weighting is then re-normalized across those doses that are left.\nNote that, because in Stage 2 it is usually not known at design time which doses will be in Stage 2, nor often how many, the static weight table does not disable entries for doses that may not be present in Stage 2.\n\n\n\n\n\n\nFigure 24: The adaptive allocation targets panel in Stage 2 allocation with static weights being used in the RAR calculation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#interims",
    "href": "documentation/v71/userguides/staged.html#interims",
    "title": "Seamless Trial User Guide",
    "section": "Interims",
    "text": "Interims\nInterims are specified in the Stage 2 design in the same way as FACTS Core and Stage 1, except in Stage 2 there is an option to specify whether the Stage 2 information requirements are based solely on Stage 2 data, or on the Stage 2 plus Stage 1.\n\n\n\n\n\n\nFigure 25: Stage 2 design Interims tab with the new prompt at the bottom of the left hand section.\n\n\n\nNote that the interim numbers in Stage 2 “start over” after the transition. So the interim numbers written to the stage 2 “weeks” file will start at 1, and will correspond to the interim decisions specified on the Success/Futility Criteria tab.\nInterim timing in the second stage may be affected by the presence of first stage data, if the option is selected to base interim information on both the first and second stage data. In this case, interims are handled as follows:\n\nIf interims are specified by time, then the “first interim” may be skipped if the information carried forward from the first stage exceeds the amount of information specified for the “first interim.” The “Interim 1” success/futility criteria are then ignored, and would be superceded by the “Inteirm 2” success futility criteria, if they exist. The initial analysis is performed at X weeks into the second stage, according to the first “Time: every X weeks” specification that occurs after Stage 2 starts.\nIf interims are specified by time, and the first interim is by number of subjects enrolled the “first interim” condition may never be reached (particularly in a Stage 2 where the amount of information can be very dependent on when Stage 1 graduates to Stage 2). In this instance the timed interims start at full accrual.\nIf interims are specified by information, then the first interim performed is the first interim that has information greater than the information available from the first stage data. Interim numbering still corresponds to the rows of the “Information at” specifications.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#successfutility-criteria",
    "href": "documentation/v71/userguides/staged.html#successfutility-criteria",
    "title": "Seamless Trial User Guide",
    "section": "Success/Futility Criteria",
    "text": "Success/Futility Criteria\nIn Stage 2 the “Success/Futility Criteria” tab is laid out as in FACTS Core. It differs from the Stage 1 Design &gt; Success/Futility/Graduation Criteria tab in not having any graduation criteria conditions.\nSee the Stage 2 Interims tab description for specific details on Success/Futility rule assessment if interims are unattainable or already attained at the start of Stage 2.\n\nThe Staged Design Decisions for Stage 1 and Stage 2 Combined\nThe decisions that can be made in staged designs, called Comb. Outcome in the output files, are expanded to accomodate the two stages. Instead of the 7 possible outcomes in a single stage FACTS Core design, there are more available in the staged engine. Every possible combination decision that can be made in a FACTS Staged designs falls into exactly one of 23 possible decisions. They are as follows:\n\nTrials that stopped in Stage 1\n\n1. Stage 1 Early Success\n\nStage 1 Early Success is achieved if and only if the trial meets the success condition at an interim analysis during Stage 1, and does not meet the futility criteria at the Stage 1 final analysis. The final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 1.\n\n2. Stage 1 Late Success\n\nStage 1 Late Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis success criteria.\n\n3. Stage 1 Late Futility\n\nStage 1 Late Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis futility criteria. Stage 1 Late Futility is not the complement of Stage 1 Late Success since the decision made if neither Late Success nor Late Futility are achieved at the Stage 1 final analysis is to graduate to Stage 2.\n\n4. Stage 1 Early Futility\n\nStage 1 Early Futility is achieved if and only if the trial meets the futility condition at an interim analysis during Stage 1, and does not meet the success criteria at the Stage 1 final analysis. The final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 1.\n\n5. Stage 1 Success to Futility Flip-Flop\n\nStage 1 Success to Futility Flip-Flop is achieved if and only if the trial meets the success condition at an interim analysis in Stage 1, but meets the futility condition at the Stage 1 final analysis. Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 1.\n\n6. Stage 1 Futility to Success Flip-Flop\n\nStage 1 Futility to Success Flip-Flop is achieved if and only if the trial meets the futility condition at an interim analysis in Stage 1, but meets the success condition at the Stage 1 final analysis. Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 1.\n\n\n\n\nTrials that graduated early from Stage 1\n\n21. Stage 1 Early Graduation, Stage 2 Early Success\n\nStage 1 Early Graduation, Stage 2 Early Success is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the success criteria at an interim analysis during Stage 2, and does not meet the futility criteria at the Stage 2 final analysis. The Stage 2 final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 2.\n\n22. Stage 1 Early Graduation, Stage 2 Late Success\n\nStage 1 Early Graduation, Stage 2 Late Success is achieved if and only if the trial graduates at an interim analysis in Stage 1, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis success criteria.\n\n23. Stage 1 Early Graduation, Stage 2 Late Futility\n\nStage 1 Early Graduation, Stage 2 Late Futility is achieved if and only if the trial graduates at an interim analysis in Stage 1, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis futility criteria. Stage 2 Late futility is not automatically the complement of Stage 2 Late Success; the Stage 2 final analysis futility rule must be specified as the complement of the success rule to make it true.\n\n24. Stage 1 Early Graduation, Stage 2 Early Futility\n\nStage 1 Early Graduation, Stage 2 Early Futility is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the futility criteria at an interim analysis during Stage 2, and does not meet the success criteria at the Stage 2 final analysis. The Stage 2 final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 2.\n\n25. Stage 1 Early Graduation, Stage 2 Success to Futility Flip-Flop\n\nStage 1 Early Graduation, Stage 2 Success to Futility Flip-Flop is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the success condition at an interim analysis in Stage 2, but meets the futility condition at the Stage 2 final analysis. Stage 2 Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 2.\n\n26. Stage 1 Early Graduation, Stage 2 Futility to Success Flip-Flop\n\nStage 1 Early Graduation, Stage 2 Futility to Success Flip-Flop is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the futility condition at an interim analysis in Stage 2, but meets the success condition at the Stage 2 final analysis. Stage 2 Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 2.\n\n27. Stage 1 Early Graduation, Stage 2 Inconclusive\n\nStage 1 Early Graduation, Stage 2 Inconclusive is achieved if and only if the trial graduates at an interim analysis in Stage 1, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then does not meet the Stage 2 final success or final futility criteria.\n\n\n\n\nTrials that graduated at the Stage 1 final analysis\n\n31. Stage 1 Late Graduation, Stage 2 Early Success\n\nStage 1 Late Graduation, Stage 2 Early Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the success criteria at an interim analysis during Stage 2, and does not meet the futility criteria at the Stage 2 final analysis. The Stage 2 final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 2.\n\n32. Stage 1 Late Graduation, Stage 2 Late Success\n\nStage 1 Late Graduation, Stage 2 Late Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis success criteria.\n\n33. Stage 1 Late Graduation, Stage 2 Late Futility\n\nStage 1 Late Graduation, Stage 2 Late Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis futility criteria. Stage 2 Late futility is not automatically the complement of Stage 2 Late Success; the Stage 2 final analysis futility rule must be specified as the complement of the success rule to make it true.\n\n34. Stage 1 Late Graduation, Stage 2 Early Futility\n\nStage 1 Late Graduation, Stage 2 Early Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the futility criteria at an interim analysis during Stage 2, and does not meet the success criteria at the Stage 2 final analysis. The Stage 2 final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 2.\n\n35. Stage 1 Late Graduation, Stage 2 Success to Futility Flip-Flop\n\nStage 1 Late Graduation, Stage 2 Success to Futility Flip-Flop is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the success condition at an interim analysis in Stage 2, but meets the futility condition at the Stage 2 final analysis. Stage 2 Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 2.\n\n36. Stage 1 Late Graduation, Stage 2 Futility to Success Flip-Flop\n\nStage 1 Late Graduation, Stage 2 Futility to Success Flip-Flop is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the futility condition at an interim analysis in Stage 2, but meets the success condition at the Stage 2 final analysis. Stage 2 Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 2.\n\n37. Stage 1 Late Graduation, Stage 2 Inconclusive\n\nStage 1 Late Graduation, Stage 2 Inconclusive is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then does not meet the Stage 2 final success or final futility criteria.\n\n\n\n\nOther\n\n10. Stage 1 Early Graduation, Dose Selection Unsuccessful\n\nStage 1 Early Graduation, Dose Selection Unsuccessful is achieved if and only if the trial graduates at an interim analysis in Stage 1, but when selecting treatments to carry forward into Stage 2, no treatment arms satisfy the required Transition criteria. In this case, Stage 2 is considered “Null” and is not performed.\n\n11. Stage 1 Late Graduation, Dose Selection Unsuccessful\n\nStage 1 Early Graduation, Dose Selection Unsuccessful is achieved if and only if the the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, but when selecting treatments to carry forward into Stage 2, no treatment arms satisfy the required Transition criteria. In this case, Stage 2 is considered “Null” and is not performed.\n\n40. Stage 1 Graduation, Max sample size reached\n\nStage 1 Graduation, Max sample size reached is achieved if and only if the trial graduates at a Stage 1 analysis, but the calculated Stage 2 sample size is 0. In this case, Stage 2 is considered “Null” and is not performed.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#simulation-results",
    "href": "documentation/v71/userguides/staged.html#simulation-results",
    "title": "Seamless Trial User Guide",
    "section": "Simulation Results",
    "text": "Simulation Results\nIn the center of the simulation tab, the summary simulation results are displayed. There are many columns that are output by FACTS. Only the columns considered “Highlights” are displayed by default. Other groups of output columns can be displayed by clicking on the “Show More Columns” button.\nThese windows will show:\n\n\n\n\n\n\nFigure 26: The options available in “Show More Columns” for a staged design.\n\n\n\n\n\n\n\n\n\n\nName\nColumn Description\n\n\n\n\nAll\nAll summary columns\n\n\nHighlights\nOnly the columns shown on the main tab\n\n\nStage 2 Highlights\nThe columns from the main tab that pertain to Stage 2\n\n\nAllocation\nThe columns that report on participant recruitment and allocation\n\n\nResponse\nThe columns that report that estimate treatment response, the SD of the estimate, the estimate of the SD of the response, the true treatment response and the true SD of the response.\n\n\nObserved\nThe raw endpoint output and the dropout rates by arm and visit\n\n\nProbabilities\nThe final estimates for the QOIs that were computed for the trial.\n\n\nTiming\nThe duration to the end of the trial under certain conditions.\n\n\nModel Parameters\nThe columns that report the estimates of the values of the model parameters.\n\n\nStage 2 Simulation Results\nA window that displays the individual Stage 2 simulation results for the currently selected scenario. The results initially displayed are the Stage 2 ‘highlights’ columns. Other groups of these simulation results can be opened from the Right Click menu of this window.\n\n\nStage 1 Summary\nA window that displays the end of Stage 1 Summary results similar to FACTS Core summary\n\n\nStage 1 Simulation Results\nA window that displays the individual Stage 1 simulation results for the currently selected scenario. The results initially displayed are the Stage 1 ‘highlights’ columns. Other groups of these simulation results can be opened from the Right Click menu of this window.\n\n\nStage 1 DS Summary\nA window that displays a summary of the end of stage 1 arm selection results\n\n\nFrequentist results\nIf frequentist analysis is enabled, the summary results can be viewed, these are grouped by how missing data has been treated: Last Observation Carried Forward (LOCF), Baseline Observation Carried Forward (BOCF – if baseline has been simulated), Per-Protocol (PP).\n\n\nSimulation Duration\nA window that displays how long each scenario took to simulate",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#right-click-menu",
    "href": "documentation/v71/userguides/staged.html#right-click-menu",
    "title": "Seamless Trial User Guide",
    "section": "Right Click Menu",
    "text": "Right Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 27: The right click menu on the Staged design simulations tab.\n\n\n\nThese will respectively:\n\nOpen results folder\n\nOpens a new Windows directory browser window showing the contents of the simulation results for that scenario.\n\nStage 2 Simulation Results\n\nOpens a window that displays the individual Stage 2 simulation results for the currently selected scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of this window.\n\nStage 1 Summary\n\nOpens a window that displays the end of Stage 1 Summary results similar to FACTS Core summary\n\nStage 1 Simulation Results\n\nOpens a window that displays the individual Stage 1 simulation results for the currently selected scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of this window.\n\nStage 1 DS Summary\n\nOpens a window that displays a summary of the end of stage 1 arm selection results\n\nFrequentist Stage 2 Results\n\nIf frequentist analysis is enabled, this opens a window that displays the summary frequentist results grouped by how missing data has been treated: Last Observation Carried Forward (LOCF), Baseline Observation Carried Forward (BOCF – if baseline has been simulated), Per-Protocol (PP).\n\nOpen R\n\nStarts R, loading in the result files for that scenario as separate dataframes.\n\nShow ... Graphs\n\nOpens the FACTS results graph control displaying the corresponding graphs for that scenario. See below for a description of the different groups of graphs.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#graphs-of-simulation-results",
    "href": "documentation/v71/userguides/staged.html#graphs-of-simulation-results",
    "title": "Seamless Trial User Guide",
    "section": "Graphs of Simulation Results",
    "text": "Graphs of Simulation Results\nThe graphs produced in the staged engine are usually very similar to the graphs produced in the FACTS Core engine. It is certainly worth familiarizing yourself with those graphs when using Staged design.\nGraphs available for continuous or dichotomous core designs\nGraphs available for time-to-event core designs\nGraphs available for multiple endpoint core designs\n\nPer Scenario Graphs\nThe staged design graphs include the FACTS Core graphs, broken out by stage, the complete data analysis graphs (if performed), the arm selection graphs, and overall graphs, which are combined across stages.\n\nStage 1 Graphs\n\nThese are the same graphs as are available in FACTS Core, based on the analysis in Stage 1 when the Futility / Graduation / Success decision was made. There are 3 additional graphs not in FACTS Core: Time Course for Stopping/Graduation, Proportion Times Arms Selected, and Numbers of Arms Selected\n\nStage 1 CD Graphs\n\nThese are the same graphs as are available in FACTS Core, based on the Stage 1 Complete Data Analysis, (if the “Perform Stage 1 complete data analysis” option was de-selected then these graphs are not available). It does not include “interim” based graphs or arm selection graphs.\n\nStage 1 DS Graphs\n\nThese are the same graphs as are available in FACTS Core, based on the Stage 1 Arm selection data analysis. It does not include “interim” based graphs.\n\nStage 2 Graphs\n\nThese are mostly the same graphs as are available in FACTS Core, based on Stage 2. There are 3 additional graphs not in FACTS Core: Time Course for Stopping, Outcomes by Arms Selected, and Outcomes by Number of Arms Selected\n\nOverall Graphs\n\nThese graphs are based on the combined Stage 1/Stage 2. They comprise:\n\n\n\nCombined Allocation Boxplot\n\nSimilar to this\n\nTarget Response Scatter plot Combined\n\nSimilar to this\n\nCombined Time Course for Stopping\n\nSimilar to this\n\nSample Size/Duration by Arm Selected\n\nSee below\n\nSample Size/Duration by Num Arms Selected\n\nSee below\n\n\n\nTime Course Graphs\nSome of the most useful graphs for getting an overview of the results of Staged Design simulations of a scenario are the time course graphs. In particular the graph in the “Overall Graph” set: Combined Time Course for Stopping\n\n\n\n\n\n\nFigure 28: The Time Course for Stopping Graph from the “Overall” graph set with time on the x-axis.\n\n\n\nThis graph shows a stacked histogram for the state of all the simulations at any given week / sample size. The user can select whether the x-axis is time of number of subjects enrolled.\nAt each time point / number of subjects the column of the graph above that point is coloured reflecting the state of the simulations – if they have not yet achieved their final outcome they are shown grey for “Continuing / Inconclusive”. Otherwise they are coloured according to final outcome. Once a simulation is in a final outcomes state it never changes, so the grey area is the only area that reduces as the graph is read from left to right.\n\n\n\n\n\n\nFigure 29: The Time Course for Stopping Graph from the “Overall” graph set with number of subjects randomized on the x-axis.\n\n\n\nThere is also a time course graph in the Stage 1 graphs group that shows the time to make a Stage 1 decision, and ignores Stage 2. It’s called Time Course for Stopping/Graduation.\n\n\n\n\n\n\nFigure 30: The Time Course for Stopping/Graduation Graph from the “Stage 1” graph set with time on the x-axis.\n\n\n\nThe time course plot in the Stage 2 graphs group shows the decisions made in Stage 2, using only trials that actually moved on the Stage 2. The timing and number of subjects resets at 0 for the start of Stage 2 in these graphs.\n\n\n\nThe Time Course for Stopping Graph from the “Stage 2” graph set with time since the start of Stage 2 on the x-axis.\n\n\nThe time course plot in the Stage 1 DS section only includes simulations that graduated and moved to dose selection.\nThe time course plot in the Stage 1 CD section only includes the assessment of the final analysis after full follow-up of all Stage 1 subjects. It ignores the Stage 1 decision made by the trial.\n\n\nStage 1 - Proportion of Times Arms Selected\nThis is a histogram of the proportion of times each arm was selected to be used in Stage 2. Notice in the example image below that Control does not achieve a proportion of 1 despite being automatically selected for Stage 2: the proportion is across all simulations of the scenario. Including those that did not graduate to Stage 2. In this example, only one arm as well as control was selected for Stage 2, so the sum of the proportion of times the other arms were selected equals the proportion of times Control was selected, which is also the proportion of trials that graduated. If more than one arm would have been selected, these columns would have been higher.\n\n\n\n\n\n\nFigure 31: The proportion of simulated trials in which each arm was selected for inclusion in Stage 2.\n\n\n\n\n\nStage 1 - Number of Arms Selected\nThis is a straightforward histogram of the proportion of times different numbers of treatment arms (excluding Control and Active Comparator) were selected to go forward to the Second Stage.\n\n\n\n\n\n\nFigure 32: Plot of how many active treatment arms were carried forward into Stage 2.\n\n\n\n\n\nStage 2 - Outcomes by Arms Selected\nThis graph shows a stacked bar chart of trial outcomes for all trials in which the x-axis arm was included in Stage 2. The higher the bar, the more often the arm was included in Stage 2.\n\n\n\n\n\n\nFigure 33: Plot of the outcomes for trials that included each arm in Stage 2.\n\n\n\n\n\nStage 2 - Outcomes by Number of Arms Selected\nThis graph shows a stacked bar chart of trial outcomes separated by the number of arms that were carried forwards into Stage 2 (not including the control or active comparator). The higher the bar, the more often that number of arms were carried forwards. Which arms were included in stage 2 is irrelevant in this plot.\n\n\n\n\n\n\nFigure 34: Plot of the outcomes for trials based on the number of arms included in Stage 2.\n\n\n\n\n\nOverall - Sample Size/Duration by Arm Selected\nThis graph shows a box plot of either the total sample size or the total duration combined across the two stages conditioning on trials in which each arm was included. Each simulation counts towards all arms that were included in Stage 2, so a trial that graduated and had Control, Dose 2, and Dose 4 move into Stage 2 would contribute to the box plots of the Control, Dose 2, and Dose 4.\n\n\n\n\n\n\nFigure 35: Boxplot of the Combined Sample Size of Stages 1 and 2 for trials that included each arm in thier second stage.\n\n\n\n\n\nOverall - Sample Size/Duration by Num Arms Selected\nThis graph shows a box plot of either the total sample size or the total duration combined across the two stages separating out trials by the number of arms that moved forward into Stage 2. Each simulation counts towards exactly one boxplot in this graph.\n\n\n\n\n\n\nFigure 36: Boxplot of the Combined Duration of Stages 1 and 2 for trials that had a specific number of arms move forwards into Stage 2.\n\n\n\n\n\n\nAcross Scenario Graphs\nSimilar to the Across Scenario Graphs in FACTS Core these show trellis plots of graphs for the operating characteristics that are most often compared across different designs. For most of these, there are different versions of the graph for the two stages:\n\nArm selection Stage 1\n\nSee below\n\nArm selection Stage 2\n\nSimilar to this\n\nEstimates of QOIs (Stage 1 and Stage 2)\n\nSimilar to this\n\nOverall ppn of success\n\nSimilar to this\n\nSubject allocation (Stage 1 and Combined)\n\nSimilar to this\n\nOverall sample size (Stage 1 and Combined)\n\nSimilar to this\n\nResponse (Stage 1 and Stage 2)\n\nSimilar to this\n\n\n\nStage 1 - Selected Arms\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. This is the one Across Scenarios graph that differs from its FACTS Core counterpart.\nEach bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” – the arm was correctly selected (it was selected and marked as “Should succeed” on the VSR tab) and the trial was successful at the end of Stage 1.\n“Should not succeed” – the arm was incorrectly selected (it was selected but not marked as “Should succeed” on the VSR tab) and the trial was successful at the end of Stage 1.\n“Selected” – the arm was correctly selected for graduation (it was selected and marked as “Should succeed” on the VSR tab) and the trial graduated to Stage 2.\n“Mis-selected” – the arm was incorrectly selected for graduation (it was selected but not marked as “Should succeed” on the VSR tab) and the trial graduated to Stage 2\nUnsuccessful – the arm was selected and the trial failed.\n\n\n\n\n\n\n\nFigure 37: The Across Scenarios Arm Selection graph for Stage 1.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#viewing-simulation-results-in-the-simulations-tab",
    "href": "documentation/v71/userguides/staged.html#viewing-simulation-results-in-the-simulations-tab",
    "title": "Seamless Trial User Guide",
    "section": "Viewing Simulation Results in the Simulations Tab",
    "text": "Viewing Simulation Results in the Simulations Tab\n\nSummary Per Scenario\nMost of the summary results are as for FACTS CORE. The main differences are in the Highlights – which summarise over both stages, and a Staged Design specific results set “Stage 1 DS Summary”.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, they can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nExternal Data File\nOptional\nThis column is visible if any of the scenarios use an external data file to define the subject responses on each treatment arm, specified on the ‘Virtual Subject Response &gt; External Files’ tab. For those scenarios this gives the name of the external data file, otherwise it is blank.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nComb. Mean Subj.\n1\nThis is the mean (over the simulations) of the combined number of subjects recruited across both stages in this scenario.\n\n\nSD Comb. Subj.\n1\nThis is the standard deviation across the simulations of the combined number of subjects recruited across both stages in this scenario.\n\n\nComb. Subj 80%\n1\nThis is the eightieth percentile across the simulations of the combined number of subjects recruited across both stages in this scenario\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that were successful – whether that was early success in Stage 1, late success in Stage 1, early success in Stage 2 or late success in Stage 2.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that were futile – whether that was early futility in Stage 1, late futility in Stage 1, early futility in Stage 2 or late futility in Stage 2.\n\n\n\nThere are then columns that report the type of outcome depending on whether that outcome happened in Stage 1 (S1), in Stage 2 after graduating early (at a Stage 1 interim) from Stage 1 (S2EG) or in Stage 2 after graduating late (at the Stage 1 final analysis) from Stage 1 (S2LG)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Suc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile nor (Stage 1 only) graduated in the final analysis.\n\n\nPpn Early Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at a Stage 1 interim, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn Late Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at the Stage 1 final analysis, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn NoN Stage 2\n1\nThis is the proportion of simulations where Stage 2 was due to be able to run but was unable to because the overall maximum sample size had been reached.\n\n\nMean Comb Alloc &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the combined allocation of subjects to this arm over both stages.\n\n\nSD Comb Alloc &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the combined allocation of subjects to this arm over both stages.\n\n\nMean Comb. Duration\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit in Stage 1 to last patient last visit in Stage 2.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#stage-1-ds-summary",
    "href": "documentation/v71/userguides/staged.html#stage-1-ds-summary",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 DS Summary",
    "text": "Stage 1 DS Summary\nThese columns summarise the Stage 1 transition to Stage 2 after graduation.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nExternal Data File\nOptional\nThis column is visible if any of the scenarios use an external data file to define the subject responses on each treatment arm, specified on the ‘Virtual Subject Response &gt; External Files’ tab. For those scenarios this gives the name of the external data file, otherwise it is blank.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in Stage 1 in this scenario.\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success in Stage 1 (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early in Stage 1 but were successful in the final analysis of Stage 1.\n\n\nPpn Early Graduation\n1\nThis is the proportion of simulations that stopped early in Stage 1 for graduation to Stage 2.\n\n\nPpn Late Graduation\n1\nThis is the proportion of simulations that did not stop early in Stage 1 and graduated to Stage 2 at the final Stage 1 analysis.\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early in Stage 1 but were futile in the final analysis of Stage 1.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility in Stage 1 (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Suc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success in Stage 1 but regressed to futility in the final analysis.\n\n\nPpn Fut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility in Stage 1 but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile nor graduated in the final analysis of Stage 1.\n\n\nPpn Early Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at a Stage 1 interim, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn Late Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at the Stage 1 final analysis, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn Selected &lt;Dose&gt;\nOne per arm\nThis is the proportion of simulations that graduated to Stage 2 and selected this arm for inclusion in Stage 2.\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the mean response on this arm at the ‘decision analysis’ in Stage 1.\n\n\nPpn Arms Drop: &lt;Dose&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped during Stage1 by the Arm Dropping rules. Not selecting the arm for inclusion in Stage 2 is not counted as “dropping the arm” for the purposes of this count.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to the enrollment of the Last Patient (LP) in Stage 1 (i.e. the duration of accrual).\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#notes-on-the-differences-in-the-sd-output-files-from-facts-core",
    "href": "documentation/v71/userguides/staged.html#notes-on-the-differences-in-the-sd-output-files-from-facts-core",
    "title": "Seamless Trial User Guide",
    "section": "Notes on the differences in the SD output files from FACTS Core",
    "text": "Notes on the differences in the SD output files from FACTS Core\nSee the FACTS Core User Guide for the appropriate endpoint for a detailed listing of the columns in each type of output file.\n\nInterims in the weeks files\nIn the Stage 1 “weeks” files that report the results of each analysis in a simulation, there are two “special” analyses that are numbered 999 and 1000, respectively. The 999 interim is the Arm selection analysis after the Stage 1 outcome decision was made, and the 1000 analysis is the “Complete Data Analysis” which may or may not be enabled if early stopping in Stage 1 is possible, and subjects are followed-up after the stopping decision.\nThe s1-simulations.csv file contains the 999 rows of each s1-weeksXXXXX.csv, while s1-cd-simulations.csv data corresponds to the 1000 row.\nThe 999 row is always included (even if no graduation and hence no Arm selection is actually occurring). If no arm selection is taking place or if there is no delay in performing a Arm selection following the decision to graduate, then the analysis is of exactly the same data as that of the analysis before it, the estimates will differ however from the preceding analysis just due to MCMC sampling differences.\n\n\nAdditional columns in Stage 1 results files\nThe Stage 1 simulations, weeks, and summary files have some additional columns regarding new measurements relevant in a staged design. Besides these columns, the staged output files are similar to the FACTS Core output files:\n\nSimulations.csv and weeksNNNNN.csv for continuous/dichotomous, time-to-event, or multiple endpoints\nSummary.csv for continuous/dichotomous, time-to-event, or multiple endpoints\n\nThe new columns in the staged design output files are:\n\n\nNew s1-summary and s1-ds-summary file fields\nIn s1-summary and s1-ds-summary.csv:\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nP(EG)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2.\n\n\nP(LG)\n1\nThe proportion of simulations where there was successful late graduation (at the final analysis) to Stage 2\n\n\nP(ENG)\n1\nThe proportion of simulations where there was a decision to graduate early (at an interim) to Stage 2, but then Stage 2 could not be run because no treatment arms met the dose selection criteria. (ENG=Early Non-Graduation)\n\n\nP(LNG)\n1\nThe proportion of simulations where there was a decision to graduate late (at the final analysis) to Stage 2, but then Stage 2 could not be run because no treatment arms met the dose selection criteria. (LNG=Late Non-Graduation)\n\n\nNum Selected\n1\nIn s1-ds-summary this is the average over the simulations where stage 1 graduated, of the number of doses selected.\nIn s1-summary this is the average over all the simulations of the number of doses selected.\n\n\nPpn Selected &lt;dose&gt;\nD\nIn s1-ds-summary this is the proportion of the simulations where stage 1 graduated, where this dose was selected\nIn s1-summary this is the proportion of all the simulations where this dose was selected.\n\n\n\n\n\nNew s1-cd-summary file fields\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nUndec\n1\nThe proportion of simulations that reached final analysis and did not meet the Stage 1 final success or futility criteria (and thus graduated late to Stage 2).\n\n\nP(G2F)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2 but where the Stage 1 analysis after complete data met the Stage 1 futility criteria\n\n\nP(G2S)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2 but where the Stage 1 analysis after complete data met the Stage 1 success criteria\n\n\nP(G2I)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2 but where the Stage 1 analysis after complete data did not meet the Stage 1 final success or futility criteria (and thus fall into the Stage 1 late graduation criteria)\n\n\n\n\n\nNew s1-weeks & s1-simulation file fields\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nOutcome\n1\nThis is the outcome status for the Stage 1 simulation – which has some additional possible outcome values compared to FACTS Core:\n\n8: Early Graduation (EG)\n9: Late Graduation (LG)\n10: Early Non-Graduation (ENG) – the graduation criteria were met but no treatment arms met the dose selection criteria, hence Stage 2 was ‘Null’.\n11: Late Non-Graduation (LNG) – the graduation criteria were met but no treatment arms met the dose selection criteria, or the overall max sample size was reached in Stage 1, hence Stage 2 was ‘Null’.\n15: Early graduation, and meets Stage 1 final futility criteria in the complete data analysis\n16: Early graduation, and meets Stage 1 final success criteria in the complete data analysis\n17: Early graduation, and does not meet Stage 1 final futility or success criteria in the complete data analysis\n\n\n\nNum Selected\n1\nThe number of treatment arms selected for inclusion in Stage 2\n\n\nSelect &lt;dose&gt;\nD\nA flag per arm (including Control) indicating whether it was selected for use in Stage 2\n\n\nGraduation &lt;QOI&gt;\n-\nA flag for each QOI in the Early Graduation criteria. In the weeks file on interim analysis rows these will be 1 if met and 0 if not met. However the row in simulations.csv corresponds to the 999 row (dose selection) where the EG criteria are not re-evaluated. Thus in s1-simulations these columns either have the value -1 not evaluated because stage 1 reached final analysis when early graduation criteria are not evaluated or -9999 because the values correspond to the dose selection analysis when no decision criteria are evaluated.\n\n\nGraduation Combined\n1\nA flag for whether the complete graduation criteria were met, including at the final analysis when graduation occurs if neither the final analysis Success or Futility criteria are met. If the trial stopped at an interim the value here will be -9999 as the row corresponds to the 999 row (dose selection) where the early decision criteria are not re-evaluated.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#additional-columns-in-stage-2-results-files",
    "href": "documentation/v71/userguides/staged.html#additional-columns-in-stage-2-results-files",
    "title": "Seamless Trial User Guide",
    "section": "Additional columns in Stage 2 results files",
    "text": "Additional columns in Stage 2 results files\n\nNew s2-summary file fields\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNtrials\n1\nThis is the number of trials simulated. (Whereas the column Nsims reports the number of Stage 2’s simulated).\n\n\nComb No. subj\n1\nThe average (over all the trial simulations) of the combined number of subjects in the trial from Stage 1 and Stage 2\n\n\nSE No. Subj\n1\nThe standard error of the combined number of subjects\n\n\nComb No.Subj 80%-ile\n1\nThe 80th percentile of the combined number of subjects\n\n\nP(ES1)\n1\nThe proportion of trials that ended early for success in Stage 1\n\n\nP(LS1)\n1\nThe proportion of trials that ended late for success in Stage 1\n\n\nP(LF1)\n1\nThe proportion of trials that ended late for futility in Stage 1\n\n\nP(EF1)\n1\nThe proportion of trials that ended early for futility in Stage 1\n\n\nSFFF1\n1\nThe proportion of trials that ended early for success in Stage 1 but “flip-flopped” to futility after completing follow-up\n\n\nFSFF1\n1\nThe proportion of trials that ended early for futility in Stage 1 but “flip-flopped” to success after completing follow-up\n\n\nUndec 1\n1\nThe proportion of trials that ended undecided at the end of Stage 1. (This will be 0. All trials that are undecided at the end of Stage 1 graduate - but they may fail dose selection, which in FACTS output is labeled “Non-Graduation”).\n\n\nP(ENG)\n1\nThe proportion of trials that graduated early from Stage 1 but Stage 2 was unable to run because no treatment arms were selected.\n\n\nP(LNG)\n1\nThe proportion of trials that graduated late from Stage 1 but Stage 2 was unable to run because no treatment arms were selected.\n\n\nP(ES2E)\n1\nThe proportion of trials that ended early for success in Stage 2 having graduated early from Stage 1\n\n\nP(LS2E)\n1\nThe proportion of trials that ended late for success in Stage 2 having graduated early from Stage 1\n\n\nP(LF2E)\n1\nThe proportion of trials that ended late for futility in Stage 2 having graduated early from Stage 1\n\n\nP(EF2E)\n1\nThe proportion of trials that ended early for futility in Stage 2 having graduated early from Stage 2 having graduated early from Stage 1\n\n\nSFFF2E\n1\nThe proportion of trials that ended early for success in Stage 2 but “flip-flopped” to futility after completing follow-up, having graduated early from Stage 1\n\n\nFSFF2E\n1\nThe proportion of trials that ended early for futility in Stage 2 but “flip-flopped” to success after completing follow-up, having graduated early from Stage 1\n\n\nUndec.2E\n1\nThe proportion of trials that ended undecided at the end of Stage 2 having graduated early from Stage 1.\n\n\nP(ES2L)\n1\nThe proportion of trials that ended early for success in Stage 2 having graduated late from Stage 1\n\n\nP(LS2L)\n1\nThe proportion of trials that ended late for success in Stage 2 having graduated late from Stage 1\n\n\nP(LF2L)\n1\nThe proportion of trials that ended late for futility in Stage 2 having graduated late from Stage 1\n\n\nP(EF2L)\n1\nThe proportion of trials that ended early for futility in Stage 2 having graduated late from Stage 2 having graduated early from Stage 1\n\n\nSFFF2L\n1\nThe proportion of trials that ended early for success in Stage 2 but “flip-flopped” to futility after completing follow-up, having graduated late from Stage 1,\n\n\nFSFF2L\n1\nThe proportion of trials that ended early for futility in Stage 2 but “flip-flopped” to success after completing follow-up, having graduated late from Stage 1\n\n\nUndec.2L\n1\nThe proportion of trials that ended undecided at the end of Stage 2 having graduated late from Stage 1.\n\n\nNoNSt2\n1\nThe proportion of trials where Stage 1 graduated to Stage 2 and dose selection was successful, but Stage 2 could not be run because the trial overall sample size had been reached in Stage 1 and Stage 1 data was not being used in Stage 2.\n\n\nMean Comb Alloc &lt;dose&gt;\nD\nFor each dose the mean of the combined number of subjects (over the simulations) allocated to that dose in both Stage 1 and Stage 2.\n\n\nSE Comb Alloc &lt;dose&gt;\nD\nThe standard error of the mean of the combined number of subjects allocated to each dose.\n\n\nComb Duration\n1\nThe mean (over the simulations total duration of the trial from the start of Stage 1 to the end of Stage 2, if a Stage 2 was run, otherwise the end of Stage 1.\n\n\nComb LPFV\n1\nThe mean (over the simulations) of the total accrual period from the start of Stage 1 to the Last Patient First Visit (whether that was in Stage 1 or Stage 2)\n\n\n\nThe remaining columns are as per the summary file for this endpoint in FACTS Core, but note that averages, proportions etc. are taken over only those Stage 2’s that ran.\nIn addition:\n\nThe summaries of mean treatment effect (and mean treatment effect SD) are done over the simulations in which S2 was run AND that treatment arm was kept in stage 2.\nIf on the Transition &gt; Data Inclusion tab, under the option “Min and Max Decision QOIs and Target QOIs are selectable from:”, “only arms selected for Stage 2” has been s elected, then these QOIs will be different simulation to simulation compared to a design where “all arms” was selected, and this will result in different values for the summary.\n\n\n\nNew s2-weeks and s2-simulation file fields\nThe preceding columns are as per the Weeks and Simulations files for this endpoint in FACTS Core (see the corresponding User Guide for detailed listings), but note that if the Stage 2 did not run these columns will contain -9999 in the Simulations file and the Weeks file are empty of data rows.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nComb Weeks\n1\nNumber of weeks into the trial from the start of Stage 1.\n\n\nComb Outcome\n1\nThis is the final outcome of both stages. If the outcome is in the range 1-6then the trial did not graduate to Stage 2 and the outcome of the trial is the outcome of Stage 1.\nOutcome 7 does not occur as failing to meet final success or futility in the final analysis of Stage 1 results in Graduation to Stage 2, not an inconclusive outcome.\nOutcomes:\n10: Early No Graduation – Stage 1 graduated early but no treatment arms were selected for Stage 2\n11: Late No Graduation – Stage 1 graduated late but no treatment arms were selected for Stage 2\n2N: Stage 1 graduated early, N will be in the range 1-7 and indicates the outcome of Stage 2 which is the outcome of the trial.\n3N: Stage 1 graduated late, N will be in the range 1-7 and indicates the outcome of Stage 2 which is the outcome of the trial.\n40: Stage 1 graduated to Stage 2 and dose selection was successful, but Stage 2 could not be run because the trial overall sample size had been reached in Stage 1 and Stage 1 data was not being used in Stage 2.\n\n\nComb # Subjects\n1\nThe total number of subjects enrolled in the trial from both Stage 1 and Stage 2.\n\n\nComb Alloc &lt;Dose&gt;\nD\nThe total number of subjects allocated to each arm from both Stage 1 and Stage 2\n\n\nComb Duration\n1\nThe total duration of the trial from the FPFV of Stage 1 to the final analysis of Stage 2.\n\n\nComb LPFV\n1\nThe total accrual period of the trial from the FPFV of Stage 1 to the LPFV of Stage 2\n\n\nNull Stage\n1\nA flag value 0 or 1. A value of 1 indicates that Stage 2 was not run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/staged.html#data-in-the-patients-files",
    "href": "documentation/v71/userguides/staged.html#data-in-the-patients-files",
    "title": "Seamless Trial User Guide",
    "section": "Data in the patients files",
    "text": "Data in the patients files\nThe data provided in the patients files is the same as the data provided in a patients file for FACTS Core. See the continuous/dichotomous, time-to-event, or multiple endpoint documentation.\nThe contents of the patients files reflect the following principles:\n\ns1-patients – contain the data at the furthest point in time that any analysis is done.  So if the CDA (Complete Data Analysis) option is checked, s1 always has the data associated with the 1000 line. If it isn't checked, it's whatever data is associated with the 999 line.\n\nUnless ‘no censoring is checked’ in which case s1-patients has the same data as if a CDA had not been carried out.\n\ns2-patients – contains the data associated with the Stage 2 999 analysis (i.e. with all appropriate Stage 1 inclusions, arm-relabelling, etc.)\n\nIf in Stage 1 there was follow up, but no CDA and Stage 1 patients’ data was censored then the s1 data in s2-patients will differ from that in s1-patients as it will reflect the full follow up of s1 subjects in S2.\n\nmaster-patients – contains the union of s1-patients and s2-patients. (Note: In TTE, if different follow-up rules are applied in Stages 1 and 2, and S1 subjects are followed into S2, the data that is recorded for these subjects is the data available after stage 2).\n\n\nS1-patients\n\nContents of the s1-patients file after early graduation based on three different inputs in the design.\n\n\n\n\n\n\n\n\nIs Follow up after Interim Graduation Checked?\nIs there a S1 Complete data analysis?\nIs S1 data censored at dose selection?\nContents of the s1-patients file\n\n\nN\n-\n-\nS1 data as at the last s1 analysis.\n\n\nY\nY\n-\nS1 data as at the complete data analysis (full follow up)\n\n\nY\nN\nN\nS1 data after full follow up.\n\n\nY\nN\nY\nS1 data as at the last S1 analysis.\n\n\n\n\nContents of the s1-patients file after a decision that is not early graduation.\n\n\n\n\n\n\nS1 data after …\nContents of the s1-patients file\n\n\nS1 Interim Success/Futility with no follow up checked\nS1 data as at last s1 analysis – the stopping interim\n\n\nS1 Interim Success/Futility with follow up checked\nS1 data as at last s1 analysis – the analysis after follow up\n\n\nS1 Final Success/Futility\nS1 data as at the s1 final analysis\n\n\nFinal Graduation\nS1 data as at the s1 final analysis.\n\n\nSuccess/Futility at Max Time\nS1 data as at the s1 final analysis.\n\n\nGraduation at Max Time\nS1 data as at the s1 final analysis.\n\n\n\n\n\nS2-patients and master-patients\n\nThe data lock timing for the s2-patients and master-patients file based on how stage 1 data is used in stage 2.\n\n\n\n\n\n\n\nS1 use in S2 method\nContents of s2-patients file\nContents of Master-patients file\n\n\nNot used\nS2 data as at last s2 analysis\nCombined s1-patients and s2 patients data\n\n\nUsed in full\nS2 data as at last s2 analysis hence including all s1 data\nCombined s1-patients and s2 patients data, with the s1 use in s2 set\n\n\nSubjects on arms kept in S2 are used\nS2 data as at last s2 analysis including those s1 subjects used in s2\nCombined s1-patients and s2 patients data, with the s1 use in s2 set for those s1 subjects on the arms kept in S2\n\n\nUsed in full and pooled onto the single arm in S2\nS2 data as at last s2 analysis, hence including the s1 data that was used in s2 but with the arm re-labeled\nCombined s1-patients and s2 patients data, with the s1 use in s2 set for those s1 subjects included and which arm in s2 the s1 subjects were pooled with.\n\n\nSubjects not complete in S1 are included in S2\nS2 data as at last s2 analysis, hence including the partial s1 data that was used in s2\nCombined s1-patients and s2 patients data, with the s1 use in s2 set for those s1 subjects included\n\n\n\nThere are two columns in the master-patients file that require explanation:\nStage2Use: This field is a flag indicating how and whether the data from a patient in stage 1 is used in stage 2, the values of the flag are:\n\nNot used in stage 2\n&lt;This value is not used.&gt;\nRetained in stage 2, treated as on the same arm as in stage 1.\nRetailed in stage 2, but pooled onto a different arm, the arm that was retained in stage 2.\n&lt;This value is not used.&gt;\nRetained in stage 2, the patient did not complete in stage 1, so was retained to complete in stage 2.\n\nStage2Arm: This field records which arm (if any) the patient’s data is treated as belonging to in stage 2. This will be same as the arm in stage 1 unless the stage 1 data has been pooled onto the 1 arm retained in stage 2. If the patient’s data is not used in stage 2 this field is set to -9999.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Seamless Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/tte.html",
    "href": "documentation/v71/userguides/enrichment/study/tte.html",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether an event represents patient improvement or deterioration, whether the trial is attempting to show the treatment’s superiority over the control, or its non-inferiority to it, and the length of time subjects are to be followed.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether an event is subject improvement or subject deterioration. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf an event indicates success (subject improvement), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) is greater than 1 by some Target Hazard Ratio Difference for Success (THRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is greater than 1 by more than the Target Hazard Ratio Difference for Futility (THRDF).\nIf an event indicates failure (subject condition worsening), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) to be less than 1 by some Target Hazard Ratio Difference for Success (TRDS). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is less than 1, by the Target Hazard Ratio Difference for Futility (THRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference (CSD), a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\nThe limit on subject follow up, this can be either:\n\nThe maximum follow-up time, no subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event.\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event.\n\n\nNote that as a result of being able to specify whether a higher or lower event rate represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a THRD of 0.3 for success corresponds to a hazard ratio of 0.7”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/tte.html#study-info",
    "href": "documentation/v71/userguides/enrichment/study/tte.html#study-info",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether an event represents patient improvement or deterioration, whether the trial is attempting to show the treatment’s superiority over the control, or its non-inferiority to it, and the length of time subjects are to be followed.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether an event is subject improvement or subject deterioration. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf an event indicates success (subject improvement), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) is greater than 1 by some Target Hazard Ratio Difference for Success (THRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is greater than 1 by more than the Target Hazard Ratio Difference for Futility (THRDF).\nIf an event indicates failure (subject condition worsening), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) to be less than 1 by some Target Hazard Ratio Difference for Success (TRDS). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is less than 1, by the Target Hazard Ratio Difference for Futility (THRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference (CSD), a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\nThe limit on subject follow up, this can be either:\n\nThe maximum follow-up time, no subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event.\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event.\n\n\nNote that as a result of being able to specify whether a higher or lower event rate represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a THRD of 0.3 for success corresponds to a hazard ratio of 0.7”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/tte.html#sec-groups",
    "href": "documentation/v71/userguides/enrichment/study/tte.html#sec-groups",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Group Info",
    "text": "Group Info\nOn this tab values to define the analysis for each group are specified.\nIn a trial to show superiority, for each group the Target Clinically Significant Difference can be set. Optionally the parameters for a phase 3 may be specified to allow the predictive probability of subsequent success in such a phase 3 trial to be used as decision criteria.\n\nWith a time-to-event endpoint the Clinically Significant Difference is in terms of Target Hazard Ratio Difference – that is a difference from 1 in the ratio of the hazard rate in the treatment arm and the hazard rate in the control arm. Separate differences can be set for determining success and futility, and separate differences can be set for each group and for the across groups analysis. Their use in practice is specified on the “Design &gt; Stopping Criteria” and “Design &gt; Evaluation Criteria” tabs.\nThe phase 3 success criteria are:\n\nPhase 3 total number of subjects per arm\nThe required one-sided alpha\nWhether the phase 3 is to use a test for superiority or non-inferiority (set independently from whether the ED trial is for superiority or non-inferiority)\nA super-superiority margin / non-inferiority margin (depending on whether the phase 3 trial is for superiority or non-inferiority), this margin is independent from any margins specified for the ED trial.\n\nGiven these criteria FACTS calculates the predicted probability of success in the subsequent phase 3, given the estimate of the hazard ratio integrated over the uncertainty in that estimate. The conventional expected power of the specified phase 3 is calculated for the treatment effect in each MCMC sample and then averaged. The resulting predicted probability of success in phase 3 can then be used in the stopping criteria and final evaluation criteria.\n\n\n\n\n\n\n\nFigure 3: The Group Info sub-tab.\n\n\n\nSeparate ‘Target Hazard Ratio Differences’ can be set for success and for futility, specifying the THRD for success and THRD for futility. We use the terms Target Hazard Rate Difference here as it makes it clearer that the value to be entered should be positive. Elsewhere (in column headings for instance) the more conventional term CSD may be used.\n\nIf used to decide to stop a group for success or judge whether the group was successful in the final evaluation, the criteria will test whether \\(PR(1 - HR &gt; CSHRD\\ for\\ success) &gt; Success\\ threshold\\ \\).\nThat is, whether the posterior probability that the hazard ratio is less than one by more than the target hazard ratio difference is greater than a specified threshold (which is set on the Design tabs).\nIf used to decide to stop a group for futility or judge whether the group was futile in the final evaluation, the criteria will test whether \\(\\Pr(1 - HR &gt; CSHRD\\ for\\ futility) &lt; Futility\\ threshold\\)\nThat is, whether he posterior probability that the hazard ratio is less than one by more than the target hazard ratio difference for futility is less than a specified threshold (which is set on the Design tabs).\nIf the endpoint is such that an event means subject improvement, then the comparison is reversed (the treatment difference becomes \\(HR - 1\\) and if the trial is non-inferiority then the test for “being greater than the CSD” is replaced with testing for “being less than the NIM” (Non-Inferiority Margin). Thus the meaning of the user specified Difference or Margin is interpreted taking into account both whether a response means ‘better’ or ‘worse’ and whether the trial aim is ‘superiority’ or ‘non-inferiority’. The result is that for normal usage the value entered will be +ve, as the following diagrams should make clear:\n\n\n\n\n\n\nFigure 4\n\n\n\n\nNote that in Superiority trials the required THRD for success will be greater than or equal to the THRD for futility, whereas in the Non-Inferiority trials the required NIM for futility will be greater than or equal to the NIM for success.\n\nNotes on setting Target Hazard Ratio Differences\nTarget Hazard Ratio Differences (THRD) are also referred to as Clinically Significant Differences (CSD). A “standard” hypothesis test for demonstrating superiority to control uses a CSD of 0. Testing with a non-zero CSD is different, and the implications need to be carefully understood.\nThe first mistake to avoid is setting the target hazard ratio difference for success too large. The decisions for success are in terms of the estimated posterior probability that the difference of the hazard ratio from 1 is greater than the target. If the CSD is set to what might be the treatment difference in the “alternate hypothesis” of a standard hypothesis test, we could only expect on average to have a posterior probability that the treatment difference is greater than the CSD of 50%.\nTo achieve posterior probabilities of &gt; 50%, the observed difference must be greater than the target difference. To achieve the desired power by lowering the required posterior probability would be a mistake, as posterior probability thresholds of &lt; 50% have the undesirable characteristic that the criteria can be met in circumstances where it can be seen that if further data was gathered consistent with what had been seen already, it would lead the threshold no longer being met. The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a target hazard ratio difference for success that, should the treatment have the value that it is hoped to achieve, we would expect to see some &gt;50% probability of being greater than it. Thus rather than using what might be termed ‘the target value’ for the THRDS, it is better to use ‘the minimum acceptable value’.\nThe same target difference can be used to decide futility, requiring a &lt;&lt; 50% confidence that the difference of the hazard ratio from 1 is greater than the target difference. However, particularly if there are other endpoints not being explored in the simulation, or other properties of the treatment (such as convenience, compliance, cost, tolerability etc.) that might justify continued development even if it is not an outright winner on the primary endpoint, it may be that a lower target hazard ration difference needs to be set as the threshold for determining futility – for example sufficient to demonstrate that development even on the basis of non-inferiority on the primary endpoint is likely to fail. Hence FACTS allows separate CSDs to be set for assessing success and futility.\n\n\nNon-inferiority\nIn a trial to show non-inferiority, the tab is the same except that ‘Target Hazard Ratio Differences’ are now ‘Margin deltas’ to be set.\n\n\n\n\n\n\nFigure 5: Group Info tab, in a non-inferiority design.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/tte.html#visits",
    "href": "documentation/v71/userguides/enrichment/study/tte.html#visits",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Visits",
    "text": "Visits\nIf events are observed at any time that they occur during follow up, then there is no need to specify a visit schedule, the length of follow up will be as it has been specified on the Study &gt; Study Info tab (above), leave the “Events Are Only Observed at Visits” check box unchecked:\n\n\n\n\n\n\nFigure 6: Events Observed When They Occur.\n\n\n\nIf observation of event is to be only at visits, the visit schedule needs to be specified. There is an additional observation at the end of follow-up. Visits can be specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 7: Observation of Events Restricted to Visits.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/tte.html#variants",
    "href": "documentation/v71/userguides/enrichment/study/tte.html#variants",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of subjects).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different maximum subjects for each group that has had a cap specified on the Study &gt; Study Info tab and maximum “Total Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\nIn FACTS Enrichment Designs, as well as trial Success and Failure rates, a major Operating Characteristic that we often wish to estimate is the ability of the design (if the trial is successful) to select the ‘right’ groups, depending of course on the scenario being simulated. To enable FACTS to report this the user must specify on the Virtual Subject Response &gt; Explicitly Defined &gt; Group Response profiles which of the groups “Should succeed”, that is, it would constitute a ‘correct selection’ by the design in that scenario.\n\n\n\n\n\n\nFigure 8: The Variants tab, specifying 3 variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/continuous.html",
    "href": "documentation/v71/userguides/enrichment/study/continuous.html",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included, the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether higher response or lower response is subject improvement. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a higher response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be greater than that of the subjects in the control arm (or of a specified mean change based on historical data), or greater than control by some Target mean difference for success (The Clinically Significant Difference, CSD, for success).\nIf a lower response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be less than that of the subjects in the control arm (or of a specified mean change based on historical data), or less than control by some Target Mean Difference for Futility (The CSD for futility).\nSee Section 2 for more information\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be “not worse than” control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSDs or the NI Margins will almost always be positive values. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 10 for success corresponds to lowering the mean by 10”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\nTo ‘Include baseline data’ or not. If baseline is included then additional options are enabled to model subjects’ baselines and their possible interaction with response. If baseline is included response may be:\n\nChange from baseline\nFinal endpoint value\n\nNote that this setting changes both how responses are simulated and how they are modelled, so for most purposes this choice of response merely changes how things are labelled. The exceptions are ITP and Baseline Carried Forward where the choice of ‘change from baseline’ or ‘final endpoint value’ does affect behavior.\n\n\n\n\n\n\n\n\nFigure 2: The Group Info sub-tab\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nThe groups’ names edited to something meaningful for the trial and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall study cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall study cap (which to have any effect, must be less than the sum of the individual group caps).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/continuous.html#study-info",
    "href": "documentation/v71/userguides/enrichment/study/continuous.html#study-info",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included, the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether higher response or lower response is subject improvement. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a higher response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be greater than that of the subjects in the control arm (or of a specified mean change based on historical data), or greater than control by some Target mean difference for success (The Clinically Significant Difference, CSD, for success).\nIf a lower response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be less than that of the subjects in the control arm (or of a specified mean change based on historical data), or less than control by some Target Mean Difference for Futility (The CSD for futility).\nSee Section 2 for more information\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be “not worse than” control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSDs or the NI Margins will almost always be positive values. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 10 for success corresponds to lowering the mean by 10”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\nTo ‘Include baseline data’ or not. If baseline is included then additional options are enabled to model subjects’ baselines and their possible interaction with response. If baseline is included response may be:\n\nChange from baseline\nFinal endpoint value\n\nNote that this setting changes both how responses are simulated and how they are modelled, so for most purposes this choice of response merely changes how things are labelled. The exceptions are ITP and Baseline Carried Forward where the choice of ‘change from baseline’ or ‘final endpoint value’ does affect behavior.\n\n\n\n\n\n\n\n\nFigure 2: The Group Info sub-tab\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nThe groups’ names edited to something meaningful for the trial and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall study cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall study cap (which to have any effect, must be less than the sum of the individual group caps).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/continuous.html#sec-group-info",
    "href": "documentation/v71/userguides/enrichment/study/continuous.html#sec-group-info",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Group Info",
    "text": "Group Info\nOn this tab values that govern the analysis for each group are specified.\n\nTrials to show Superiority\nIn a trial to show superiority, for each group the Target mean difference for success and Target mean difference for futility may be specified, these are referred to as the CSDs (Clinically Significant Differences). Optionally the parameters for a phase 3 may be to allow the predictive probability of subsequent success in such a phase 3 trial to be used as decision criteria.\n\nWith a continuous endpoint the Clinically Significant Difference is in terms of Target Mean Difference – that is a difference between the mean change from baseline in the treatment arm and the mean changed from baseline in the control arm. Separate differences can be set for determining success and futility, and separate differences can be set for each group and for the across groups analysis. Their use in practice is specified on the “Design &gt; Stopping Criteria” and “Design &gt; Evaluation Criteria” tabs.\n\n\n\n\n\n\n\nFigure 3: The Group Info sub-tab\n\n\n\n\nThe phase 3 criteria are:\n\nPhase 3 total number of subjects per arm\nThe required one-sided alpha\nWhether the phase 3 is to use a test for superiority or non-inferiority (set independently from whether the ED trial is for superiority or non-inferiority)\nA super-superiority margin / non-inferiority margin (depending on whether the phase 3 trial is for superiority or non-inferiority), this margin is independent from any margins specified for the ED trial.\n\nGiven these criteria FACTS calculates the predicted probability of success in such a trial for each treatment arm given the estimate of the treatment difference, integrated over the uncertainty in that estimate. The conventional expected power of the specified phase 3 is calculated for the treatment effect in each MCMC sample and then averaged. The resulting predicted probability of success in phase 3 can then be used in the stopping criteria and final evaluation criteria.\n\nSeparate ‘Target Mean Differences’ can be set, specifying the TMD for success and TMD for futility. We use the terms Target Mean Difference here, as it makes it clearer that the value to be entered should be positive. Elsewhere (in column headings for instance) the more conventional term CSD is used.\n\nIf the “Posterior probability” criteria is used for stopping a group for success or judging if a group is successful in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\theta_{d} - \\theta_{0} &gt; CSD\\ for\\ success \\right) &gt; Success\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target mean difference for success is greater than a specified threshold (set on the Design tabs).\nIf the “Posterior probability” criteria is used for stopping a group for futility or judging if a group is futile in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\theta_{d} - \\theta_{0} &gt; CSD\\ for\\ futility \\right) &lt; Futility\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target mean difference for futility by is less than a specified threshold (set on the Design tabs).\nIf the endpoint is such that a lower response means improvement, then the comparison is reversed (becomes \\(\\theta_{0} - \\theta_{d}\\)) and if the trial is non-inferiority then the test for “being greater than the CSD” is replaced with testing for “being less than the NIM” (Non-Inferiority Margin). Thus the meaning of the user specified Difference or Margin is interpreted taking into account both whether a higher response means ‘better’ or ‘worse’ and whether the trial aim is ‘superiority’ or ‘non-inferiority’. The result is that for normal usage the value entered will be +ve, as the following diagrams should make clear:\n\n\n\n\n\n\nFigure 4\n\n\n\n\nNote that in Superiority trials the required TMD for success will be greater than or equal to the TMD for futility, whereas in the Non-Inferiority trials the required NIM for futility will be greater than or equal to the NIM for success.\n\n\nNotes on setting Target Mean Differences\nA “standard” hypothesis test for demonstrating superiority to control uses an effective TMD, or CSD, of 0. Testing with a non-zero CSD is different, and the implications need to be carefully understood.\nThe first mistake to avoid is setting the target mean difference for success too large. The decisions for success are in terms of the estimated posterior probability that the mean difference between the response on the treatment arm and the response on the control arm is greater than the target. If the CSD is set to what might be the treatment difference in the “alternate hypothesis” in standard hypothesis test, we could only expect on average to have a posterior probability that the treatment difference is greater than the CSD of 50%.\nTo achieve posterior probabilities of &gt; 50%, we must set a CSD that we expect the treatment to exceed. To achieve the desired power by instead lowering the required posterior probability threshold would be a mistake, as posterior probability thresholds of &lt; 50% have the undesirable characteristic that the criteria can be met in circumstances where it can be seen that if further data was gathered consistent with what had been seen already, it would lead the threshold no longer being met. The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a target mean difference for success that, should the treatment have the value that it is hoped to achieve, we would expect to see some &gt;50% probability of being greater than it. Thus rather than using what might be termed ‘the target value’ for the CSD, it is better to use ‘the minimum acceptable value’.\nThe same target difference can be used to decide futility, requiring a &lt;&lt; 50% confidence that the mean difference of the response on the treatment arm from response on the control arm is greater than the target. However, particularly if there are other endpoints not being explored in the simulation, or other properties of the treatment (such as convenience, compliance, cost, tolerability etc.) that might justify continued development even if it is not an outright winner on the primary endpoint, it may be that a lower target mean difference needs to be set as the threshold for determining futility – for example sufficient to demonstrate that development even on the basis of non-inferiority on the primary endpoint is likely to fail. Hence FACTS allows separate CSDs to be set for assessing success and futility.\n\n\nTrials to show Non-inferiority\nIn a trial to show non-inferiority, the tab is the same except that ‘Target Mean Differences’ are now ‘Target Non-inferiority Margins’.\n\n\n\n\n\n\nFigure 5: Group Info tab, in a non-inferiority design\n\n\n\nAs with trials to show Superiority, optionally the phase 3 criteria can be set (see Section 2.1).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/continuous.html#visits",
    "href": "documentation/v71/userguides/enrichment/study/continuous.html#visits",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Visits",
    "text": "Visits\nIf longitudinal modeling is not being used, then simply the time to the final visit is specified:\n\n\n\n\n\n\nFigure 6: Time to endpoint\n\n\n\nOtherwise the visit schedule needs to be specified. The last visit in the schedule is taken to be when the final endpoint is observed. Visits can be specified one at a time by entering the week of the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week of the visit and the visit index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 7: Visit schedule",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/study/continuous.html#variants",
    "href": "documentation/v71/userguides/enrichment/study/continuous.html#variants",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of subjects).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different maximum subjects for each group that has had a cap specified on the Study &gt; Study Info tab and maximum “Total Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\nIn FACTS Enrichment Designs, as well as trial Success and Failure rates, a major Operating Characteristic that we often wish to estimate is the ability of the design (if the trial is successful) to select the ‘right’ groups, depending of course on the scenario being simulated. To enable FACTS to report this the user must specify on the Virtual Subject Response &gt; Explicitly Defined &gt; Group Response profiles which of the groups “Should succeed”, that is, it would constitute a ‘correct selection’ by the design in that scenario.\n\n\n\n\n\n\nFigure 8: The Variants tab, specifying 5 variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/dichotomous.html",
    "href": "documentation/v71/userguides/enrichment/vsr/dichotomous.html",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual subject response from externally simulated PK/PD data. When simulations are executed, they will be executed for each profile specified by the user.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/dichotomous.html#explicitly-defined",
    "href": "documentation/v71/userguides/enrichment/vsr/dichotomous.html#explicitly-defined",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\n\nGroup Response\nResponse profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 1 Response rate values are entered for the treatment and control arms (if present) for each group directly into the Treatment Response Rate and Control Response Rate columns of the table. The graphical representation of these values updates accordingly.\nIn addition it is possible for the user to specify if a group “Should Succeed” using the “Should succeed” checkbox on each row. This is then used in the summary of the simulation results to compute how often the simulated trial was successful and groups that ‘Should Succeed’ were successful (reported in the column “Ppn Correct Groups”) and how often the simulated trial was successful and groups that were not marked ‘Should Succeed’ were successful (reported in the column “Ppn Incorrect Groups”).\n\n\n\n\n\n\nFigure 1: Virtual Subject Response – Explicitly-Defined - Group Response sub-tab.\n\n\n\nThe graph that shows the response defined – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right-clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\n\n\nLoad Scenario Responses From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses.\nEach individual simulation uses one set of responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 2: Virtual Subject Response - Loading scenario group response rates from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual mean responses and the overall mean response over all the VSRs.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. In the normal case the format is:\n\nIf a control arm is being used: Each line should contain columns [PT1, PT2, … , PTG, PCG, PC1, PC2, … , PCG] giving the true mean response probabilities (PTi) for the Treatment arm in each of the G groups, followed by the analogous parameters for the Control arms.\nWithout a control arm, using Objective Control: Each line should contain columns [PT1, PT2, … , PTG] giving the true mean response probabilities (PTi) for the Treatment arm in each of the G groups.\n\nWhen using the Restricted Markov Model the format is:\n\nIf a control arm is being used: Each line should contain columns [P1T1, P1T2, … , P1TG, P0T1, P0T2, … , P0TG, P1C1, P1C2, … , P1CG, P0C1, P0C2, … , P0CG] giving the true mean response probabilities (P1Ti) and true mean non-response probabilities (P0Ti) for the Treatment arm in each of the G groups, followed by the analogous parameters for the Control arms.\nWithout a control arm, using Objective Control: Each line should contain columns [P1T1, P1T2, … , P1TG, P0T1, P0T2, … , P0TG] giving the true mean response probabilities (P1Ti) and true mean non-response probabilities (P0Ti) for the Treatment arm in each of the G groups.\n\nFor example:\n# Dichotomous, 4 groups with control\n#T1 T2 T3 T4 C1 C2 C3 C4\n# 5 null cases\n0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23\n0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24\n0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25\n0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26\n0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27\n# 5 increasingly good\n0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25\n0.275, 0.31, 0.26, 0.30, 0.25, 0.25, 0.25, 0.25\n0.30, 0.37, 0.27, 0.35, 0.25, 0.25, 0.25, 0.25\n0.325, 0.43, 0.28, 0.40, 0.25, 0.25, 0.25, 0.25\n0.35, 0.49, 0.29, 0.45, 0.25, 0.25, 0.25, 0.25",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/dichotomous.html#longitudinal-models",
    "href": "documentation/v71/userguides/enrichment/vsr/dichotomous.html#longitudinal-models",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Longitudinal Models",
    "text": "Longitudinal Models\nIf ‘Use longitudinal modeling’ has been checked on the ‘Study Info’ tab, and explicitly defined group responses have been specified, then it will be necessary to specify how to simulate the subjects’ responses at intermediate visits. This is done by specifying a transition method that can be combined with any of the response profiles to generate the intermediate results for subjects on treatment or control in all the groups.\nThe methods are parameterized so that its inclusion does not affect the response rate of the final endpoint to be simulated, and they scale automatically to suit each group response profile.\nClick here for an overview of longitudinal models for dichotomous endpoints in the FACTS Core engine.\n\nEnter / Retain Method\nThe Enter/Retain method is parameterized by setting probabilities for transition from non-responder to responder, and for remaining a responder once observed for each visit.\n\n\n\n\n\n\nFigure 3: Simulated Longitudinal Results – enter/retain method.\n\n\n\nThe transition probabilities will be modified for each treatment arm in each group response profile, to preserve the specified rate of response to simulate. This is done by finding (using numerical iteration) for each response rate to be simulated, the offset which, when added to the log odds of the all the transition probabilities, results in transition probabilities that give the required response rate.\n\n\nRestricted Markov Model\nIf the restricted Markov model was selected on the Study Info tab, the subject simulation method is fully described by the overall probability of response and failure for each group.\n\n\n\n\n\n\nFigure 4: Simulated subject response - restricted Markov method.\n\n\n\nFor each group, a final probability of response and failure are specified. The Probability of final stability is then inferred as the probability that neither response nor failure is observed. The response and failure rate can then be calculated as:\n\\[\n\\lambda_{r} = \\frac{\\Pr(resp)\\left\\lbrack - \\ln{\\Pr(stable)} \\right\\rbrack}{\\nu_{T}\\left( 1 - Pr(stable) \\right)}\n\\]\n\\[\n\\lambda_{f} = \\frac{\\Pr(fail)\\left\\lbrack - \\ln{\\Pr(stable)} \\right\\rbrack}{\\nu_{T}\\left( 1 - Pr(stable) \\right)}\n\\]\nwhere \\(\\nu_{T}\\) is the time of the final visit. When a virtual subject is recruited into the trial, a response and failure time are simulated based on the response and failure rates. If both events (response and failure) occur after the final visit, the simulated subject is counted as a final stability and their outcome is based on whether final stabilities count as success or failure as specified on the Study &gt; Study Info tab. If one or both events occur before the final visit, the event that occurred first is taken as the subject’s final response because this is an absorbing state model. Once an outcome has been observed, that outcome remains for the rest of the treatment schedule.\nIf values are entered for the probability of response and failure that sum to greater than one, the probability of stability displays an error and the response rates are reported as NaN (not a number).\nIf values are entered for the probability of response and failure that sum to exactly one, the probability of stability is 0 and both response rates are effectively infinite. In this case, “**” is displayed as the response rate although these are valid settings.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/vsr/dichotomous.html#external",
    "href": "documentation/v71/userguides/enrichment/vsr/dichotomous.html#external",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subject response within FACTS they can be simulated externally, from a PK-PD model for instance, and imported into FACTS, and the supplied responses are sampled from (with replacement) to provide the subject responses in the simulation. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below.\nTo import an external file, the user must first add a profile to the table. After adding the profile, a file selector window is opened and the user must select the file of externally simulated data. To change the selection, click the ‘Browse’ button.\n\n\n\n\n\n\nFigure 5: External Data.\n\n\n\n\nRequired Format of Externally Simulated Data\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id (must be positive and change from subject to subject)\nGroup index (1, 2, 3,… )\nArm Index (1 = Control, 2 = Treatment)\nVisit Id (1, 2, 3, …)\nResponse (0, 1), if the Restricted Markov model is being used then possible Response values are (0, 1, -1 = stable).\n\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file. Note that all visits for each subject must be grouped together. Thus all the data for the first subject comes before that of the second, and so on.\n#Patient ID, Group Index, Arm Index, Visit, Response\n1, 1, 1, 1, 0\n1, 1, 1, 2, 0\n1, 1, 1, 3, 1\n1, 1, 1, 4, 0\n2, 1, 2, 1, 0\n2, 1, 2, 2, 0\n2, 1, 2, 3, 0\n2, 1, 2, 4, 0\n3, 2, 1, 1, 0\n3, 2, 1, 2, 0\n3, 2, 1, 3, 1\n3, 2, 1, 4, 1\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/design.html",
    "href": "documentation/v71/userguides/enrichment/design.html",
    "title": "Design Overview",
    "section": "",
    "text": "The Design Tab allows the user to specify how control responses, treatment responses and longitudinal responses are modeled, how subjects are allocation, the timing of interims, and the criteria for stopping groups or the study for early success or futility, and the criteria for judging final success or futility of each group and the study. Wherever applicable, this user guide is separated by type of endpoint used (continuous/dichotomous or time-to-event).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Design"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/design.html#allocation",
    "href": "documentation/v71/userguides/enrichment/design.html#allocation",
    "title": "Design Overview",
    "section": "Allocation",
    "text": "Allocation\nOn the allocation tab:\n\nIf there are control arms included in the trial - the user specifies the relative proportion of subjects allocated to each arm in each group. The proportions are only relative within a group, not across groups. The relative proportion of subjects between groups depends on the relative accrual rates into the different groups and the points at which each group stops accruing.\nIf no Control Arm is included in the trial this tab is not displayed.\n\n\n\n\n\n\n\nFigure 34: Design - Allocation tab\n\n\n\nIn the example screenshot above, 2:2 randomizations has been specified for all groups, giving them 1:1 allocation with a block size of 4.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Design"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html",
    "title": "Simulation",
    "section": "",
    "text": "The Simulation tab allows the user to execute simulations for each of the scenarios specified for the study. The user may choose the number of simulations, whether to execute locally or on the Grid, and modify the random number seeds.\nIn the Simulation tab the user can provide simulation configuration parameters like the number of simulations to run, whether the simulations can be run on the Grid, the parallelization strategy, the random number seed used in the simulations, and the number of certain output files that should be kept during the simulation execution.\nFACTS uses Markov Chain Monte Carlo (MCMC) methods in the generation of simulated patient response data and trial results. In order to exactly reproduce a statistical set of results, it is necessary to start the Markov Chain from an identical “Random Seed”. The initial random seed for FACTS simulations is set from the simulation tab, the first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. It is possible to re-run a specific simulation, for example to have more detailed output files generated, by specifying ‘start at simulation’.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#number-of-simulations",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#number-of-simulations",
    "title": "Simulation",
    "section": "Number of simulations",
    "text": "Number of simulations\nThis box allows the user to enter the number of simulations that they would like FACTS to run for each scenario listed in the table at the bottom of the simulation tab. There is no set number of simulations that is always appropriate.\n\n10 simulations\n\nYou might want to run 10 simulations if you just want to look at a few simulated trials and assess how the decision rules work and if FACTS is simulating what you expected based on what you input on the previous tabs. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\n\n100 simulations\n\nYou might want to run 100 simulations if you want to look at many individual trials to make sure that what you want to happen is nearly always happening. You can also start to get a very loose idea about operating characteristics like power based on 100 sims. 100 simulations is also usually sufficient to spot big problems with the data analysis such as poor model fits or significant bias in the posterior estimates.\n\n1,000 simulations\n\nYou might want to run 1,000 simulations if you want estimates of operating characteristics like power, sample size, and Type I error for internal use or while iterating the design. This generally isn’t considered enough simulations for something like a regulatory submission. With 1,000 simulations the standard error for a typical type I error calculation is on the order of \\(0.005\\).\n\n10,000 simulations\n\nYou might want to run 10,000 simulations per scenario if you are finalizing a design and are preparing a report. This is generally enough simulations for a regulatory submission, especially in non-null simulation scenarios. The standard error for a typical type I error calculation using 10,000 simulations is on the order of \\(0.0015\\).\n\n&gt; 10,000\n\nYou might want to run more than 10,000 simulations if you want to be very certain of an operating characteristic’s value - like Type I error. And plan to use the measurement of the quantity for something important like a regulatory submission. The standard error of a Type I error calculation with 100,000 simulations, e.g., is on the order of \\(0.0005\\).\n\n&gt; 100,000\n\nYou probably don’t want to run more than 100,000 simulations per scenario. Maybe your finger slipped an hit an extra 0, or you thought there were 5 zeroes in that number when there were actually 6. If the simulated trial is adaptive, this is going to take a while.\n\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. Not all scenarios must be run with the same number of simulations. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#sec-startatsimulation",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#sec-startatsimulation",
    "title": "Simulation",
    "section": "Start at Simulation",
    "text": "Start at Simulation\nThe “Start at simulation” option allows for the simulation of a particular trial seen in a previous set of simulations without having to simulate all of the previous trials in that previous set to get to it.\nThe initial random seed for FACTS simulations is set in the simulation tab. The first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. Thus, it is possible to re-run a specific simulation out of a large set without re-running all of them. For example, say the 999th simulation out of a set displayed some unusual behavior, in order to understand why, one might want to see the individual interim analyses for that simulation (the “weeks” file), the sampled subject results for that simulation (the “Subjects” files) and possibly even the MCMC samples from the analyses in that simulation. You can save the .facts file with a slightly different name (to preserve the existing simulation results), then run 1 simulation of the specific scenario, specifying that the simulations start at simulation 999 and that at least 1 weeks file, 1 subjects file and the MCMC samples file (see the “MCMC settings” dialog) are output.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#parallelization-packet-size",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#parallelization-packet-size",
    "title": "Simulation",
    "section": "Parallelization Packet Size",
    "text": "Parallelization Packet Size\nThe parallelization packet size option allows simulation jobs to be split into runs of no-more than the specified number of trials that are run in parallel. If more simulations of a scenario are requested than can be done in one packet, the simulations are broken into the requisite number of packets, run, and combined and summarized when they are all complete. The final results files will look just as though all the simulations were run as one job or packet.\nThe packet size must be a perfect divisor of the number of simulations. This is usually easy since common numbers of simulations are multiples of 100, but don’t try to use a prime number for Number of Simulations or you’re stuck with only 2 packet size options.\nBy default (if the check box with Choose Parallel Packet Size is not checked) the number of simulations per packet depends on the number of simulations per scenario. If the number of simulations is less than 1000, then each scenario is packaged as a single packed and simulated. If the number of simulations per scenario is greater than or equal to 1000, the default packet size is 10 and all simulations are decomposed into packets of size 10.\n\n\n\n\n\n\nPacketization with VSRs using .mvsr files\n\n\n\nIf an external file is used to create explicit VSRs (a .mvsr file), then the packet size should be a multiple of the number of rows in that MVSR file. Each packet will get passed the entire .mvsr file to run. If there are multiple .mvsr files with differing numbers of lines then only the VSR scenarios that have a .mvsr file that has a number of rows that is a divisor of the packet size will be run. The rest will error. The packet size can then be modified to get each of the .mvsr specified VSR files to be run.\nCare should be taken when packetizing a scenario that includes an external data file to supply the virtual subject responses; in this situation, a of copy of the external file is included in each packet which can cause the packetisation process to run out of memory as the packets are being created. In this case, use a smaller number of larger packets, such as packets that are 1/10th of the total number of simulations.\n\n\nWhen running simulations, FACTS will create and run as many packets in parallel as there are execution threads on the local machine. In general, the overhead of packetization is quite low, so a packet size of 10 to 100 can help speed up the overall simulation process. Threads used to simulate scenarios that finish quickly can pick up packets for scenarios that take longer. The progress bar updates as simulation packets complete, so the smaller the packet size, the more accurately FACTS can report the overall progress of the simulation execution.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#sec-randomseed",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#sec-randomseed",
    "title": "Simulation",
    "section": "Random Seed",
    "text": "Random Seed\nRandom number generation plays a huge role in FACTS’s virtual patient generation and statistical analyses. In order to exactly reproduce a statistical set of results, it is necessary to start the random number generation process from an identical “Random Seed”. Using the same random seed in the same version of FACTS guarantees that simulated trials will always be reproducible. Changing the design parameters or the version of FACTS may or may not remove this reproducibility depending on the change.\nEven a small change in the random seed will produce very different simulation results.\nIn addition to setting the seed, the user can choose whether they want the “Same seed for all scenarios” or “Different seed” for different scenarios. If “Same seed for all scenarios” is selected, the subjects generated for each simulated trial will match for the different scenarios. This induces a correlation among the simulation output for different scenarios. This can be good if you’re trying to compare operating characteristics for different scenarios, but it can also be misleading. To disable this option select the “Different Seed” option. If “Different seed” is selected, then each scenario has its own seed that samples a different set of subjects than any other scenario. This uncorrelates the simulation output across scenarios, which can be advantageous if the absolute value of the operating characteristics are more valuable to you than the comparison of operating characteristics across scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#mcmc-settings",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#mcmc-settings",
    "title": "Simulation",
    "section": "MCMC Settings",
    "text": "MCMC Settings\nTo set advanced settings for simulation, the user may click the “MCMC Settings” button, which will display a number of additional specifiable parameters for simulation in a separate window.\n\n\n\n\n\n\nFigure 2: Advanced MCMC settings.\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the MCMC chain to reach its stationary distribution. Burn-in samples are output in MCMC files if the files are output.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest.\n\nThe third parameter controls the number of MCMC samples taken after each imputation of missing data using the longitudinal model. The default value is 1. This parameter only has an effect if Bayesian imputation is being used to impute missing or partially observed data. Increasing the value of this parameter allows the parameter estimates to converge somewhat to a potentially new stationary distribution for each new set of imputed data. If the imputed data is only a small percentage of the overall data this is likely unnecessary. As a rough guide, if it at some early interims &gt; 5% of the data being analyzed will be imputed, a value in the range 2 to 10 is recommended to avoid underestimating the uncertainty. A higher number should be used the greater the proportion of imputed data.\nThe next parameter concerns the output of the MCMC samples to a file. It is possible to have the design engine output the sampled values from the MCMC in all of the interims of the first N simulated trials of each scenario by specifying the “Number of MCMC files to output” to be greater than 0. The resulting files, ‘mcmcNNNN.csv’, will be in the results directory with all the other results files for that scenario. These files include the burn-in samples from the MCMC chains.\nThe final parameter in MCMC Settings is the thinning parameter. This parameter will only keep every \\(N^{th}\\) sample taken during MCMC where \\(N\\) is the thinning parameter. Thinning MCMC samples can reduce the autocorrelation of consecutive MCMC iterations, which increases the effective samples per retained sample, but also results in needing many more MCMC iterations to reach the same number of retained samples. Generally, we do not recommend thinning for standard simulation runs.\n\n\n\n\n\n\nWarning about thinning\n\n\n\nUnlike other software that performs MCMC, when you choose to thin by a value, FACTS does not increase the number of MCMC iterations it performs in order to retain the value specified in “Number of Samples”. So if you leave “Number of Samples” at its default value, \\(2500\\), and thin by \\(10\\), you will be left with \\(250\\) retained samples. You should adjust for this by increasing the “Number of Samples” if you choose to thin.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#results-output",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#results-output",
    "title": "Simulation",
    "section": "Results Output",
    "text": "Results Output\nThe results output section of the Simulation tab allows for the specification of how many output files should be generated for files that are individually created for each simulation. Summary files (summary.csv) that have 1 line per scenario are always created. Simulations files (simulations.csv) that have 1 line per simulation are always created. Weeks files (weeksXXXXX.csv), patients files (patientsXXXXX.csv), and frequentist weeks files (weeks_freq_{missingness}_XXXXX.csv) are not created for every single simulation. Instead, the number of simulation specific output files can be set per type. This limits the amount of output files that FACTS will save.\nSee the endpoint specific descriptions of the output files for descriptions of what the previously mentioned output files report (continuous, dichotomous and time-to-event).\nSome plots in FACTS that are created based on weeks files, and if very few weeks files are saved, the plots will not be as accurate or descriptive.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#facts-grid-simulation-settings",
    "title": "Simulation",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nA user with access to a computational grid may choose to run simulations on the grid instead of running them locally. This frees the user’s computer from the computationally intensive task of simulating so that they can continue other work or even shutdown their PC or laptop. In order to run simulations on the grid, it must first be configured. This is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#right-click-menu",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#right-click-menu",
    "title": "Simulation",
    "section": "Right Click Menu",
    "text": "Right Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 4: The menu that appears when you right click on the table within the simulation tab.\n\n\n\nThese will respectively:\n\nOpen a new Windows directory browser window showing the contents of the simulation results for that scenario.\nOpen a window that displays the individual simulation results for that scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results (see below) the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of the Simulation Results highlights window.\nOpen a window that displays the frequentist analysis summary results. This option is only available if one or more frequentist analyses have been selected on the Design &gt; Frequentist Analysis tab. (If more than one analysis has been requested – using different treatments of missing data there will be separate options in the menu to display each summary).\nOpen R loading in the result files for that scenario as separate dataframes.\nOpens the FACTS graph control displaying the graphs for that scenario.\nOpens the FACTS graph control that displays the trellis plot of graphs of selected scenarios for selected design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#open-in-r",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#open-in-r",
    "title": "Simulation",
    "section": "Open in R",
    "text": "Open in R\nThe “Open in R” button allows for the creation of an R script that has pre-populated code for loading in output files created by the FACTS simulations.\nBy default, any/all of the simulation output files can be included in the created script. If “Aggregation” (see below) has been performed, then only the aggregated files will be available for being loaded in R.\nWhen the button is clicked, FACTS will create an R script with the correct file paths to load in the data, as well as creating a function that will read the files in correctly. The file is then opened in the default R editor for the user. If there is no default program for opening a .R file, your operating system should ask how you want to open the file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#aggregation",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#aggregation",
    "title": "Simulation",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 5: Window that appears when aggregating simulation results.\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per group, plus an extra across groups row.\n\nWhere there is a group of columns for each group, they will be turned into a single column with each value on a new row.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. WeeksNNNNN.csv files are aggregated into a single agg_[pivot_]weeks.csv file. PatientsNNNNN.csv files are aggregated into a single agg_patients.csv file, but they are never pivoted because each row already refers to a single group. Similarly the various frequentist results at the summary, simulation and weeks level are aggregated (if they’ve been output).\nRegionIndex.csv is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nRecruitment Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g. External Subjects Profile if there are no external scenarios)\n\n\nDropouts Profile\n\n\n\nLongitudinal Rates Profile\n\n\n\nGroup Response Profile\n\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nP(TS)\nProportion of trial success (early success + late success)\n\n\nP(TF)\nProportion of trial futility (early futility + late futility)\n\n\nSims\nSimulation number. Only present in weeks and patients files.\n\n\nGroup\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#design-report",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#design-report",
    "title": "Simulation",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#per-scenario-graphs",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#per-scenario-graphs",
    "title": "Simulation",
    "section": "Per Scenario Graphs",
    "text": "Per Scenario Graphs\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select that row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks. (Not displayed if the ‘y’ value must lie in the interval 0-1.\nGroup – on some graphs the results show results for the treatment effect in a specific group or across groups, this drop down allows the user to select which.\nSimulation – on some graphs the data shown is from a specific simulation, this control allows the user to select which one.\nInterim – on some graphs the data shown is from a specific interim in a specific simulation, this control allows the user to select which one.\n\n\nOutcome and Subject Allocation\n\nContinuousDichotomousTime-to-Event\n\n\n\nRelative Response and Allocation\n\n\n\n\n\n\nFigure 6: Relative Response and Allocation.\n\n\n\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations in each group plotted as a green bar.\nThe true difference in response between the study treatment and control in each group as a black cross.\nThe estimated mean difference in response (“treatment effect”) and the 2.5-97.5% interquartile range of the observed estimates across the simulations in each group as a red circle with vertical red error bars.\nThe estimated mean overall difference from the individual control responses and 2.5-97.5% interquartile range of the observed estimates across the simulations across the groups as a red circle with vertical red error bars.\nThe true population weighted across groups difference in response between the study treatment and control, calculated as the average of the true difference in response in each group weighted by the true population fractions of each group as defined in the actual profile, as a grey triangle.\nThe true design enriched across groups difference between the study treatment and control, calculated as the average of the true difference in response in each group weighted by the actual numbers of subjects recruited into each group, as a black star.\n\n\n\nResponse and Allocation\n\n\n\n\n\n\nFigure 7: Response and Allocation.\n\n\n\nThis is similar to the previous graph, except it shows the allocation to, and response on, the study treatment and control arms separately and not the treatment difference.\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations to control as a blue bar and to the study treatment arm as a green bar.\nThe true mean response to the study treatment in each group as a black cross.\nThe true mean response to the control in each group as a black diamond.\nThe estimated mean response on the study treatment arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as a red circle with vertical red error bars.\nThe estimated mean response on the control arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as a orange circle with vertical orange error bars.\n\n\n\n\n\nOdds Ratio and Allocation\n\n\n\n\n\n\nFigure 8: Odds Ratio and Allocation.\n\n\n\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations in each group plotted as a green bar.\nThe true response odds ratio between the study treatment and control in each group as a black cross.\nThe estimated response odds ratio (“treatment effect”) and the 2.5-97.5% interquartile range of the observed estimates across the simulations in each group as a red circle with vertical red error bars.\nThe estimated mean overall response odds ratio and 2.5-97.5% interquartile range of the observed estimates across the simulations across the groups as a red circle with vertical red error bars.\nThe true population weighted across groups response odds ratio between the study treatment and control, calculated as the average of the true odds ratio in each group weighted by the true population fractions of each group as defined in the actual profile, as a grey triangle.\nThe true design enriched across groups odds ratio between the study treatment and control, calculated as the average of the true odds ratio in each group weighted by the actual numbers of subjects recruited into each group, as a black star.\n\n\n\nResponse and Allocation\n\n\n\n\n\n\nFigure 9: Response and Allocation.\n\n\n\nThis is similar to the previous graph, except it shows the response rate on the study treatment and control arms separately and not the odds ratio.\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations to control as a blue bar and to the study treatment arm as a green bar.\nThe true mean response rate for the study treatment in each group as a black cross.\nThe true mean response rate for the control in each group as a blue triangle.\nThe estimated mean response rate on the study treatment arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as a red circle with vertical red error bars.\nThe estimated mean response rate on the control arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as an orange circle with vertical orange error bars.\n\n\n\n\n\nHazard Ratio and Allocation\n\n\n\n\n\n\nFigure 10: Hazard Ratio and Allocation.\n\n\n\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations in each group plotted as a green bar.\nThe true hazard ratio between the study treatment and control in each group as a black cross.\nThe estimated hazard ratio (“treatment effect”) and the 2.5-97.5% interquartile range of the observed estimates across the simulations in each group as a red circle with vertical red error bars.\nThe estimated mean overall hazard ratio and 2.5-97.5% interquartile range of the observed estimates across the simulations across the groups as a red circle with vertical red error bars.\nThe true population weighted across groups hazard odds ratio between the study treatment and control, calculated as the average of the true hazard ratio in each group weighted by the true population fractions of each group as defined in the actual profile, as a grey triangle.\nThe true design enriched across groups hazard ratio between the study treatment and control, calculated as the average of the true hazard ratio in each group weighted by the actual numbers of subjects recruited into each group, as a black star.\n\n\n\nHazard Rates\n\n\n\n\n\n\nFigure 11: Hazard Ratio graph.\n\n\n\nThe Hazard Rates graphs shows the number of events in each arm, the raw hazard ratio and fitted hazard ratios.\n\nThe blue show the number of events in the control arm.\nThe gray bar shows the number of events in the treatment arm.\nThe raw hazard ratio in each arm is shown by a gray circle.\nThe fitted hazard ratio in the control arm is shown by an orange diamond with orange bars indicating the 2.5-97.5% interquartile range.\nThe fitted hazard ratio in the treatment arm is shown by a red diamond, with red bars indicating the 2.5-97.5% interquartile range.\n\n\n\n\n\n\n\nProb. Group Compared to CSD/CSHRD Futility\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 12: Probability Response Compared to CSD for Futility plot.\n\n\n\nThis graph shows for each group and the across groups analysis, the probability of ‘beating’ the CSD for futility (the definition of ‘beating’ will depend on whether a higher endpoint score is a better or worse outcome for the subject and whether the trial is for superiority or non-inferiority).\nNote that though This is the comparison against the Futility CSD it is the probability of being better than it, higher probabilities mean less likelihood of stopping early for futility or declaring futility in the final evaluation.\n\nThe mean probability is plotted as solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\n\n\n\nFigure 13: Probability Response Compared to CSD for Futility plot.\n\n\n\nThis graph shows for each group and the across groups analysis, the probability of ‘beating’ the CSD for futility (the definition of ‘beating’ will depend on whether a higher endpoint score is a better or worse outcome for the subject and whether the trial is for superiority or non-inferiority).\nNote that though this is the comparison against the Futility CSD it is the probability of being better than it, higher probabilities mean less likelihood of stopping early for futility or declaring futility in the final evaluation.\n\nThe mean probability is plotted as a solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\n\n\n\nFigure 14: Probability Response Compared to CSHRD for Futility plot.\n\n\n\nThis graph shows for each group and the across groups analysis, the probability of ‘beating’ the CSHRD for futility (the definition of ‘beating’ will depend on whether a higher endpoint score is a better or worse outcome for the subject and whether the trial is for superiority or non-inferiority).\nNote that though this is the comparison against the Futility CSHRD it is the probability of being better than it, higher probabilities mean less likelihood of stopping early for futility or declaring futility in the final evaluation.\n\nThe mean probability is plotted as a solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\nProb. Group Compared to CSD/CSHRD Success and Group Phase III Success\nThis is the same as the “Prob. Group Compared to CSD/CSHRD Futility” plot, except that the probabilities are either\n\nRelative to control and the CSHRD for success.\nThe probability of Phase III success\n\n\n\nTrial Outcomes by Group\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 15: Relative Response and Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Outcomes by Group plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Outcomes by Group plot.\n\n\n\n\n\n\nThis plot shows as a stacked bar chart the proportion of different outcomes by group, across group and whole study.\nThe outcome types are:\n\nEarly Success (dark green): the group stopped early for success and had not regressed to futile (but it could have regressed to inconclusive) at the final analysis (if there was one).\nLate Success (light green): the group recruitment stopped because the group or study recruitment cap was reached; in the final evaluation of the group data the final evaluation success criteria were met.\nLate Futility (light red): the group recruitment stopped because the group or study recruitment cap was reached; in the final evaluation of the group data the final evaluation futility criteria were met.\nEarly Futility (dark red): the group stopped early for futility and had not regressed to success (but it could have regressed to inconclusive) at the final analysis (if there was one).\nSuccess to Futility Flip-Flop (pink): the group stopped early for success but had regressed to futility at the final analysis.\nFutility to Success Flip-Flop (purple): the group stopped early for futility but had regressed to success at the final analysis.\nInconclusive - Study Cap (dark brown): the group recruitment stopped because the study recruitment cap was reached; in the final evaluation of the group data neither the final evaluation success not the final evaluation futility criteria were met.\nInconclusive – Group Cap (light brown): the group recruitment stopped because the group recruitment cap was reached; in the final evaluation of the group data neither the final evaluation success not the final evaluation futility criteria were met.\n\n\n\nOutcome by Scatterplot\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 18: Group Outcome Scatterplot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Group Outcome Scatterplot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 20: Group Outcome Scatterplot.\n\n\n\n\n\n\nThis is a scatter plot graph that plots the result of a particular group or the ‘across groups’ plotting the estimate of response against the number of subjects recruited into the group or whole trial.\nThe symbol used to plot each simulation indicates the reason for stopping / outcome.\n\nLight blue circle: the group stopped early for success\nDark blue circle: the group did not stop early and was a success in the final analysis\nBrown square: the group did not stop early and the was futile in the final analysis\nRed square: the group stopped early for futility\nLight pink diamond: the group stopped early for success but was futile in the final analysis.\nBrown diamond: the group stopped early for futility but was successful in the final analysis\nYellow cross: the group outcome was inconclusive; the study reached the study cap.\nBlue cross: the group outcome was inconclusive; the group reached the group cap.\nPink cross: the group outcome was inconclusive; the study stopped early.\n\nThere is a control that allows the user to select whether the points are plotted for a particular group or for the whole study – using the across groups treatment estimate.\n\n\nDistribution of Early Stopping (Futility)\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 21: Distribution of Early Stopping plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 22: Distribution of Early Stopping plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Distribution of Early Stopping plot.\n\n\n\n\n\n\nThis plot shows the proportion of times each group has stopped early for futility as a brown bar, plus box and whisker plots showing the distribution, in time in weeks, of when those early stops occurred.\n\n\nDistribution of Early Stopping (Success)\nThis plot is the same as for the “Distribution of Early Stopping (Futility)” plot, except it shows the proportion of times each group has stopped early for success and the distribution in stopping times of those stops.\n\n\nCumulative Operating Characteristics Plot\n\n\n\n\n\n\n\n\nFigure 24\n\n\n\n\n\n\n\n\n\n\nFigure 25\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\nPer Sim and Interim Relative Response and Allocation\nThese graphs exclusive to continuous and dichotomous endpoint designs show similar information to the Relative Response and Allocation graph above, but show the results for a single simulation, or single interim of a single simulation. The individual interim results can only be shown for simulations for which ‘weeks’ files were output.\n\n\nPer Sim and Interim Relative Response and Allocation\nThese graphs show similar information to the Outcome and Allocation graph above, but show the results for a single simulation, or single interim of a single simulation. The individual interim results can only be shown for simulations for which ‘weeks’ files were output.\n\n\nExplore Final Futility/Success Criteria\n\n\n\n\n\n\n\n\nFigure 26\n\n\n\n\n\n\n\n\n\n\nFigure 27\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select criteria to use: posterior probability of beating the CSD/CSHRD or probability of phase III success, and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis).\nFor the given target the proportion of trials that would meet each of the criteria over the range of threshold values is plotted for each group and across group treatment effects.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\nExplore Early Success/Futility Eval Criteria\n\n\n\n\n\n\n\n\nFigure 28\n\n\n\n\n\n\n\n\n\n\nFigure 29\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (as in the examples above where the shape of the “existing stopping rules” line indicates that no early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which stopping criteria is evaluated and from which interim stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\nMCMC Trace plots\n\n\n\n\n\n\nFigure 30: MCMC Trace Plot.\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC. (See the description of the MCMC file contents below 16.6).\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/index.html#across-scenario-graphs",
    "href": "documentation/v71/userguides/enrichment/simulation/index.html#across-scenario-graphs",
    "title": "Simulation",
    "section": "Across Scenario Graphs",
    "text": "Across Scenario Graphs\nTo view multiple graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot. You can select the graph type, filter the design variants and filter which scenarios displayed:\n\n\n\n\n\n\nFigure 31: Window that appears when aggregating simulation results.\n\n\n\n\nSelected Groups\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each group was successful in a trial that was successful.\n\n“Successful” –the arm was correctly successful: it was successful, marked as “Should succeed” on the VSR tab and the trial was successful.\n“Should not succeed” – the arm was incorrectly successful: it was successful but not marked as “Should succeed” on the VSR tab and the trial was successful.\nUnsuccessful – the trial was successful but the group was not.\n\n\n\n\n\n\n\nFigure 32\n\n\n\n\n\nQOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each group. There is a dropdown control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\n\n\n\n\n\nFigure 33\n\n\n\n\n\nPpn Success\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\n\n\n\n\n\nFigure 34\n\n\n\n\n\nResponse\nThis graph shows a group response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\n\n\n\n\n\nFigure 35\n\n\n\n\n\nAllocation\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm in each group over the simulations.\n\n\n\n\n\n\nFigure 36\n\n\n\n\n\nTotal Subjects\nThis graph shows the mean total sample size for each scenario at different maximum sample sizes (the different variants).\n\n\n\n\n\n\nFigure 37",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, they can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response (change from baseline) for the control arm in each group.\n\n\nMean Ctrl Response: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of response (change from baseline) for the study treatment in each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the across groups effect size (the estimate across the groups of the difference between response on the study treatment and the historic control rate or the mean response on the control arm).\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘sigma’ that is the common standard deviation of the distributions of the final endpoints for each treatment arm.\n\n\nSD Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for sigma.\n\n\nMean Overall Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘the across groups sigma’ that is the common standard deviation of the distributions of the final endpoints for the pooled control subjects and pooled study treatment subjects.\n\n\nSD Overall Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for the pooled sigma.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response for the scenario\n\n\nTrue SD Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. This is the true standard deviation of the control response for the scenario.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response in each group for the scenario.\n\n\nTrue SD Trt Resp: &lt;Group&gt;\nOne per group\nThe standard deviation of the treatment response in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met\n\n\nPpn Futility Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met at the final evaluation\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Baseline Beta\n1\nThis is the average (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nSD Baseline Beta\n1\nThis is the standard deviation (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nMean Baseline: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nSD Baseline &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nTrue Mean Baseline &lt;Group&gt;\nOne per group\nTrue mean baseline for the scenario\n\n\nTrue SD Baseline &lt;Group&gt;\nOne per group\nTrue standard deviation of baseline for the scenario\n\n\n\n\n\n\nThese parameters are unused in ED unless the Time Course Hierarchical or ITP longitudinal analysis models are used.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt;Visit&gt;\nGroup * Arms * Visits\nThis is the mean (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit.\n\n\nSE Mean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt; Visit&gt;\nGroup * Arms * Visits\nThis is the Standard Error (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit.\n\n\n\n\n\n\nThis is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nSD Sigma\n1\nThis is the SD (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nMean Beta\n1\nThis is the mean (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nSD Beta\n1\nThis is the SD (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nPPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the within group difference between treatment and control was significant.\n\n\n\n\n\n\nThe columns in this table are the same as for the “Frequentist” results, but are the results of an ANCOVA test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#highlights",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#highlights",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, they can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#allocation",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#allocation",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#response",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#response",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response (change from baseline) for the control arm in each group.\n\n\nMean Ctrl Response: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of response (change from baseline) for the study treatment in each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the across groups effect size (the estimate across the groups of the difference between response on the study treatment and the historic control rate or the mean response on the control arm).\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘sigma’ that is the common standard deviation of the distributions of the final endpoints for each treatment arm.\n\n\nSD Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for sigma.\n\n\nMean Overall Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘the across groups sigma’ that is the common standard deviation of the distributions of the final endpoints for the pooled control subjects and pooled study treatment subjects.\n\n\nSD Overall Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for the pooled sigma.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response for the scenario\n\n\nTrue SD Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. This is the true standard deviation of the control response for the scenario.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response in each group for the scenario.\n\n\nTrue SD Trt Resp: &lt;Group&gt;\nOne per group\nThe standard deviation of the treatment response in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#observed",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#observed",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#probabilities",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#probabilities",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#stopping-rules",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#stopping-rules",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met\n\n\nPpn Futility Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#evaluation-rules",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#evaluation-rules",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met at the final evaluation\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#hierarchical-prior-parameters",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#hierarchical-prior-parameters",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#baseline",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#baseline",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Baseline Beta\n1\nThis is the average (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nSD Baseline Beta\n1\nThis is the standard deviation (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nMean Baseline: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nSD Baseline &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nTrue Mean Baseline &lt;Group&gt;\nOne per group\nTrue mean baseline for the scenario\n\n\nTrue SD Baseline &lt;Group&gt;\nOne per group\nTrue standard deviation of baseline for the scenario",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#model-parameters",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#model-parameters",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "These parameters are unused in ED unless the Time Course Hierarchical or ITP longitudinal analysis models are used.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt;Visit&gt;\nGroup * Arms * Visits\nThis is the mean (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit.\n\n\nSE Mean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt; Visit&gt;\nGroup * Arms * Visits\nThis is the Standard Error (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#simulation-results",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#simulation-results",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "This is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#frequentist-results",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#frequentist-results",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nSD Sigma\n1\nThis is the SD (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nMean Beta\n1\nThis is the mean (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nSD Beta\n1\nThis is the SD (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nPPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the within group difference between treatment and control was significant.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#frequentist-ancova-results",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#frequentist-ancova-results",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "The columns in this table are the same as for the “Frequentist” results, but are the results of an ANCOVA test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-summary.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe columns in summary.csv are common across all the FACST Dose Finding design engines, hence there are columns in the file when using the N-CRM design engine with no contents as they are for results no applicable to this engine.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Project\n1\nThe name of the “.facts” file in the FACTS GUI that was used to generate the simulations.\n\n\nScenario\n1\nThe name of the scenario – This is the various profile names that make up the scenario, concatenated together.\n\n\nTimestamp\n1\nThe date and time when the simulations started.\n\n\nVersion\n1\nThe version number of the FACTS GUI that ran the simulations.\n\n\nNsim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\n80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nMean Alloc &lt;Group&gt; \n2*G\nThe mean number of subjects recruited into each arm in each group. If a control arm is not included the column is still present, with value ‘0’.\n\n\nMean Trt Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response on the study treatment arm in each group.\n\n\nMean Trt Effect (Overall)\n1\nThe mean of the mean estimate of the across groups treatment difference.\n\n\nSE Trt Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response on the study treatment arm in each group.\n\n\nSE Trt Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups treatment difference.\n\n\nMean Control Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response on the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\nThe mean of the average of the control response across the groups\n\n\nSE Control Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response rate on the control arm in each group.\n\n\nSE Avg. Control Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups control response rate.\n\n\nMean Sigma\n1\nThe mean of the estimate of the standard deviation in the endpoint from the estimates of the response of the individual arms in each group.\n\n\nSE Sigma\n1\nThe standard error of the estimate of the standard deviation in the endpoint from the estimates of the response of the individual arms in each group.\n\n\nMean Overall Sigma\n1\nThe mean of the estimate of the standard deviation in the endpoint from the estimates of the treatment difference across the groups.\n\n\nSE Overall Sigma\n1\nThe standard error of the estimate of the standard deviation in the endpoint from the estimates of the treatment difference across the groups.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\nTrue mean treatment response for the scenario\n\n\nTrue Mean Control Resp &lt;Group&gt;\nG\nTrue mean control response for the scenario\n\n\nMean Baseline Beta\n1\nThe mean of the mean estimates of the baseline adjustment\n\n\nSE Baseline Beta\n1\nThe standard error of the mean estimates of the baseline adjustment\n\n\nMean Baseline &lt;Group&gt;\nG\nThe mean of the mean estimate of the baseline\n\n\nSE Baseline &lt;Group&gt;\nG\nThe standard error of the mean estimates of the baseline\n\n\nSD Baseline &lt;Group&gt;\nG\nThe mean of the SDs of the estimates of the baseline\n\n\nSE SD Baseline &lt;Group&gt;\nG\nThe standard error of the estimates of the SD of the baseline\n\n\nTrue Mean Baseline &lt;Group&gt;\nG\nThe true mean baseline from the scenario\n\n\nTrue SD Baseline &lt;Group&gt;\nG\nThe true standard deviation of the baseline for the scenario\n\n\nMean Mu Theta\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group.\n\n\nSE Mu Theta\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean study treatment difference from control in each group.\n\n\nMean Tau Theta\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group.\n\n\nSE Tau Theta\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group.\n\n\nMean Mu Gamma\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group.\n\n\nSE Mu Gamma\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean response on the control arm in each group.\n\n\nMean Tau Gamma\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group.\n\n\nSE Tau Gamma\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group.\n\n\nMean Longmod Resp &lt;Group&gt;  \nG * A * V\nThis is the mean, over the simulations, of the response in a particular group, on a particular arm at a particular visit. These columns are only present if a TCH or ITP longitudinal model is being fitted (other models do not produce an estimate of the mean response at a visit).\n\n\nSE Mean Longmod Resp &lt;Group&gt;  \nG * A *V\nThis is the Standard Error, over the simulations, of the mean estimate of response at a visit. These columns are only present if a TCH or ITP longitudinal model is being fitted (other models do not produce an estimate of the mean response at a visit).\n\n\nMean Pr Ph3 Success &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of success in phase 3 of the study treatment in each group.\n\n\nMean Pr Ph3 Success 99\n1\nThe mean, across the simulations, of the mean probability of success in phase 3 of the across groups treatment difference.\n\n\nMean Pr CSD (Success) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for success in each group.\n\n\nMean Pr CSD (Success) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for success.\n\n\nMean Pr CSD (Futility) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for futility in each group.\n\n\nMean Pr CSD (Futility) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for futility.\n\n\nPpn CSD Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD success criterion.\n\n\nPpn Ph3 Success &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility &lt;Group&gt;\nG\nThe proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success &lt;Group&gt;\nG\nThe proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations where each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThe proportion of simulations where the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations where each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile\n1\nThe proportion of simulations where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn CSD Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility (Final) 99\n1\nThe proportion of simulations in which after all data has been collected the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its futility criterion in each group.\n\n\nPpn Ph3 Futility (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its CSD success criterion.\n\n\nPpn Ph3 Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its success criterion in each group.\n\n\nPpn Ph3 Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility (Final) 99\n1\nThe proportion of simulations where the across groups treatment effect after all data has been collected met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success (Final) 99\n1\nThe proportion of simulations where after all data has been collected the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where after all data has been collected the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations after all data has been collected where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn Outcome 1\n1\nThe proportion of simulations that stopped early for success.\n\n\nPpn Outcome 2\n1\nThe proportion of simulations that reached full accrual and declared success on final evaluation.\n\n\nPpn Outcome 3\n1\nThe proportion of simulations that reached full accrual and declared futility on final evaluation.\n\n\nPpn Outcome 4\n1\nThe proportion of simulations that stopped early for futility\n\n\nPpn Outcome 5\n1\nThe proportion of simulations that stopped early for success but were deemed futile on final evaluation.\n\n\nPpn Outcome 6\n1\nThe proportion of simulations that stopped early for futility but were deemed successful on final evaluation.\n\n\nPpn Outcome 7\n1\nThe proportion of simulations that reached full accrual and were inconclusive.\n\n\nMean Study Accrual Stop Week\n1\nThe mean study duration of accrual – from start of accrual to last patient first visit.\n\n\nMean BAC Mu &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Mu &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nMean BAC Tau &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Tau &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-summary_freq.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-summary_freq.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\nsignificant (GrpxTrt)\n1\nThe proportion of simulations where the group interaction term was significant.\n\n\nMean Trt Effect &lt;group&gt;\nG\nThe mean, over the simulations, of the estimate of the mean treatment effect in each group.\n\n\nSE Trt Effect &lt;group&gt;\nG\nThe standard error, over the simulations, of the estimate of the mean treatment effect in each group\n\n\nMean Sigma\n1\nThe mean, over the simulations, of the estimate of the standard deviation in the final response.\n\n\nSE Sigma\n1\nThe standard error, over the simulations, of the estimate of the standard deviation in the final response.\n\n\nMean Beta\n1\nThis is the mean (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nSE Beta\n1\nThis is the SD (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nPpn Significant &lt;group&gt;\nG\nThe proportion of simulations in the which the treatment effect was significant in each group.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n#Sims\n1\n✔\n\nSimulation number\n\n\nWeeks (Duration)\n1\n✔\n\nThe week of final analysis – the total duration of the simulation.\n\n\n#Week\n1\n\n✔\nWeek\n\n\nNo. Subj\n\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nAlloc &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe number of subjects allocated to each arm in each group.\n\n\nMean Trt Resp &lt;group&gt;\nG\n✔\n✔\nThe estimated mean response of the study treatment in the group.\n\n\nTrt Effect (Overall)\n1\n✔\n✔\nThe estimated mean treatment difference across the groups.\n\n\nSD Mean Trt Resp &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the study treatment in the group.\n\n\nSD Trt Effect (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the mean treatment difference across the groups.\n\n\nMu Theta\n1\n✔\n✔\nThe mean of the hierarchical distribution of the treatment differences over all the groups.\n\n\nTau Theta\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the treatment differences over all the groups.\n\n\nMean Control Resp &lt;group&gt;\nG\n✔\n✔\nThe estimated mean response of the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\n✔\n✔\nThe average overall mean response of the control arm over all the groups. (Note: this is not part of the response model, but estimated separately).\n\n\nSD Mean Control Resp &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the control arm in each group.\n\n\nSD Avg. Control Resp (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the average response over all the control arms.\n\n\nMu Gamma\n1\n✔\n✔\nThe mean of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nTau Gamma\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nSigma\n1\n✔\n✔\nThe estimate of the standard deviation of the final responses given the per-group response model.\n\n\nSD Sigma\n1\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the final responses given the per-group response model.\n\n\nSigma Overall\n1\n✔\n✔\nThe estimate of the standard deviation of the final response given the across-groups response model.\n\n\nSD Sigma Overall\n1\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the final response given the across group response model.\n\n\nTrue Mean Trt Resp &lt;group&gt;\nG\n✔\n✔\nTrue mean treatment response for the scenario\n\n\nTrue SD Trt Resp &lt;group&gt;\nG\n✔\n✔\nTrue standard deviation of treatment response for the scenario\n\n\nTrue Mean Control Resp &lt;group&gt;\nG\n✔\n✔\nTrue mean control response for the scenario\n\n\nTrue SD Control Resp &lt;group&gt;\nG\n✔\n✔\nTrue standard deviation of control response for the scenario\n\n\nBaseline Beta\n1\n✔\n✔\nThe estimated mean of the baseline adjustment\n\n\nSD Baseline Beta\n1\n✔\n✔\nThe estimated standard deviation of the baseline adjustment\n\n\nMean Baseline &lt;group&gt;\nG\n✔\n✔\nThe estimated mean of the baseline\n\n\nSE Baseline &lt;group&gt;\nG\n✔\n✔\nThe estimated standard error of the baseline\n\n\nSD Baseline &lt;group&gt;\nG\n✔\n✔\nThe estimated mean of the standard deviation of baseline\n\n\nTrue Mean Baseline &lt;group&gt;\nG\n✔\n✔\nTrue mean baseline from the scenario\n\n\nTrue SD Baseline &lt;group&gt;\nG\n✔\n✔\nTrue standard deviation of baseline from the scenario\n\n\nNum Completed &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe number of subjects completed (final endpoint available) in each arm in each group.\n\n\nComplete Info &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe amount of complete information in each arm in each group – however that has been defined in the definition of interim timing – subjects enrolled, subjects complete at a certain visit or subjects who had the opportunity to complete at a certain visit.\n\n\nMean Raw Response &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe mean raw response\n\n\nSD Mean Raw Response &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe standard deviation of the raw response\n\n\nNum Dropouts &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nNumber of dropouts seen\n\n\nPr Ph3 Success &lt;group&gt;\nG\n✔\n✔\nThe probability of success in phase 3 of the study treatment for each group.\n\n\nPr Ph3 Success 99\n1\n✔\n✔\nThe probability of success in phase 3 of the across groups treatment difference.\n\n\nPr CSD (Success) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the success NIM) of the study treatment for each group.\n\n\nPr CSD (Success) 99\n1\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the NIM) of the across groups treatment difference.\n\n\nPr CSD (Futility) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the study treatment for each group.\n\n\nPr CSD (Futility) 99\n1\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the across groups treatment difference.\n\n\nCSD Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Futility 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Success 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required success criteria were met. 0 = Not met.\n\n\nSuccess Criteria Met 99\n1\n✔\n✔\nA flag: 1 = the required success criteria were met. 0 = Not met.\n\n\nFutility Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required futility criteria were met. 0 = Not met.\n\n\nFutility Criteria Met 99\n1\n✔\n✔\nA flag: 1 = the required futility criteria were met. 0 = Not met.\n\n\nCSD Futility (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are met for the final evaluation for futility for each group, 0 = they are not all met. Whether the across group final futility condition is also met if required is not included in this flag.\n\n\nCombined Futility (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for futility in the across group analysis, 0 = they are not all met.\n\n\nCombined Success (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success for each group, 0 = they are not all met. Whether the across group final success condition is also met if required is not included in this flag.\n\n\nCombined Success (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nSuccess Criteria Met (Final) 99\n1\n✔\n\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met (Final) 99\n1\n✔\n\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nFutile Study\n1\n✔\n✔\nA flag: 1 = the study was futile overall, 0 = otherwise.\n\n\nSuccessful Study\n1\n✔\n✔\nA flag: 1 = the study was successful overall, 0 = otherwise.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome: = Early success = Late success = Late futility = Early futility = Success to futility flip-flop = Futility to success flip-flop = Inconclusive\n\n\nGroup Outcome &lt;group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: = Early success = Late success = Late futility = Early futility = Success to futility flip-flop = Futility to success flip-flop = Inconclusive\n\n\nGroup Stop Type &lt;group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: = Success = Group Cap = Futility = Other (group stopped because study stopped early) = Study cap\n\n\nGroup Stop Week &lt;group&gt;\nG\n✔\n✔\nThe week the group stop decision was taken. There may be further follow-up time before the group analysis was completed.\n\n\nAccrual Stop Week\n1\n✔\n✔\nThe week the study stop decision was taken. There may be further follow-up time before the study analysis was completed.\n\n\nBAC Mu &lt;group&gt;\nG\n✔\n✔\nThe mean estimated value of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Mu &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nBAC Tau &lt;group&gt;\nG\n✔\n✔\nThe mean estimated value of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Tau &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nLinear Regression Alpha &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the linear regression longitudinal model the mean estimate of the constant offset in the change in response from this visit to the final visit\n\n\nLinear Regression Beta &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the linear regression longitudinal model the mean estimate of the coefficient of change in response from this visit to the final visit\n\n\nLinear Regression Lambda &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the linear regression longitudinal model the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\n\n\nTCH Alpha &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the time course hierarchical longitudinal model the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit.\n\n\nTCH Lambda &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the time course hierarchical longitudinal model the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\n\n\nTCH Tau &lt;model&gt;\nLM\n✔\n✔\nOnly if using the time course hierarchical longitudinal model the mean estimate of the SD of the per subject random effect\n\n\nITP K &lt;model&gt;\nLM\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the ITP shape parameter\n\n\nITP Lambda &lt;model&gt;\nLM\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the SD of the residual error.\n\n\nITP Tau &lt;model&gt;\nLM\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the SD of the per subject random effect\n\n\nITP Omega &lt;group&gt; &lt;arm&gt;\nG*2\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the mean treatment arm effect.\n\n\nLongmod Resp &lt;Group&gt; &lt;Arm&gt; &lt;Visit&gt;\nG*A*V\n✔\n✔\nOnly if using the time course hierarchical or ITP longitudinal model the mean estimate of the response at each visit, reported per group per arm per visit",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-simulations_freq.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-simulations_freq.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nOverall Group p-value\n1\nThe ANCOVA overall group p-value\n\n\nOverall Trt p-value\n1\nThe ANCOVA overall treatment p-value\n\n\nGroup x Trt p-value\n1\nThe ANCOVA treatment-group interaction p-value\n\n\nEst. Trt Effect &lt;group&gt;\nG\nThe mean estimate of the treatment effect for each group.\n\n\nLower CI Trt Effect &lt;group&gt;\nG\nThe lower bound of the 95% confidence interval for the estimate of the treatment effect for each group.\n\n\nUpper CI Trt Effect &lt;group&gt;\nG\nThe upper bound of the 95% confidence interval for the estimate of the treatment effect for each group.\n\n\np-value Trt Effect &lt;group&gt;\nG\nThe p-value for the within group treatment effect for each group.\n\n\nSigma\n1\nThe estimate of the standard deviation in the subjects’ final response.\n\n\nBeta\n1\nThe mean estimate of the baseline adjustment\n\n\nBeta SD\n1\nThe standard deviation of the estimate of the baseline adjustment",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-patientsnnnnn.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nRegion index\n\n\nDateInWeeks\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nGroup\n1\nThe index number of the group the subject belongs to.\n\n\nArm\n1\nA flag indicating the arm the subject was randomized to: 0 = Control, 1 = Study Treatment.\n\n\nLastVisit#\n1\nThe index of the last visit for which subject data is available\n\n\nDropout\n1\nA flag indicating if the subject has dropped out, 0 = not dropped out, 1= dropped out.\n\n\nBaseline\n1\nThe baseline. Column is only present if baseline is included.\n\n\nVisit &lt;visit&gt;\nV\nThe subject’s endpoint score for that visit: -9999 = not available.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/enrichment/simulation/continuous.html#contents-of-mcmcnnnnn.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nTrtResp &lt;group&gt;\nD\nThe estimate of the mean treatment difference in each group\n\n\nOverall Theta\n1\nThe overall estimate of the treatment difference\n\n\nCtlResp &lt;group&gt;\n\nThe estimate of the mean response on the control arm in each group\n\n\nOverall Gamma\n\nThe overall mean response on the control arms\n\n\nSigma\n1\nThe estimate of the SD of the endpoint\n\n\nLongmod &lt;model&gt; &lt;param&gt; &lt;visit&gt; these vary significantly from design to design depending on the model used and the number of model instances. The parameter names correspond to the symbols used for the parameters on the Design &gt; Longitudinal tab\nL * P * V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Simulation",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/analysis.html",
    "href": "documentation/v71/userguides/enrichment/analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "The analysis tab allows the user to supply a specific data set for analysis by the design specified in the Design tab of the “.facts” file.\nClicking on the “Use Design to Analyze Data” button, will create an empty “subject.csv” file in the main simulation results directory and an ‘Analysis’ sub-directory there for running the analysis and saving the outputs.\nAlternatively, clicking on the “Import Data to Analyze” launches a file browser, allowing the user to select a ‘.csv’ file to load as the data to analyze. This is a shortcut for first clicking on the “Use Design to Analyze Data” button, and then clicking on the “Select File to Create New Analysis” button on the subject data tab.\nAfter enabling data analysis, the analysis screen is shown with no data loaded. By clicking on the “Subject Data” tab the user is now able to enter data values directly, or to load a ‘.csv’ file already containing data:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/analysis.html#the-subject.csv-file-format",
    "href": "documentation/v71/userguides/enrichment/analysis.html#the-subject.csv-file-format",
    "title": "Analysis",
    "section": "The subject.csv file format",
    "text": "The subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nThe format of the file is the same as the ‘patientsNNNN.csv’ output file (continuous, dichotomous), or time-to-event) and the column values described above.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/enrichment/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "href": "documentation/v71/userguides/enrichment/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "title": "Analysis",
    "section": "Converting arrival date value from days to weeks",
    "text": "Converting arrival date value from days to weeks\nFrom FACTS 7.0 the value in the Date field is interpreted as being in weeks (rather than days as in previous versions). If you have existing data, a simple conversion tool is provided “Convert Date from Days to Weeks” that simply divides all of thete values by 7. Having run the conversion, you then need to save the modified data. The values of the Date field will only make a difference to TTE analyses.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: The data as provided to the analysis tab with dates in days.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: The same dates, but converted to weeks.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Enrichment Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/dr.html",
    "href": "documentation/v71/userguides/dr.html",
    "title": "Design Report",
    "section": "",
    "text": "This document describes usage and how to update the installation of the Design Report Generator, an automated report generation tool, included with FACTS. It is intended for anyone concerned with using the automated report generation facility. The Report Generator is available only for the desktop version of FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Design Report"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/dr.html#purpose-and-scope-of-this-document",
    "href": "documentation/v71/userguides/dr.html#purpose-and-scope-of-this-document",
    "title": "Design Report",
    "section": "",
    "text": "This document describes usage and how to update the installation of the Design Report Generator, an automated report generation tool, included with FACTS. It is intended for anyone concerned with using the automated report generation facility. The Report Generator is available only for the desktop version of FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Design Report"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/dr.html#software-prerequisites",
    "href": "documentation/v71/userguides/dr.html#software-prerequisites",
    "title": "Design Report",
    "section": "Software prerequisites",
    "text": "Software prerequisites\nIn order to run the report generator directly from FACTS, you will need the following installed on your Windows machine on which you are using FACTS:\n\nA reasonably recent version of R (v3.5.3+) along with the R libraries ‘rmarkdown’, ‘xtable’ and ‘stringi’ installed.\nRStudio (the Design Report Generator uses two packages that come with RStudio – “mathjax” and “pandoc”, these can now be obtained separately but given the ubiquity of RStudio FACTS simply requires you to have that installed).\nMicrosoft Word or Open Office\nFACTS v7.1 or later\n\n\n\n\n\n\n\nNote\n\n\n\nRecently we’ve experienced some problems with “pandoc”, downloading and installing from the pandoc website (https://pandoc.org), then re-starting Windows has fixed them.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Design Report"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/dr.html#sec-setup",
    "href": "documentation/v71/userguides/dr.html#sec-setup",
    "title": "Design Report",
    "section": "Setup",
    "text": "Setup\nYou will need to inform FACTS of the location of R and RStudio on your computer.\nTo do this start FACTS and go to menu option: “Settings &gt; Options” and then to the “R Configuration” tab.\n\n\n\n\n\n\nFigure 1\n\n\n\nSelect the R and RStudio versions you would like to use.\nIf the version you wish to use isn’t shown, or the path is incorrect, use the “Edit” button to open a file browser to navigate to the version of R that you wish to use and select the appropriate version of “R.exe”. Similarly for RStudio.exe.\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Design Report"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/dr.html#steps-for-generating-a-design-report",
    "href": "documentation/v71/userguides/dr.html#steps-for-generating-a-design-report",
    "title": "Design Report",
    "section": "Steps for generating a design report",
    "text": "Steps for generating a design report\nTo generate a design report from FACTS, you will need to do the following sequence of steps:\n\nSimulate your design in FACTS.\n\n\n\n\n\n\n\nNote\n\n\n\nIn case you have previously generated a report for your design using the report generator, you will need to make sure that the previous version of the report is not open in Word when you generate a new report.\n\n\n\nAfter setting valid R and RStudio versions/paths in Settings &gt; Options, go to the “Simulation” tab and click on the “Design Report” button. A command prompt Window will open detailing the report generation process. The very first run of the design report generation process will take a bit longer than subsequent runs as it will have to install all relevant R packages in a FACTS specific location.\nOnce the command prompt closes, the Word document will be automatically opened. It will first give you a warning regarding the document containing fields that may refer to other files, click “Yes” and then a subsequent warning regrading update the Table of Contents will appear. Click “Update entire table” and your report should appear.\n\n\n\n\n\n\n\nFigure 3: Example FACTS design report for a Core continuous endpoint design",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Design Report"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/dr.html#usage-notes",
    "href": "documentation/v71/userguides/dr.html#usage-notes",
    "title": "Design Report",
    "section": "Usage Notes",
    "text": "Usage Notes\n\nLocation of the generated Word file\nThe generated report document is stored in the “results” folder of your FACTS simulation (e.g., if your FACTS project is saved as CoreDesign.facts, the folder named CoreDesign_results is the corresponding ‘results’ folder).\n\n\nSuggested steps after generating the report\nA typical workflow after initially generating the report is as follows:\n\nReview the generated report for correctness, and fix minor typographical and formatting errors if any.\nIf you would like to use a predefined Word template, you could apply the template to your report.\nModify the default table style and add table cross-references as needed.\nAdd additional text and figures as needed. For example, you can copy graphs displayed in the FACTS GUI by clicking on the “Export options” button next to the r graph, selecting “Copy image to clipboard” and pasting it in the generated report.\n\n\n\nLocation of the Report Generator source files\nThe Design Report Generator uses R and Rmarkdown to create the report document. The script files used by the Report Generator are stored in the “ReportGenerator” folder under your FACTS installation folder. (It might be named something like C:\\Program Files (x86)\\BerryConsultants\\FACTS 7.1.0\\ReportGenerator). The files within this folder with file extensions of “.R”, and “.Rmd” are the ones required by the Report Generator. If an updated version of the Report Generator is made available either as a bug-fix or intermediate release, it will consist of updated “.R” and “.Rmd” files. Simply replacing the corresponding files in the original installation folder will deploy the updated version.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Design Report"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html",
    "href": "documentation/v71/userguides/2dcrm.html",
    "title": "2D-CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) 2D-CRM design engine. It is intended for all end users of the system.\n\n\n\nThis document covers the FACTS Dose Escalation 2D-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6 or later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nPlease cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#purpose-of-this-document",
    "href": "documentation/v71/userguides/2dcrm.html#purpose-of-this-document",
    "title": "2D-CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) 2D-CRM design engine. It is intended for all end users of the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#scope-of-this-document",
    "href": "documentation/v71/userguides/2dcrm.html#scope-of-this-document",
    "title": "2D-CRM",
    "section": "",
    "text": "This document covers the FACTS Dose Escalation 2D-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6 or later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#citing-facts",
    "href": "documentation/v71/userguides/2dcrm.html#citing-facts",
    "title": "2D-CRM",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#facts-7.0",
    "href": "documentation/v71/userguides/2dcrm.html#facts-7.0",
    "title": "2D-CRM",
    "section": "FACTS 7.0",
    "text": "FACTS 7.0\nThere have been no changes to 2D-CRM in FACTS 7.0.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#facts-6.5",
    "href": "documentation/v71/userguides/2dcrm.html#facts-6.5",
    "title": "2D-CRM",
    "section": "FACTS 6.5",
    "text": "FACTS 6.5\nThere have been no changes to 2D-CRM in FACTS 6.5.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#facts-6.4.1-changes-to-2d-crm",
    "href": "documentation/v71/userguides/2dcrm.html#facts-6.4.1-changes-to-2d-crm",
    "title": "2D-CRM",
    "section": "FACTS 6.4.1 Changes to 2D-CRM",
    "text": "FACTS 6.4.1 Changes to 2D-CRM\nCompared to 6.4 there have been slight changes to address circumstances where the model could fail to converge, and parameter estimates could reach very large values. This was due to wanting to allow 0 doses in 2D-CRM, because in the 2D setting it is sometimes the case that there have been monotherapy escalation studies and there is a desire to incorporate this data in the 2D trial. To do this a 0 dose is included for the other drug so there are monotherapy combinations in the design (possibly excluded from being actually given during the 2D trial). This means the posterior parameter estimates from the monotherapy can be used for that drug’s prior, or the data from the monotherapy trial can be included as “Prior Toxicities”.\nUp to this release (6.4.1) there were two problems doing this:\n\nInherited from the N-CRM code, FACTS was still storing the logs of the dose strengths, so doses could not be 0, and the advice was to use small values instead such as 0.001.\nThe presence of a combination with very small values for the transformed dose strength (and now 0 values) causes problems for the model’s stability, in particular when the dose strengths are 0 it becomes impossible for the model to have anything other than 0 DLT rate at the 0,0 combination.\n\nIn FACTS 6.4.1 these problems have been addressed by the following changes:\n\nFACTS now allows 0 strength doses.\nIt there is any combination where both dose strengths after transformation are less than 0.001 then this combination must be excluded from the trial and can have no prior toxicities specified for it (however prior observations of no toxicities are allowed).\nIf both drugs include 0 strength doses, the lower asymptote for the Response Model (the range is usually [0, 1]) must be raised slightly, e.g. to 0.0001.\n\nRestrictions 2 and 3 are checked when there is an attempt to run simulations, and if violated a warning message is displayed and the simulations not run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#facts-6.4-changes-to-2d-crm",
    "href": "documentation/v71/userguides/2dcrm.html#facts-6.4-changes-to-2d-crm",
    "title": "2D-CRM",
    "section": "FACTS 6.4 Changes to 2D-CRM",
    "text": "FACTS 6.4 Changes to 2D-CRM\nThere were no changes to the 2D-CRM simulator for FACTS 6.4.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#facts-6.3-changes-to-2d-crm",
    "href": "documentation/v71/userguides/2dcrm.html#facts-6.3-changes-to-2d-crm",
    "title": "2D-CRM",
    "section": "FACTS 6.3 Changes to 2D-CRM",
    "text": "FACTS 6.3 Changes to 2D-CRM\nThere were no changes to the 2D-CRM simulator for FACTS 6.3.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#the-use-of-the-2d-crm-simulator-to-design-a-trial",
    "href": "documentation/v71/userguides/2dcrm.html#the-use-of-the-2d-crm-simulator-to-design-a-trial",
    "title": "2D-CRM",
    "section": "The use of the 2D-CRM simulator to design a trial",
    "text": "The use of the 2D-CRM simulator to design a trial\nThe typical pattern of use of the simulator is as follows:\n\nEnter the constraints and objectives of the study: maximum sample size and the bounds of the different toxicity bands.\nSelect the choice of and overdose control thresholds.\nSpecify a small number of expected toxicity responses as VSR scenarios.\nSelect the desired run-in and dose escalation rules.\nSpecify an initial set of priors for the model.\nRun a small number of simulations (25 is usually adequate at this stage) of each scenario.\nCheck the performance of the design:\n\nDo the majority of the final combinations being selected have an acceptable level of toxicity?\nIs the overall level of toxicity observed in the trial acceptable?\nIn the scenarios where the performance is worst, observe the behavior in individual simulations.\n\nTry a number of modifications to the design and simulate each modification individually. Increasing the number of simulations (usually 100 is still adequate at this stage).\nConsider if more VSRs need to be included to check a greater range of possible actual toxicity rate patterns.\nCombine modifications with promising results and re-simulate, (usually 500 is an adequate number at this stage).\n\nThe design changes that are typically considered are:\n\nExclude low dose combinations that are certain to be too weak.\nExclude high dose combinations that are certain to be too toxic.\nModify the run-in to be more aggressive if it is taking too many subjects and cycles to reach the escalation phase.\n\nThe ”Huang” diagonals method is the least aggressive, both the “Ivanova” and “Tri-axial” can skip some combinations, finally consider a user specified run-in sequence.\nThe “Huang” and the “Tri-axial” can make use of the “multiple simultaneous cohorts” option in the run-in phase to test multiple combinations simultaneously.\n\nIf the escalation phase is too aggressive – incurring too many toxicities or selecting combinations with high toxicity true toxicity too often - then options are:\n\nMake the run-in expand at the dose below observed toxicities, not at the toxicity\nMake the priors for the model less informative (if the priors are informative and predicting low toxicity at higher combinations), the greater uncertainty in the toxicity at higher doses may then result in the estimate of their toxicity exceeding the overdose control threshold.\nMake the priors for the model predict higher levels of toxicity.\nIncrease the overdose control thresholds.\nShift the toxicity band bounds downwards.\nAdd prior toxicity data.\nIncrease the number of subjects before escalation.\n\nIf the escalation phase is not aggressive enough – selecting combinations with too low a toxicity too often - then options are:\n\nMake the run-in expand at the dose where toxicity observed not a dose below\nMake the priors for the model more informative (if the priors are uninformative) this will allow lack of toxicity on lower doses reduce the predicted toxicity at higher, untested doses, bringing the probability of toxicity at the higher doses below the dose escalation threshold.\nMake the priors for the model predict lower levels of toxicity.\nLower the overdose control thresholds.\nShift the toxicity band bounds upwards.\nAdd prior toxicity data.\nDecrease the number of subjects before escalation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#general-constraints-and-assumptions",
    "href": "documentation/v71/userguides/2dcrm.html#general-constraints-and-assumptions",
    "title": "2D-CRM",
    "section": "General constraints and assumptions",
    "text": "General constraints and assumptions\nThe design engine is limited to phase 1/2a designs with dichotomous endpoints toxicity/no-toxicity. There is no longitudinal modeling or analysis of covariates included. All results of a cohort are assumed to be available before the next cohort is treated.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#models-and-methods",
    "href": "documentation/v71/userguides/2dcrm.html#models-and-methods",
    "title": "2D-CRM",
    "section": "Models and Methods",
    "text": "Models and Methods\nFor a full description of the models and methods implemented in the 2D-CRM see the Software Specification document [Spec].\nIn the current version there is a single statistical model used to analyse the data – the 5 parameter Bayesian Logistic Regression Model as described in (Neuenschwander, Branson, and Gsponer 2008). The model is used to compute the posterior probability that the toxicity rate at each combination is in one of 4 toxicity bands – “underdosing”, “target toxicity”, “excess toxicity” and “unacceptable toxicity”. The boundaries between the bounds are settable by the user.\nThe posterior probability that the rate is in the target toxicity rate is used to target the dose allocation – the method attempts to allocate to the Maximum Target Toxicity (MTT). The posterior probability that the rate is in the “excess toxicity” and “unacceptable” or just the “unacceptable” bands can be used to enforce an “overdose control”. Combinations with a posterior probability that exceed the specified threshold are excluded from selection or allocation.\nThe dose combinations are explored in two phases:\n\nOptionally the trial may begin with a “small cohort run-in” or the allocation of a single cohort to a specified combination. This phase ends when the sequence of combinations to explore is exhausted or excluded by combinations at which toxicities have been observed. There is no model fitting during this phase.\nThe trial then proceeds looking for the dose combination with the highest probability of having a toxicity rate in the target toxicity range using dose escalation, limited by dose escalation rules.\n\n\nRun-in methods\nThere are 4 run-in methods available:\n\nContour\n\nThis explores the reverse diagonals, assuming that the expected toxicity at the dose increments of the two doses are roughly equal.\n\nRow by row\n\nThis explores the doses of drug 1 at the first dose of drug 2, then increments drug 2, steps back a specified number of doses in drug 1 and then increments through the doses of drug 1 again. This method favours escalation of drug 1 over escalation of drug 2.\n\nTri-axial\n\nThis explores the combinations on each axis and on the diagonal. For small numbers of dose increments it is similar to the Huang run-in. For larger numbers of dose increments it is more parsimonious.\n\nCustom\n\nThis explores a specific sequence of combinations stopping when the first toxicity is observed or the sequence is completed.\n\n\nOr run-in can be skipped.\nFor more details see the Software Specification document [Spec].\n\n\nEscalation methods\nThere are 4 escalation methods available. They all work in the same basic framework:\n\nThe dose combinations where sufficient subjects have been tested and lesser dose combinations are found.\nThe dose combinations that can be escalated to beyond these are found.\nThe dose combinations where the overdose control limits are exceeded are removed.\nThe target dose combinations from the remaining set are selected according to the selected method.\n\nThe 4 methods are:\n\nContour\n\nThis explores every MTT on every row and every column (frequently these overlap, but not necessarily).\n\nWalk\n\nThis explores the MTT that is at or adjacent to the last combination tested (excluding the dose that is an increment in both drugs from the last combination tested).\n\nTri-axial\n\nThis explores the MTT on each axis and on the diagonal. Once these have the required maximum subjects on MTT, any combinations off the axis and diagonal that are the MTT are explored.\n\nAllocate MTT\n\nThis allocates to the combination that is the overall MTT of all the combinations that it is permitted to allocate to.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#outstanding-issues-absent-features",
    "href": "documentation/v71/userguides/2dcrm.html#outstanding-issues-absent-features",
    "title": "2D-CRM",
    "section": "Outstanding Issues / Absent Features",
    "text": "Outstanding Issues / Absent Features\nCurrently the most obvious absent features are:\n\nmodeling of additional endpoints such as efficacy or drug exposure\nthe modeling of ordinal toxicity\nthe simulation of open enrolment where subjects are allocated on subject by subject basis as they become available for enrolment, with a limit on the number of subjects who can being treated but for whom final endpoint has not been observed.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#study-info",
    "href": "documentation/v71/userguides/2dcrm.html#study-info",
    "title": "2D-CRM",
    "section": "Study Info",
    "text": "Study Info\nOn the Study Info tab the user can\n\nSpecify the maximum trial size (in subjects not cohorts, as subjects tested in the run-on phase count to the overall total).\nSpecify the parameters of the target, the user specifies the boundaries between the different toxicity bands, and whether using overdose limits and if so which ones and what the overdosing thresholds are.\n\n\n\n\n\n\n\nFigure 4: Study info tab: 2D-CRM",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#treatment-arms",
    "href": "documentation/v71/userguides/2dcrm.html#treatment-arms",
    "title": "2D-CRM",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nOn the Treatment Arms tab the user specifies the names of the drugs, and the number, relative strength and names of the doses of each drug. The user can either click the ‘Add’ button the required number of times or use Auto generate doses, specifying the number of doses to create and initial dose level. Auto generate deletes all the pre-existing doses before generating the new ones.\nAfter creating the desired number of entries the user can edit the strengths and names of the doses.\nIt is up to the user to ensure that the dose strengths are in monotonically increasing order.\nIt is now (FACTS 6.4.1 and later) possible to enter dose strengths of 0, to create dose combinations that are monotherapies. This is particularly useful for enabling prior information from monotherapy trials to be included in this design.\n\n\n\n\n\n\nFigure 5: Treatment Arms tab",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#exclude",
    "href": "documentation/v71/userguides/2dcrm.html#exclude",
    "title": "2D-CRM",
    "section": "Exclude",
    "text": "Exclude\nOn the Study &gt; Exclude tab the user can specify dose combinations that cannot be used in the trial. By clicking on any square on the grid the corresponding dose combination is cycled through the following states: ‘Toxic’, ‘Not Available’, ‘Ineffective’ and ‘Available’. The initial state is ‘Available’ for all combinations.\nIf a combination is specified as ‘Toxic’ then all the combinations where both drugs are at the same or higher strength are also excluded.\nIf a combination is specified as ‘Ineffective’ then all combinations where both drugs are at the same or lower strength are also excluded.\nIf a combination is specified as ‘Not Available’ then just that combination is excluded with no consequences for the combinations around it.\nIf there are any combinations where, after transformation of the dose strengths, both drugs have a dose strength of less than 0.001 this must be excluded from the trial (mark them “ineffective” or “not available”). Any toxicities occurring on such a combination would be data that the model would have difficulty fitting.\n\n\n\n\n\n\nFigure 6: The Exclude tab",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#explicitly-defined",
    "href": "documentation/v71/userguides/2dcrm.html#explicitly-defined",
    "title": "2D-CRM",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nOn the VSR &gt; Explicitly Defined tab, the user enters the toxicity rate to use when simulating subject responses, for every dose combination. The cells are colored according to the toxicity band coloring convention used in FACTS (under-dosing: blue, target: green, excess: orange, unacceptable: red) according to the toxicity rate entered.\n\n\n\n\n\n\nFigure 7: VSR Explicitly Defined tab\n\n\n\n\nParametric\nOn the VSR &gt; Parametric tab, the user enters the parameters for a model that generates a toxicity rate to use when simulating subject responses, for every dose combination. The cells are colored using toxic band coloring according to the toxicity rate of that dose combination.\nThe model used is the 5 parameter BLRM model that is used as the Toxicity response model on the Design &gt; Response Model tab, but the user can specify different ‘effective dose strength’ models from those used in the analysis.\n\n\n\n\n\n\nFigure 8: VSR Parametric tab\n\n\n\nFor each profile the user specifies:\n\nFor each drug the ‘effective dose strength’ model.\n\nThe reference dose (where the effective dose strength is 1 and the toxicity rate due to that drug is given by the Alpha parameter alone).\nWhether the effective dose strength is the ratio of dose strength to the reference dose or the exponential of the difference of the to dose strength from the reference dose.\n\nFor each drug the values of the coefficients of the single drug models entered as ln(alpha) and ln(Beta)\nThe value of Eta, the drug-drug interaction term – Eta is the log-odds ratio between the interaction and the non-interaction model at the reference dose combination.\n\nIt is perfectly reasonable to use the parametric models by simply varying the parameters using trial and error until the desired set of toxicity rates is achieved.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#start-up",
    "href": "documentation/v71/userguides/2dcrm.html#start-up",
    "title": "2D-CRM",
    "section": "Start-up",
    "text": "Start-up\nThe Start-up tab allows the user to specify what is commonly referred to as a single patient run-in, in the FACTS 2D-CRM it’s allowed to be a “small cohort“ run-in. (The paper (Ivanova et al. 2003) showed that smaller target toxicity rates benefited from larger cohort sizes in the run-in).\nOn this tab the user can specify\n\nThe scheme to use for the run-in: “None”, “Contour”, “Row by Row”, “Tri-axial” or “Custom”. These are described above and in [Spec].\n\nIf “None” is specified then the table for specifying specific dose combinations is used to specify the dose combination(s) to be assigned to the starting cohort(s). These will be the full sized cohorts used in the escalation phase.\n\nThe cohort size to use in the run-in. A size of ‘1’ yields a ‘single patient run-in’, but other values are possible and may give better results, (see (Ivanova et al. 2003)).\nFor the ‘Row by row’ run-in scheme, the number of drug 1 dose increments to back-off each time drug 2 is incremented.\nThe range of the run-in. If the user does not want the run-in to possibly run all the way up to the maximum dose strengths, even if no toxicities have been observed, it is possible to specify a rectangular area of dose combinations for the run-in to include by specifying the starting combination (the lower left corner of the rectangle) and the stopping dose (the upper right corner combination). Note that as you would expect, regardless of whether this feature is used or not, dose combinations that have been excluded from the study on the Stud &gt; Exclude tab are excluded from the run-in.\nThe dose allocation scheme: (only applies to the Contour and Tri-axial run-in schemes):\n\nallocate a single cohort;\nallocate the cohort across the next possible dose combinations;\nor allocate a cohort to each of the next dose combinations and optionally specify a maximum number of cohorts / dose combinations that can be tested at a single step.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Row by row and Custom run-ins only identify one dose combination to test next, and so use “single cohort” allocation regardless of the setting of this control.\n\n\n\nWhere to allocate after the end of the run-in it it ends due to observing toxicities, either:\n\nexpand the allocation at the combinations where toxicity has been observed,\nor at one dose increment below where toxicity has been observed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the run-in ends because it has reached the end of the combinations to be tested during run-in without seeing a toxicity, then dose escalation starts by expanding at the allocation of the maximum dose combination(s).\n\n\n\nIf use of a Custom run-in has been specified, then the sequence of dose combinations is specified.\n\n\n\n\n\n\n\nFigure 9: Design Start-up tab",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#dose-escalation",
    "href": "documentation/v71/userguides/2dcrm.html#dose-escalation",
    "title": "2D-CRM",
    "section": "Dose Escalation",
    "text": "Dose Escalation\nThe Dose Escalation tab allows the user to specify the dose escalation and early stopping rules to use during the Dose Escalation phase.\nOn this tab the user can specify:\n\nThe cohort size.\nThe minimum number of subjects: that must be have been tested at a combination before the trial can escalate to a higher dose combination (that is not excluded by the overdose control).\nThe maximum dose increment: in either drug that can be applied from a combination that has been tested, and where the minimum of subjects have been tested, to a higher dose combination.\nThe dose escalation scheme: “Contour”, “Walk”, “Tri-Axial” or “Allocate to MTT”. These are described in (2.3.2) and in [Spec].\nThe dose allocation scheme: (only applies to the “Contour” and “Tri-axial” schemes):\n\nallocate a single cohort;\nallocate the cohort across the next possible doses;\nor allocate a cohort to each of the next doses.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe “Walk” and “Allocate to MTT” escalation schemes only identify one next combination, and so use “single cohort” allocation regardless of the setting of this control.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf “Multiple simultaneous cohorts” is specified, then it is possible to set a limit on the maximum number of cohorts / dose combinations tested in the next step.\n\n\n\nMaximum subjects on MTT. This is the maximum number of subjects to allocate to a combination under Dose Escalation. In the “Contour”, “Walk”, “Tri-Axial” escalation schemes this is currently the only stopping rule. If all the targeted combinations have the maximum number of subjects on them, the trial stops.\n\n\n\n\n\n\n\nFigure 10: Design Dose Escalation\n\n\n\nAll the escalation schemes stop if the maximum number of subjects has been allocated or all combinations are too toxic (the posterior probabilities of toxicity exceed the overdose threshold). The schemes other than “Allocate to MTT” can also stop when the maximum number of subjects on MTT is reached on all the target doses.\nIf the “Allocate to MTT” escalation scheme, is being used then additional stopping rules similar to those available in N-CRM can be enabled. If they are enabled, then the following may be specified:\n\nMinimum subjects on MTT: the trial can only stop with a combination selection if the combination to be allocated to have at least these number of subjects on them.\nStop when Posterior Probability of target Toxicity at MTT is at least: the trial can stop with a combination selection when the combination to be allocated to has a posterior probability that its toxicity rate is in the Target Toxicity band is at least the specified amount.\nMinimum cohort’s accrued: the trial can only stop with a combination selection if the specified minimum number of cohorts has been allocated and completed.\nMinimum toxicities: the trial can only stop for all combinations too toxic if the required minimum number of toxicities have been observed. Whilst all combinations are too toxic but this minimum number of toxicities havs not been observed, subjects are allocated to the lowest dose combination.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#response-model",
    "href": "documentation/v71/userguides/2dcrm.html#response-model",
    "title": "2D-CRM",
    "section": "Response Model",
    "text": "Response Model\nThe Response Model tab displays a description of the model and 4 other components:\n\nA panel where the prior distributions for the model parameters can be input. Unless prior toxicity information is also to be included, it is recommended that uninformative priors are not used as these tend to lead to a failure to re-escalate to a dose if just one toxicity has been observed there, due to overdose control.\n\n\n\n\n\n\n\nTip\n\n\n\nValues that represent weakly informative priors are recommended instead. The prior mean and SD for each Alpha should represent the prior expectation (in log-odds) of toxicity at the reference dose for that drug in isolation. Having set Alpha the prior for Beta should be set to allow gradients that are as steep as plausible, but if set too high, or too broad it may cause problems of not being able to escalate to high doses because of overdose control (even with few toxicities at low doses the uncertainty of the model at higher doses where there is currently no data may mean the posterior estimates of toxicity at these doses exceed the overdose control thresholds). This can be checked looking at example simulations or manually by entering test data on the Analysis tab. The prior for the correlation should usually be zero or slightly negative. A positive prior for Rho means that if the value for Alpha is estimated as low (due to low toxicity being observed near the reference dose) then the value of Beta will be estimated as being low – and this may lead to too low an estimate of toxicity at higher doses. A very strongly negative prior for Rho can lead to the same problems as a too high or broad prior for Beta.\n\n\n\nA panel where the transformations of the virtual dose strengths to values that can be used by the model is specified. This transformation is usually the dose ratio of each dose to the median dose of that drug. Options to use a linear transform and a different reference dose are available.\n\n\n\n\n\n\n\nTip\n\n\n\nThe recommended values for the reference dose are either the center of the dose range (the default “Median dose”) or the dose thought most likely to be in the middle of the target toxicity range for that drug alone.\n\n\n\nRe-scaling the response model. The model estimates rates between 0 and 1 and works best when the observed rates are asymptotically 0 or 1. If the observed toxicity is known to have different minimum and maximum rates (0.1 and 0.5 say) then the model fits is much, much better if these are specified as the asymptotes in place of 0 and 1.\n\n\n\n\n\n\n\nTip\n\n\n\nIf a dose combination of 0,0 is included in the design (even when excluded from be used in the trial, as it must be) the response model must be re-scaled with a minimum asymptote above 0, even if only very slightly (such as 0.0001).\n\n\n\nThe last panel displays the values of the model at the dose combinations using the specified parameters.\n\n\n\n\n\n\n\nFigure 11: Design Response Model\n\n\n\nSpecifying the Priors: The priors for each pair of parameters (α,β) for the response model for each drug are specified via a bivariate normal distribution (as in the single drug N-CRM), with a separate mean and SD for α and β, and a correlation term ρ (Rho). The prior for the interaction term η (Eta) is the mean and Sd of a Normal distribution. Using a Normal prior for Eta allows Eta to be -ve, modeling a negative interaction between the drugs and resulting in a non-monotonic surface. Unless a negative interaction is thought possible, it is recommended that the lognormal prior for Eta is used\nSpecifying the dose transform: For both drugs the transformation of the dose strengths to a range of values suited to the model is specified. For each drug the transformations are relative to ‘reference’ dose, by default the median value in the dose range. The transformation can be linear log(d – d*) or the dose ratio ((d/d*))\nSpecifying the asymptotes: The model fits values in the range (0-1), asymptotically approaching either limit. If desired the range can be reduced by re-scaling so that it asymptotically approaches a specified lower limit &gt; 0 and upper limit &lt; 1.\nChecking the prior: The influence of the prior on the model can be checked by viewing the results of sampling of the model parameters from the specified prior. Options are to view a) 100 samples of the 1-dimensional model for drug 1 or drug 2, or b) 1,000 samples of the 2-dimensional model over both drugs showing the 10%-ile, median or 90%-ile toxicities at each dose combination.\n\n\n\n\n\n\nFigure 12: Design Response Model showing individual drug prior\n\n\n\nThe individual drug graph plots the doses at the log of their transformed dose strength (“Log(X^)”), this enables the logits to be plotted linearly.\nIf there is a transformed dose strength of 0, this is plotted at some notional value that represents -infinity, and to see the plot at the other doses the “Plot excl. first dose” option should be checked.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#prior-toxicities",
    "href": "documentation/v71/userguides/2dcrm.html#prior-toxicities",
    "title": "2D-CRM",
    "section": "Prior Toxicities",
    "text": "Prior Toxicities\nThis tab allows prior information to be included with the data collected during the trial.\nThe user can specify, for any dose combinations a prior observed number of toxicities and number of observations. To allow the prior data to be down weighted, the user is allowed to enter fractional amounts.\nThe specified prior data is simply added to the observed data when fitting the model, but this is logically equivalent to fitting the model with the specified prior to the specified prior data and then using the resulting posterior estimate of the model parameters as the prior for fitting the model to the observed data.\nSome idea of the effect of the prior data can be gained by using the “Analysis” tab, and performing an analysis on a dataset without any additional subject data.\nIf there is a combination where the transformed dose strengths of both drugs are below 0.001, there can be no prior observed toxicities for that combination (it would be impossible for the model to fit them).\n\n\n\n\n\n\nFigure 13: Design Prior Toxicities",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#to-run-simulations",
    "href": "documentation/v71/userguides/2dcrm.html#to-run-simulations",
    "title": "2D-CRM",
    "section": "To run simulations",
    "text": "To run simulations\nEven before simulations have been run, FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Each such combination is a ‘simulation scenario’.\nThe user clicks on the check box for each scenario to be simulated, or simply clicks on “Select All”, then clicks on the “Simulate” button.\nDuring simulation, the user is prevented from modifying any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters into the current “.facts” file, and when the simulations are complete all the simulation results are saved in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there are sub-folders that holds the results for each scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#mcmc-settings",
    "href": "documentation/v71/userguides/2dcrm.html#mcmc-settings",
    "title": "2D-CRM",
    "section": "MCMC Settings",
    "text": "MCMC Settings\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nNumber of samples per imputation (not used in 2D-CRM, the model does not include imputation)\nIf the Number of MCMC samples to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output in a file per simulation, allowing the user to check convergence.\nThe MCMC output thinning parameter can be used to reduce the amount of data output to the MCMC file. It does not reduce the amount of MCMC samples used within the model fitting.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation.\n\n\n\n\n\n\nFigure 15: MCMC Settings",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#how-many-simulations-to-run",
    "href": "documentation/v71/userguides/2dcrm.html#how-many-simulations-to-run",
    "title": "2D-CRM",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%).\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#simulation-results",
    "href": "documentation/v71/userguides/2dcrm.html#simulation-results",
    "title": "2D-CRM",
    "section": "Simulation results",
    "text": "Simulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nAll: A window containing all the summary results columns\nHighlights: a separate window with the results shown on the main tab\nAllocation, Observed: summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity: summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc: summary results of the posterior probabilities of the properties of interest\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\n\nView Graph: opens the FACTS built in graph utility displaying the results for the currently selected scenario. See Error! Reference source not found. below for a description of the graphs.\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options:\n\nOpen results folder: Opens a file browser in the results folder of the scenario, allowing swift access to any of the results files.\nSimulation results: Opens a window displaying the individual simulation results for each simulation of the currently selected scenario\nOpen in R: opens a control that will launch R, first loading the selected files in the results folder as data frames.\nShow Graphs: launches the graph viewer to view the results of the currently selected scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/2dcrm.html#facts-grid-simulation-settings",
    "title": "2D-CRM",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#detailed-simulation-results",
    "href": "documentation/v71/userguides/2dcrm.html#detailed-simulation-results",
    "title": "2D-CRM",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 17) shows the results simulation by simulation. Like the summary results – the initial window hold “highlights”, right clicking on the window displays a menu with options for viewing the “Fitted Toxicity”, “Observed Toxicity” or “All” Columns.\n\n\n\n\n\n\nFigure 16: Detailed Simulation Results\n\n\n\n\n\n\n\n\n\nFigure 17: Detailed Simulation Results",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#cohort-simulation-results",
    "href": "documentation/v71/userguides/2dcrm.html#cohort-simulation-results",
    "title": "2D-CRM",
    "section": "Cohort Simulation Results",
    "text": "Cohort Simulation Results\nAfter viewing individual simulation results, the user may examine detailed results for any simulation with per-cohort data in the table by right-clicking on the row and selecting “Cohort Results”. A separate window opens, showing the result for the selected simulation cohort by cohort. Like the summary and simulation results the initial window displays “highlights” columns, right clicking on the window displays a menu with options for viewing the “Fitted Toxicity”, “Observed Toxicity” or “All” Columns.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#aggregation",
    "href": "documentation/v71/userguides/2dcrm.html#aggregation",
    "title": "2D-CRM",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 18: Aggregation\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nIn other design engines there is the option to pivot the results by dose, this is more complicated in 2D-CRM and the option is not available.\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_, so agg_summary.csv will contain the rows from each of the summary.csv files, cohortsNNN.csv files are aggregated into a single agg_cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#highlights",
    "href": "documentation/v71/userguides/2dcrm.html#highlights",
    "title": "2D-CRM",
    "section": "Highlights",
    "text": "Highlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\nThese results are in the ‘summar.csv’ files in each scenario results folder.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nNum Sims\n1\nThe number of simulations that are being summarised\n\n\nMean subjects\n1\nThe mean number (over the simulations) of subjects randomized\n\n\nPpn Toxic\n1\nThe mean (over the simulations) proportion of the randomized subjects that had a toxic outcome\n\n\nSd Ppn Toxic\n1\nThe standard deviation (over the simulations) of the proportion of subjects that had a toxic outcome.\n\n\nMean true toxicity\n1\nThe mean (over the simulations) of the true probabilities of toxicity that subjects have been exposed to.\n\n\nDuration\n1\nThe mean duration of the simulations in weeks\n\n\nSd Duration\n1\nThe standard deviation of the durations of the simulations\n\n\nPpn All Toxic\n1\nThe proportion of the simulations that stopped because all combinations were judged too toxic (exceeded the overdose control limit).\n\n\nPpn Early Succ\n1\nThe proportion of the simulations that stopped because the stopping condition(s) had been satisfied.\n\n\nPpn Reached Cap\n1\nThe proportion of the simulations that stopped because the maximum nmber of subjects had been reached.\n\n\nPpn MTT Under\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “under-dosing” range.\n\n\nPpn MTT Target\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “target” range.\n\n\nPpn MTT Excess\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “excess” range.\n\n\nPpn MTT Unacceptable\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “unacceptable” range.\n\n\nPpn Correct Under\n1\nThe proportion of the dose combinations that have a true toxicity rate in the “under-dosing” band over all the simulations that are determined to be “under-dosing”. That is the posterior probability at the end of the simulation that the toxicity rate of that dose combination of being in the “under-dosing” band is greater than the probabilities of being in the “target” band or the combined “excess” and “unacceptable” toxicity bands, and the combination is not excluded by the overdose control conditions.\n\n\nPpn Correct Target\n1\nThe proportion of the dose combinations that have a true toxicity rate in the “target” band over all the simulations that are determined to be “target”. That is the posterior probability at the end of the simulation that the toxicity rate of that dose combination of being in the “target” band is greater than the probabilities of being in the “under-dosing” band or the combined “excess” and “unacceptable” toxicity bands, and the combination is not excluded by the overdose control conditions.\n\n\nPpn Excess+Unacc\n1\nThe proportion of the dose combinations that have a true toxicity rate in the “excess” or “unacceptable” bands over all the simulations that are determined to be “excess” or “unacceptable”. That is the posterior probability at the end of the simulation that the toxicity rate of that dose combination of being in the “excess” or “unacceptable” bands is greater than the probabilities of being in the “under-dosing” band or the “target band”, or the combination is excluded by the overdose control conditions.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#fitted-toxicity",
    "href": "documentation/v71/userguides/2dcrm.html#fitted-toxicity",
    "title": "2D-CRM",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Tox Coeffs\n5\nThe mean (over the simulations) of the estimate of the 5 parameters of the BLRM Dose-Toxicity model\n\n\nSd Tox Coeffs\n5\nThe standard deviation (over the simulations) of the estimate of the 5 parameters of the BLRM Dose-Toxicity model\n\n\nPr(Under): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the posterior probability of being under the target toxicity at each dose combination.\n\n\nPr(Target): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the posterior probability of being in the target toxicity band at each dose combination\n\n\nPr(Excess): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the posterior probability of being in the excess toxicity band at each dose combination\n\n\nPr(Unacc): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulation) of the posterior probability of being in the unacceptable toxicity band at each dose combination\n\n\nPr(Sel-MTT): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe proportion of times that each dose combination was selected as the combination with the MTT (Maximum Target Toxicity). Some methods may select more than one such combination so that these proportions sum to more than one for each scenario.\n\n\nMean Fitted Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the estimate of the toxicity rate at each dose combination.\n\n\nSd Fitted Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe standard deviation (over the simulations) of the toxicity rate at each dose combination.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#observed-toxicity",
    "href": "documentation/v71/userguides/2dcrm.html#observed-toxicity",
    "title": "2D-CRM",
    "section": "Observed Toxicity",
    "text": "Observed Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Subjects: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the number of subjects allocated to each dose combination.\n\n\nSd Subjects: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe standard deviation (over the simulations) of the number of subjects allocated to each dose combination.\n\n\nMean Obs Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the number of observed toxicities at each dose combination.\n\n\nSd Obs Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe standard deviation (over the simulations) of the number of observed toxicities at each dose combination.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#the-per-simulation-results",
    "href": "documentation/v71/userguides/2dcrm.html#the-per-simulation-results",
    "title": "2D-CRM",
    "section": "The Per-Simulation Results",
    "text": "The Per-Simulation Results\nDouble clicking on any row in the simulation results table, opens a window displaying the individual simulation results. This table shows the individual results that are averaged in the summary table.\nThese values are in the ‘simulations.csv’ for each scenario in the scenario results folder.\nLike the summary results, the per-simulation results are organized into ‘Highlights’, ‘Fitted Toxicity’ and ‘Observed Toxicity’ tables, with the corresponding columns.\nRight clicking on any summary results table brings up a menu that allows the user to:\n\nOpen the per-simulations results window with all columns\nOpen the per-simulations results window with the highlights columns\nOpen the per-simulations results window with the fitted toxicity columns\nOpen the per-simulations results window with the observed toxicity columns\nOpen the results folder for the scenario line that the cursor is on\nOpen the cohort results widow for the currently selected simulation.\n\n\n\n\n\n\n\nFigure 20",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#fitted-ppntoxicity",
    "href": "documentation/v71/userguides/2dcrm.html#fitted-ppntoxicity",
    "title": "2D-CRM",
    "section": "Fitted Ppn(Toxicity)",
    "text": "Fitted Ppn(Toxicity)\nThe Fitted Ppn(Toxicity) graph shows the average mean posterior estimate of the toxicity at each combination, with the cells colored in increasing intensity the higher the fitted toxicity. In addition in each cell the average number of observed toxicities and average total observations is reported.\nThe format is:\nFit: &lt;Average Mean estimate of toxicity&gt;\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\n\n\n\n\n\n\nFigure 21",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#ppntoxicity-bands",
    "href": "documentation/v71/userguides/2dcrm.html#ppntoxicity-bands",
    "title": "2D-CRM",
    "section": "Ppn(Toxicity Bands)",
    "text": "Ppn(Toxicity Bands)\nThe Ppn(Toxicity Bands) graph shows the average mean posterior estimate that the toxicity falls in one of the defined toxicity bands, with the cells colored from faint to intense the higher the probability. The color used depends on the band being displayed: blue for under-dosing, green for target, orange for excess and red for unacceptable.\nIn addition in each cell the average number of observed toxicities and average total observations is reported.\nThe format is:\nProb: &lt;Average Posterior Probability&gt;\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\n\n\n\n\n\n\nFigure 22",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#toxicity-summary-graph",
    "href": "documentation/v71/userguides/2dcrm.html#toxicity-summary-graph",
    "title": "2D-CRM",
    "section": "Toxicity Summary Graph",
    "text": "Toxicity Summary Graph\nThe Toxicity Summary Graph displays a small histogram for each dose combination. In each histogram it shows for that dose combination the posterior probability its toxicity rate is in each of the four toxicity bands.\nIf the posterior probabilities are such that the dose combination is excluded by the overdose control rules, then the background of the histogram is in pink to highlight it.\n\n\n\n\n\n\nFigure 23",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#mean-prselected-mtt",
    "href": "documentation/v71/userguides/2dcrm.html#mean-prselected-mtt",
    "title": "2D-CRM",
    "section": "Mean Pr(Selected MTT)",
    "text": "Mean Pr(Selected MTT)\nThe Mean Pr(Selected MTT) graph shows the average number of times each combination was selected as the, or one of the, combinations with Maximum Target Toxicity. The cells are colored more intensely the greater the Ppn of simulations that the dose combination was selected as MTT.\nThe ‘Selected as MTT’ dose combination is the one with the maximum posterior probability of having a toxicity rate in the target band and not excluded by overdose control. The ‘Walk’ and ‘Allocate to MTT’ dose escalation methods select a single MTT at the end of a trial, but the ‘Contour’ and ‘Tri-Axial’ can select multiple MTTs so when using these dose escalation methods these probabilities can sum to more than 1.\nThe format is:\n&lt;PPn of simulations this combination selected as MTT&gt;\n\n\n\n\n\n\nFigure 24",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#mean-subject-allocation",
    "href": "documentation/v71/userguides/2dcrm.html#mean-subject-allocation",
    "title": "2D-CRM",
    "section": "Mean Subject Allocation",
    "text": "Mean Subject Allocation\nThe Mean Subject Allocation graph shows the average number of subjects allocated to each combination over all the simulations, with the cells colored from white to dark blue the greater the number allocated.\nThe format is:\n&lt;Number of subjects allocated to this combination&gt;\n\n\n\n\n\n\nFigure 25",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#mean-observed-prtoxicity",
    "href": "documentation/v71/userguides/2dcrm.html#mean-observed-prtoxicity",
    "title": "2D-CRM",
    "section": "Mean Observed Pr(Toxicity)",
    "text": "Mean Observed Pr(Toxicity)\nThe Mean Observed Pr(Toxicity) graph shows the average number of observations and number of toxicities along with the mean fitted toxicity across all the simulations. The color intensity reflects the fraction of observations that resulted in toxicity, with the color more intense the closer that fraction gets to 1..\nThe format is:\nFit: &lt;Average Mean Estimate of Toxicity&gt;\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\n\n\n\n\n\n\nFigure 26",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#study-sizes-and-outcomes",
    "href": "documentation/v71/userguides/2dcrm.html#study-sizes-and-outcomes",
    "title": "2D-CRM",
    "section": "Study Sizes and Outcomes",
    "text": "Study Sizes and Outcomes\nThe Study Sizes and Outcomes graph plots a histogram, of the sample sizes of the simulations. The bars are colored to show why the simulated trial stopped:\n\nBecause all dose combinations were too toxic (barred by the overdose control rules)\nBecause defined stopping rule had been met.\nBecause the trial maximum sample size had been reached.\n\n\n\n\n\n\n\nFigure 27",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#fitted-prtoxicity-per-simulationper-cohort",
    "href": "documentation/v71/userguides/2dcrm.html#fitted-prtoxicity-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Fitted Pr(Toxicity) per simulation/per cohort",
    "text": "Fitted Pr(Toxicity) per simulation/per cohort\nThe Fitted Ppn(Toxicity) graph shows the mean posterior estimate of the toxicity at each combination for each simulation, with the cells colored from beige to intense red the higher the toxicity. In addition in each cell the number of observed toxicities and total observations is reported.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can be viewed for the result of the analysis after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\nFit: &lt;Mean estimate of toxicity&gt;\nObs: &lt; Toxicity observations&gt; / &lt;Total observations&gt;\n\n\n\n\n\n\nFigure 28",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#prtoxicity-band-per-simulationper-cohort",
    "href": "documentation/v71/userguides/2dcrm.html#prtoxicity-band-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Pr(Toxicity Band) per simulation/per cohort",
    "text": "Pr(Toxicity Band) per simulation/per cohort\nThe Ppn(Toxicity Bands) graph shows the posterior estimate that the toxicity falls in one of the defined toxicity bands for each simulation, with the cells colored from faint to intense the higher the probability. The color used depends on the band being displayed: blue for under-dosing, green for target, orange for excess and red for unacceptable.\nIn addition in each cell the number of observed toxicities and total observations is reported.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can also be viewed for the result of the analysis after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\nProb: &lt;The Posterior Probability&gt;\nObs: &lt;The number toxicity observations&gt; / &lt;The number of total observations&gt;\n\n\n\n\n\n\nFigure 29",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#summary-graph-per-simulationper-cohort",
    "href": "documentation/v71/userguides/2dcrm.html#summary-graph-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Summary Graph per simulation/per cohort",
    "text": "Summary Graph per simulation/per cohort\nThe Summary graph shows the posterior estimate that the toxicity falls into each of the defined toxicity bands as a small histogram for each dose combination\nIn addition in the cells where the dose combination is excluded by the overdose rules are highlighted with a pink background.\n\n\n\n\n\n\nFigure 30",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#subject-allocation-per-simulationper-cohort",
    "href": "documentation/v71/userguides/2dcrm.html#subject-allocation-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Subject Allocation per simulation/per cohort",
    "text": "Subject Allocation per simulation/per cohort\nThe Subject Allocation graph shows the number of subjects allocated to each combination in a particular simulation with the cells colored from white to dark blue the greater the number allocated.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can also be viewed for the subject allocation after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\n&lt;Number of subjects allocated to this combination&gt;\n\n\n\n\n\n\nFigure 31",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#observed-prtoxicity-per-simulationper-cohort",
    "href": "documentation/v71/userguides/2dcrm.html#observed-prtoxicity-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Observed Pr(Toxicity) per simulation/per cohort",
    "text": "Observed Pr(Toxicity) per simulation/per cohort\nThe Observed Pr(Toxicity) graph shows the number of observations and number of toxicities for each combination, for each the simulation. The color intensity reflects the fraction of observations that resulted in toxicity, with the color more intense the closer that fraction gets to 1.The size of the colored square is proportional to the number of observations.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can also be viewed for the subject allocation after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\n\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\nsignifies the combination that is (one of) the combination(s) selected for allocation to for the next cohort. At the end of the simulation, these are the combinations that are selected as the MTT.\nCells excluded by the overdose control rules are marked with a “*”.\n\n\n\n\n\n\n\nFigure 32",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#true-prtoxicity",
    "href": "documentation/v71/userguides/2dcrm.html#true-prtoxicity",
    "title": "2D-CRM",
    "section": "True Pr(Toxicity)",
    "text": "True Pr(Toxicity)\nThe True Ppn(Toxicity) graph shows the toxicity rate simulated at each combination for all the simulations, with the cells colored from beige to intense red the higher the toxicity.\nThe format is:\n&lt;Simulated toxicity rate&gt;\n\n\n\n\n\n\nFigure 33",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/2dcrm.html#analysis-tab-graphs",
    "href": "documentation/v71/userguides/2dcrm.html#analysis-tab-graphs",
    "title": "2D-CRM",
    "section": "Analysis tab graphs",
    "text": "Analysis tab graphs\nAfter the analysis the graph shown by default is the “Observed Toxicities” graphs (shown above).\nThe other three graphs available are similar to the “per cohort” graphs available for simulations, namely:\n\nFitted Pr(Toxicity)\nPr(Toxicity band)\nBand Summary Graph\n\n\nFitted Pr(Toxicity)\nThis graph shows the mean fitted toxicity rate at each combination, colored with increasing intensity the higher the rate, and a summary of the observed data:\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\nPr(Toxicity Band)\nThis graph shows the posterior probability of being in a particular toxicity band at each combination, colored with increasing intensity the higher the probability, and a summary of the observed data:\n\n\n\n\n\n\nFigure 42\n\n\n\nThere is a control to select for which toxicity band the probabilities are shown.\n\n\nBand Summary Graph\nThis graph shows the posterior probability of being in each toxicity band at each combination as a simple histogram. Common y-axis range of 0-1 is used for all the histograms. The background of the histograms is white if the combination is within the overdose control rules and pink if the combination is excluded by the overdose control rules. At the top of each histogram is a summary of the observed data (toxicities / total observed):\n\n\n\n\n\n\nFigure 43",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "2D-CRM"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts625.html",
    "href": "documentation/versions/v6/facts625.html",
    "title": "FACTS 6.2.5 Release Notes",
    "section": "",
    "text": "Berry Consultants would like to announce a new maintenance release, FACTS 6.2.5. FACTS 6.2.5 contains the following improvements to the FACTS 6.2.4 version:\nThis release addresses three rare situations in FACTS 6.2.4. If any of your designs replicate these exact circumstances you are recommended to upgrade to FACTS 6.2.5:\n\nIn FACTS Staged Design with a Time-to-Event end point and a predictor, if using, in stage 1, a predictive probability of success in stage 2, the imputation from the predictor was not being performed correctly.\nIn FACTS Staged Design, if all recruitment is completed in the first stage, so that only follow up remains in the second stage, if the second stage contains interims by time, these interims were not simulated.\nIn FACTS Core, if a Dunnett’s adjusted p-value QOI was defined and there was an additional p-value QOI defined after it, the results reported for the Dunnett’s adjusted QOI were corrupted.\n\nThe remaining, minor enhancement is in the FACTS 6.2.5 GUI:\n\nIn FACTS Core TTE, if QOIs using a Predictor endpoint were defined over and above the default ones, the GUI could delete these on re-opening the “.facts” file. Should this have happened to you, you would have seen FACTS display a warning message that it was deleting these QOIs. The GUI has been fixed so that this deletion no longer occurs. There have been no changes to Dose Escalation or Enrichment Designs. There have been no updates to the documentation or examples.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.2.5 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts630.html",
    "href": "documentation/versions/v6/facts630.html",
    "title": "FACTS 6.3.0 Release Notes",
    "section": "",
    "text": "FACTS 6.3.0 is now available for official release. This version contains significant changes to FACTS N-CRM Open Enrollment to make it more efficient, and adds to FACTS Core and FACTS Staged Designs (Continuous, Dichotomous and Multiple Endpoint) options to model arms that differ in strength along 2 dimensions (for example, but not limited to: dose strength and dosing frequency). Please contact us regarding any questions.\nIn detail the new features in FACTS 6.3.0 are:\n\nIn FACTS Dose Escalation N-CRM the open enrollment option has been improved:\n\nThe limit of the “maximum number of subjects without final results” is now applied per dose not overall. This means that after escalating to a higher dose, allocation is not held up waiting for later subjects on the lower dose to complete. Accrual is faster, fewer subjects are lost. If you are thinking of doing an open enrollment N-CRM design, we strongly recommend you update to FACTS 6.3.0.\nThe user supplies two limits, one used while allocating to an “uncleared” dose, the other used when allocating to a “cleared” dose (and hence allocating to the MTD).\nIf recruiting 2 groups, different maximums can be specified for the second group.\nThere is now an option so that the simulation of Open enrollment only “pauses” when the early stopping criteria are met, allowing enrollment to be re-started if the final follow up data move MTD to a dose where the stopping criteria are not met.\n\nIn FACTS Dose Escalation N-CRM the open enrollment option has a new feature, the option to use “backfill”. Enabling “backfill” allows a subject who would otherwise be lost (because the “maximum number of subjects without final results” is currently met) to be allocated to a lower dose. There are parameters to control the backfill:\n\nseparate trial maximums can be specified for the subjects allocated in escalation or to the MTD, or in backfill.\nlimits on how many subjects can be on a dose for it to be open for backfill.\nlimits on how high the dose must be before it is open for backfill.\nlimits on how close a dose must be to the current dose for it to be open for backfill.\n\nIn FACTS Dose Escalation N-CRM there are now more “run-in” options:\n\nsimple run-in (as in FACTS 6.2.0)\ncustom run-in – where the user precisely specifies the sequence of doses to be tested and the number of subjects to test at each dose.\nsmall cohort pre-escalation – this follows the full escalation rules, including overdose control but with a smaller cohort size – and the same number of cohorts required to clear doses. Like all run-ins, it ends when a toxicity is observed.\n\nIn FACTS Dose Escalation N-CRM the calculation of the likelihood when analyzing an Ordinal Toxicity endpoint has been improved. This means however that a design using Ordinal Toxicity created under FACTS 6.2.0 is likely to behave noticeably differently under FACTS 6.3.0. If the design is well advanced, or in use, you are advised to stay with using FACTS 6.2.0 for that design. If you are just starting out designing an Ordinal Toxicity endpoint N-CRM we recommend upgrading to FACTS 6.3.0.\nFACTS Core and FACTS Staged Designs features a new 2D treatment arm option and associated 2D response models. The 2D options are available for the Continuous, Dichotomous and Multiple Endpoints. The 2D treatment arm option allows:\n\nArms to be defined as a combination of 2 “factors” e.g. dose strength and dosing frequency, or dose strengths of two different agents.\nThe combinations can be analyzed independently, mapped onto a 1D ordering and analyzed with any of the standard 1D dose models, or with one of the three new 2D response models: a 2D NDLM, a 2D continuous factorial model, or a 2D discrete factorial model.\nTarget Quantities of Interest can be defined to be confined to those combinations in a particular row or column (e.g. the calculate the Pr(max) of the once a day doses).\n\nIn FACTS Enrichment Designs the implementation of fitting of the Hierarchical model (options for treatment arms across groups and control arms across group) have been improved. They should converge somewhat faster and at the FACTS default MCMC sample length (2500), will typically be more accurate than before.\n\nThis release addresses some situations in FACTS 6.2.0 that could cause errors. If any of your designs replicate these circumstances, you are recommended to upgrade to FACTS 6.3.0:\n\nIn FACTS 6.2.0 Dose Escalation 3+3, the simulations don’t properly implement the re-escalation rules after de-escalation. This was introduced when we made the significant extensions to N-CRM in FACTS 6.2.0.\nIn FACTS 6.2.0 Dose Escalation N-CRM many “pseudo-patients” parameters are not interpreted correctly.\nIn FACTS 6.2.0 Enrichment Designs with a Continuous endpoint, when using the Linear Regression Longitudinal Model, it fitted incorrectly when informative priors were used.\nIn FACTS 6 Core with a Continuous endpoint and simulating baseline, calculating a p-value QOI, with BOCF for missing data, the BOCF value for missing subjects was being set incorrectly (only a problem if baseline values are very difference from 0).\n\nThe following minor issues in the FACTS GUI were also fixed:\n\nIn FACTS Dose Escalation with N-CRM when specifying an open enrollment design, maximum subjects on MTD for “clearing” a dose and for stopping are meant to be entered in “subjects” but the GUI interpreted the input as “cohorts’ using whatever was the last cohort size in that “.facts” file.\nWhen using the “Ppn Correct Arm” in FACTS Core by marking arms as “should succeed” in the VSR profiles, if variants were not enabled, the variant target QOI arm selection criteria would incorrectly re-set to “Pr(Max)” when re-opening the file.\nWhen using a large external data file, running simulations with lots of packets could cause “out-of-memory” issues. Finally, some enhancements and fixes in the Design Report in FACTS Core have been implemented.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.3.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts641.html",
    "href": "documentation/versions/v6/facts641.html",
    "title": "FACTS 6.4.1 Release Notes",
    "section": "",
    "text": "1 Introduction\nBerry Consultants would like to announce a new maintenance release, FACTS 6.4.1. FACTS 6.4.1 contains the following improvements to the FACTS 6.4.0 version. Please contact us regarding any questions.\n\n\n2 FACTS (Staged) Core Improvements\n\nIn Time-to-Event designs, the sigmoidal, 3-parameter logistic and hierarchical logistic dose response models have been improved to better handle their respective likelihood evaluation. Namely, when the dose response is non-monotone, or the doses are widely separated.\nIn Time-to-Event designs, the prior for the sigmoidal model’s a2 parameter is now properly applied. As a result, estimates for the sigmoidal model’s a1 and a2 parameter have now been corrected.\nIn Time-to-Event designs, the option to model control separately in TTE predictor models is now applied correctly.\nIn Dichotomous designs, selecting the “Log-odds” parametrization of Posterior Probability QOIs will no longer be rejected as invalid if the Delta values for comparison are outside of [-1, 1].\nIn Multiple Endpoint designs with a dichotomous endpoint, Posterior Probability QOIs with the “Log-odds” parametrization will now be computed correctly.\nA very rare bug has been fixed that occurred when an adaptive design was converted back to a fixed design. The simulator would check the now irrelevant details of the interims and crash.\n\n\n\n3 FACTS Dose Escalation Improvements\n\nIn CRM(Efficacy) designs, FACTS files created with FACTS 6.1.0 or older versions will have their “Model control separately” setting correctly migrated over in FACTS 6.4.1 and later versions.\nIn N-CRM designs, the number of beta distribution samples in the specific quantiles prior derivation algorithm has been increased from 1,000 to 10,000.\nIn Dose Escalation designs, Windows and Linux simulation result differences have been resolved.\nIn 2D-CRM dose values of 0 are now allowed with some restrictions:\n\nany combination where the transformed dose strengths of both drugs are very low (or 0) must be excluded from the study and not have any prior toxicities specified as to have occurred on that combination. The model cannot fit toxicity on such combinations.\nif there is a combination where the transformed dose strength of both drugs are 0, the response model must be re-scaled (using the “Asymptotes” option) so the lower bound is not asymptotically 0, but some value slightly above that (such as 0.0001).\n\nIn 2D-CRM the prior graph on the Response Model tab can now show the sampled priors for the individual drugs without the lowest dose being plotted (when a dose 0 or very low dose is included this can compress the plot for all the other doses). The x-axis has also been re-labelled to make it clear the doses are being plotted at the log of their transformed dose values.\nIn N-CRM if using Open Enrolment and Backfill, the “Max Study Allocation for Escalation” was not being respected, this is fixed in FACTS 6.4.1.\n\n\n\n4 FACTS Enrichment Design Improvements\n\nFACTS will no longer error when running multiple scenarios when using external data files.\n\n\n\n5 Framework Improvements\n\nSimulation of FACTS files stored on a shared drive will be handled more robustly in the case of intermittent connectivity to the shared drive.\nRenaming of FACTS analyses on the Analysis tab will now correctly handle the situation when the analysis name has been unchanged.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.4.1 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts600.html",
    "href": "documentation/versions/v6/facts600.html",
    "title": "FACTS 6.0.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.0.0\nBerry Consultants is delighted to announce that FACTS 6.0.0 is ready for release!\nBuilding on FACTS 5, FACTS 6.0.0 adds a new simulation type: “FACTS Staged Design”.\n\nFACTS “Staged Design” is a simulator that runs a “FACTS Core” simulation followed by a second “FACTS Core” simulation that can take decisions based on the result of the first simulation and include data from the first simulation. This allows, for example, the simulation of a Phase II trial followed by a Phase III trial, whether as separate trials or as a seamless Phase II/III.\nFACTS Enrichment Designs includes the flexibility over the timing of interims and the ability to set different decision thresholds at different interims.\n\nFACTS 6.0.0 is fully backwards compatible with FACTS 5 – it can load and run all your FACTS 5 designs – and then add new FACTS 6.0.0 features to them. In particular you can load a FACTS Core design into FACTS Staged Design as the starting point for the design of the first stage. You can have FACTS 5 and FACTS 6.0.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Staged Design:\n\nThe simulation of one treatment selection stage followed by another.\nThe stages can be connected on a scale from completely seamless to completely independent.\nFACTS Staged Design can be used to simulate:\n\na Phase II and the consequential Phase III trials, or a seamless Phase II/III trial\na Phase IIA and the consequential Phase IIB trials, or a seamless Phase IIA/B trial\na Phase II trial with a treatment arm selection and expansion stage\n\nThe simulations include:\n\nDifferent options for specifying the interval between the stages\nDifferent options for which data from the first stage can be included in the second stage: all of it, none of it, all the data on the arms retained in the second stage, all the data on the study drug arms in the first stage pooled on the one study drug arm retained in the second stage and just subjects from the first stage who did not complete in that stage.\nRules for selecting which treatment arms are kept in the second stage or are dropped after the first stage, including rules on specific arms (such as “retain the top dose if …”), rules on specific target arms (such as “retain the Minimum Efficacious Dose which has a Hazard Ratio of X or less compared to the Control Arm”) rules across all arms (such as “retain the 2 treatment arms with the highest probability of having a response greater than control, as long as their probability of toxicity is less than …”) and rules applied to groups of treatment arms (such as “retain the two arms that are once a day treatments rather than the two that are twice a day treatments if …”).\nDifferent analysis models, allocation rules, interims and decision criteria for each stage.\n\nThe ability to take decision in Stage 1 based on the predictive probability of the outcome of stage 2.\nThe full simulation output of both stages.\nGraphs of the Stage 1, Stage 2, Dose Selection and Overall results.\n\nFACTS Enrichment Designs:\n\nAs in FACTS Core, the scheduling of interims can now be specified by the number of subjects who have completed or have completed up to a particular visit.\nThe decision criteria thresholds can be specified separately for different interims.\n\nFACTS Core:\n\nThe option to specify a deterministic accrual and/or deterministic allocation sequence, for example allowing custom dose escalation trials with cohort accrual, while allowing the full functionality of the Core engine\n\nFACTS Dose Escalation:\n\nIs unchanged.\n\n\n\n\n3 Downloading FACTS 6.0.0\nThe FACTS 6.0.0 release is available for download from the Berry Consultants FACTS 6 ftp repository at http://berry1.berryconsultants.com/facts_6/.\nLogin using:\n\nUsername: facts_6\nPassword: DynIrgyur4\n\nThere are, as usual, 4 files to download: Documents.zip, Examples.zip, Setup.msi, setup.exe. Obfuscated versions of setup.exe and Setup.msi have been included for those of you accessing through firewalls that prevent the download of .exe and .msi files. Having downloaded setup.e_e and Setup.m_i simply rename the file suffix before using.\n\n\n4 Installing FACTS 6.0.0\nAs with previous version of FACTS, FACTS 6.0.0 installs on a PC. Simply place the downloaded files on the PC / laptop it is to be installed on and run the file “setup.exe”. You will need a new FACTS license key for this new version. It will be sent to you separately.\nFor any support questions or issues, please contact us.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.0.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v6/facts610.html",
    "href": "documentation/versions/v6/facts610.html",
    "title": "FACTS 6.1.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.1.0\nBerry Consultants is delighted to announce that FACTS 6.1.0 is ready for release! Building on FACTS 6.0.0, FACTS 6.1.0 adds two new Dose Escalation simulation types: “FACTS 2D-CRM” and “FACTS mTPI”:\n\nFACTS “2D-CRM” is a simulator that runs simulations of dose escalation trials testing combinations of doses from 2 drugs. The implementation follows that of the 2D-CRM prototype that was available earlier this year.\n\n\n\n\n\n\n\nFACTS mTPI is an implementation of Yuan Ji’s “Modified toxicity probability interval method for dose-finding trials”.\n\n\n\n\n\n\nFACTS 6.1.0 also adds a major piece of simulation functionality across (almost) all FACTS engines: ‘Design Variants’, these allow you to have within one “.facts” file, multiple designs with different maximum sample sizes. This makes it much easier to estimate the required sample size for a design. The feature includes the ability to mark specific treatment arms or groups as ‘correct choices’, and FACTS now summarizes not only the proportions of successful and unsuccessful trials, but also proportions of successful trials that also made correct choices.\n\n\n\n\n\nFACTS 6.1.0 is fully backwards compatible with FACTS 6.0.0 and 5 – it can load and run all your FACTS 6.0.0 and FACTS 5 designs – and then use new FACTS 6.1.0 features with those designs. You can have FACTS 6.1.0 and FACTS 6.0.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Dose Escalation:\n\nDesign Variants in N-CRM.\n2D-CRM\nmTPI\n\nFACTS Enrichment Designs:\n\nDesign Variants\nThe ability to extend hierarchical modeling with clustered model.\n\nFACTS Core:\n\nDesign Variants\nBetter control over which frequentist calculations are performed.\nThe ability to use p-value QOIs for early success/futility decision making.\n\nFACTS Staged Design:\n\nDesign Variants\nThere is now an ‘Analysis’ tab in Staged Design.\n\n\n\n\n3 Downloading FACTS 6.1.0\nThe FACTS 6.1.0 release is available for download from the Berry Consultants FACTS 6 ftp repository at http://berry1.berryconsultants.com/facts_6/.\nLogin using:\n\nUsername: facts_6\nPassword: DynIrgyur4\n\nThere are, as usual, 4 files to download: Documents.zip, Examples.zip, Setup.msi, setup.exe. Obfuscated versions of setup.exe and Setup.msi have been included for those of you accessing through firewalls that prevent the download of .exe and .msi files. Having downloaded setup.e_e and Setup.m_i simply rename the file suffix before using.\n\n\n4 Installing FACTS 6.1.0\nAs with previous version of FACTS, FACTS 6.1.0 installs on a PC. Simply place the downloaded files on the PC / laptop it is to be installed on and run the file “setup.exe”. You will need a new FACTS license key for this new version. It will be sent to you separately.\nFor any support questions or issues, please contact us.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 6 Release Notes",
      "FACTS 6.1.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v7/facts700.html",
    "href": "documentation/versions/v7/facts700.html",
    "title": "FACTS 7.0.0 Release Notes",
    "section": "",
    "text": "Introduction\nFACTS 7.0.0 is now available for download via App Center. This release marks the addition of two new FACTS design types: Platform Trial Design – Continuous and Platform Trial Design – Dichotomous.\nPlease contact us regarding any questions.\n\n\nFACTS Platform Trial Features\nWithin these new Platform Trial design types, FACTS users can now:\n\nSimulate a platform trial, for a continuous/dichotomous endpoint, with various trial level participant and arm constraints. In particular, users can specify a maximum enrollment time, number of participants, successful treatments, participants per arm and concurrent treatments.\nSimulate a platform trial with treatments arriving at different times during the trial.\nSpecify simulated mean arm responses/effects to be a fixed value or sampled from a distribution.\nSimulate participant accrual, responses, and dropout rates as per FACTS Core.\nSpecify a constant proportion of participants allocated to the control arm, or an allocation dependent on the number of treatments currently in the trial, with the option of performing response adaptive randomization.\nAnalyze participant data and estimate mean treatment responses using a Bayesian independent arm model, or frequentist p-values, comparing treatment arms to a common control arm.\nSpecify “Trial Update” information and frequency, at which analyses are performed and allocation ratios may get updated.\nSpecify when to evaluate “Treatment Milestones”, at which decisions are made about treatment outcomes.\nSpecify success/futility criteria that apply to all treatments, or to specific treatments.\nClassify treatments as Good, Mediocre or Unacceptable to get summary statistics such as the proportion of ‘Good’ treatments that are successful/inconclusive/unsuccessful and similarly for the other classifications.\nView granular simulation and summary results of various Platform Trial operating characteristics.\nGenerate a Platform Trial design report outlining the characteristics of the simulated design in a Word document.\n\n\n\nFACTS Core and Staged Improvements\n\nIn FACTS Core and Staged designs (except Time-to-Event designs), users can now simulate single arm trials, with options for both Bayesian and Frequentist p-values to be calculated comparing the data on the experimental arm to an objective reference response/response rate specified on the QOI tab.\nFACTS Core and Staged designs (except Time-to-Event designs) will now correctly handle frequentist calculations when a control arm is not present and comparison is performed against an objective reference response/response rate.\np-value calculations have been updated to better accommodate their use at interims, with dropouts and incomplete subjects now handled differently. No incomplete subjects have a final endpoint imputed, but subjects that are known dropouts and have had the opportunity to complete are imputed/ignored according to the “Handle missingness” option for the p-value.\nLOCF behavior has been made consistent. LOCF will impute a participant’s baseline value as their final outcome if a baseline value is observed and no non-baseline visit data is observed.\nFACTS Staged designs will now correctly handle the mirroring of Stage 1 data in Stage 2 for the Dose Response and Longitudinal models.\n\n\n\nFACTS Enrichment Design Improvements\n\nIn FACTS Enrichment Time-to-Event designs, FACTS will now correctly apply the user specified alpha levels per group when calculating frequentist output summaries.\nIn FACTS Enrichment Time-to-Event designs, FACTS will now correctly calculate frequentist output when the underlying design has only specified one group.\nIn FACTS Enrichment designs, FACTS will now enforce group caps to be strictly greater than zero.\n\n\n\nFACTS Dose Escalation Improvements\n\nOn the Analysis tab, FACTS will now enforce the specification of the cohort number when uploading a subject data file to run an analysis.\nIn 2D-CRM, FACTS will now correctly handle a rare situation in the row-by-row run-in scheme.\nIn 2D-CRM, the engines when run in a Linux environment will have a correctly formatted simulation results output header.\n\n\n\nGeneral Improvements\n\nBREAKING CHANGE: FACTS will now consistently handle the “Date” column in a patients file to be in weeks rather than days, making it consistent with the rest of FACTS. “Patients” files generated from FACTS simulations will report the “Date” column as “DateInWeeks” to avoid any ambiguity.\nBREAKING CHANGE: The “Date” column in Deterministic Accrual external data files will need to be manually updated to specify the date in weeks rather than days.\nBREAKING CHANGE: The “Date” column in subject data file provided when running a FACTS Analysis will need to be updated to specify the date in weeks rather than days. If performing FACTS Analysis via the GUI, the FACTS Analysis tab provides a “Convert Date from Days to Weeks” utility that does the conversion.\nThe precision of results output in FACTS will now consistently be up to 6 decimal places for all design types, except for Time-to-Event designs which will display output up to 8 decimal places.\nFACTS will now correctly handle interactions with the latest version of RStudio to date (2023.03.0). This includes the generation of design reports and the importing of FACTS results output to RStudio via the “Open in R” button on the Simulation tab. Note that FACTS will continue to support older version of RStudio.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.0.0 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v7/facts711.html",
    "href": "documentation/versions/v7/facts711.html",
    "title": "FACTS 7.1.1 Release Notes",
    "section": "",
    "text": "There are 4 main improvements with FACTS 7.1.1:\n\nThe FACTS Cloud download website has replaced the Microsoft App Center (which they are arbitrarily withdrawing from service).\nFACTS User Guides have been moved online to the FACTS knowledge hub: See here\nThe equations displayed on the FACTS user interface are being comprehensively overhauled so that they are clearer, more consistent and more correct. So far the main equation group of the FACTS Core, FACTS Staged and FACTS Platform Trial dose response models have been done.\nThe FACTS user interface tooltips (started in 7.1.0) have been extended to all of FACTS Core Continuous, Dichotomous and Time-to-Event. There is a setting in the Settings &gt; Options menu command to turn them off if you don’t need them.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.1.1 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v7/facts711.html#facts-core-and-staged-improvements",
    "href": "documentation/versions/v7/facts711.html#facts-core-and-staged-improvements",
    "title": "FACTS 7.1.1 Release Notes",
    "section": "FACTS Core and Staged Improvements",
    "text": "FACTS Core and Staged Improvements\n\nIn Core and Staged designs, when recruiting subjects deterministically, specifying a deterministic accrual profile without an associated file and saving the associated design will no longer cause FACTS to close unexpectedly.\nIn Core and Staged designs, the Virtual Subject longitudinal responses will handle the fraction of final SD per visit correctly when visits are deleted.\nIn Core and Staged designs, global frequentist analyses will not longer be performed by default when creating new designs.\nIn FACTS Core and Staged designs, the output flag specified when calling engine executables in execution mode will now correctly create the execdatasummary.csv file in the specified output directory.\nIn FACTS Core and Staged designs, new designs will by default no longer perform trial level frequentist analysis calculations.\nIn Staged designs, conditional power of current stage 2 when no control arm is carried to stage 2 will now be handled correctly.\nIn Staged designs, the 80% percentile of the number of subjects reported in the summary results will be consistent with that displayed in Core designs.\nIn Stage Multiple Endpoint designs, the sort criterion on the Transition tab will now be saved correctly when the sort criterion uses a non-primary endpoint QOI.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.1.1 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v7/facts711.html#facts-enrichment-design-improvements",
    "href": "documentation/versions/v7/facts711.html#facts-enrichment-design-improvements",
    "title": "FACTS 7.1.1 Release Notes",
    "section": "FACTS Enrichment Design Improvements",
    "text": "FACTS Enrichment Design Improvements\n\nIn Enrichment designs, new designs will by default no longer perform trial level frequentist analysis calculations.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.1.1 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/v7/facts711.html#facts-dose-escalation-improvements",
    "href": "documentation/versions/v7/facts711.html#facts-dose-escalation-improvements",
    "title": "FACTS 7.1.1 Release Notes",
    "section": "FACTS Dose Escalation Improvements",
    "text": "FACTS Dose Escalation Improvements\n\nIn 2D-CRM, the default overdose control settings have been updated to match those in CRM.\nIn 2D-CRM, auto generating doses will now correctly allow for dose strengths of zero.\nIn 2D-CRM, the response model by default will have the eta parameter prior specified in lognormal space.\n\nPlease contact us regarding any questions.",
    "crumbs": [
      "Documentation",
      "Release Notes",
      "FACTS 7 Release Notes",
      "FACTS 7.1.1 Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/index.html",
    "href": "documentation/versions/index.html",
    "title": "Release Notes",
    "section": "",
    "text": "Welcome to our section on FACTS Releases — your go-to source for detailed information on what’s new in each version of FACTS and a roadmap about what’s to come.",
    "crumbs": [
      "Documentation",
      "Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/index.html#what-youll-find-here",
    "href": "documentation/versions/index.html#what-youll-find-here",
    "title": "Release Notes",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nNew Features: Understand the core functionalities, enhancements, and integrations introduced with each update and planned for the future.\nImprovements & Optimizations: Learn about refinements in performance, stability, and efficiency that help ensure you’re always getting the best experience.\nFixes & Patches: Stay informed about resolved bugs, security patches, and other maintenance updates that keep our product reliable and secure.\nVersion & Update History: Trace the growth of our product through a chronological record of releases, so you’ll know exactly when key changes were introduced.",
    "crumbs": [
      "Documentation",
      "Release Notes"
    ]
  },
  {
    "objectID": "documentation/versions/index.html#how-to-use-these-notes",
    "href": "documentation/versions/index.html#how-to-use-these-notes",
    "title": "Release Notes",
    "section": "How to Use These Notes",
    "text": "How to Use These Notes\nExplore the roadmap of upcoming features, the latest release notes or browse previous versions to see how our product has evolved over time by selecting the major version (FACTS 6, 7 …) and then the release of interest from the sidebar. With every iteration, we’re committed to delivering meaningful enhancements that align with your needs. Check back often to stay current on all the ways we’re working to make our solution faster, more powerful, and easier to use.",
    "crumbs": [
      "Documentation",
      "Release Notes"
    ]
  },
  {
    "objectID": "documentation/glossary.html",
    "href": "documentation/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page provides definitions of terms, acronyms, and abbreviations that are commonly used across FACTS documentation.\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nName\nDefinition\n\n\n\n\n3+3\n3+3 The conventional 3-person cohort phase 1 design for Oncology trials with fixed dose escalation and de-escalation rules based only on the toxicities observed in the last cohort.\n\n\nAcross Groups\nA key feature of the Enrichment Design is the inclusion of analysis both within individual groups and across all the groups, and the ability to make decisions using both analyses. The across groups analysis is more than just pooling all the data and should be fully understood if it is to be used correctly.\n\n\nActive Comparator\nA treatment arm where subjects are treated with some existing therapy which the user either wishes to compare the novel treatment to (in addition to control), or wishes to use to check that the study has been properly designed to show a difference from control.\n\n\nArm Selection\nIn a Staged Design, there are a number of ways the user can specify which of the available arms are used in the second stage. The selection of the arms takes place at or after the specified time after the analysis at which the first stage decides to “graduate” to the second stage, and before the second stage starts.\n\n\nBaseline\nThe subject’s baseline is their endpoint value before treatment on the trial is started. In FACTS, the subject’s baseline is measured at their first visit at follow-up time 0, any prior visits (e.g. for screening to see if the patient is eligible for the trial) are not included in the simulation.\n\n\nbCRM\nA “bi-variate” form of the CRM that analyses both an efficacy and a toxicity endpoint.\n\n\nCap\nA limit on the number of subjects recruited. In FACTS N-CRM users can specify a cap on the overall number of subjects to be recruited in the trial (the ‘Overall Cap’) and specify stopping rules to define when the trial should stop before it reaches cap.\n\n\nControl\nIs the treatment arm with which the novel treatment(s) are principally being compared. Control may be placebo, or some existing standard of care, or therapy, against which the novel treatment has to be benchmarked in order to determine its likely usefulness.\n\n\nCore\nFACTS Core: A mode of FACTS for designing trials where multiple treatments, (possibly different doses of a novel treatment) are tested against a control and optionally an active comparator.\n\n\nCRM\nContinual Reassessment Method – a dose escalation design where the dose-toxicity is estimated using a simple Bayesian model, and the resulting estimates used to control the dose escalation and estimate the Maximum Tolerated Dose (MTD).\n\n\nCSD\nClinically Significant Difference\n\n\nCSHRD\nClinically Significant Hazard-Ratio Difference\n\n\nDE\nDose Escalation: a mode of FACTS where subjects are treated in cohorts and dose escalation is determined by the number of toxicities observed.\n\n\nDichotomized endpoint\nA dichotomous endpoint is created by measuring a continuous endpoint and scoring a subject as a responder if their score is above or below a specified threshold. FACTS allows the underlying continuous endpoint to be modeled longitudinally in order to provide a better prediction of whether a subject will be a responder at their final visit, or not.\n\n\nDose Response Model\nA model used in the statistical analysis of the final response as a function of the treatment dose strength. FACTS includes both parametric and non-parametric models, including ‘no model’.\n\n\nED\nEnrichment Designs: a mode of FACTS for designing trials where the same treatment is testing in different settings for example different sub-populations or different but related indications.\n\n\nEndpoint\nAn endpoint is a measure of the subject’s health that is being analyzed in order to learn about the effects of the treatments being studied in the trial.\n\n\nFACTS\nFixed and Adaptive Clinical Trial Simulator.\n\n\nFinal Endpoint\nThe value, or state, of a subject’s endpoint at the last visit in the follow-up schedule.\n\n\nGraduation\nThis is the analysis at which the first stage decides to stop, having decided that the second stage should be run. At an early interim Stage 1 can decide to stop the whole trial for success or futility, or to graduate to the second stage: this is referred to as Early Graduation. If Stage 1 reaches its final analysis, either because all the subjects in the first stage have completed follow-up, or a Stage 1 guillotine – a date when Stage 2 must start - has been reached, then at this final analysis it can again be decided to stop the whole trial for success or futility; otherwise the trial graduates to the second stage: this is referred to as Late Graduation.\n\n\nGroup\nThe very neutral term ‘groups’ is used in ED, because the ED design could be used in a number of different settings. The groups could be for instance different patient sub-populations with the same disease, different disease sub-populations such as different sarcoma types, or even different but related diseases that the same treatment might be effective against (migraine, dental pain and post-operative pain for instance). The key characteristic is that there is a need to study each independently (with their own control) but also look across the different groups for some purpose – this might be for analysis where there is an expectation of commonality in response on control, or in the difference from control on the treatment arms; or operationally where there is limited budget, drug supply or time across the groups; or for the purposes of decision making – where the treatment has to be successful in more than one group to be worth considering for further development.\n\n\nGUI\nGraphical User Interface, the visual part of the FACTS application that the user interacts with.\n\n\nHistorical Control\nA ‘historic control’ arm is used when no control arm is randomized to in the study, and the response on the arms where the novel treatment administered are compared to combined data from control arms from other already complete studies.\n\n\nImputation\nWhen the Bayesian statistical models are fitted to the simulated data within FACTS, if longitudinal modeling has been included, then multiple imputation is used for missing subject final endpoint values, whether missing due to the subject having dropped out or their final visit simply not occurred yet. The value is separately sampled at each iteration of the MCMC, from the posterior distribution of the longitudinal model, given the subject’s interim endpoint values.\n\n\nInformation\nThe timing of the first interim timing is in terms of when a specified amount of information has been collected – this can be number of subjects enrolled, number of subjects complete (up to a specified visits) or subjects who could have completed (up to a specified visit) – i.e. it includes those who would have completed if they hadn’t dropped out. Subsequent interims can be defined in terms of time (e.g. every 4 weeks) or information.\n\n\nInterim Visit\nA visit between the baseline visit and final visit, at which a subject’s endpoints are measured.\n\n\nIntermediate Endpoint\nThe value of a subject’s endpoint at a visit after their baseline visit, but before their final visit. Intermediate endpoint values can be used to estimate, using a longitudinal model, what the subject’s final endpoint will be (or would have been) thus providing additional information for analyses. Multiple imputation is used ensure that these estimated responses are included in the analysis with all due uncertainty. Sometimes called an early endpoint.\n\n\nLongitudinal Model\nAn analysis model that models the relationship between a subject’s interim endpoint values and their final endpoint value. In FACTS, all longitudinal models are simply multiple imputation models that can be used to impute a subjects final endpoint value when it is not available.\n\n\nMethod\nIn the FACTS documentation we try to reserve the term ‘method’ for the algorithms used in the simulation (as opposed to the analysis) part of the program. In the analysis part we use the term ‘Model’, see below.\n\n\nModel\nIn the FACTS documentation we try to reserve the term ‘model’ for the statistical models used in the analysis of the trial data (in the ‘design’ section of the FACTS user interface). Where mathematical algorithms are used for other purposes in FACTS (for instance in the generation of the simulated data) we try to use the term ‘method’. It is common for users to confuse the data generation and the trial implementation components of FACTS, and using distinct terminology may help to reduce this.\n\n\nMTD\nThe dose most likely to be the Maximum Tolerated Dose (MTD) – the dose with the highest Pr(MTD).\n\n\nMTD+\nThe dose most likely to be the MTD+ – the dose with the highest Pr(MTD+).\n\n\nMultiple Imputation\nWhen the Bayesian statistical models are fit to simulated data within FACTS, if longitudinal modeling has been included, then multiple imputation is used for missing subject final endpoint values. This includes subjects whose final endpoint data are missing due to the subject having dropped out and subjects who have not yet had enough follow-up time to observe their final visit response yet. Imputed final endpoint values are separately sampled at each iteration of the MCMC from the posterior distribution of the longitudinal model, given the subject’s interim endpoint values.\n\n\nNIM\nNon-Inferiority Margin\n\n\nNIHRM\nNon-Inferiority Hazard-Rate Margin\n\n\nOSD\nFor those designs with both a toxicity and efficacy endpoint the OSD is the Optimum Selected Dose, this will be the MED if the MED is below the MTD, otherwise it will be the MTD.\n\n\nOSD+\nFor those designs with both a toxicity and efficacy endpoint the OSD+ is the Optimum Selected Dose, this will be the MED+ if the MED+ is below the MTD+, otherwise it will be the MTD+.\n\n\nPr(MTD)\nA dose’s probability of being MTD is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable toxicity band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. This estimate of MTD is constrained to select one of the available doses.\n\n\nPr(MTD+)\nA dose’s probability of being the MTD+ is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. Unlike Pr(MTD), Pr(MTD+) includes estimating whether a dose below or a dose above the range of those being tested is more likely to have a toxicity in the acceptable band than any of the doses in the range.\n\n\nProfile\nA profile is a specification of one aspect of a scenario. Examples of these aspects are Accrual Rate, VSR, Dropout Rate, and others. A scenario is made up of one profile of each of the required types for the type of trial being simulated. FACTS allows the user to specify multiple profiles of each type and then presents all the possible combinations of profiles as scenarios that can be used to drive simulations.\n\n\nQOI\nQuantity Of Interest - A value to be calculated because it is of interest to the proceeding of the simulated trials. Quantities may be of interest because they are to be used in an interim decision, to adapt allocation, for final evaluation, or to ensure it is value is written out in the FACTS results files for analysis outside of FACTS.\n\n\nResponse\nA synonym of endpoint value. May be the value of a subjects final endpoint, or a change in a subject’s endpoint compared to their baseline state.\n\n\nRestricted Markov\nA form of the Dichotomous endpoint where, rather than the subject’s endpoint being either 0 or 1 and able to switch between them from visit to visit, their endpoint value is ‘stable’ until it becomes either ‘response’ or ‘failure’. Once their state has become ‘response’ or ‘failure’ it can then not change. For example ‘failure’ could be “subject has resumed smoking” in a smoking cessation trial, or death in an oncology trial.\n\n\nScenario\nA scenario is the complete specification of the unknown external factors that determine the data observed on the trial and its timing. The exact factors depend on the type of trial being simulated, but typically include:\n\nthe distribution of the final change from base line, or probability of response or rate of events in the different treatment groups\nthe properties of subjects’ early responses and the correlation with their final outcome\nthe rate at which subjects are recruited into the trial\nthe rate at which subjects drop out of the trial.\n\n\n\nSPEC\nThe Design Engine Specification document, describes the system algorithms, and meaning of parameters in a more technical context. SPEC documents have been deprecated as of FACTS 7.1.1\n\n\nSubject\nAn entity recruited onto a clinical trial for the purposes of learning about the properties of a treatment. Depending on the type of trial they might be patients or they might be healthy volunteers.\n\n\nTreatment Arm\nAn arm being studied in a clinical trial. A treatment arm may refer to different doses of the same treatment or completely separate therapies. Subjects, upon entering a study, are randomized to a treatment arm.\n\n\nUG\nThe User Guide document - describes in detail how to use a FACTS engine.",
    "crumbs": [
      "Documentation",
      "Glossary"
    ]
  },
  {
    "objectID": "documentation/glossary.html#overview",
    "href": "documentation/glossary.html#overview",
    "title": "Glossary",
    "section": "",
    "text": "This page provides definitions of terms, acronyms, and abbreviations that are commonly used across FACTS documentation.\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nName\nDefinition\n\n\n\n\n3+3\n3+3 The conventional 3-person cohort phase 1 design for Oncology trials with fixed dose escalation and de-escalation rules based only on the toxicities observed in the last cohort.\n\n\nAcross Groups\nA key feature of the Enrichment Design is the inclusion of analysis both within individual groups and across all the groups, and the ability to make decisions using both analyses. The across groups analysis is more than just pooling all the data and should be fully understood if it is to be used correctly.\n\n\nActive Comparator\nA treatment arm where subjects are treated with some existing therapy which the user either wishes to compare the novel treatment to (in addition to control), or wishes to use to check that the study has been properly designed to show a difference from control.\n\n\nArm Selection\nIn a Staged Design, there are a number of ways the user can specify which of the available arms are used in the second stage. The selection of the arms takes place at or after the specified time after the analysis at which the first stage decides to “graduate” to the second stage, and before the second stage starts.\n\n\nBaseline\nThe subject’s baseline is their endpoint value before treatment on the trial is started. In FACTS, the subject’s baseline is measured at their first visit at follow-up time 0, any prior visits (e.g. for screening to see if the patient is eligible for the trial) are not included in the simulation.\n\n\nbCRM\nA “bi-variate” form of the CRM that analyses both an efficacy and a toxicity endpoint.\n\n\nCap\nA limit on the number of subjects recruited. In FACTS N-CRM users can specify a cap on the overall number of subjects to be recruited in the trial (the ‘Overall Cap’) and specify stopping rules to define when the trial should stop before it reaches cap.\n\n\nControl\nIs the treatment arm with which the novel treatment(s) are principally being compared. Control may be placebo, or some existing standard of care, or therapy, against which the novel treatment has to be benchmarked in order to determine its likely usefulness.\n\n\nCore\nFACTS Core: A mode of FACTS for designing trials where multiple treatments, (possibly different doses of a novel treatment) are tested against a control and optionally an active comparator.\n\n\nCRM\nContinual Reassessment Method – a dose escalation design where the dose-toxicity is estimated using a simple Bayesian model, and the resulting estimates used to control the dose escalation and estimate the Maximum Tolerated Dose (MTD).\n\n\nCSD\nClinically Significant Difference\n\n\nCSHRD\nClinically Significant Hazard-Ratio Difference\n\n\nDE\nDose Escalation: a mode of FACTS where subjects are treated in cohorts and dose escalation is determined by the number of toxicities observed.\n\n\nDichotomized endpoint\nA dichotomous endpoint is created by measuring a continuous endpoint and scoring a subject as a responder if their score is above or below a specified threshold. FACTS allows the underlying continuous endpoint to be modeled longitudinally in order to provide a better prediction of whether a subject will be a responder at their final visit, or not.\n\n\nDose Response Model\nA model used in the statistical analysis of the final response as a function of the treatment dose strength. FACTS includes both parametric and non-parametric models, including ‘no model’.\n\n\nED\nEnrichment Designs: a mode of FACTS for designing trials where the same treatment is testing in different settings for example different sub-populations or different but related indications.\n\n\nEndpoint\nAn endpoint is a measure of the subject’s health that is being analyzed in order to learn about the effects of the treatments being studied in the trial.\n\n\nFACTS\nFixed and Adaptive Clinical Trial Simulator.\n\n\nFinal Endpoint\nThe value, or state, of a subject’s endpoint at the last visit in the follow-up schedule.\n\n\nGraduation\nThis is the analysis at which the first stage decides to stop, having decided that the second stage should be run. At an early interim Stage 1 can decide to stop the whole trial for success or futility, or to graduate to the second stage: this is referred to as Early Graduation. If Stage 1 reaches its final analysis, either because all the subjects in the first stage have completed follow-up, or a Stage 1 guillotine – a date when Stage 2 must start - has been reached, then at this final analysis it can again be decided to stop the whole trial for success or futility; otherwise the trial graduates to the second stage: this is referred to as Late Graduation.\n\n\nGroup\nThe very neutral term ‘groups’ is used in ED, because the ED design could be used in a number of different settings. The groups could be for instance different patient sub-populations with the same disease, different disease sub-populations such as different sarcoma types, or even different but related diseases that the same treatment might be effective against (migraine, dental pain and post-operative pain for instance). The key characteristic is that there is a need to study each independently (with their own control) but also look across the different groups for some purpose – this might be for analysis where there is an expectation of commonality in response on control, or in the difference from control on the treatment arms; or operationally where there is limited budget, drug supply or time across the groups; or for the purposes of decision making – where the treatment has to be successful in more than one group to be worth considering for further development.\n\n\nGUI\nGraphical User Interface, the visual part of the FACTS application that the user interacts with.\n\n\nHistorical Control\nA ‘historic control’ arm is used when no control arm is randomized to in the study, and the response on the arms where the novel treatment administered are compared to combined data from control arms from other already complete studies.\n\n\nImputation\nWhen the Bayesian statistical models are fitted to the simulated data within FACTS, if longitudinal modeling has been included, then multiple imputation is used for missing subject final endpoint values, whether missing due to the subject having dropped out or their final visit simply not occurred yet. The value is separately sampled at each iteration of the MCMC, from the posterior distribution of the longitudinal model, given the subject’s interim endpoint values.\n\n\nInformation\nThe timing of the first interim timing is in terms of when a specified amount of information has been collected – this can be number of subjects enrolled, number of subjects complete (up to a specified visits) or subjects who could have completed (up to a specified visit) – i.e. it includes those who would have completed if they hadn’t dropped out. Subsequent interims can be defined in terms of time (e.g. every 4 weeks) or information.\n\n\nInterim Visit\nA visit between the baseline visit and final visit, at which a subject’s endpoints are measured.\n\n\nIntermediate Endpoint\nThe value of a subject’s endpoint at a visit after their baseline visit, but before their final visit. Intermediate endpoint values can be used to estimate, using a longitudinal model, what the subject’s final endpoint will be (or would have been) thus providing additional information for analyses. Multiple imputation is used ensure that these estimated responses are included in the analysis with all due uncertainty. Sometimes called an early endpoint.\n\n\nLongitudinal Model\nAn analysis model that models the relationship between a subject’s interim endpoint values and their final endpoint value. In FACTS, all longitudinal models are simply multiple imputation models that can be used to impute a subjects final endpoint value when it is not available.\n\n\nMethod\nIn the FACTS documentation we try to reserve the term ‘method’ for the algorithms used in the simulation (as opposed to the analysis) part of the program. In the analysis part we use the term ‘Model’, see below.\n\n\nModel\nIn the FACTS documentation we try to reserve the term ‘model’ for the statistical models used in the analysis of the trial data (in the ‘design’ section of the FACTS user interface). Where mathematical algorithms are used for other purposes in FACTS (for instance in the generation of the simulated data) we try to use the term ‘method’. It is common for users to confuse the data generation and the trial implementation components of FACTS, and using distinct terminology may help to reduce this.\n\n\nMTD\nThe dose most likely to be the Maximum Tolerated Dose (MTD) – the dose with the highest Pr(MTD).\n\n\nMTD+\nThe dose most likely to be the MTD+ – the dose with the highest Pr(MTD+).\n\n\nMultiple Imputation\nWhen the Bayesian statistical models are fit to simulated data within FACTS, if longitudinal modeling has been included, then multiple imputation is used for missing subject final endpoint values. This includes subjects whose final endpoint data are missing due to the subject having dropped out and subjects who have not yet had enough follow-up time to observe their final visit response yet. Imputed final endpoint values are separately sampled at each iteration of the MCMC from the posterior distribution of the longitudinal model, given the subject’s interim endpoint values.\n\n\nNIM\nNon-Inferiority Margin\n\n\nNIHRM\nNon-Inferiority Hazard-Rate Margin\n\n\nOSD\nFor those designs with both a toxicity and efficacy endpoint the OSD is the Optimum Selected Dose, this will be the MED if the MED is below the MTD, otherwise it will be the MTD.\n\n\nOSD+\nFor those designs with both a toxicity and efficacy endpoint the OSD+ is the Optimum Selected Dose, this will be the MED+ if the MED+ is below the MTD+, otherwise it will be the MTD+.\n\n\nPr(MTD)\nA dose’s probability of being MTD is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable toxicity band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. This estimate of MTD is constrained to select one of the available doses.\n\n\nPr(MTD+)\nA dose’s probability of being the MTD+ is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. Unlike Pr(MTD), Pr(MTD+) includes estimating whether a dose below or a dose above the range of those being tested is more likely to have a toxicity in the acceptable band than any of the doses in the range.\n\n\nProfile\nA profile is a specification of one aspect of a scenario. Examples of these aspects are Accrual Rate, VSR, Dropout Rate, and others. A scenario is made up of one profile of each of the required types for the type of trial being simulated. FACTS allows the user to specify multiple profiles of each type and then presents all the possible combinations of profiles as scenarios that can be used to drive simulations.\n\n\nQOI\nQuantity Of Interest - A value to be calculated because it is of interest to the proceeding of the simulated trials. Quantities may be of interest because they are to be used in an interim decision, to adapt allocation, for final evaluation, or to ensure it is value is written out in the FACTS results files for analysis outside of FACTS.\n\n\nResponse\nA synonym of endpoint value. May be the value of a subjects final endpoint, or a change in a subject’s endpoint compared to their baseline state.\n\n\nRestricted Markov\nA form of the Dichotomous endpoint where, rather than the subject’s endpoint being either 0 or 1 and able to switch between them from visit to visit, their endpoint value is ‘stable’ until it becomes either ‘response’ or ‘failure’. Once their state has become ‘response’ or ‘failure’ it can then not change. For example ‘failure’ could be “subject has resumed smoking” in a smoking cessation trial, or death in an oncology trial.\n\n\nScenario\nA scenario is the complete specification of the unknown external factors that determine the data observed on the trial and its timing. The exact factors depend on the type of trial being simulated, but typically include:\n\nthe distribution of the final change from base line, or probability of response or rate of events in the different treatment groups\nthe properties of subjects’ early responses and the correlation with their final outcome\nthe rate at which subjects are recruited into the trial\nthe rate at which subjects drop out of the trial.\n\n\n\nSPEC\nThe Design Engine Specification document, describes the system algorithms, and meaning of parameters in a more technical context. SPEC documents have been deprecated as of FACTS 7.1.1\n\n\nSubject\nAn entity recruited onto a clinical trial for the purposes of learning about the properties of a treatment. Depending on the type of trial they might be patients or they might be healthy volunteers.\n\n\nTreatment Arm\nAn arm being studied in a clinical trial. A treatment arm may refer to different doses of the same treatment or completely separate therapies. Subjects, upon entering a study, are randomized to a treatment arm.\n\n\nUG\nThe User Guide document - describes in detail how to use a FACTS engine.",
    "crumbs": [
      "Documentation",
      "Glossary"
    ]
  },
  {
    "objectID": "documentation/v72/userguides/core/design/frequentistanalysis.html",
    "href": "documentation/v72/userguides/core/design/frequentistanalysis.html",
    "title": "Frequentist Analysis",
    "section": "",
    "text": "On the Frequentist Analysis tab the user can specify that some standard frequentist analyses be performed at trial analyses. The frequentist analysis tab is completely separate from, and independent of, any p-value QOIs that have been defined. The analyses specified on this tab cannot be used for simulated trial decisions - they are for storing in output only.\nEach specified analysis can be conducted using a variety of ways of handling missingness. Select all ways of handling missingness that are desired:\n\nMissing data replaced by last observation carried forward (LOCF)\nMissing data replaced by baseline observation carried forward (BOCF). This is only available if the endpoint is continuous and Baseline is being simulated.\nMissing data is ignored (a “per-protocol” analysis).\nMissing data is treated as a failure. This is only available if the endpoint is dichotomous.\n\nIf the trial has interim analyses, then for the simulations for which frequentist weeks files are to be output (specified on the Simulation tab) the standard frequentist analyses will be performed. If the trial has p-values QOIs, those QOIs are calculated every interim in all simulations.\nHaving the frequentist analysis include Dunnett’s adjusted p-values is a separate option (that applies to all the analysis type requested) because of the significant run-time overhead this can entail. Dunnett’s adjustment is available for continuous and dichotomous frequentist analyses.\nThe frequentist analysis tabs for the continuous and dichotomous engines also have trend tests, and allow the user to specify contrast coefficients to conduct those tests.\nNote that the reported frequentist estimates of the treatment effect take the specified direction of response on the Study tab (whether a response indicates subject improving or worsening) into account. They are adjusted so that a treatment that is estimated to be better than the control always has a positive treatment effect.\n\n\n\n\n\n\nFigure 1: The frequentist analysis tab for a continuous endpoint.\n\n\n\n\nContinuous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing unadjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\n(p-value)[## “1-sided p-value is reported in all the frequentist results. This is done in order to be consistent with comparisons with 1-sided α-values elsewhere.”],\nconfidence interval for the mean difference,\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nIf selected, using Dunnett-adjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\np-value,\nconfidence interval for the mean difference\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nIf neither placebo nor an active comparator are simulated, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out.\nIf high values of the endpoint are good, P-values are calculated using a one-sided t-test testing \\[H_0: \\mu_T &lt; \\mu_C\\] against \\[H_1: \\mu_T \\ge \\mu_C\\] with \\(\\mu_T\\) being the true treatment response mean and \\(\\mu_C\\) being the true control response mean. If low values of the endpoint are good, then the signs of the hypotheses are flipped.\n\n\nDichotomous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing the methodology described by Agresti, Mee and Nurminem for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nIf checked, using Dunnett-adjusted dose-placebo comparisons for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nP-values are calculated by user choice either using a normal approximation (see previous section on p-value calculations for continuous endpoints) or a Fisher’s exact test, which is a conservative test to test the null hypothesis that the outcome is independent of the treatment assignment. For the normality approximation to have good asymptotic coverage, the expected number of successes and failures under the null hypothesis should be greater or equal to 5 for both groups. In cases where this is not guaranteed, Fisher’s exact test is recommended when strict type 1 error control is required.\nIf neither placebo nor active comparator are specified, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out.\n\n\nTime-to-Event Frequentist Analysis\nFor each simulated trial, the following frequentist analyses will be performed:\n\nDose-placebo comparisons based on log-rank test and the Cox proportional hazards model. Summaries include:\n\nThe log-rank and Wilcoxon test statistics and the corresponding p-values,\nEstimated hazard ratio and its confidence interval from Cox model,\nFor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were, based on the hazard ratio inference from the Cox model).\n\nMedian survival times based on the Kaplan-Meier method.\nFor the predictor, descriptive statistics are computes – a 7 number summary for a continuous predictor, percentages for the dichotomous predictor, and median time, average hazard, and a cox model with predictor as covariate are computed for a time to event predictor.\n\nThe following variations for adjusting the alpha level for multiple comparisons should be available using the user-specified 1-sided alpha: unadjusted, and Bonferonni adjusted."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html",
    "href": "documentation/v72/userguides/core/design/doseresponse.html",
    "title": "Dose Response Models",
    "section": "",
    "text": "Dose response models in FACTS may be more accurately called final endpoint models. They create and model a relationship across the doses specified in the Treatment Arms tab. Often, but not always, the dose strength, called “Effective Dose Strength” in the Study &gt; Treatment Arms tab of FACTS, is used in the dose response models to determine the order of doses, and which doses are more related to others.\nThe dose response models can be simple, and model the doses largely independently, as is done with the Independent Dose Model or the Independent Beta Binomial Model (dichotomous only). They can have logistic style models with interpretable parameters, like the 3-parameter logistic or the \\(E_{max}\\) model (called Sigmoidal in FACTS). The dose response model can also be a model that creates a smooth, spline like, model over the doses using a normal dynamic linear model (NDLM), a monotonic NDLM, or a 2nd order NDLM.\nFor all endpoints, we model the response at each dose, d, in terms of \\(\\theta_d\\) on a continuous scale, allowing a consistent and rich range of dose response models to be used for all endpoint types. Transformations (see below) of the dichotomous and time-to-event responses are used to achieve this."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#independent-dose-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#independent-dose-model",
    "title": "Dose Response Models",
    "section": "Independent Dose Model",
    "text": "Independent Dose Model\nThe “Independent Dose Model” providing a simple pair-wise comparison between the study drug arms and the control arm and/or the active comparator arm. The doses are modelled as independent and normally distributed with a prior of:\n\\[\\theta_d \\sim \\text{N}(\\mu_d, \\nu_d^2)\\]\nWhere \\(\\mu_d\\) and \\(\\nu_d^2\\) are specified in FACTS and can either be the same or vary across arms.\nThis model is useful:\n\nWhen there is only one or two experimental arms\nWhen the study drug arms don’t differ by dose but in other ways, so there is no ordered sequence of treatments – e.g. each arm is the study drug in combination with a different additional drug.\nFor simulating simple trial designs\nFor simulating a ‘conventional’ or ‘strawman’ design to compare more complex designs against\n\nOtherwise for trials with multiple related arms that differ by dose, it is usually more efficient to use a dose-response model."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#independent-beta-binomial-model-dichotomous-only",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#independent-beta-binomial-model-dichotomous-only",
    "title": "Dose Response Models",
    "section": "Independent Beta-Binomial Model (Dichotomous Only)",
    "text": "Independent Beta-Binomial Model (Dichotomous Only)\nThis is a “no model” option similar to the above, but only available for the Dichotomous endpoint. Unlike any other dose response model, this model uses the beta distribution to model the probability of response directly rather than fitting a model to the log-odds of the probability.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(P_d)\\] where \\(P_d\\) is the probability that a patient is a response at the final endpoint for subjects randomized to dose \\(d\\). With posterior\n\\[P_d \\sim \\text{Beta}(\\alpha_d + \\text{responders}_d, \\beta_d + \\text{non-responders}_d)\\]\nWhere \\(\\alpha_d\\), \\(\\beta_d\\) are the priors for the arm \\(d\\), \\(\\text{responders}_d\\) is the number of responders on arm \\(d\\) and \\(\\text{non-responders}_d\\) is the number of non-responders on arm \\(d\\).\nThis model has the advantages of an easier to understand prior, and better estimation of \\(P_d\\) when the number of responders and non-responders is small (either is &lt; 5) compared to the log-odds model. As it’s a independent dose model, it is well suited to analyzing single arm, two arm or three arm trials, or trials where the experimental arms are unrelated, and so there is no basis on which to borrow information between them. With trials with multiple arms of the same treatment at different doses it is usually more advantageous to fit a dose response model."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#simple-ndlm",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#simple-ndlm",
    "title": "Dose Response Models",
    "section": "Simple NDLM",
    "text": "Simple NDLM\nThe Normal Dynamic Linear Model (NDLM) estimates the final endpoint as a smoothed curve across doses included in the model. Doses are tied directly to their nearest neighbor, and the prior expectation for a dose is that it is equal to its neighboring doses, tending to a constant dose response across doses in the absence of data. The model is defined as follows:\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately.\nThe dose response of the first dose, \\(d'\\), has a prior of:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nwhere \\(\\mu_{d'}\\) and \\(\\tau_{d'}^2\\) are specified directly in FACTS. Subsequent dose response estimates \\(\\theta_{d'+1}, \\ldots, \\theta_D\\) have priors centered at the previous dose response with variances based on the distance between the dose \\(d\\) strength and the dose \\(d-1\\) strength. Specifically,\n\\[\\theta_d \\sim N\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\text{ for } d=d'+1, \\ldots, D\\]\nwhere for dose strengths \\(\\nu_d\\) and \\(\\nu_{d-1}\\), \\(\\tau_{d-1}^2\\) is defined as \\[\\tau^2_{d-1}=\\tau^2\\left(\\nu_d-\\nu_{d-1}\\right)\\]\nThe prior distribution for the “drift” parameter, which controls the amount of smoothing is:\n\\[\\tau^{2}\\sim IG\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) and \\(\\tau_n\\) are specified in the Dose Response tab in FACTS under Model Parameters. See here for help with specifying an inverse gamma distribution with center and weight.\nIn the continuous case the residual error around the estimated dose response is\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nwhere \\(\\sigma_\\mu\\) and \\(\\sigma_n\\) are specified on the Dose Response tab in FACTS under Error Parameters.\nThe Simple NDLM is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Simple NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a null scenario where the response on all the doses is the same as control, the simple NDLM reduces type-1 error. The estimate of \\(\\tau^2\\) tends towards zero and the estimate of the dose response tends to a flat line. The smoothing of the estimates of the response reduces the random fluctuations in the estimates from dose to dose and is a very effective counter to the problem of type-1 error inflation due to the multiplicity of doses being tested. A small prior for the mean value of \\(\\tau\\) will accentuate this effect. The price to be paid for this benefit is a tendency to underestimate large changes in response or where there is a peak in the response. This over-smoothing can reduce power. A prior for the mean of \\(\\tau\\) centered at large values will minimize over-smoothing, and a very large prior would simply remove any smoothing effect at all. A standard prior for the mean of \\(\\tau\\) would be the expected average change in response from dose to dose, scaled to take into account the differences in the specified effective dose strengths.\nUsually, the choice of prior for \\(\\tau^2\\) is tuned based on simulation results, trading off reducing type-1 error in the null case against loss of power and reduced estimate of maximum response in the effective scenario with the largest jump in response or steepest peak.\n\n\n\n\n\n\nNote on the use of an NDLM with doses without subjects\n\n\n\n\n\nWhen using the NDLM model or any of its alternatives (2\\(^{nd}\\) order or Monotonic NDLM), and including doses in a design that receive no subject allocation (ie. Allocation ratio set to 0), some important characteristics of the model should be considered in the interpretation of results. First, the NDLM model “borrows” information from neighboring doses, and therefore including unallocated intermediate doses will increase uncertainty in the fitted response. Information must “pass through” these intermediate doses, and uncertainty should be expected to increase as the number of intermediate unallocated doses between two allocated doses increases. Secondly, the NDLM model estimates the response at unallocated doses, but without subject data at these doses the uncertainty in the estimate can be considerably larger than doses with subject data. This high degree of uncertainty can lead to a vestigial probability that an unallocated dose is the target dose (Pr(EDq), Pr(MED), Pr(Max)) even though the mean response estimate and probabilities at neighboring doses with subject data would not suggest this to be the case."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#monotonic-ndlm",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#monotonic-ndlm",
    "title": "Dose Response Models",
    "section": "Monotonic NDLM",
    "text": "Monotonic NDLM\nThe Monotonic NDLM endpoint model is similar to the NDLM in the sense that it smooths dose response estimates using neighboring doses, but is constrained to be either monotonically increasing or monotonically decreasing across doses. The direction of monotonicity depends on the value specified in the “Monotonic:” selection in the Model Parameters section of the Monotonic NDLM Dose Response page.\nThe use of this model is generally limited to endpoints where a strong presumption of increasing/decreasing response with dose is reasonable – such as a biomarker or a toxicity endpoint.\nLet doses \\(d = d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately. The following model is the monotonically positive NDLM:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nand\n\\[\\theta_d \\sim N^+\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\] where \\(\\tau_{d-1}^2\\) is defined as in the NDLM, and \\(X \\sim \\text{N}^+(\\mu, \\sigma^2)\\) refers to a positive truncated normal distribution with density function:\n\\[f_{X}(x) = \\frac{1 - \\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&gt;0\\]\nThe result of this dose-response model is that the curve is monotonically increasing, in that \\(\\theta_d&gt;\\theta_{d-1}\\).\nThe monotonically decreasing NDLM is similar except: \\[\\theta_d \\sim N^-\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\]\nwhere \\(X \\sim \\text{N}^-(\\mu, \\sigma^2)\\) refers to a negative truncated normal distribution:\n\\[f_{X}(x) = \\frac{\\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&lt;0\\]\nThe result of this dose-response model is that the curve is monotonically decreasing, in that \\(\\theta_d&lt;\\theta_{d-1}\\)."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#second-order-ndlm",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#second-order-ndlm",
    "title": "Dose Response Models",
    "section": "Second Order NDLM",
    "text": "Second Order NDLM\nThe second order NDLM serves as a “smoother” similar to the NDLM, however it tends to smooth toward a linear fit with a slope (the NDLM itself tends to shrink each dose’s fit toward its neighbors, while the second order NDLM prefers any trend in the neighbors).\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control arm or control is included in the dose response model, and \\(d'=2\\) if the control arm is modelled separately. The initial dose \\(d'\\) is modeled:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{0},\\tau^2_{0}\\right)\\]\nwhere \\(\\mu_0\\) and \\(\\tau_0^2\\) are specified directly in FACTS.\nIn the case of a time-to-event endpoint, the initial dose \\(d'\\) is the control arm, and has a \\(\\theta_{d'}= 0\\) by definition, so no prior distribution is needed.\nThe prior distribution for the dose response for the dose directly after the initial dose is specified on the difference between the \\(d'\\) and \\(d'+1\\) level doses:\n\\[\\theta_{d'+1} - \\theta_{d'} \\sim N\\left(\\mu_{1},\\tau^2_{1}\\right)\\]\nSuccessive doses are then modeled based on differences in slope between the dose and the two doses below them. Let:\n\\[\\theta_{d} = \\theta_{d - 1} + \\Delta_{d}\\zeta_{d} + \\frac{\\Delta_{d}}{\\Delta_{d - 1}}\\left( \\theta_{d - 1} - \\theta_{d - 2} \\right)\\]\nfor doses \\(d=d'+2,\\ldots,D\\), where \\(\\Delta_d=\\nu_d-\\nu_{d-1}\\) and \\(\\Delta_{d-1}=\\nu_{d-1}-\\nu_{d-2}\\). The priors for the dose response smoothing terms \\(\\zeta_d\\) are:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau_2^2)\\] The smoothing is determined by the parameter \\(\\tau_2\\). Small values of \\(\\tau_2\\) lead to more smoothing, while large values of \\(\\tau_2\\) lead to little information sharing between doses (above dose 1). Note that “small” and “large” is dependent on the scale of the doses. The prior on the smoothing parameter is:\n\\[\\tau_{2}^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau_2\\), and \\(\\tau_n\\) is the prior weight. See here for help with specifying an inverse gamma distribution with center and weight.\nNote that that in this formulation, \\(\\tau_2^2\\) can be thought of as the deviation from linearity. The linear scaling of deviations thus leads to a conditional variance that scales as the square of the dose difference:\n\\[\\text{Var}[\\theta_d \\mid \\theta_{d-1}, \\theta_{d-2}]=\\tau_2^2\\cdot (\\nu_d-\\nu_{d-1})^2\\]\nThe second order NDLM, like the simple NDLM, is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Second Order NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a null scenario where the response on all the doses is the same as control the Second Order NDLM, like the Simple NDLM, tends to reduce type-1 error. As the estimate of \\(\\tau^2\\) tends to zero the estimate of the dose response tends to a line (with non-zero slope if appropriate).\nThe second order NDLM has more power than the Simple NDLM when the dose response is smooth, but will tend to shrink estimates to the control by more than the Simple NDLM if there are marked peaks or steps in the response. Because it borrows from two neighboring doses, it is better at estimating a response for a dose where there is no data than the simple NDLM can, and can be used with burn-in’s where not all doses have been allocated to, but doesn’t cope well if there are consecutive doses where there is no data.\nHowever, unlike the simple NDLMs where the variance structure is preserved if doses are added to or dropped from the design, this is not true for the 2nd order NLDM. Thus, if using the 2nd order NDLM and the doses that are available to the model are changed, then the parameters for the prior for \\(\\tau_2^2\\) may need to be re-visited.\nThe simple NDLM is likely to be preferred where there are fewer doses (4-5), and the second order NDLM is often preferred when there are 7 or more (it will depend on the expected smoothness of the response – the smoother it is the more useful the second order NDLM will be).\nAs with the simple NDLM, the choice of prior for \\(\\tau_2^2\\) can be tuned based on simulation results: balancing having a smaller prior that limits type-1 error in the null case, against having a larger prior that yields more power and better estimate of maximum response in the effective scenarios with the large jumps in response or pronounced peaks.\n\n\n\n\n\n\nNote on the Second Order NDLM before FACTS 4.0\n\n\n\n\n\nThe second order NDLM described in this section is the version utilized in FACTS version 4.0 and later. The model labelled “Second Order NDLM” in versions before 4.0 is was maintained as the model labelled “Legacy 2nd Order NDLM” until the release of FACTS 7.1, at which time it was removed."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#parameter-logistic",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#parameter-logistic",
    "title": "Dose Response Models",
    "section": "3-Parameter Logistic",
    "text": "3-Parameter Logistic\nThe 3-parameter logistic dose response model estimates efficacy as a smooth curve using an intuitive parameterization. The dose response for a dose \\(d\\) with effective dose strength \\(\\nu_d\\) is:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}}\\]\nWhere the \\(a\\) parameters have the following description:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated change in dose response for a dose with strength ∞ when compared to a dose of strength 0.\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum (\\(a_2\\))\n\n\nThe shape of the 3-parameter logistic curve can vary wildly based on the parameter values, but always starts at \\(a_1\\) at dose strength 0 and monotonically increases to \\(a_1+a_2\\) as the effective dose strength goes to infinity.\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nIn the continuous case, the prior distribution for the error term in the 3-Parameter Logistic is Inverse Gamma:\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nAn advantage of the 3-Parameter Logistic model is that it is has a small number of parameters that have defined, intuitive interpretations. Since there are few parameters it’s generally easy to estimate the parameters even on small amounts of data. Because of the limited flexibility of the logistic curve, the 3-parameter logistic model should only be used when there is evidence that it is appropriate. See the Hierarchical Logistic and Sigmoid (Emax) models for dose response models with a similar pattern, but slightly more flexibility in shape."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-logistic",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-logistic",
    "title": "Dose Response Models",
    "section": "Hierarchical Logistic",
    "text": "Hierarchical Logistic\nThe  hierarchical logistic model Scott Berry’s favorite dose response model.  is an extension of the 3-parameter logistic with the form:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}} + \\zeta_{d}\\]\nwhere \\(\\zeta_d\\) is a random intercept term that modifies \\(a_1\\) differently for each dose under the constraint that all \\(\\zeta_d\\) must sum to 0.\nThe additional term \\(\\zeta_d\\) is a per dose ‘off-curve’ effect. It allows for a secondary non-parametric dose effect over and above the logistic model. It has been used in circumstances in which there is actually some falling away of effect at the highest dose. This is an example mean fit from 100 simulations. The model fit and error bars are in green, and the true dose responses are shown by the black line.\n\n\n\n\n\n\nFigure 1: An example of a fit from the hierarchical logistic model averaged over 100 simulations. The black line shows the truth, and the green line with error bars shows the model fits.\n\n\n\n\\(\\zeta_d\\) is modelled as:\n\\[\\zeta_d \\sim \\text{N}(0, a_4^2)\\]\nconditioned that\n\\[\\sum_{d}^{}\\zeta_{d} = 0\\]\nAnd \\(a_4^2\\) has an inverse gamma prior:\n\\[a_{4}^{2}\\sim IG\\left( \\frac{\\Lambda_{n}}{2},\\frac{\\Lambda_{\\mu}^{2}\\Lambda_{n}}{2} \\right)\\]\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nA typical recommended value for the center of the prior distribution of \\(\\alpha_4\\) is the largest expected likely deviation in true response from the underlying logistic with a weight of 1. The effect of this prior is then checked through simulation of a range of plausible dose response profiles, with a range of designs to check the sensitivity of the final estimate of response to this prior. Additionally, see here for help with specifying an inverse gamma distribution with center and weight.\nIn this model, compared to the ‘plain’ 3-parameter logistic above, it is important that the true ED50 lies within the expected dose range, and that the prior for the ED50, \\(a_3\\), has the majority of its probability mass in the available dose range. For example, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be:\n\\[a_{3}\\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\nUsing a weaker prior, such as \\(\\text{N}^+(\\nu_D, \\nu_D^2)\\) leads to a more linear fit. With just this change to the prior for \\(a_3\\) the average of the estimated of the mean response changes from the graph above to:\n\n\n\n\n\n\nFigure 2: An example of hierarchical logistic model fits using a prior that results in a flatter dose response model fit."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#sigmoid-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#sigmoid-model",
    "title": "Dose Response Models",
    "section": "Sigmoid Model",
    "text": "Sigmoid Model\nA sigmoid model (\\(\\text{E}_{\\text{max}}\\)) provides a coherent parameterization of the dose response trend similar to the 3-parameter logistic, but with one extra shape parameter, \\(a_4\\).\nThe model formula is:\n\\[\\theta_{d} = a_{1} + \\frac{(a_{2} - a_{1})v_{d}^{a_{4}}}{{a_{3}}^{a_{4}} + v_{d}^{a_{4}}}\\]\nThe interpretation of the four parameters is:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated dose response for a dose of strength \\(\\infty\\) (slight difference from Logistic models)\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum attainable effect (\\(a_2-a_1\\))\n\n\\(a_4\\)\n\ncontrols the slope of the dose response model at the ED50. A larger value of \\(a_4\\) corresponds to a steeper slope. A value of \\(a_4=1\\) makes the Sigmoid model equivalent to a Three Parameter Logistic model with \\(a_2\\) equal to \\(a_1 + a_2\\) from the Sigmoid model. A value of \\(a_4\\) approaching 0 corresponds to a dose response model that is nearly flat at \\(\\frac{a_{1} + a_{2}}{2}\\). By differentiation, it can be seen that the slope where the effective dose \\(\\nu_d=a_3\\) is \\((a_{2} - a_{1})\\frac{a_{4}}{4a_{3}}\\).\n\n\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_4, \\lambda_4^2)\\]\nThe advantage of this model is that it is considerably more flexible than the logistic, but still more efficient to estimate than an NDLM. This model (if applicable) is ideal if the target is an Effective Dose (ED) such as an ED90. Estimating an ED requires estimating the response on control, the maximum response, and the gradient which is requires a lot of data to do well if using no model or a smoothing model such as one of the NDLMs. With the 4-parameter Sigmoid model (or 3-parameter logistic – but that has a more limited applicability due to being significantly less flexible in shape), it’s usually not an issue to estimate the EDx well.\nThe caveats to using this model are:\n\nWhilst being much more flexible than the logistic, it still has model assumptions, in particular monotonicity, that must be met for this to be a reasonable model to be used.\nThe curve is only well estimated if the true ED50 lies within the doses tested.\nLike the hierarchical logistic model above, the prior for \\(a_3\\) should be weakly informative, with the majority of its probability mass in the available dose range. So similarly, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be: \\[a_3 \\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\n\n\n\n\n\n\n\nFigure 3: Example of a sigmoid model with \\(\\alpha_1=5\\), \\(\\alpha_2=10\\), \\(\\alpha_3=3\\), and \\(\\alpha_4=5\\)."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#u-shaped-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#u-shaped-model",
    "title": "Dose Response Models",
    "section": "U-Shaped Model",
    "text": "U-Shaped Model\nThe U-shaped model when the allows for an initial increase (decrease) in the dose response with dose, followed by a leveling out of the mean, then a decrease (increase). Whether the U-Shaped model increases then decreases or decreases then increases is specified by the user on the Dose Response tab. An example curve for this model is given in the figure below.\nThe dose-response curve can be characterized in four different regions of doses. For the first region, which contains the lowest doses, \\(0&lt;\\nu_d&lt;p_{min}\\), the dose-response curve is increasing (decreasing):\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( \\frac{\\nu_{d}}{p_{\\min}} \\right)^{\\alpha}\\]\nThe next region is the plateau, where the dose-response curve is constant. For \\(p_{min} &lt; \\nu_d &lt; p_{min}+p_{width}\\): \\[\\theta_d=\\theta_0 + S\\cdot\\delta\\] For the third region, the dose-response curve is decreasing (increasing). For \\(p_{min}+p_{width} &lt; \\nu_d &lt; p_{min}+p_{width} + w_{width}\\),\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( 1 - \\frac{\\nu_{d} - \\left( p_{\\min} + p_{width} \\right)}{w_{width}} \\right)^{\\beta}\\]\nFor the final region, the dose-response curve is again constant, at the same level as the zero-dose. For \\(\\nu_d &gt; p_{min}+p_{width} + w_{width}\\), \\[\\theta_d = \\theta_0\\]\nThe parameters of the model are described below:\n\n\\(S\\) is \\(1\\) or \\(-1\\), as determined by the Model is increasing/decreasing radio buttons. \\(S=1\\) if Model is Increasing is selected, indicating that the model starts increasing at low doses.\n\\(\\theta_0\\) represents the zero-strength dose response. Its prior is: \\[\\theta_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\]\n\\(\\delta\\) represents the maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[\\delta \\sim \\text{N}^+(\\mu_\\delta, \\sigma_\\delta^2)\\]\n\\(p_{min}\\) represents the lowest dose at which the maximal change in response from the zero-strength is achieved. It is restricted to be positive, and has a prior of: \\[p_{min} \\sim \\text{N}^+(\\mu_{min}, \\sigma_{min}^2)\\]\n\\(p_{width}\\) represents the width of the plateau – the region where all doses achieve maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[p_{width} ~ \\sim \\text{N}^+(\\mu_{width}, \\sigma_{width}^2)\\]\n\\(w_{width}\\) represents the width of the region for doses beyond the plateau, where response is returning to the zero-strength dose level. It is restricted to be positive, and has a prior of: \\[w_{width} ~ \\sim \\text{N}^+(\\mu_{w}, \\sigma_{w}^2)\\]\n\\(\\alpha\\) determines the rate of change of the dose response curve for doses below the plateau. Values less than \\(1\\) indicate a rapid initial change from baseline, with slower change as the plateau is reached. Values greater than \\(1\\) indicate a slow initial change from baseline, with more rapid change as the plateau is reached. To help avoid identifiability issues, \\(\\alpha\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). \\(\\alpha\\)’s prior is: \\[\\alpha \\sim \\text{LN}^*(\\mu_\\alpha, \\sigma_\\alpha^2)\\] where \\(\\text{LN}^*()\\) represents the lognormal distribution with truncation constraints at \\(10^{-1}\\) and \\(10^{1}\\).\n\\(\\beta\\) determines the rate of change of the dose response curve for doses beyond the plateau. Values less than \\(1\\) indicate a slow initial change from the plateau level, with more rapid change subsequently. Values greater than \\(1\\) indicate a rapid initial change from the plateau level, with slower change subsequently. To help avoid identifiability issues, \\(\\beta\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). The prior on \\(\\beta\\) is: \\[\\beta \\sim \\text{LN}^*(\\mu_\\beta, \\sigma_\\beta^2)\\]\n\nThe U-shaped model has seven free parameters, and thus it is inadvisable to use such a model for dose-finding trial designs that contain only a few dose levels. When a small number of doses are available, it may help to constrain the values of \\(\\alpha\\) and \\(\\beta\\) by utilizing small standard deviations in the priors.\n\n\n\n\n\n\nFigure 4: An example dose response curve for the U-shaped model. In this example, \\(\\alpha&lt;1\\) and \\(\\beta &gt; 1\\)."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#plateau-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#plateau-model",
    "title": "Dose Response Models",
    "section": "Plateau Model",
    "text": "Plateau Model\nThe plateau model is a special case of the U-shaped model, in which \\(p_{width}=\\infty\\). That is, there is no return to baseline for high doses. This model eliminates three parameters from the U-Shaped model, since \\(p_{width}\\), \\(w_{width}\\), and \\(\\beta\\) are not used. See the figure below to see an example dose response model for the plateau model that mimics the beginning of the U-Shaped model above.\n\n\n\n\n\n\nFigure 5: An example dose response curve for the Plateau model."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#parameter-exponential-logistic-dichotomous-only",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#parameter-exponential-logistic-dichotomous-only",
    "title": "Dose Response Models",
    "section": "3 Parameter Exponential Logistic (Dichotomous Only)",
    "text": "3 Parameter Exponential Logistic (Dichotomous Only)\nThe 3-parameter exponential logistic model has the following structure:\n\\[\\theta_d = a_1 + a_2 \\nu_d^{a_3}\\]\nWhere \\(\\nu_d\\) is the effective dose strength of dose \\(d\\). This is a logistic model for the dichotomous endpoint because \\(\\theta_d\\) is the log odds ratio of the probability of the response, \\(P_d\\) at dose \\(d\\).\nThe exponent parameter \\(\\alpha_3\\) allows the change in slope at the lower end of the curve to be asymmetric in comparison to the change in curve at the upper end of the curve.\nThe priors for the parameters are:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] The interpretations of the parameters defining this model are:\n\n\\(a_1\\)\n\nthe dose response for a dose with strength 0\n\n\\(a_2\\)\n\nthe slope associated with the exponentiated dose strength\n\n\\(a_3\\)\n\na shape parameter modifying the effective dose strength through exponentiation.\n\n\nThe figure below shows an example of two different 3-parameter exponential logistic model fits. Notably, the fit shown in green has an \\(a_3\\) parameter greater than \\(1\\), which leads to faster increases of the response rate model as the effective dose strength increases.\n\n\n\n\n\n\nFigure 6: Two different examples of 3-parameter exponential logistic model fits. The green model has an \\(\\alpha_3\\) parameter greater than 1, which leads to faster increases of the sigmoid model as the effective dose strength increases."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-model",
    "title": "Dose Response Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nLike the Independent Dose Model, the hierarchical model treats the arms as if they are an unordered set of treatments, but unlike the Independent Dose Model the Hierarchical Model shares information between doses included in the model. The hierarchical model is a way of encouraging the estimates of arm responses to be similar, and the degree of this encouragement can be controlled using parameters of the prior distribution. The hierarchical model assumes that the arm parameters are drawn from the same normal distribution:\n\\[\\theta_d \\sim \\text{N}(\\mu, \\tau^2)\\]\nWhere \\(d\\) is the set of doses included in the model. The prior distributions for \\(\\mu\\) and \\(\\tau^2\\) are\n\\(\\mu \\sim \\text{N}(\\Lambda_\\mu, \\lambda_\\mu^2)\\)\nand\n\\[\\tau^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau\\), and \\(\\tau_n\\) is the prior weight. \\(\\tau^2\\) governs the amount of information shared between doses in the model. The estimates of the dose parameters can be encouraged to be close together by choosing a small value for \\(\\tau_\\mu\\) and a large value for \\(\\tau_n\\). See here for a tool to help understand the inverse gamma distribution specified by center and weight parameters.\nThe control arm can be included in the hierarchical model if it is desired to encourage the experimental arms to be similar to the control arm, or excluded from it if the experimental arms are encouraged to be similar to each other but not necessarily to the control arm. This is not true with time-to-event data, when the control arm can only be excluded from the hierarchical model."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#linear-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#linear-model",
    "title": "Dose Response Models",
    "section": "Linear Model",
    "text": "Linear Model\nThe linear model uses a strong assumption of exact linearity between the responses and the dose strengths. (For dichotomous models, the linearity is on the log-odds scale, and the linearity is on the log hazard ratio scale for TTE models.) The model is\n\\[\\theta_d=\\alpha+\\beta\\nu_d\\] for all doses \\(d\\) in the model. Both \\(\\alpha\\) and \\(\\beta\\) are given normal prior distributions:\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\]\nThe linear model is appropriate when there is strong scientific knowledge that the shape of the response curve should be linear as a function of dose strength. Note that if an arm has a dose strength between two other arms in the model, its response will always be estimated to be between the responses of those two arms. As a result, the linear model may not be appropriate in combination with certain decision quantities such as Pr(Max), because some doses are guaranteed to have probability zero of being the best arm. If the control arm is included in the linear model and it has the smallest dose value, then all experimental arms will always have exactly the same probability of being better than control.\nWe recommend creating scenarios where the true parameters deviate from linearity and simulating them to test the robustness of a design based on the linear model.\n\n\n\n\n\n\nNote\n\n\n\nFor dichotomous models, the linear model replaces the 2-parameter logistic model available in FACTS versions before 6.4. The two models are identical."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-linear-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-linear-model",
    "title": "Dose Response Models",
    "section": "Hierarchical Linear Model",
    "text": "Hierarchical Linear Model\nA more flexible version of the linear model is the hierarchical linear model. This model encourages the arm responses to have a linear-like shape, but also includes a term allowing parameters to deviate from linearity when appropriate. The arm parameters satisfy the relationship\n\\[\\theta_d = \\alpha + \\beta \\nu_d + \\zeta_d\\] where the \\(\\alpha\\) and \\(\\beta\\) parameters are as in the linear model, with prior distributions\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\] and the \\(\\zeta_d\\) parameters are the deviations from linearity. These deviations are assumed to have a normal distribution constrained so that their average value is zero:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau^2) \\text{ with } \\sum_d\\zeta_d=0.\\]\nThe prior distribution for \\(\\tau^2\\) is\n\\[\\tau^{2}\\sim\\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nIf \\(\\tau^2\\) is small, which can be encouraged by choosing \\(\\tau_\\mu\\) to be small and \\(\\tau_n\\) to be large, then the dose parameter estimates will lie close to a line. See here for help understanding FACTS’s parameterization of the inverse gamma distribution.\nThe hierarchical linear model is a good choice when, for example, one expects a linear relationship between the dose strengths and the response but one is not prepared to assume exact linearity. It can also be a good way of encouraging, but not requiring, monotonicity in the response."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#d-continuous-factorial-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#d-continuous-factorial-model",
    "title": "Dose Response Models",
    "section": "2D Continuous Factorial Model",
    "text": "2D Continuous Factorial Model\nThe 2D Continuous Factorial Model fits a regression on the dose response that uses the numeric dose strength as a continuous valued covariate. The dose response (with \\(\\eta_r\\) and \\(\\zeta_c\\) denoting dose strength of the row level and column level, respectively) is modeled as:\n\\[\\theta_{rc} = \\alpha_0 + \\alpha_1 \\zeta_c + \\alpha_2 \\eta_r + \\alpha_3\\zeta_c \\eta_r\\] With priors\n\\[\\alpha_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\] \\[\\alpha_1 \\sim \\text{N}(\\mu_1, \\sigma_1^2)\\] \\[\\alpha_2 \\sim \\text{N}(\\mu_2, \\sigma_2^2)\\] \\[\\alpha_3 \\sim \\text{N}(\\mu_3, \\sigma_3^2)\\]\nThen, \\(\\alpha_0\\) is the response at the control combination, \\(\\alpha_1\\) is the linear coefficient of the response to the column factor strengths \\(\\zeta_c\\), and \\(\\alpha_2\\) is the linear coefficient of the response to the row factor strengths \\(\\eta_r\\).\nThe user has the option to simplify the model and exclude the interaction term \\(\\alpha_3\\), which is the coefficient of the product of the two factor strengths.\nNote that despite being called “continuous” this dose response model applies to all endpoints. It’s called continuous because the coefficients act on the dose strengths as continuous measures of strength.\n\n\n\n\n\n\nFigure 7: 2D Continuous Factorial Dose Response model specification page."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#d-discrete-factorial-model",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#d-discrete-factorial-model",
    "title": "Dose Response Models",
    "section": "2D Discrete Factorial Model",
    "text": "2D Discrete Factorial Model\nThe 2D Discrete Factorial Model fits a regression on the dose response that uses the dose strength as a factor covariate rather than a numeric dose strength. Each dose is an independent level within each row or column factor, so any dose is equidistant from all other doses\n\\[\\theta_{rc} = \\alpha + \\gamma_r + \\beta_c\\]\nWith priors\n\\[\\alpha \\sim \\text{N}(\\mu_\\alpha, \\sigma_\\alpha^2)\\]\n\\[\\beta_c \\sim \\text{N}(\\mu_{\\beta_c}, \\sigma_{\\beta_c}^2)\\] \\[\\gamma_r \\sim \\text{N}(\\mu_{\\gamma_r}, \\sigma_{\\gamma_r}^2)\\]\nThe parameters associated with lowest level of each factor, \\(\\gamma_0\\) and \\(\\beta_0\\), are constrained to be \\(0\\).\n\n\n\n\n\n\nFigure 8: 2D Discrete Factorial Dose Response model specification page."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#d-ndlm",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#d-ndlm",
    "title": "Dose Response Models",
    "section": "2D NDLM",
    "text": "2D NDLM\nThe 2-D NDLM Model takes the idea of smoothing across doses that was used in the standard NDLM model, but smooths across row factors and column factors separately.\n\nThe Base Model, with Control Included\nThe treatment effect for the combination of level \\(r\\) in the row factor and level \\(c\\) in the column factor is denoted as \\(\\theta_{rc}\\), and \\(Y_{rc}\\) is the observed data in that cell. The borrowing parameters are denoted as \\(\\phi\\) for the row factor smoothing, and \\(\\tau\\) for the column factor smoothing. The dose strengths are denoted as \\(\\nu_r\\) for the row factors, and \\(\\omega_c\\) for the column factors. Let \\(\\Delta \\nu_r = \\nu_r - \\nu_{r-1}\\) and \\(\\Delta \\omega_c = \\omega_c - \\omega_{c-1}\\) (for \\(r&gt;0\\) and \\(c&gt;0\\)). For notational convenience at the grid edge, let \\(\\theta_{-1, c} = 0\\), \\(\\theta_{r,-1}\\), \\(\\Delta\\nu_0\\equiv\\infty\\), and \\(\\Delta\\omega_0\\equiv\\infty\\).\nThe 2-D NDLM Model with control included in the model can then be specified as:\n\\[\\theta_{0,0} \\sim \\text{N}(\\mu_0, \\tau_0^2)\\] \\[\\theta_{rc} \\sim \\text{N}(\\mu_{rc}, \\tau_{rc}^2)\\] where\n\\[\\tau_{rc}^{2} = \\left( \\frac{1}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{1}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)^{- 1}\\]\n\\[\\mu_{rc} = \\tau_{rc}^{2}\\left( \\frac{\\theta_{r - 1,c}}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{\\theta_{r,c - 1}}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)\\]\nwith priors\n\\[\\tau^{2}\\sim\\text{IG}\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\n\\[\\phi^{2}\\sim\\text{IG}\\left( \\frac{\\phi_{n}}{2},\\frac{\\phi_{\\mu}^{2}\\phi_{n}}{2} \\right)\\]\nNote: that not all combinations of \\(r\\) and \\(c\\) will have data associated with them. However, FACTS will still evaluate the prior at all combinations. Thus, in the situation depicted below where the grayed-out point indicates a treatment combination with no allocation, \\(\\theta_{1,2}\\) is not modeled conditioned only on \\(\\theta_{1,1}\\). \\(\\theta_{0,1}\\) also informs on \\(\\theta_{1,2}\\) via \\(\\theta_{0,2}\\).\n\n\n\n\n\n\nFigure 9: 2D dosing grid example with no data for (0,2).\n\n\n\n\n\nFix smoothing ratio for row factor and column factor\nOptionally, the user might want to specify the row factor borrowing as a function of the column factor borrowing:\n\\[\\phi\\equiv k \\cdot \\tau\\] where \\(k\\) is a user-specified constant. This can be performed by selecting “Fixed smoothing ratio” in the \\(\\phi^2\\) prior specification area.\n\n\nControl not in model, no zero-level doses\nIf neither treatment arm allows zero-level doses (e.g. like one would expect if the column factor represented doses and the row factor represented frequency), then a prior for the lowest dose would instead be needed as input from the GUI:\n\\[\\theta_{1,1} \\sim \\text{N}(\\mu_1, \\tau_1^2)\\]\n\n\n\n\n\n\nFigure 10: 2D NDLM Dose Response model specification page."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-priors",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#hierarchical-priors",
    "title": "Dose Response Models",
    "section": "Hierarchical Priors",
    "text": "Hierarchical Priors\nIf a hierarchical prior is used for either the Control or Active Comparator arm, then FACTS will estimate a prior for that arm using external data that the user provides. This prior is estimated through a hierarchical model, and then the in-study data collected on subjects randomized to that arm are used to estimate the posterior distribution for the arm.\nTo specify a hierarchical prior, the user specifies the sufficient statistics from each historical study. These are:\n\nContinuous: the mean response and the SD of the response of the control or active comparator arm and the number of subjects observed.\nDichotomous: the observed number of responders and the number of subjects observed in the study.\nTime-to-Event: the number of events and the amount of exposure within each bin in the piecewise model.\n\nThe information from the historical studies can be ‘down-weighted’ by decreasing the effective information in the sample size. For continuous, this can be done by decreasing the sample size by a percentage. For dichotomous, both the number of responders and the number of subjects would be decreased by a percentage. For time-to-event, multiplying the number of events and the exposure by the same fraction will reduce the information in the study without changing the reported hazard rate.\n\n\n\n\n\n\nFigure 11: The Hierarchical Priors tab for a continuous study when both the Control and Active Comparator are given hierarchical priors.\n\n\n\nThe hierarchical models for the control or active comparator rates are very similar across the endpoints. They are briefly described below.\n\nContinuousDichotomousTime-to-Event\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\theta_{0t} \\sim \\text{N}(\\mu_0, \\tau_0^2) \\text{ for } \\tau=0,1,2,\\ldots,T\\] where \\(\\theta_{0t}\\) is the mean for the control arm in trial \\(t\\) (\\(t=0\\) for the current trial and \\(t=1,2,\\ldots,T\\) for previous trials). A user needs to specify appropriate priors for the hyper-parameters:\n\\[\\mu_0 \\sim \\text{N}(m_0, \\sigma_0^2)\\] \\[\\tau_0^2 \\sim \\text{IG}(a_0, b_0)\\]\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\theta_{0t} \\sim \\text{N}(\\mu_0, \\tau_0^2) \\text{ for } \\tau=0,1,2,\\ldots,T\\] where \\(\\theta_{0t}\\) is the log-odds for the control arm in trial \\(t\\) (\\(t=0\\) for the current trial; \\(t=1,2,\\ldots,T\\) for previous trials). A user needs to specify appropriate priors for the hyper-parameters:\n\\[\\mu_0 \\sim \\text{N}(m_0, \\sigma_0^2)\\] \\[\\tau_0^2 \\sim \\text{IG}(a_0, b_0)\\]\nThe prior distributions for the hyper-parameters of the hierarchical model. The hyper-parameters are the mean and standard deviation of Normal distribution for the log hazard ratios of the event rates of the historical studies and the current study.\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\lambda_{st}=\\lambda_s \\exp(\\gamma_t) \\text{ for } t=0,1,2,\\ldots,T\\] where \\(\\lambda_{st}\\) is the hazard rate for the control arm in segment \\(s\\) (\\(s=1,2,\\ldots,S\\)) for previous trial \\(t\\) (\\(t=1,2,\\ldots,T\\)) and \\(\\lambda_{s0}\\) is the hazard rate for the current control arm in segment \\(s\\); \\(\\lambda_s\\) is a base hazard for segment \\(s\\); and \\(\\gamma_t\\) is the log hazard ratio between that base rate and the \\(\\lambda_{st}\\) values.\nThe following hierarchical model is used\n\\[\\gamma_t \\sim \\text{N}(\\mu_\\gamma, \\tau_\\gamma^2) \\text{ for } t=0,1,2,\\ldots,T\\] Users specify priors for the hyper-parameters:\n\\[\\mu_\\gamma \\sim \\text{N}(m_\\gamma, t_\\gamma^2)\\] \\[\\tau^2 \\sim \\text{IG}(a_\\gamma, b_\\gamma)\\]\nThe formulation above is not identifiable as changes in \\(\\lambda_s\\) can be compensated for by changes in the \\(\\gamma_t\\) values (thus, one can use different combination of \\(\\lambda_s\\) and \\(\\gamma_t\\), but acquire the same set of values \\(\\lambda_{st}\\) and thus the same likelihood). To avoid this difficulty, we use the above formulation but fix \\(\\gamma_0 = 0\\). In addition to preserving the identifiability of the structure, this constraint allows \\(\\lambda_s\\) to have the interpretation of being the hazard rate for the current control arm, and thus the prior on \\(\\lambda_s\\) from the main dose response may be used as the prior for \\(\\lambda_s\\).\n\n\n\n\nSetting Priors for Hierarchical Model Hyper Parameters\nUnless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.\nIn this case the following would be reasonable:\n\nSet the prior mean value for Mu as the mean of the mean responses on the control arm in the historic studies\nSet the prior SD for Mu equal to the average SD of response divided by the square root of the number of studies.\nSet the center for tau to the same value as the prior SD for Mu.\nSet the weight for tau to be &lt; 1.\n\nOne can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small (relative to the number of studies), then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.\nTo give some prior preference towards pooling or separate analysis the weight for tau has to be large (relative to the number of historic studies) – to have a design that is like pooling all the historic studies the mean for tau needs to be small (say 10% or less of the value suggested above). For there to be no borrowing from the historic studies the value for tau needs to be large (say 10x or more the value suggested above).\nThe best way to understand the impact of the priors is try different values and run simulations.\n\n\nBayesian Augmented Control (BAC) Example:\nIt is easiest to study the impact of BAC in a simplified version of the trial being designed, for example use a 2 arm fixed design with ‘no model’, no adaptation, no longitudinal modeling, etc.\nFor instance, in a continuous setting with an expected sample size of 50 on control, a mean response of 5 and an SD in the response of 2, looking at the effect of having 4 prior, similarly sized studies:\n\n\n\nExternal Study Sufficient Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of subjects\n\n\nMean Response\n\n\nSD of Response\n\n\n\n\n\n\nStudy 1\n\n\n50\n\n\n4.76\n\n\n2\n\n\n\n\nStudy 2\n\n\n50\n\n\n4.93\n\n\n2\n\n\n\n\nStudy 3\n\n\n50\n\n\n5.07\n\n\n2\n\n\n\n\nStudy 4\n\n\n50\n\n\n5.24\n\n\n2\n\n\n\n\nFor simplicity we have just varied the means of the studies – they are all consistent with a normally distributed estimate, with mean of 5 and a standard error of \\(\\frac{2}{\\sqrt{50}}\\).\nBy simulating a fixed trial with a sample size of 100 and fixed 1:1 allocation between control and a study arm we can easily observe the effects of a BAC on the control arm. Creating 6 VSR profiles with sigma = 2, and the same mean response for control and the study arm in each profile of: 4.53, 4.76, 5.07, 5.24 and 5.47 we can look at the bias of the final estimate and the shrinkage in the SD of the estimate that could be brought about by using BAC:\n\n\n\nQuick simulation study of how the hierarchical model for BAC effects estimates of the control rate under different true control rate scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Mean\n\n\nRaw mean\n\n\nRaw SD\n\n\nEstimate inc BAC\n\n\nSD inc BAC\n\n\nBias\n\n\nEffective additional Subjects\n\n\n\n\n\n\n4.53\n\n\n4.55\n\n\n0.28\n\n\n4.64\n\n\n0.255\n\n\n2.1%\n\n\n11.1\n\n\n\n\n4.76\n\n\n4.78\n\n\n0.28\n\n\n4.83\n\n\n0.250\n\n\n1.0%\n\n\n13.5\n\n\n\n\n4.93\n\n\n4.95\n\n\n0.28\n\n\n4.96\n\n\n0.248\n\n\n0.2%\n\n\n14.3\n\n\n\n\n5.07\n\n\n5.09\n\n\n0.28\n\n\n5.07\n\n\n0.248\n\n\n-0.4%\n\n\n14.2\n\n\n\n\n5.24\n\n\n5.26\n\n\n0.28\n\n\n5.20\n\n\n0.250\n\n\n-1.1%\n\n\n13.2\n\n\n\n\n5.46\n\n\n5.49\n\n\n0.28\n\n\n5.38\n\n\n0.255\n\n\n-1.9%\n\n\n10.7\n\n\n\n\nNote it is important to use the column “SD Mean resp 1” from the simulations file to see the SD of the estimate and not use the SD of the estimates in the summary table.\nThe small shrinkage in the SD is more impressive when viewed in terms of how many additional subjects would have to be recruited to achieve the same shrinkage in the raw estimate. These are useful savings for small additional bias.\nThe “effective additional subjects” was calculated: \\[\\left( \\frac{\\text{True sigma}}{\\text{AVG(SD Mean resp)}} \\right)^{2} - \\left( \\frac{\\text{True sigma}}{\\text{AVG(SE Mean Raw Response)}} \\right)^{2}\\] where in this example \\(\\text{True sigma}\\) was 2."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/doseresponse.html#time-to-event-missingness",
    "href": "documentation/v72/userguides/core/design/doseresponse.html#time-to-event-missingness",
    "title": "Dose Response Models",
    "section": "Time-to-Event Missingness",
    "text": "Time-to-Event Missingness\nFor a time-to-event endpoint, as is the convention in this endpoint, subjects are observed up to the time they have an event (or the end of follow up), so no subject drops out after having had an event. Subjects that do drop-out are included in the analysis as not having had an event up to the last observation of the subject."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/interims.html",
    "href": "documentation/v72/userguides/core/design/interims.html",
    "title": "Interims",
    "section": "",
    "text": "Interim analyses allow for decision making throughout the lifecycle of an adaptive trial in FACTS. Interim analyses can adjust allocation probabilities, drop arms, or allow for early success/futility of the trial. Interims can either be specified with calendar frequency – occurring every specified number of weeks or specified to occur after a specified amount of information has been collected."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/interims.html#continuous-and-dichotomous-endpoint",
    "href": "documentation/v72/userguides/core/design/interims.html#continuous-and-dichotomous-endpoint",
    "title": "Interims",
    "section": "Continuous and Dichotomous Endpoint",
    "text": "Continuous and Dichotomous Endpoint\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have actually completed a specified visit\nthe number of subjects who have had the opportunity to complete a specified visit (includes drop-outs)\n\nWhen specifying an interim analysis schedule, it can be done either based on time or based on one of the information categories above.\nIf specifying interims based on time the first interim analysis timing must be based on information, and each subsequent interim is triggered after the provided amount of time has elapsed. If accrual completes before the first interim threshold is reached, and the first interim was defined in terms of the number of subjects enrolled, then the interims by time start at full accrual. If the first interim is defined in any other terms (by events, subjects complete or subjects with opportunity to complete) then interims only start when this is reached (which might be never).\nIf specifying interims based solely on information, the table on the “Interims” tab determines when the analyses will be triggered. Each interim is defined individually by the number of patients/observations that have satisfied some criteria. If information is If information is defined as Subjects Enrolled, then interim are triggered immediately upon enrollment of the subject satisfying the criteria. If information is defined as completers or opportunity to complete, then interims are triggered immediately upon the visit being reached that satisfies the specified criteria. Successive interims must be in terms of the same or more observations at the same or later visit, and either Visit or Subject must increase. Different types of information cannot be mixed to trigger interim analyses except in using time to trigger interims after the first based on information.\n \nIf interims are governed by time, there is the option as to whether interims should continue after full accrual, or discontinue."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/interims.html#time-to-event-endpoint",
    "href": "documentation/v72/userguides/core/design/interims.html#time-to-event-endpoint",
    "title": "Interims",
    "section": "Time-to-Event Endpoint",
    "text": "Time-to-Event Endpoint\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have observed their predictor endpoint\nthe number of subjects who have had the opportunity to observe their predictor endpoint (includes drop-outs)\nspecified numbers of events observed\nspecified number of predictor events observed\n\nOutside of the new types of information, the time-to-event triggers work in exactly the same way that continuous and dichotomous triggers do."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/allocation.html",
    "href": "documentation/v72/userguides/core/design/allocation.html",
    "title": "Allocation",
    "section": "",
    "text": "The options on allocation tab depend on whether an adaptive or non-adaptive design has been selected on the ‘Study &gt; Study Info’ tab, and if adaptive whether subjects are recruited sequentially or in cohorts.\nIf the design is non-adaptive then the only allocation option is blocked with fixed allocation ratios.\nIf the design is adaptive with Continuous or Deterministic recruitment, then there are 4 allocation options.\nIf the design is adaptive with cohort recruitment then there are 3 allocation options, all of which can be combined with early stopping:"
  },
  {
    "objectID": "documentation/v72/userguides/core/design/allocation.html#randomization-ratio-and-blocking",
    "href": "documentation/v72/userguides/core/design/allocation.html#randomization-ratio-and-blocking",
    "title": "Allocation",
    "section": "Randomization Ratio and Blocking",
    "text": "Randomization Ratio and Blocking\nIn the Randomization Ratio and Blocking section of the Allocation tab the user inputs the components of randomization blocks that enroll from the onset of the study and until any arm is dropped. These blocks work like the Fixed Allocation tab blocks.\nOnce an arm is dropped the “Upon arm drop:” option in the Setup section of the allocation tab will determine how randomization proceeds."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/allocation.html#arm-dropping-criteria",
    "href": "documentation/v72/userguides/core/design/allocation.html#arm-dropping-criteria",
    "title": "Allocation",
    "section": "Arm Dropping Criteria",
    "text": "Arm Dropping Criteria\nThe user may select any combination of Bayesian “Per Dose” QOIs: Posterior Probabilities or Predictive Probabilities, or a Target Dose QOI. For each criteria selected (at least one must be selected), the user supplies a direction of comparison and probability threshold. If at any interim, any arm meets the specified arm dropping criteria, then that arm becomes a candidate for dropping. After all candidates are identified for dropping, the Setup rules determine which, if any, of the candidates will be dropped."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/allocation.html#setup",
    "href": "documentation/v72/userguides/core/design/allocation.html#setup",
    "title": "Allocation",
    "section": "Setup",
    "text": "Setup\nA variety of rules are specified in the Setup section of the Allocation tab.\n\nMax number of arms that can be dropped during the study\nThe maximum number of arms that can be dropped – this can be set to from 0 to the number of active arms, the Control and Active Comparator are never dropped. This figure is the ‘maximum number of arms that can be dropped over the duration of the trial’. At any interim any number between 0 and the ’maximum number of arms can be dropped as long as this limit is not exceeded on the overall number of arms that would have been dropped.\nIf the maximum number of arms that can be dropped is set to the number of study drug arms, then if all arms are dropped this is recorded as an “early termination for futility”.\n\n\nPrune from lowest/highest dose\nArm dropping can be restricted to ‘pruning from the lowest dose’ and/or ‘pruning from the highest dose’. If ‘prune from the lowest dose’ is selected, then until the lowest dose meets the arm dropping criteria the second lowest dose cannot be dropped and so on. At an interim when the lowest dose does meet the arm dropping criteria, and the second dose does too, then the second dose can be dropped as well (and third and fourth …), it does not have to wait until the next interim. Pruning from the highest dose does the same thing, except that no dose can be dropped unless every larger dose will also be dropped.\nIf no pruning is selected or pruning from both ends is selected, then the user must select to prioritize the higher or lower dose to cover the instance when more doses are candidates for dropping than are allowed to drop by the maximum threshold. If “Prioritize dropping the lowest dose” is selected then when forced to choose between two or more doses to drop, the one with the lowest effective dose strength will be dropped.\n\n\nUpon arm drop\nFinally, specify what is to be done with the unused subjects that would have been allocated to an arm that has now been dropped. There are three options:\n\nMaintain study size, maintain combined block size of treatments\n\nSubjects that would have been allocated to arms that have been dropped are now randomized between all the remaining active treatment arms (excluding control and active comparator) using their original allocation ratios. For example, if the allocation ratio was 2:1:1:1 between Control and D1, D2 and D3, then after dropping D1 the block size remains size 5 (2 to Control and 1 to each of D2 and D3) with the \\(5^{th}\\) slot being allocated 1:1 between the remaining two study arms D2 & D3. Thus, the overall proportion of subjects on Control (and Active Comparator) is maintained.\n\nMaintain study size, reduce combined block size of treatments\n\nSubjects that would have been allocated to any arms that have been dropped are randomized between all remaining arms (including control) using a reduced total block size. Thus, if the allocation ratio was 2:1:1 between Control and D1 and D2, then after dropping D1 the block size drops to total size three: 2 to Control and 1 to D2.\n\nDecrease the study size, reduce combined block size of treatments\n\nSubjects that would have been allocated to any arms that have been dropped are no longer recruited, and the total sample size of the study is reduced; the remaining subjects are recruited using a reduced block. If interims are specified by number of subjects recruited, these interim sizes are reduced proportionally. As an example of this, if an arm that gets 20% of the randomization is dropped when there are 100 subjects enrolled, then if the next interim is scheduled at 200 subjects enrolled, it will actually be performed at 180 subjects enrolled."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/allocation.html#early-stopping-only",
    "href": "documentation/v72/userguides/core/design/allocation.html#early-stopping-only",
    "title": "Allocation",
    "section": "Early Stopping Only",
    "text": "Early Stopping Only\nIf allocation is to be fixed the “Early Stopping Only” option should be selected. Then on this tab the fixed allocation ratios and block size are specified – for the first cohort and then for all subsequent cohorts. In each case the user specifies the number of each arm to allocate in each cohort and the sum must equal the size of the cohort.\n\n\n\n\n\n\nFigure 7: Cohort allocation tab with early stopping, but no adaptive allocation."
  },
  {
    "objectID": "documentation/v72/userguides/core/design/allocation.html#adaptive-allocation-1",
    "href": "documentation/v72/userguides/core/design/allocation.html#adaptive-allocation-1",
    "title": "Allocation",
    "section": "Adaptive Allocation",
    "text": "Adaptive Allocation\nIf allocation is to be adaptive the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort by sampling from allocation vector” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation ratios for the remaining subjects in the cohort as for adaptive allocation (see Adaptive Allocation above).\n\n\n\n\n\n\n\n\nFigure 8: Cohort allocation tab adaptive allocation.\n\n\n\n\nAdaptively allocate to best dose\nIf allocation is to be adaptive, but at each cohort just Control and the “best dose” are to be allocated to, the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort to single treatment (plus control)” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation weighting to calculate for the non-Control arms, (see Adaptive Allocation above) and all the subjects not allocated to control will be allocated to this arm.\n\n\n\n\n\n\n\n\nFigure 9: Cohort allocation tab with adaptive allocation allocating to only the best dose."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/tte.html",
    "href": "documentation/v72/userguides/core/study/tte.html",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include whether interim analyses will be simulated, if a predictor is used to impute subject event times, sample size, maximum number of events, follow-up times, and more.\n\n\n\n\n\n\nFigure 1: The study tab for a time-to-event trial.\n\n\n\n\n\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether an early predictor is to be included in the design. An early predictor can be a continuous or dichotomous outcome, or a precursor event. The user specifies whether higher score, a response, or precursor event is a positive or negative outcome for the subject (this is used in the comparison with the CSD – not in the relation between the predictor and the final event, that is solely down to the parameters entered on the virtual subject response tabs). For continuous or dichotomous predictors, the user specifies after what time (weeks after randomization) the predictor is observed. For all predictors a clinically significant difference can be specified which can be used to specify early stopping or final evaluation conditions on the predictor. If the predictor is continuous, the value is delta on the mean difference between a study arm and control, if dichotomous, the value is the difference in rate and if the predictor is a precursor event then the value is a clinically significant hazard ratio.\nIf an early predictor is included, then a ‘Predictor’ tab is added to the Virtual Subject Response tabs, and a Predictor tab is added to the Design tabs, with a predictor Dose Response sub-tab and Relationship to Endpoint sub-tab.\n\n\n\n\n\n\nThe maximum number of subjects that can be recruited into the study and the maximum number of events to be observed (if there is a time-to-event predictor, the maximum events to be observed can be specified to be the predictor events).\n\n\n\nDetermine what counts as an event when determining when the trial should be stopped due to reaching the number of events specified in the “Maximum number of events” box. The options are “Final events” and “Predictor events.”\n\n\n\nThe maximum number of events (type of event specified above) allowed in the trial. When this value is reached the trial moves on to the final analysis stage.\n\n\n\nThe maximum follow-up time per subject. No subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event. With this option all subjects have the same maximum follow-up.\n\n\n\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event. With this option subjects recruited earlier may be follow-up for longer than subjects recruited later in the trial and the overall amount of exposure is maximized.\n\n\n\nWhether an event indicates a success (time to recovery) or a failure (death or progression) for a subject. The value specified here also dictates the direction of frequentist tests and predictive probabilities.\n\n\n\nDictate whether subjects are recruited sequentially or deterministically.\nIf recruited sequentially, the user recruitment will be simulated stochastically using a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/tte.html#design-options",
    "href": "documentation/v72/userguides/core/study/tte.html#design-options",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Whether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether an early predictor is to be included in the design. An early predictor can be a continuous or dichotomous outcome, or a precursor event. The user specifies whether higher score, a response, or precursor event is a positive or negative outcome for the subject (this is used in the comparison with the CSD – not in the relation between the predictor and the final event, that is solely down to the parameters entered on the virtual subject response tabs). For continuous or dichotomous predictors, the user specifies after what time (weeks after randomization) the predictor is observed. For all predictors a clinically significant difference can be specified which can be used to specify early stopping or final evaluation conditions on the predictor. If the predictor is continuous, the value is delta on the mean difference between a study arm and control, if dichotomous, the value is the difference in rate and if the predictor is a precursor event then the value is a clinically significant hazard ratio.\nIf an early predictor is included, then a ‘Predictor’ tab is added to the Virtual Subject Response tabs, and a Predictor tab is added to the Design tabs, with a predictor Dose Response sub-tab and Relationship to Endpoint sub-tab."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/tte.html#study-information",
    "href": "documentation/v72/userguides/core/study/tte.html#study-information",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The maximum number of subjects that can be recruited into the study and the maximum number of events to be observed (if there is a time-to-event predictor, the maximum events to be observed can be specified to be the predictor events).\n\n\n\nDetermine what counts as an event when determining when the trial should be stopped due to reaching the number of events specified in the “Maximum number of events” box. The options are “Final events” and “Predictor events.”\n\n\n\nThe maximum number of events (type of event specified above) allowed in the trial. When this value is reached the trial moves on to the final analysis stage.\n\n\n\nThe maximum follow-up time per subject. No subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event. With this option all subjects have the same maximum follow-up.\n\n\n\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event. With this option subjects recruited earlier may be follow-up for longer than subjects recruited later in the trial and the overall amount of exposure is maximized.\n\n\n\nWhether an event indicates a success (time to recovery) or a failure (death or progression) for a subject. The value specified here also dictates the direction of frequentist tests and predictive probabilities.\n\n\n\nDictate whether subjects are recruited sequentially or deterministically.\nIf recruited sequentially, the user recruitment will be simulated stochastically using a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/dichotomous.html",
    "href": "documentation/v72/userguides/core/study/dichotomous.html",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include accrual style, visit schedule, whether interim analyses will be simulated, and more.\n\n\n\n\n\n\nFigure 1: The study tab for a dichotomous trial.\n\n\n\n\n\nIn the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nIf “Use Longitudinal Modeling” is selected, then special longitudinal options can be turned on.\n\n\nIf this option is selected, then the endpoint is actually simulated as a dichotomized continuous response – e.g. a “responder analysis” where a subject with a change from baseline above (or below) a certain threshold is a ‘responder’ and otherwise is not.\nWhen using a dichotomized response, the longitudinal analysis is on the underlying continuous response rather than the dichotomous responder endpoint. After imputing subject values based on intermediate subject data they are dichotomized.\n\n\n\nWhen using the restricted Markov model a subject can be in one of 3 states “Success”, “Failure” or “Stable.” Once the subject has become a “Success” or “Failure” they are  fixed in that state Success and Failure are called “absorbing states. , and it will be their final outcome.\nAll subjects are initially in the “Stable” state and may or may not transition to another state during follow-up. If a subject finishes follow-up in the “Stable” state, they must be classified as failure or success.\n\n\n\n\n\nThe study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuously, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a response indicates the subject improving or worsening (and thus whether a higher or lower response rate is a good thing). The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/dichotomous.html#design-options",
    "href": "documentation/v72/userguides/core/study/dichotomous.html#design-options",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "In the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nIf “Use Longitudinal Modeling” is selected, then special longitudinal options can be turned on.\n\n\nIf this option is selected, then the endpoint is actually simulated as a dichotomized continuous response – e.g. a “responder analysis” where a subject with a change from baseline above (or below) a certain threshold is a ‘responder’ and otherwise is not.\nWhen using a dichotomized response, the longitudinal analysis is on the underlying continuous response rather than the dichotomous responder endpoint. After imputing subject values based on intermediate subject data they are dichotomized.\n\n\n\nWhen using the restricted Markov model a subject can be in one of 3 states “Success”, “Failure” or “Stable.” Once the subject has become a “Success” or “Failure” they are  fixed in that state Success and Failure are called “absorbing states. , and it will be their final outcome.\nAll subjects are initially in the “Stable” state and may or may not transition to another state during follow-up. If a subject finishes follow-up in the “Stable” state, they must be classified as failure or success."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/dichotomous.html#study-information",
    "href": "documentation/v72/userguides/core/study/dichotomous.html#study-information",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "The study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuously, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a response indicates the subject improving or worsening (and thus whether a higher or lower response rate is a good thing). The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/dichotomous.html#d-treatment-arm-model",
    "href": "documentation/v72/userguides/core/study/dichotomous.html#d-treatment-arm-model",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens.\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/continuous.html",
    "href": "documentation/v72/userguides/core/study/continuous.html",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying rules and methods for common clinical trial simulation features. These include accrual style, visit schedule, whether interim analyses will be simulated, and more.\n\n\n\n\n\n\nFigure 1: The study tab for a continuous trial.\n\n\n\n\n\nIn the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nWhether to include simulation of subject’s baseline score, and if so, whether the response to be modeled is change from baseline or final endpoint value.\n\n\n\n\nThe study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a larger endpoint value indicates subjects improvement or if a lower endpoint value is better. The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and with the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/continuous.html#design-options",
    "href": "documentation/v72/userguides/core/study/continuous.html#design-options",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "In the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\n\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\n\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\n\nWhether to include simulation of subject’s baseline score, and if so, whether the response to be modeled is change from baseline or final endpoint value."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/continuous.html#study-information",
    "href": "documentation/v72/userguides/core/study/continuous.html#study-information",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\n\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\n\nIndicate whether a larger endpoint value indicates subjects improvement or if a lower endpoint value is better. The value provided here informs the  direction of frequentist hypothesis tests All FACTS frequentist tests are 1-sided.  and with the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\n\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule."
  },
  {
    "objectID": "documentation/v72/userguides/core/study/continuous.html#d-treatment-arm-model",
    "href": "documentation/v72/userguides/core/study/continuous.html#d-treatment-arm-model",
    "title": "Study Tab - Continuous Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens[^3].\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/index.html",
    "href": "documentation/v72/userguides/core/longitudinalmodels/index.html",
    "title": "Longitudinal Models",
    "section": "",
    "text": "The time-to-event endpoints also allow for using an early predictor endpoint to help in the estimation of the primary endpoint. The time-to-event predictor model works differently than the continuous or dichotomous longitudinal models. See here for a description of the time-to-event predictor models."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/index.html#how-many-longitudinal-models",
    "href": "documentation/v72/userguides/core/longitudinalmodels/index.html#how-many-longitudinal-models",
    "title": "Longitudinal Models",
    "section": "How many longitudinal models?",
    "text": "How many longitudinal models?\nWhen specifying a longitudinal model in FACTS, the user must clarify whether the longitudinal model is shared across all doses, or if different arms are to estimate different functions connecting early endpoints to the final endpoint.\nThe options that may be selected for the number of model instances are:\n\n“Single model for all arms” – just one set of longitudinal parameters is estimated using all the subject data regardless of their treatment.\n“Model control separately” – two sets of longitudinal parameters are estimated, one using data from just those subjects on the control arm, and the other using all the subject data from all the other arms.\n“Model comparator separately” (only available if there is an active comparator arm) - two sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, and the other using all the subject data from all the other arms (control and active treatments).\n“Model control and comparator separately” (only available if there is an active comparator arm) - three sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, one using data from just those subjects on the control arm, and the third using all the subject data from all the other arms.\n“Model all arms separately” – a set of longitudinal parameters is independently estimated separately for each arm, each set estimated just using the data available from the subjects on that arm.\n\nThe fewer models used, the more data is pooled, the more precisely the model parameters can be estimated, and the more informative the intermediate data can be. Pooling data assumes that subjects in different groups have the same longitudinal profile, although they may still have different magnitude or probability of response (estimated by Dose Response Model).\nIf the response profile could be different on different treatment arms – for example, if subjects on control recover constantly over time, but those on an effective dose of the study drug recover very quickly early on, and more gradually later in their follow-up - a longitudinal model based on pooled data would tend to over-estimate the final endpoint of subjects on an effective dose of the study drug when only their early data was available. In a case like that, it would be better to use separate models. If the rapid early response was likely on all study drug treatment arms, then possibly use “model control separately.” If, however, the rapid response will only be seen on some study drug arms then use “model all arms separately”.\nIn addition to declaring how many longitudinal models should be estimated, when there is more than one model instance the user must declare if the prior distributions should be specified differently across each model instance or not. The options that may be selected for prior specification (for all LMs but linear regression) are:\n\nSame priors across all model instances\n\n\nEach instance of the model has identical prior distributions on its parameters. Estimation is still performed independently on each model instance.\n\n\nSpecify priors per model instance\n\n\nEach instance of the model has its own priors that may vary across instances.\n\nThe linear regression longitudinal model has a different parameter set estimated for each visit, so its prior specification rules are slightly different. See the linear regression section below for specifics about prior specification specific to this model."
  },
  {
    "objectID": "documentation/v72/userguides/core/longitudinalmodels/continuous.html",
    "href": "documentation/v72/userguides/core/longitudinalmodels/continuous.html",
    "title": "Longitudinal Models for Continuous Endpoints",
    "section": "",
    "text": "LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {\\(y_{it}\\)} is the set of observed responses from early visits, and \\(y_{i t_m}\\) is the last observed value of \\(y_{i t}\\), then the LOCF model for the final endpoint \\(Y_i\\) is\n\\[Y_i\\mid \\{y_{it}\\} = y_{it_m}\\]\nIn the continuous engine \\(t_m\\) can be any earlier observed visit including the baseline value.\n\n\nLinear Regression\n\n\n\n\n\n\nShiny App\n\n\n\nThe following shiny application for a tool that helps visualize and set priors for the linear regression longitudinal model.\nSee here.\n\n\nThe linear regression model fits a simple linear model from the data at each visit with the final visit\n\\[Y_i \\mid y_{it} \\sim \\alpha_t + \\beta_t y_{it} + \\text{N}(0,\\lambda_t^2)\\]\nThe parameter \\(\\alpha_t\\) is the intercept of the model for visit t, and the parameter \\(\\beta_t\\) is a multiplicative modifier (slope) of the response observed longitudinal at visit \\(t\\) to adjust the prediction of the final endpoint.\nImputation of the final endpoint value for a subject using the linear regression longitudinal model is based on only the latest observed visit’s endpoint value.\nThe default setting of “Same priors across all model instances and visits,” implies that each parameter \\(\\alpha\\), β, and λ have the same prior for all t. Estimation of those parameters is still done independently for each model instance. The one prior across all model instance are formulated as:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_\\mu, \\alpha_\\sigma^2)\\] \\[\\beta_t \\sim \\text{N}(\\beta_\\mu, \\beta_\\sigma^2)\\] \\[\\lambda_{t}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nThe above prior formulation may not be desirable if specifying priors that are not extremely diffuse – especially on the \\(\\beta\\) parameters. Instead, selecting “Specify priors per visit across all model instances,” will share the prior specification across all instances of the model, but allows for different priors to be put on the parameters associated with each visit. The user inputted prior parameters are now subscripted with \\(t\\) to denote the visit they correspond to. These priors apply to all model instances:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_{\\mu_t}, \\alpha_{\\sigma_t}^2)\\] \\[\\beta_t \\sim \\text{N}(\\beta_{\\mu_t}, \\beta_{\\sigma_t}^2)\\] \\[\\lambda_{t}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n_t}}{2},\\frac{\\lambda_{\\mu_t}^{2}\\lambda_{n_t}}{2} \\right)\\]\nIt is also possible to specify priors “Per model instance and visit,” in which every visit has separate priors, and those differing priors vary across model instances. This is the most flexible prior specification method. The user inputted prior parameters are now subscripted by both t for visit and i for model instance.\n\\[\\alpha_{ti} \\sim \\text{N}(\\alpha_{\\mu_{ti}}, \\alpha_{\\sigma_{ti}}^2)\\] \\[\\beta_{ti} \\sim \\text{N}(\\beta_{\\mu_{ti}}, \\beta_{\\sigma_{ti}}^2)\\] \\[\\lambda_{ti}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n_{ti}}}{2},\\frac{\\lambda_{\\mu_{ti}}^{2}\\lambda_{n_{ti}}}{2} \\right)\\]\nA potential starting place for non-informative prior values would be\n\n\\(\\alpha\\)\n\nmean of 0, SD \\(\\ge\\) largest expected response\n\n\\(\\beta\\)\n\nmean of either 0 or \\(\\frac{\\text{final visit time}}{\\text{early visit time}}\\), SD \\(\\ge\\) largest expected ratio of final visit to first visit\n\n\\(\\lambda\\)\n\nmean of expected SD of the endpoint (‘sigma’), weight of 1. The variability of the prediction from the longitudinal model (based on an observed intermediate response) should be less than that based solely on the treatment allocation, thus this is a weakly pessimistic prior on the effectiveness of the longitudinal model that should be quickly overwhelmed by the data.\n\n\nThis model is easy to understand and can be used even if there is only one visit, but doesn’t get more powerful if there are more visits. Its power will depend on the degree of correlation between the intermediate visit response and the final response.\n\n\nTime Course Hierarchical\nThe Time Course Hierarchical models the relationship between subjects’ early responses and their final response. It incorporates a per-subject offset from the mean response and a scaling factor from each visit to the final endpoint, but with no model of the change from visit-to-visit.\nThe response at the \\(t_{th}\\) visit for the \\(i^{th}\\) subject, having been randomized to the \\(d^{th}\\) dose is modeled as:\n\\[y_{it} \\sim e^{\\alpha_t}(\\theta_d + \\delta_i) + \\text{N}(0, \\lambda_t^2)\\]\nThe imputed final response (visit \\(T\\)$) for the \\(i^{th}\\) subject, having been randomized to the \\(d^{th}\\) dose is modeled as:\n\\[Y_{iT} \\sim \\theta_d + \\delta_i + \\text{N}(0, \\lambda_T^2)\\]\n(i.e. \\(\\alpha_T\\) is 0).\nThe model parameters can be interpreted as follows:\n\n\\(\\theta_d\\)\n\nthe estimated mean response at the final visit in dose \\(d\\) from the dose response model.\n\n\\(\\delta_i\\)\n\nthe estimated patient level random effect around the mean final response (\\(\\theta_d\\)) for the dose \\(d\\) that patient \\(i\\) is randomized to.\n\n\\(\\alpha_t\\)\n\na scaling parameter that determines the proportion of the final response that is observable at visit \\(t\\). A value of \\(\\alpha_t=0\\) indicates that the expected value of early visit \\(t\\) is equal to the estimated final visit mean \\(\\theta_d\\). A value of \\(\\alpha_t= −0.69315\\) indicates that the expected value of early visit \\(t\\) is 50% of the estimated final visit mean \\(\\theta_d\\).\n\n\\(\\lambda_t^2\\)\n\nthe variance of the endpoint around the estimated mean response at visit \\(t\\).\n\n\nThe prior for \\(\\alpha_t\\) is a normal distribution with a user specified the mean and standard deviation:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_\\mu, \\alpha_\\sigma^2)\\]\nThe prior for the \\(\\delta_i\\) terms is a normal distribution with a mean of 0 and variance τ2.\n\\[\\delta_i \\sim \\text{N}(0, \\tau^2)\\]\n\\(\\tau^2\\) is estimated as part of the longitudinal model, and has an inverse gamma prior distribution with prior central value \\(\\tau_\\mu\\) and weight (in terms of “equivalent number of observations”) \\(\\tau_n\\):\n\\[\\tau^{2} \\sim \\text{IG}\\left( \\frac{\\tau_{n}}{2},\\\\\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nThe prior for the \\(\\lambda_t^2\\) terms is an inverse gamma distribution with prior central value \\(\\lambda_\\mu\\) and weight (in terms of “equivalent number of observations”) \\(\\lambda_n\\):\n\\[\\lambda_{t}^{2}\\sim\\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be\n\n\\(\\alpha_t\\)\n\nmean of -2, SD of 2, … so the prior ~70% interval for \\(\\alpha_t\\) is between -4 and 0 (+/1 1 SD) and thus the prior 70% interval for \\(e^{\\alpha_t}\\) to be between 0.02 and 1.\n\n\\(\\tau\\)\n\nmean set to the expected SD of the endpoint (‘sigma’), with a weight of 1.\n\n\\(\\lambda_t\\)\n\nmean set to the expected SD of the endpoint (‘sigma’), with weight of 1.\n\n\nWe would expect \\(\\tau^2 + \\lambda^2 \\approx \\sigma^2\\), thus to specify a prior mean of \\(\\sigma\\) for each with a weight of 1 is a weakly pessimistic prior that should be quickly overwhelmed by the data.\nThis model is useful if there is thought to be a significant per-subject component to the response that should be manifest at the intermediate visits, and sufficient visits for the per-subject component to be estimated.\n\n\nKernel Density\nThe Kernel Density Model longitudinal model is a non-parametric re-sampling approach that is ideal for circumstances where the relationship between the interim time and the final endpoint is not known or not canonical.\nThe procedure is as follows. Assume an interim value for patient \\(i\\) at time \\(t\\), \\(Y_{it}\\). Patient \\(i\\) does not have an observed final endpoint at time \\(T\\), so one is to be imputed. Let \\((X_{1t},X_{1T}), \\ldots, (X_{nt}, X_{nT})\\) be the set of values for the previous subjects for whom there exists an interim value \\(X_{*t}\\) and final value \\(X_{*T}\\).\nTo impute a value of \\(Y_{iT}\\) given \\(Y_{it}\\), a pair \\((X_{kt},X_{kT})\\) is selected with probability based on the pair’s time \\(t\\) visit response’s proximity to the observed \\(Y_{it}\\):\n\\[\\Pr\\left(\\text{Selecting}\\left( X_{kt},\\\\X_{kT} \\right) \\right) = \\frac{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}{\\sum_{k = 1}^{n}{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}}\\]\nThen, a value of \\(Y_{iT}\\) is imputed from the following distribution, which uses the selected pair’s final endpoint response \\(X_{kT}\\):\n\\[Y_{iT} \\sim \\text{N}(X_{kT}, h_{X_T}^2)\\]\nThe bandwidths \\(h_{X_t}\\) and \\(h_{X_T}\\) are selected based on the criterion given by Scott (1992). That is,\n\\[h_{X_{j}} = \\sigma_{X_{j}} \\left( 1 - \\rho^{2} \\right)^{\\frac{5}{12}} \\left( 1 + \\frac{\\rho^{2}}{2} \\right)^{- \\frac{1}{6}}{\\\\n}^{- \\frac{1}{6}}\\text{   for } j = t \\text{ and } T\\]\nwhere \\(\\sigma_{X_j}\\) is the standard deviation of the observed responses at time \\(j\\), \\(n\\) is the number of pairs \\((X_{*t},X_{*T})\\) that were chosen between, and \\(\\rho\\) is the correlation coefficient between \\(X_t\\) and \\(X_T\\) in the pairs \\((X_{1t},X_{1T}), \\ldots, (X_{nt}, X_{nT})\\).\nThe Kernel Density model does not take prior distributions as input. So long as each early visit has more subjects with an observed early response and final response than the value entered in “Minimum number of participants with an early visit and final visit needed to estimate kernel bandwidths for that early visit:” then this algorithm runs without regard for user input.\nIf any visit has fewer subjects with early data and final data than the specified minimum number of participants, then instead of calculating the values of \\(h_{X_t}\\) or \\(h_{X_T}\\) the input values of “Fixed kernel bandwidth \\(h_x\\):” and “Fixed kernel bandwidth \\(h_y\\):” are used.\nFor \\(h_x\\) and \\(h_y\\), possible starting values are the expected SD of the endpoint (‘sigma’). The default value for the minimum number of subjects with complete early and final visits is 6, but this value can be set to anything greater than 0 that the user desires.\nThe Kernel Density model is effective and flexible with no model assumptions, but its computational overhead is large. Simulations may take \\(\\sim 10\\) times longer to run, and having no prior model there has to be ‘in trial’ data before it can come into play.\n\n\nITP\nThe ITP (Integrated Two-component Prediction) model fits an observation for patient \\(i\\) on dose \\(d\\) at visit \\(t\\) as:\n\\[y_{idt} = \\left( \\theta_{d} + s_{id} + \\epsilon_{idt} \\right)\\left( \\frac{1 - \\text{exp}\\left( kx_{idt} \\right)}{1 - \\text{exp}(kX)} \\right)\\]\nwhere \\[\\epsilon_{idt} \\sim \\text{N}(0, \\lambda^2)\\] \\[s_{id} \\sim \\text{N}(0, \\tau^2)\\]\nand \\(\\theta_d\\) is the mean estimate of the final endpoint for dose d using all complete and partial data and assuming an independent dose response model on the doses. \\(s_{id}\\) is a subject specific random effect, \\(k\\) is a shape parameter, \\(x_{idt}\\) is the time \\(y_{idt}\\) is observed, \\(X\\) is the time to final endpoint, and each \\(\\epsilon_{idt}\\) is a residual error.\nThe ITP model is similar to the Time Course Hierarchical above in that both models estimate subject specific component of the response. The biggest difference between the two is that in the ITP models the response changes over time as a parametric function based on the parameter \\(k\\), rather than having a separately estimated \\(e^{\\alpha_t}\\) for each visit.\nThe shape parameter \\(k\\) determines the rate at which the final endpoint’s eventual effect is observed during a subject’s follow-up. A value of \\(k=0\\) indicates that the proportion of effect observed moves linearly with time. A value of \\(k&lt;0\\) means that the eventual final effect is observed earlier in follow-up and plateaus off as time moves towards the final endpoint. A value of \\(k&gt;0\\) indicates that less of the total final endpoint effect is observed early in follow up, but as time approaches the final endpoint time the proportion of the effect observed increases rapidly. Values of \\(k\\) less than 0 tend to be more common than values of \\(k\\) greater than 0. See the figure below for a visualization of the different response-over-time curves that can be estimated with the ITP model.\n\n\n\n\n\n\nFigure 1: Examples of a variety of ITP models with the shape parameter k ranging from -1 to 1.\n\n\n\nThe priors for the parameters in the ITP model are: \\[k \\sim \\text{N}(\\mu_k, \\sigma_k^2)\\] \\[\\theta_d \\sim \\text{N}(\\mu_{\\theta_d}, \\sigma_{\\theta_d}^2)\\] \\[\\tau^2 \\sim \\text{IG}\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\] \\[\\lambda^2 \\sim \\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be:\n\n\\(\\theta_d\\)\n\nmean of the expected mean improvement from baseline on the Control arm, with SD greater than or equal to the expected treatment effect size.\n\n\\(k\\)\n\na mean of 0 and SD of 2, if subject improvement is expected to be rapid and then diminishing, the prior mean might be -0.5 or -1, if subject improvement is expected to be initially slow or non-existent then increasing, the prior mean might be 0.5 or 1.\n\n\\(\\tau\\)\n\nmean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\n\n\\(\\lambda\\)\n\nmean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\n\n\nThe ITP model implies that the variance of the observations shrinks towards 0 with the mean (so early visits have reduced expected responses and variances). The ITP model may result in biased estimates of \\(\\theta_d\\) and/or the variance terms \\(\\tau^2\\) and \\(\\lambda^2\\) if the mean-variance relationship assumed by the ITP model is not present in the observed data. If the model assumptions are correct this can be a very effective longitudinal model."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/multendpt.html",
    "href": "documentation/v72/userguides/core/vsr/multendpt.html",
    "title": "Virtual Subject Response - Multiple Endpoint",
    "section": "",
    "text": "When simulating multiple endpoints, each endpoint will have its own set of explicitly defined VSR tabs for specifying their response profiles. See the continuous or dichotomous VSR pages for descriptions on these tabs.\n\n\nIn addition to the standard VSR tabs that all endpoints have, the multiple endpoint engine also has a Composite VSR tab that allows for the creation of a set of VSR scenarios that are made up of one response VSR for each endpoint and one longitudinal VSR for each endpoint that uses a longitudinal model.\nThe number of different profiles that need to be combined to fully define the responses to be simulated in FACTS Core Multiple Endpoint grows rapidly with the number of endpoints, and the number of different combinations quickly becomes far greater than in the single endpoint design engines. The Composite Response tab has been added to the FACTS GUI to help the user manage which combinations of profiles they would like to include in the simulations.\n\n\n\n\n\n\nFigure 1: The Composite Response tab for a multiple endpoint design with 2 endpoints.\n\n\n\nThere essentially are two approaches available to the user to specify the combinations of response profiles:\n\nClicking on the ‘Generate’ button to generate all possible combinations and then deleting the combinations that are not of interest\nBy adding composite profiles and manually selecting the individual response profiles that go together to comprise the composite profile.\n\nThe tab is divided into three areas:\n\nThe list of profiles on the left hand side, with controls at the top to add another profile or delete the currently selected profile, and controls at the bottom to delete the current profiles generate all possible profiles, or simple delete all current profile. The properties of the currently highlighted profile are shown on the right of the screen.\nOn the right, for each of the endpoints there are controls to show which baseline, dose response and longitudinal profile for that endpoint are used in the currently selected composite profile. The profile names are shown in ‘drop down’ lists that the user can use to show the list of all the profiles of that type defined for that endpoint and to select an alternative for the currently selected composite profile.\nOn the right below the controls, a table of the treatment arms with a check box per arm is displayed along with a graph that shows the individual and combined utilities of each endpoint for the currently selected dose response profiles. The check boxes allow the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were both successful and where a ‘good’ treatment arm selected. The arm section uses the target QOI specified on the Variants tab (whether or not Variants are being used)."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/multendpt.html#composite-response",
    "href": "documentation/v72/userguides/core/vsr/multendpt.html#composite-response",
    "title": "Virtual Subject Response - Multiple Endpoint",
    "section": "",
    "text": "In addition to the standard VSR tabs that all endpoints have, the multiple endpoint engine also has a Composite VSR tab that allows for the creation of a set of VSR scenarios that are made up of one response VSR for each endpoint and one longitudinal VSR for each endpoint that uses a longitudinal model.\nThe number of different profiles that need to be combined to fully define the responses to be simulated in FACTS Core Multiple Endpoint grows rapidly with the number of endpoints, and the number of different combinations quickly becomes far greater than in the single endpoint design engines. The Composite Response tab has been added to the FACTS GUI to help the user manage which combinations of profiles they would like to include in the simulations.\n\n\n\n\n\n\nFigure 1: The Composite Response tab for a multiple endpoint design with 2 endpoints.\n\n\n\nThere essentially are two approaches available to the user to specify the combinations of response profiles:\n\nClicking on the ‘Generate’ button to generate all possible combinations and then deleting the combinations that are not of interest\nBy adding composite profiles and manually selecting the individual response profiles that go together to comprise the composite profile.\n\nThe tab is divided into three areas:\n\nThe list of profiles on the left hand side, with controls at the top to add another profile or delete the currently selected profile, and controls at the bottom to delete the current profiles generate all possible profiles, or simple delete all current profile. The properties of the currently highlighted profile are shown on the right of the screen.\nOn the right, for each of the endpoints there are controls to show which baseline, dose response and longitudinal profile for that endpoint are used in the currently selected composite profile. The profile names are shown in ‘drop down’ lists that the user can use to show the list of all the profiles of that type defined for that endpoint and to select an alternative for the currently selected composite profile.\nOn the right below the controls, a table of the treatment arms with a check box per arm is displayed along with a graph that shows the individual and combined utilities of each endpoint for the currently selected dose response profiles. The check boxes allow the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were both successful and where a ‘good’ treatment arm selected. The arm section uses the target QOI specified on the Variants tab (whether or not Variants are being used)."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/multendpt.html#required-format-of-externally-simulated-file",
    "href": "documentation/v72/userguides/core/vsr/multendpt.html#required-format-of-externally-simulated-file",
    "title": "Virtual Subject Response - Multiple Endpoint",
    "section": "Required Format of Externally Simulated File",
    "text": "Required Format of Externally Simulated File\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nEndpoint Index (1 = primary, 2 = first auxiliary, …)\nVisit Index (1 = first visit, 2 = second visit, …), if baseline is included, then baseline is visit 0.\nResponse\n\nContinuous:\n\nIf no baseline is included, then the response provided must be change from baseline\nIf baseline is included, then the response provided must be the absolute response (and the design engine will compute change from baseline or analyze the absolute response as specified by the user on the Study tab).\n\nDichotomous: 0 for no response, 1 for response, -1 for stable (restricted Markov)\n\n\nSubjects need to have unique, positive integer IDs, all records for each subject should be contiguous and all records for a particular endpoint for each subject should be contiguous.\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file with a continuous primary endpoint and dichotomous secondary endpoint.\n#subj id, Arm ID, Endpoint ID, Visit ID, Response\n1,1,1,1,-0.4233\n1,1,1,2,-0.8466\n1,1,1,3,-1.6933\n1,1,1,4,-2.1166\n1,1,2,1,0\n1,1,2,2,0\n1,1,2,3,0\n1,1,2,4,0\n2,1,1,1,0.4710\n2,1,1,2,0.9421\n2,1,1,3,1.8843\n2,1,1,4,2.3554\n2,1,2,1,0\n2,1,2,2,1\n2,1,2,3,0\n2,1,2,4,1"
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/dichotomous.html",
    "href": "documentation/v72/userguides/core/vsr/dichotomous.html",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "",
    "text": "In FACTS Core with a dichotomous endpoint there are 4 different ways to specify the virtual subject response:"
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/dichotomous.html#dose-response",
    "href": "documentation/v72/userguides/core/vsr/dichotomous.html#dose-response",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Dose Response",
    "text": "Dose Response\n\n\n\n\n\n\nFigure 1: The Explicitly Defined &gt; Dose Response model for a dichotomous endpoint.\n\n\n\nDose response profiles can be added and deleted, and for each profile the user specifies:\n\nThe response rate for each treatment arm.\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\n\nThe graph on the tab shows the mean response rate specified and the target.\nIf a 2D treatment arm model is being used, the doses are listed in “effective dose strength order” as was defined on the treatment arm tab.\n\nLoad Scenario Rates From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses. Each individual simulation uses one set of response rates from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from one ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 2: The Explicitly Defined &gt; Dose Response tab when loading scenario means from a file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual response rates and the mean response rate over all the VSRs.\nThere is a check box per dose that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\nThe format of the “.mvsr” file is a simple CSV text file. Lines starting with a ‘#’ character are ignored, so the file can include comment and header lines. There must be one column per treatment arm, and they must be in dose index order. Each value is the underlying response rate to simulate. E.g.:\n#Cntrl, D1, D2, D3, D4\n0.05, 0.1, 0.15, 0.25, 0.5\n0.05, 0.1, 0.15, 0.23, 0.45\n0.05, 0.1, 0.15, 0.21, 0.4\n0.05, 0.1, 0.15, 0.19, 0.35\n0.05, 0.1, 0.15, 0.17, 0.3\n0.05, 0.1, 0.15, 0.15, 0.25"
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/dichotomous.html#longitudinal",
    "href": "documentation/v72/userguides/core/vsr/dichotomous.html#longitudinal",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Longitudinal",
    "text": "Longitudinal\nThe tab for defining dichotomous longitudinal response profiles allows the user to specify the overall transition probabilities between responder and non-responder.\nIf no special longitudinal options are selected on the Study Info page, then this default simulation method that uses transition probabilities to simulate sequential dichotomous endpoints is used.\nThe transition probabilities method for generating longitudinal responses simulates the response observed at each visit by using the probability that a subject becomes or remains a ‘1’ from one visit to the next – and all subjects start with a response of 0.\n\n\n\n\n\n\nFigure 3: The dichotomous endpoint Longitudinal VSR tab.\n\n\n\nThe user specifies for each visit,\n\nthe probability of a subject whose response was a ‘0’ at the previous visit having a response of ‘1’ at this visit\nthe probability of a subject whose response was a ‘1’ at the previous visit having a response of ‘1’ at this visit\n\nHowever, these probabilities imply a particular probability that a subject has a response of ‘1’ at the final visit, so they need to be modified for each arm in each dose response profile to give the desired final probability of response. This is done by numerically determining for each final response rate, a single value which when added to all the specified transition probabilities in the log-odds space yield the desired probability of final response.\nLet \\(Q_{td}\\) be the probability of transitioning from 0 to 1, and \\(R_{td}\\) be the probability of transitioning from 1 to 1 for each visit (t) and dose (d).\nFor the 1st visit:\n\\[P\\left( y_{1d} = 1 \\right) = Q_{1d}\\]\nwhich is like considering the imaginary 0th visit to have been a non-response. For subsequent visits:\n\\[P\\left( y_{td} = 1 \\middle| y_{t - 1,d} = 0 \\right) = Q_{td}\\]\n\\[P\\left( y_{td} = 1 \\middle| y_{t - 1,d} = 1 \\right) = R_{td}\\]\nThe dose response (\\(P_{d}\\)) is first specified in the VSR &gt; Explicitly Defined &gt; Dose Response tab, and then longitudinal components (\\(q_{t}\\) and \\(r_{t}\\)) are specified separately. FACTS then makes an adjustment to the longitudinal components \\(q_{t}\\) and \\(r_{t}\\) to calculate \\(Q_{td}\\) and \\(R_{td}\\) for each dose while maintaining the value for \\(P_{d}\\) specified in the Dose Response tab.\nThe matrices Q and R are calculated by applying offsets \\(f_{d}\\) in log odds space to the longitudinal values:\n\\[Q_{t,d} = \\frac{e^{q'_{td}}}{1 + e^{q'_{td}}}, \\ \\ q'_{td} = \\ln\\left( \\frac{q_{t}}{1 + q_{t}} \\right) + f_{d}\\]\n\\[R_{t,d} = \\frac{e^{r'_{td}}}{1 + e^{r'_{td}}}, \\ \\ r'_{td} = \\ln\\left( \\frac{r_{t}}{1 + r_{t}} \\right) + f_{d}\\]\nwhere \\(f_{d}\\) is calculated iteratively for each dose to ensure that Q and R give the correct final probability of response for each dose. As a result,\n\\[P_{d} = x_{Td}\\]\nwhere\n\\[x_{1d} = Q_{1d}\\]\nand\n\\[x_{t} = x_{t - 1}R_{td} + \\left( 1 - x_{t - 1} \\right)Q_{td}\\]\n\nExample\nWith 3 visits and probabilities of \\(0\\rightarrow 1\\) of \\(0.2\\) and of \\(1\\rightarrow 1\\) of \\(0.9\\) at each visit, the probability of a final response is 0.438. This 0.438 is fixed, and cannot be changed without changing the transition probabilities.\n\n\n\n\n\n\n\n\n\n\nIndex\nVisit\nProb 1 \\(\\rightarrow\\) 1\n(\\(r_t\\))\nProb 0 \\(\\rightarrow\\) 1\n(\\(q_t\\))\nCumulative Pr(Resp)\n\n\n\n\n1\nVisit 1\n\n\\(0.2\\)\n\\(0.2\\)\n\n\n2\nVisit 2\n\\(0.9\\)\n\\(0.2\\)\n\\(0.2*0.9\\) \\(+\\;(1-0.2)*0.2\\) \\(= 0.34\\)\n\n\n3\nVisit 3\n\\(0.9\\)\n\\(0.2\\)\n\\(0.34*0.9\\) \\(+\\;(1-0.34)*0.2\\) \\(= 0.438\\)\n\n\n\nIf a response profile calls for the probability of a final response to be simulated with a probability of 0.8, a fixed offset in log-odds is found which, when applied to all the transition probabilities, results in the desired final probability of a response. Replicating this by hand to an accuracy of 4 significant digits yields an offset of \\(f_d = 1.173\\), which gives:\n\\[\\text{logit}^{-1}(\\text{logit}(0.2) + 1.173) = 0.4469\\] \\[\\text{logit}^{-1}(\\text{logit}(0.9) + 1.173) = 0.9668\\]\nThen,\n\n\n\n\n\n\n\n\n\n\nIndex\nVisit\nProb 1 \\(\\rightarrow\\) 1\n(\\(r_t\\))\nProb 0 \\(\\rightarrow\\) 1\n(\\(q_t\\))\nCumulative Pr(Resp)\n\n\n\n\n1\nVisit 1\n\n\\(0.4469\\)\n\\(0.4469\\)\n\n\n2\nVisit 2\n\\(0.9668\\)\n\\(0.4469\\)\n\\(0.4469*0.9668\\) \\(+ (1-0.4469)*0.4469\\) \\(= 0.6792\\)\n\n\n3\nVisit 3\n\\(0.9668\\)\n\\(0.4469\\)\n\\(0.6792*0.9668\\) \\(+ (1-0.6792)*0.4469\\) \\(= 0.8000\\)"
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/continuous.html",
    "href": "documentation/v72/userguides/core/vsr/continuous.html",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "",
    "text": "In FACTS Core with a continuous endpoint there are 2 different ways to specify the virtual subject response:"
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/continuous.html#dose-response",
    "href": "documentation/v72/userguides/core/vsr/continuous.html#dose-response",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Dose Response",
    "text": "Dose Response\n\n\n\n\n\n\nFigure 1: The Explicitly Defined &gt; Dose Response model for a continuous endpoint.\n\n\n\nDose response profiles can be added and deleted, and for each profile the user specifies:\n\nThe mean response for each treatment arm. If baseline is being simulated the user can select (on the Study tab) whether the response to be analyzed is  the change from baseline or absolute response The mean response to be analyzed is always as specified on this tab, if the analysis is on change from baseline then the response specified here is change from baseline, if the response to be analyzed is absolute, then the response specified here is the absolute final score. , and depending on that selection the response specified on this tab is either change from baseline or absolute response.\nThe standard deviation of the response – either through a common SD of response for all treatment arms, or by specifying the standard deviation for the response on each treatment arm separately.\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario.FACTS uses this value to report on the proportion of simulations that were successful and selected a ‘good’ treatment arm. The check box values do not effect the simulation code, just how output is reported.\n\nThe graph on the Explicitly Defined &gt; Dose Response tab shows the mean response specified for each treatment arm +/- 1.96 SD, and the control response + the default  CSD Clinically significant difference  level specified on the QOI tab.\nIf a 2D treatment arm model is being used, the doses are listed in “effective dose strength order” as was defined on the treatment arm tab.\n\n\n\n\n\n\nFigure 2: The Explicitly Defined &gt; Dose Response tab when using a 2D treatment arm model.\n\n\n\n\nLoad Scenario Means From File\nIf the “Load scenario means from a file” option is selected, then in scenarios using this profile the simulations will use a range of dose responses instead of what is specified in the table.\nEach individual simulation uses one set of mean responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstance. Note that, to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 3: The Explicitly Defined &gt; Dose Response tab when loading scenario means from a file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual mean responses and the overall mean response over all the VSRs.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. There must be two columns per treatment arm giving the mean and SD of the change from baseline on each arm, the columns must be grouped first means then SDs and within each group they must be in dose index order. E.g.:\n#Cntrl, D1, D2, D3, Cntrl, D1, D2, D3\n-1, -1, -1, -1, 5, 5, 5, 5\n-1, -1.2, -1.4, -1.6, 5, 5, 5, 5\n-1, -1.8, -2.5, -2.5, 5, 5, 5, 5\n-1, -2, -2.5, -3, 5, 5, 5, 5\n-1, -2.5, -3.25, -3.25, 5, 5, 5, 5\n-1, -2.5, -3.25, -2.5, 5, 5, 5, 5"
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/continuous.html#longitudinal-vsr",
    "href": "documentation/v72/userguides/core/vsr/continuous.html#longitudinal-vsr",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Longitudinal VSR",
    "text": "Longitudinal VSR\nThe explicitly defined virtual subject longitudinal responses can be specified with any of 3 methods. No matter which longitudinal VSR simulation method is selected, it can be combined with any Dose Response VSR, and the dose response VSR is  guaranteed to have the marginal distribution Unless Use Baseline Adjustment for Subject Response is specified in the Baseline VSR tab.  specified in the dose response VSR section. The longitudinal VSR determines how to correlate early endpoint values with the final endpoint value.\nThe three longitudinal VSRs available for continuous endpoints are:\n\nCorrelated\nHierarchical, which comes in two ‘flavors’:\n\nHierarchical (as in versions of FACTS prior to 6.1): the per subject random element ‘delta’ is scaled at visit ‘t’ by the response fraction ft at that visit.\nHierarchical MMRM (first available in FACTS 6.1): the per subject random element ‘delta’ is the same at all visits.\n\nITP\n\n\nCorrelated Simulation of Longitudinal Data\nThe Correlated method for generating longitudinal responses simulates the response observed at each visit by summing 3 elements – a fraction of the final response, a fraction of the ‘noise’ from the previous observation, and additional element of noise at the current visit.\n\n\n\n\n\n\nFigure 4: The Longitudinal VSR tab when specifying the Correlated continuous longitudinal VSR.\n\n\n\nThe user specifies:\n\n\\(\\rho_{t}\\)\n\nthe correlation in the observation at visit \\(t\\) with visit \\(t - 1\\). Values should be in the range 0-1. The closer to 1, the greater the correlation between visit \\(t - 1\\) and visit \\(t\\). The \\(\\rho\\) at Visit 1 is not enterable, because there is no previous visit to correlate the first visit value with.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\varphi_{t}^{2}\\)\n\nthe fraction of the final standard deviation (SD) that will be observed at this visit. Values must be \\(&gt;0\\). An additional element of noise at visit \\(t\\) is sampled so that in combination with the noise carried forward from visit \\(t - 1\\), the overall variance in observations at visit \\(t\\) will have this fraction of the final variance.\n\n\nFor the first visit of patient \\(i\\), the response \\(y_{i,1}\\) assuming patient \\(i\\) was randomized to dose \\(j\\) is simulated as:\n\\[y_{i,1}\\ \\sim\\ N\\left( \\mu_{j,1},\\sigma_{j,1}^{2} \\right)\\]\nFor time points after the first visit, observed data is simulated as:\n\\[y_{i,t}\\ \\sim\\ \\mu_{j,t} + \\sqrt{1 - \\rho_{t}^{2}}N\\left( 0,\\sigma_{j,t}^{2} \\right) + \\rho_{t}\\left( y_{i,t - 1} - \\mu_{j,t - 1} \\right)\\frac{\\sigma_{j,t}}{\\sigma_{j,t - 1}} \\text{   for } t \\geq 2.\\]\nThere are 3 components of this equation. First, is the marginal mean of the response at the particular visit \\(t\\). This parameter is called \\(\\mu_{j,t}\\). \\(\\mu_{j,t}\\) is simply the dose response at the final endpoint times the proportion of the final effect that should be observed at visit \\(t\\).\n\\[\\mu_{j,t} = f_{t}\\mu_{j}\\]\nwhere \\(f_{t}\\) is input as the response fraction for each visit, and \\(\\mu_{j}\\) is the dose response for dose \\(j\\), which is input as the Response for each dose on the Dose Response tab.\nThe second component controls the adjustment of the variance based on the correlation of the endpoint to be sampled with the previous visit’s response. A strong correlation (\\(\\rho_{t}\\) close to \\(\\pm 1\\)) results in a variance reduction term \\(\\sqrt{1 - \\rho_{t}^{2}}\\) close to 0, which guarantees that the time \\(t\\) response is very close to the time \\(t - 1\\) response. The variance reduction term modifies a dose’s marginal visit variance \\(\\sigma_{j,t}^{2}\\). This dose by visit variance is calculated by squaring the marginal standard deviation for a dose’s final endpoint response \\(\\sigma_{j}\\) times the proportion of the total variance that is observed at visit \\(t\\), \\(\\phi_{t}\\), which is a user input on the Longitudinal VSR tab.\nSo, \\(\\sigma_{j,t}^{2} = \\phi_{t}^{2}\\sigma_{j}^{2}\\) where \\(\\phi_{t}\\) is the fraction of final SD specified in the Longitudinal VSR tab in FACTS, and \\(\\sigma_{j}\\) is the SD of the response specified in the Dose Response VSR tab in FACTS.\nThe final component of the model adjusts the mean of the visit \\(t\\) response that is to be simulated based on the residual of the time \\(t - 1\\) response. This component is:\n\\[\\rho_{t}\\left( y_{i,t - 1} - \\mu_{j,t - 1} \\right)\\frac{\\sigma_{j,t}}{\\sigma_{j,t - 1}}\\]\nValues of \\(\\rho_{t}\\) close to 1 lead to visit \\(t\\) responses with Z-scores similar to the visit \\(t - 1\\) responses, values of \\(\\rho_{t}\\) close to 0 lead to simulation of visit \\(t\\) responses with no regard to the previous visit’s residual value, and values of \\(\\rho_{t}\\) close to -1 lead to visit \\(t\\) responses with Z-scores that are -1 times the previous visit’s Z-score.\nThe specification of the visit level means \\(\\mu_{j,t}\\ \\)and variances \\(\\sigma_{j,t}^{2}\\) as response fractions and fractions of final SD, respectively, allows for each created longitudinal VSR to work with every created Dose Response VSR.\n\n\nHierarchical model\nThe hierarchical method for generating longitudinal responses simulates the response observed by sampling responses from a Normal distribution, where the mean is a combination of the mean final response of the treatment and a per-subject difference, scaled by a visit dependent coefficient, and the variance is a fraction of the variance of the final treatment effect.\nThis Hierarchical form of this model has a per-subject random effect parameter \\(\\delta_{i}\\) that is scaled down by the response scaling parameter \\(f_{t}\\). In the very similar Hierarchical (MMRM) model (described below), the random effect \\(\\delta_{i}\\) is not scaled by \\(f_{t}\\), so it provides a constant adjustment to all visits.\n\n\n\n\n\n\nFigure 5: The Longitudinal VSR tab when specifying the Hierarchical (MMRM) continuous longitudinal VSR.\n\n\n\nIn the Hierarchical longitudinal subject data simulation model, the response variance is decomposed into two components. One is the within-subject variability (called intra-subject variability), and the other is across-subject variability (Called inter-subject variability). The parameter \\(\\omega\\) determines how much of the total variability is simulated in the inter- and intra-subject variabilities. More variability in the inter-subject simulation means that there is less variability in the intra-subject simulation, so the early visit endpoints will be more correlated with the final visit endpoint, and vice versa.\nThe parameters of the Hierarchical longitudinal subject data simulation model are:\n\n\\(\\omega\\)\n\nthe fraction of the variance of the final response (\\(\\sigma_{j}^{2}\\)) on the treatment arm that will be simulated in the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\phi_{jt}^{2}\\)\n\nthe fraction of the final endpoint variance that will be observed at this visit. Values must be \\(&gt;0\\) and must be such that \\(\\phi_{jt}^{2} - f_{t}^{2}\\omega\\) is \\(&gt;0\\).\n\n\nObservation \\(y_{it}\\) is the visit response at visit \\(t\\) for a subject \\(i\\) that was randomized from dose \\(j\\). The observation is generated from the distribution described by:\n\\[y_{i,t} = f_{t}\\left( \\mu_{j} + \\delta_{i} \\right) + N\\left( {0,\\ \\left( \\phi_{jt}^{2} - f_{t}^{2}\\omega \\right)\\ \\sigma}_{j}^{2} \\right)\\]\nwhere \\(\\sigma_{j}^{2}\\) is the variance of the final endpoint response for dose \\(j\\), \\(\\phi_{jt}^{2}\\) is the fraction of total variance observed at visit \\(t\\) on dose \\(j\\), \\(\\mu_{j}\\) is the final endpoint response mean for dose \\(j\\), \\(\\delta_{i}\\) is a subject level random effect, \\(f_{t}\\) is the fraction of the final endpoint response that is observed at visit \\(t\\), and \\(\\omega\\) is the proportion of overall variance due to intersubject (across subject) variability.\nThe prior for the patient random effect term of patient \\(i\\) who was randomized to dose \\(j\\) is\n\\[\\delta_{ij}\\ \\sim\\ N\\left( 0,\\ \\omega\\sigma_{j}^{2} \\right)\\]\nIf \\(\\omega\\) is close to 1, then most of the variability in the responses comes from differences in participants and the visit-to-visit correlation within a participant’s follow-up is high. If \\(\\omega\\) is close to 0, then there is little correlation between visits within a participant’s follow-up, and most of the overall variance comes from noise in the within patient responses, rather than differences across patients. In other words, large values of \\(\\omega\\) lead to early data that is more predictive of the final endpoint.\nNote that, in this model the response fraction \\(f_{t}\\) is multiplied by both the final endpoint mean and the patient level random effect. This results in the variance of the dose \\(j\\) response at visit \\(t\\) being \\[{\\left( \\phi_{jt}^{2} - f_{t}^{2}\\omega \\right)\\sigma}_{j}^{2} + f_{t}^{2}\\omega\\sigma_{j}^{2} = \\phi_{jt}^{2}\\sigma_{j}^{2}.\\]\nAdditionally, since \\(f_{t}\\) changes the proportion of the overall variance that comes from the random effect, the simulated correlation between visits decreases when values of \\(f_{t}\\) less than 1 are provided. See the MMRM version of the Hierarchical simulation model if this is undesirable.\nIf all \\(\\phi_{jt}^{2} = 1\\) and \\(f_{t} = 1\\), then the visits will have pairwise correlations equal to \\(\\omega\\). If the \\(\\phi_{jt}^{2}\\) or \\(f_{t}\\) are less than 1, then the visit pairwise correlations will depend on the input variance fractions \\(\\phi_{jt}^{2}\\) and \\(f_{t}\\).\n\n\nHierarchical (MMRM) model\nThe Hierarchical (MMRM) longitudinal patient simulation model is very similar to the Hierarchical simulation method, except that in the MMRM version the response fraction for a visit does not modify the patient level random effect. The user inputs for the Hierarchical (MMRM) model are nearly identical to the plain Hierarchical model.\n\n\\(\\omega\\)\n\nthe fraction of the variance of the final response on the treatment arm (\\(\\sigma_{j}^{2}\\)) that will be simulated in the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\phi_{jt}^{2}\\)\n\nthe fraction of the final endpoint variance that will be observed at this visit for dose \\(j\\), values must be &gt;0 and must be such that \\(\\phi_{jt}^{2} - \\omega\\) &gt; 0.\n\n\nThe Hierarchical MMRM method simulates responses at visit \\(t\\) for a subject \\(i\\) that was randomized to dose \\(j\\) from the distribution:\n\\[y_{i,t} = f_{t}\\mu_{j} + \\delta_{i} + N\\left(0,\\ \\left( \\phi_{jt}^{2} - \\omega \\right)\\ \\sigma_{j}^{2} \\right)\\].\nwhere \\(\\sigma_{j}^{2}\\) is the variance of the final endpoint response for dose \\(j\\), \\(\\phi_{jt}^{2}\\) is the fraction of total variance observed at visit \\(t\\) on dose \\(j\\), \\(\\mu_{j}\\) is the final endpoint response mean for dose \\(j\\), \\(\\delta_{i}\\) is a subject level random effect, \\(f_{t}\\) is the fraction of the final endpoint response that is observed at visit \\(t\\), and \\(\\omega\\) is the proportion of overall variance due to intersubject (across subject) variability.\nThe prior for the patient random effect term of patient \\(i\\) who was randomized to dose \\(j\\) is\n\\[\\delta_{ij}\\ \\sim\\ N\\left( 0,\\ \\omega\\sigma_{j}^{2} \\right)\\]\nIf \\(\\omega\\) is close to 1, then most of the variability in the responses comes from differences in participants and the visit-to-visit correlation within a participant’s follow-up is high. If \\(\\omega\\) is close to 0, then there is little correlation between visits within a participant’s follow-up, and most of the overall variance comes from noise in the within patient responses rather than differences across patients. In other words, large values of \\(\\omega\\) lead to early data that is more predictive of the final endpoint.\nNote that, in this model the response fraction \\(f_{t}\\) is multiplied by only the final endpoint mean and not the patient level random effect. This results in the variance of the dose \\(j\\) response at visit \\(t\\) being \\[{\\left( \\phi_{jt}^{2} - \\omega \\right)\\sigma}_{j}^{2} + \\omega\\sigma_{j}^{2} = \\phi_{jt}^{2}\\sigma_{j}^{2}.\\]\nThis total variance is the same as the non MMRM Hierarchical method, but only the \\(\\phi_{jt}^{2}\\) parameter effects the variance of early endpoint responses. If all \\(\\phi_{jt}^{2} = 1\\), then the visits will have pairwise correlations equal to \\(\\omega\\). If the \\(\\phi_{jt}^{2}\\) are less than 1, then the visit pairwise correlations will be larger than \\(\\omega\\), with the exact value depending on the input variance fractions \\(\\phi_{jt}^{2}\\).\n\n\nIntegrated Two Component Prediction (ITP) Simulation of Longitudinal Data\nThe ITP method for generating longitudinal responses simulates the response observed at each visit by summing 3 elements and scaling them by an exponential function. The 3 elements are: the mean final response, an element of inter-subject variability, and a residual variability at the current visit.\n\n\n\n\n\n\nFigure 6: The Longitudinal VSR tab when specifying the ITP continuous longitudinal VSR.\n\n\n\nThe user specifies\n\n\\(\\omega_{j}\\)\n\nfraction (for each dose) of the variance of the final response on the treatment arm (\\(\\sigma_{j}^{2}\\)) used for the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(k_{j}\\)\n\nthe shape parameter (for each dose) of the exponential component governing the increase in the observed response. The values of \\(k\\) should be scaled to take into account the length of time (in weeks) to the intermediate and final visits. See below for more intuition on sensible values of \\(k\\).\n\n\nFor subject \\(i\\) at visit \\(t\\), who was randomized to dose \\(j\\), the response \\(y_{it}\\) is simulated as:\n\\[y_{it} = \\left( \\mu_{j} + s_{i} + \\epsilon_{it} \\right)\\left( \\frac{1 - \\text{exp}\\left( k_{j}x_{t} \\right)}{1 - \\text{exp}\\left( k_{j}x_{T} \\right)} \\right)\\]\nwhere \\(\\mu_{j}\\) is the mean of the final endpoint on dose \\(j\\), \\(s_{i}\\sim N\\left( 0,\\ \\omega_{j}\\sigma_{j}^{2} \\right)\\) is a subject specific random effect, each \\(\\epsilon_{it}\\sim N\\left( 0,{\\ \\sigma}_{j}^{2}\\left( 1 - \\omega_{j} \\right) \\right)\\) is a residual error, \\(k_{d}\\) is a shape parameter, \\(x_{t}\\) are the visit times that the \\(y_{it}\\) are observed, and \\(x_{T}\\) is the time of the final endpoint.\nThe ITP model implies that the variance of the observations shrinks towards 0 with the mean (so early visits have reduced expected responses and variances).\nThe shape parameter \\(k\\) determines the rate at which the final endpoint’s eventual effect is observed during a subject’s follow-up. A value of \\(k = 0\\) indicates that the proportion of effect observed moves linearly with time. A value of \\(k &lt; 0\\) means that the eventual final effect is observed earlier in follow-up and plateaus off as time moves towards the final endpoint. A value of \\(k &gt; 0\\) indicates that less of the total final endpoint effect is observed early in follow up, but as time approaches the final endpoint time the proportion of the effect observed increases rapidly. Values of \\(k\\) less than 0 tend to be more common than values of \\(k\\) greater than 0. See the figure below for a collection of possible shapes of the change in response using different values of \\(k\\).\nAdditionally, unlike the Correlated or Hierarchical simulation methods, the ITP method uses the actual visit time to simulate subject values.\n\n\n\n\n\n\nFigure 7: The relationship across intermediate visit means for different shape parameters \\(k\\)."
  },
  {
    "objectID": "documentation/v72/userguides/core/vsr/continuous.html#baseline-vsr",
    "href": "documentation/v72/userguides/core/vsr/continuous.html#baseline-vsr",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Baseline VSR",
    "text": "Baseline VSR\nIf simulation of baseline has been included on the Study &gt; Study Info tab, a new virtual subject response tab is available for specifying the baseline score.\n\n\n\n\n\n\nFigure 8: The Baseline VSR tab with adjustment based on the baseline response turned on.\n\n\n\nThe simulation of distribution of baseline scores is specified using a normal distribution with user specified mean and standard deviation, and optionally applied upper and lower bounds to reflect limitations on the score range or screening criteria. If the simulated baseline score is truncated, then the true mean and SD of the baseline are likely to be different from these values of the mean and SD which are before truncation.\nIf the response is chosen to be change from baseline on the Study Info tab, then the dose response VSR is specified as change from baseline, and the raw endpoint for a subject will actually be their baseline value plus their simulated change from baseline value. If the response is chosen to be Final endpoint value on the Study Info tab, then the dose response VSR specifies the direct distribution that the final endpoint will be sampled from, and changing the baseline distribution does not effect the final endpoint distribution.\nWhether the final endpoint is change from baseline or final endpoint value, it is possible to adjust the final response based on the simulated baseline value. To do so, the user selects “Use Baseline Adjustment for Subject Response” and supplies 3 parameters:\n\n\\(\\beta\\)\n\na coefficient that reflects the degree of influence of baseline on final score and the degree of variability in the final score due to baseline.\n\nc\n\na centering offset, typically the expected mean of the observed baseline scores\n\ns\n\na scaling element, typically set to the expected SD of the baseline.\n\n\nBe aware that performing a baseline adjustment for subject response can change the marginal distribution of the dose response VSR.\n\nExample\nIn the above screenshot a baseline of mean 25 and SD 10 has been specified for the distribution of the baseline values, so a centering of \\(c=25\\) and scaling of \\(s=10\\) is used. Wishing to simulate an overall SD of 5 in the final change from baseline and apportion two-thirds the variance to baseline, \\(\\beta\\) has been set as follows:\n\nThe desired final variance is 25 (\\(5^2\\)), divided into 1/3 dose response and 2/3 baseline effects.\nThe SD of the simulated response is set to \\(\\sqrt{\\left( 25*\\frac{1}{3} \\right)} = 2.89\\)\nThe SD of the scaled baseline score is 1, so to contribute half the final variance of 25, Beta is set to \\(\\sqrt{\\left( 25*\\frac{2}{3} \\right)} = 4.08\\)\nNote that when simulating a baseline effect in this way, limiting the range of baseline by specifying upper and lower cut-offs – which might be natural limits of the endpoint, or due to inclusion / exclusion criteria in the protocol – can significantly reduce the variance in the final endpoint due to the baseline effect."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html",
    "title": "Multiple Endpoint Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘.csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described below.\nThe graphs in the multiple endpoint engine are largely the same as the graphs in the single endpoint engine, except that the graphs from the single endpoint core engines that show responses for an endpoint now have a control allowing for the selection of which endpoint the plot should be made for.\nThis page will attempt to highlight differences between the single endpoint and the multiple endpoint engines, largely focusing on plots that do not exist in the single endpoint output. See Continuous/Dichotomous Simulation Output for full details about the single endpoint graphs in FACTS.\n\n\nMany of the multiple endpoint graphs are repeated from single endpoint, potentially with the ability to view for any of the simulated endpoints. When the endpoint to use can be selected in the controls for a plot, it is common that the utility can also be selected instead of an endpoint. This will allow for assessment of the distribution of the combined utility in addition to the response estimates for the individual endpoints. The repeated graphs are listed here:\n\nAllocation Box and Whisker Plot\n&lt;Endpoint&gt; Response and Subject Allocation\n&lt;Endpoint&gt; Response and Target Selection\nPer Dose: QOI Box and Whisker Plots\nTarget Response by Sample Size Scatter Plot\nCumulative Operating Characteristics Plot\nTime Course for Stopping\nTime Course for Arm Dropping/Retention\nArm Retention Proportion\nFrequentist P(significance)\nFrequentist Response and Significance\nPer Sim: &lt;Endpoint&gt; Response Alloc\nPer Sim: &lt;Endpoint&gt; Response Dist\nPer Interim: &lt;Endpoint&gt; Response Alloc\nPer Interim: &lt;Endpoint&gt; Response Dist\nExplore Final Success/Futility Criteria\nExplore Early Success/Futility Criteria\nExplore Arm Dropping Criteria\nExplore Success/Futility Contours\n\n\n\n\n\n\n\n\n\nFigure 1: Utility and Subject Allocation.\n\n\n\nThis graph displays a histogram of the mean subject allocation and a line graph of the mean estimated utility. These plots show:\n\nThe blue bars show the mean number of subjects allocated to each arm over the simulations.\nThe green dashed line is the average of the mean estimate of utility over the simulations\nThe black line is the true utility of the combination of the underlying response profiles being simulated for all the endpoints.\nThe green vertical lines around the green dashed line shows the range of the central 95% of mean estimates of utility over the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Utility and Ppn Greatest Pr(UMax)\n\n\n\n\n\n\n\n\n\n\n\n(b) Utility and Ppn Greatest Pr(UMin absolute: Delta=0.5)\n\n\n\n\n\n\n\nFigure 2\n\n\n\nThese plots show the utility of the true simulated response profiles and the mean and 95% spread of the mean estimate of utility ratios along with bars showing the proportion of simulations the different treatment arms are the finally selected target:\n\nThe dose with the maximum utility (left)\nThe Minimum Acceptable Utility Dose (Umin), the dose most likely to be the minimum dose that exceeds the Clinically Significant Minimum Utility (CSMU) of 0.5 (right)\n\n\n\n\nThis box and whisker plot shows the distribution of the estimate of utility for each dose:\n\n\n\n\n\n\nFigure 3: Utility Boxplot\n\n\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations.\n\n\n\n\n\n\nFigure 4: Estimated Utility for all endpoints and overall, as well as the number of subjects allocated to each arm.\n\n\n\nThese graphs includes a control that allows the user to select which simulation to graph the results from.\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the number of subjects finally allocated to each arm.\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations. This is the graph that’s shown when the Endpoint control is chosen to be “Utility” in the “Per Sim: Utility/Response Dist” plotting controls.\n\n\n\n\n\n\nFigure 5: Estimated Utility for all endpoints and overall, as well as the estimate of a chosen QOI\n\n\n\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the final values of a selected Utility QOI.\n\n\n\n\nThis is an identical set of graphs to the Per Sim: Utility and Subject Allocation and Per Sim: Utility and QOI plots, except that in addition to the control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100). number of files to be read and subsequent processing of the data.\n\n\n\n\nThe Across Scenario graphs for the multiple endpoint engine are the same as the graphs available in the individual continuous and dichotomous engines.\nFor reference, these are:\n\nSelected Arms\nQOI Box Plots\nPpn Success\nResponse/Utility\nAllocation\nInterim vs Final Scatter Plot\nReceiver Operating Characteristics\n\nThe only differences from the single endpoint across scenario graphs are:\n\nQOI Box Plots graph now has Utility as a selectable endpoint and the utility QOIs are available to be plotted\nRespnnse/Utility is the same as the single endpoint Response graph, except that Utility is a selectable endpoint\nInterim vs Final Scatter Plot now has utility QOIs\nReceiver Operating Characteristics plots now have utility QOIs."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#per-scenario-graphs",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#per-scenario-graphs",
    "title": "Multiple Endpoint Output",
    "section": "",
    "text": "Many of the multiple endpoint graphs are repeated from single endpoint, potentially with the ability to view for any of the simulated endpoints. When the endpoint to use can be selected in the controls for a plot, it is common that the utility can also be selected instead of an endpoint. This will allow for assessment of the distribution of the combined utility in addition to the response estimates for the individual endpoints. The repeated graphs are listed here:\n\nAllocation Box and Whisker Plot\n&lt;Endpoint&gt; Response and Subject Allocation\n&lt;Endpoint&gt; Response and Target Selection\nPer Dose: QOI Box and Whisker Plots\nTarget Response by Sample Size Scatter Plot\nCumulative Operating Characteristics Plot\nTime Course for Stopping\nTime Course for Arm Dropping/Retention\nArm Retention Proportion\nFrequentist P(significance)\nFrequentist Response and Significance\nPer Sim: &lt;Endpoint&gt; Response Alloc\nPer Sim: &lt;Endpoint&gt; Response Dist\nPer Interim: &lt;Endpoint&gt; Response Alloc\nPer Interim: &lt;Endpoint&gt; Response Dist\nExplore Final Success/Futility Criteria\nExplore Early Success/Futility Criteria\nExplore Arm Dropping Criteria\nExplore Success/Futility Contours\n\n\n\n\n\n\n\n\n\nFigure 1: Utility and Subject Allocation.\n\n\n\nThis graph displays a histogram of the mean subject allocation and a line graph of the mean estimated utility. These plots show:\n\nThe blue bars show the mean number of subjects allocated to each arm over the simulations.\nThe green dashed line is the average of the mean estimate of utility over the simulations\nThe black line is the true utility of the combination of the underlying response profiles being simulated for all the endpoints.\nThe green vertical lines around the green dashed line shows the range of the central 95% of mean estimates of utility over the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Utility and Ppn Greatest Pr(UMax)\n\n\n\n\n\n\n\n\n\n\n\n(b) Utility and Ppn Greatest Pr(UMin absolute: Delta=0.5)\n\n\n\n\n\n\n\nFigure 2\n\n\n\nThese plots show the utility of the true simulated response profiles and the mean and 95% spread of the mean estimate of utility ratios along with bars showing the proportion of simulations the different treatment arms are the finally selected target:\n\nThe dose with the maximum utility (left)\nThe Minimum Acceptable Utility Dose (Umin), the dose most likely to be the minimum dose that exceeds the Clinically Significant Minimum Utility (CSMU) of 0.5 (right)\n\n\n\n\nThis box and whisker plot shows the distribution of the estimate of utility for each dose:\n\n\n\n\n\n\nFigure 3: Utility Boxplot\n\n\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations.\n\n\n\n\n\n\nFigure 4: Estimated Utility for all endpoints and overall, as well as the number of subjects allocated to each arm.\n\n\n\nThese graphs includes a control that allows the user to select which simulation to graph the results from.\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the number of subjects finally allocated to each arm.\n\n\n\n\nThis set of graphs show the final utility analysis at the end of individual simulations. This is the graph that’s shown when the Endpoint control is chosen to be “Utility” in the “Per Sim: Utility/Response Dist” plotting controls.\n\n\n\n\n\n\nFigure 5: Estimated Utility for all endpoints and overall, as well as the estimate of a chosen QOI\n\n\n\nOn each graph\n\nThe black line shows the ‘True’ overall utility of the response profiles being simulated.\nThe green solid line shows the estimate of overall utility\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of utility.\nThe additional lines (here khaki, blue and brown) show the estimates of utility of the individual endpoints.\nThe blue bars show the final values of a selected Utility QOI.\n\n\n\n\nThis is an identical set of graphs to the Per Sim: Utility and Subject Allocation and Per Sim: Utility and QOI plots, except that in addition to the control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100). number of files to be read and subsequent processing of the data."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#across-scenario-graphs",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#across-scenario-graphs",
    "title": "Multiple Endpoint Output",
    "section": "",
    "text": "The Across Scenario graphs for the multiple endpoint engine are the same as the graphs available in the individual continuous and dichotomous engines.\nFor reference, these are:\n\nSelected Arms\nQOI Box Plots\nPpn Success\nResponse/Utility\nAllocation\nInterim vs Final Scatter Plot\nReceiver Operating Characteristics\n\nThe only differences from the single endpoint across scenario graphs are:\n\nQOI Box Plots graph now has Utility as a selectable endpoint and the utility QOIs are available to be plotted\nRespnnse/Utility is the same as the single endpoint Response graph, except that Utility is a selectable endpoint\nInterim vs Final Scatter Plot now has utility QOIs\nReceiver Operating Characteristics plots now have utility QOIs."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#summary-per-scenario",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#summary-per-scenario",
    "title": "Multiple Endpoint Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after multiple endpoint simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ -&gt; Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut -&gt; Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt;Dose&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn Incorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation columns provided in FACTS output for multiple endpoint trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nUtility\n\nThe utility columns provided in FACTS output for multiple endpoint trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Utility: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the posterior estimate of the overall utility of this dose, this is the utility of each endpoint at the dose, combined using the specified utility combination method.\n\n\nSD Utility: &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the posterior estimate of the overall utility of this dose.\n\n\n\n\n\nProbabilities\n\nThe Utility QOIs, which are visible when Probabilities columns are selected in simulation output for multiple endpoint trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per Utility QOI\nFor each Utility Posterior Probability QOI this is the mean over the simulations of the estimate of the posterior probability of the QOI for each dose.\nFor each Utility Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The Max Utility target is always identifiable, so the Ppn(target) should sum to 1 across the doses.\nA Umin target is not guaranteed to be identifiable, if no dose meets the CSMU criteria in any MCMC sample so all doses have a 0 probability of having greater utility than Control then no dose is the Umin. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\n\n\n\n\n\n&lt;Endpoint&gt; Response\nThe columns that appear when selecting &lt;Endpoint&gt; Response vary depending on whether the selected endpoint is continuous or dichotomous.\n\nContinuousDichotomous\n\n\n\nThe columns provided if selecting endpoint specific response for a continuous endpoint.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nSD Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nMean Sigma (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint.\n\n\nSD Mean Sigma (&lt;Endpoint&gt;)\n1\nThis is the standard deviation (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint\n\n\nTrue Mean Resp (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the true change from baseline / response rate from which the simulation data was sampled.\n\n\nTrue SD Resp.: &lt;Dose&gt;\n1\nIf the endpoint is continuous these columns are included and report the true standard deviation of the simulated response. If baseline and “baseline adjustment of the simulated subject response” is used, this reported SD will include that impact of that and be different from the value(s) for sigma entered on the VSR &gt; Dose Response tab.\n\n\nMean Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the posterior estimate of the utility of this endpoint on this dose.\n\n\nSD Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the posterior estimate of the overall utility of this dose.\n\n\n\n\n\n\nThe columns provided if selecting endpoint specific response for a dichotomous endpoint.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nSD Response (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nTrue Mean Resp (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the true change from baseline / response rate from which the simulation data was sampled.\n\n\nMean Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the posterior estimate of the utility of this endpoint on this dose.\n\n\nSD Utility (&lt;Endpoint&gt;): &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the posterior estimate of the overall utility of this dose.\n\n\n\n\n\n\n\n\n&lt;Endpoint&gt; Probabilities\n\nThe endpoint specific QOI columns for simulation output in the multiple endpoint engine.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI defined on this endpoint\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\n&lt;Endpoint&gt; Baseline – Continuous Endpoints Only\n\nThe baseline columns shown when &lt;Endpoint&gt; Baseline is selected. Only available for continuous endpoints.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Baseline Beta (&lt;Endpoint&gt;)\n1\nIf a baseline adjusted dose response model is being used then this is the mean (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nSD Baseline Beta (&lt;Endpoint&gt;)\n1\nIf a baseline adjusted dose response model is being used then this is the standard deviation (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nMean Baseline (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the mean observed baseline score.\n\n\nSD Baseline (&lt;Endpoint&gt;)\n1\nThis is the standard deviation (over the simulations) of the mean observed baseline score.\n\n\nTrue Mean Baseline (&lt;Endpoint&gt;)\n1\nThis is the true mean of the baseline distribution used to simulate the baseline scores.\n\n\nTrue SD Baseline (&lt;Endpoint&gt;)\n1\nThis is the true standard deviation of the distribution used to simulate the baseline scores.\n\n\n\n\n\n&lt;Endpoint&gt; Observed\n\nThe columns shown when &lt;Endpoint&gt; Observed is selected.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the mean (over the simulations) of dropouts per arm per visit in this scenario.\n\n\n\n\n\n&lt;Endpoint&gt; Model Params\n\nThe columns shown when &lt;Endpoint&gt; Model Params is selected for a specific endpoint.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nBAC Mean (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Tau (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau (&lt;Endpoint&gt;)\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Tau (&lt;Endpoint&gt;)\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\n\n\n\nFrequentist Results (&lt;Missingness&gt; &lt;Endpoint&gt;)\n\nThe columns available when selecting Frequentist results for a specific endpoint and missingness handling method.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n\nMean Theta (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\n\nThe mean response per dose.\n\n\n\nSD Mean Theta (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\n\nThe standard error of the response per dose\n\n\n\nPpn Signif (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of | simulations where the unadjusted p-value is | less than the user specified one-sided alpha. | | The marginal probability of significance per | dose (using unadjusted p-values).\n\n\n\nBias (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\n\nCoverage (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the treatment difference compared to control contains the true mean treatment difference compared to control used to simulate subject responses.\n\n\n\nPpn Signif Dunnett (&lt;Missingness&gt; &lt;Endpoint&gt;)\n1\nFor each treatment arm, the proportion of simulations where at least one of the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Dunnett (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Dunnett (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval for the treatment difference compared to control contains the true mean treatment difference compared to control used to simulate subjects responses.\n\n\n\nPpn Signif Bonf (&lt;Missingness&gt; &lt;Endpoint&gt;)\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\n\nPpn Signif Bonf (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\n\nCoverage Bonf (&lt;Missingness&gt; &lt;Endpoint&gt;) &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\n\nPpn Signif Trend (&lt;Missingness&gt; &lt;Endpoint&gt;)\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#detailed-per-simulation-results",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#detailed-per-simulation-results",
    "title": "Multiple Endpoint Output",
    "section": "Detailed Per Simulation Results",
    "text": "Detailed Per Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulations output in the table by double-clicking on the appropriate row. A separate window pops up and displays the results for each individual simulation. This is a “prettier” view of the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 6: The simulation results pop out viewer showing simulation highlights."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-summary.csv",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-summary.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first two lines are header lines, starting with a ‘#’, containing the column headings. The first line contains the lengthy “human readable” form of the QOI names, the second line contains the shorter “computer readable” alternate QOI names. For columns that are not QOI values, the column names are the same in the two rows.\n\nContents of the summary.csv file for the multiple endpoint engine.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the “.facts” file – not including the file extension.\n\n\nScenario\n1\nThe scenario name as used in FACTS – made by concatenating the names of the various profiles that make up the scenario\n\n\nTimestamp\n1\nThe time the simulations started\n\n\nVersion\n1\nThe overall version of FACTS that ran the simulations (not the version of the specific design engine)\n\n\nNSim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.subj 80%-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;Dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nNo. Dropouts &lt;Dose&gt;\nD\nThe mean (over the simulations) of the number of dropouts for each treatment arm.\n\n\nMean Util &lt;Dose&gt;\nD\nThis is the mean (over the simulations) of the posterior estimate of the utility of this dose.\n\n\nSE Util &lt;Dose&gt;\nD\nThis is the SD (over the simulations) of the posterior estimate of the utility of this dose.\n\n\nMean Resp &lt;Endpoint&gt;&lt;Dose&gt;\nD per endpoint\nThis is the mean (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nSE Resp &lt;Dose&gt;\nD per endpoint\nThis is the standard deviation (over the simulations) of the estimate of the response on this endpoint on this dose.\n\n\nMean Sigma\n1 per continuous endpoint\nThis is the mean (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint.\n\n\nSE Mean Sigma\n1 per continuous endpoint\nThis is the standard deviation (over the simulations) of the estimate of the standard deviation of the response (across all doses) on this endpoint\n\n\nTrue Mean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nThis is the true change from baseline / response rate from which the simulation data was sampled.\n\n\nTrue SD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nIf the endpoint is continuous these columns are included and report the true standard deviation of the simulated response. If baseline and “baseline adjustment of the simulated subject response” is used, this reported SD will include that impact of that and be different from the value(s) for sigma entered on the VSR &gt; Dose Response tab.\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit per endpoint\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit, based on the independent estimate of the dose response (omega_d), scaled by the longitudinal model’s estimate of the proportion of the final effect observed at the visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit per endpoint\nThis is the SD ( over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Util &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nThis is the mean (over the simulations) of the mean estimate of the utility of this endpoint on this dose.\n\n\nSE Util &lt;Endpoint&gt; &lt;Dose&gt;\nD per endpoint\nThis is the standard error (over the simulations) of the mean estimate of the utility of this endpoint on this dose.\n\n\nMean Beta &lt;Endpoint&gt;\n1 per continuous endpoint\nIf a baseline adjusted dose response model is being used then this is the mean (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nSE Beta &lt;Endpoint&gt;\n1 per continuous endpoint\nIf a baseline adjusted dose response model is being used then this is the standard error (over the simulations) of the estimate of the baseline adjustment parameter ‘Beta’.\n\n\nMean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the mean (over the simulations) of the mean observed baseline score.\n\n\nSE Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the standard error (over the simulations) of the mean observed baseline score.\n\n\nSD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the mean (over the simulations) of the SD of the observed baseline score.\n\n\nSE SD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the standard error (over the simulations) of the mean of the observed baseline score.\n\n\nTrue Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the true mean of the baseline distribution used to simulate the baseline scores.\n\n\nTrue SD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\nThis is the true SD of the baseline distribution used to simulate the baseline score.\n\n\nBAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau (&lt;Endpoint&gt;)\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first sit, to last person last visit.\n\n\nPpn Arms Dropped &lt;Dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive location.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first sit, to last person first visit (LPFV).\n\n\nQOI Columns\n\n\n\nThe QOI Columns depend on the QOIs that have been defined for this design. The columns are grouped, so that first all of the Utility QOIs are listed, then all of the Endpoint 1 QOIs are listed, and so on. Within the utility or an endpoint group, the QOIs are arranged in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-summary_freq_missing_endpoint.csv",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-summary_freq_missing_endpoint.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of Summary_freq_<missing>_<Endpoint>.csv",
    "text": "Contents of Summary_freq_&lt;missing&gt;_&lt;Endpoint&gt;.csv\nThere is a frequentist summary file for each type of treatment of missing values for each endpoint.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nThe contents of the Summary_freq_&lt;missing&gt;_&lt;Endpoint&gt;.csv file.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nMean Theta &lt;Dose&gt;\nD\nThe mean response per dose.\n\n\nSE Mean Theta &lt;Dose&gt;\nD\nThe standard error of the response per dose\n\n\nMean Beta (baseline)\n1 if endpoint continuous\nThe mean of the estimate of the baseline coefficient.\n\n\nSE Mean Beta (baseline)\n1 if endpoint continuous\nThe standard error of the estimate of the baseline coefficient\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nBias &lt;Dose&gt;\nD\nThe difference between the mean response and the true (simulated) response per dose\n\n\nCoverage &lt;Dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true response rate used to simulate subject responses.\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Dunnett &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Dunnett &lt;Dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true response rate used to simulate subjects responses.\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Bonf &lt;Dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Bonf &lt;Dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true response rate used to simulate subjects responses.\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nThe contents of the simulations.csv file and the weeksNNNNN.csv file for the multiple endpoint engine.\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis.\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index 999). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:\n\n= Early success\n= Late success\n= Late futility\n= Early futility\n= Success to futility flip-flop\n= Futility to success flip-flop\n= Inconclusive\n\n\n\nMean Utility &lt;Dose&gt;\nD\n✔\n✔\nThe mean estimate of utility per dose\n\n\nSD Utility &lt;Dose&gt;\nD\n✔\n✔\nThe SD of the estimate of utility\n\n\nEarly Success\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;Dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nComplete &lt;Dose&gt;\nD\n✔\n✔\nThe number of subjects on each dose that have completed – final endpoint data is available.\n\n\nComplete Information &lt;Dose&gt;\nD\n✔\n✔\nThe number of subject who count as complete for the purposes of timing interims – whether complete or opportunity to complete and at which visit is as defined on the Interims tab.\nIf interims are by enrolment then CompleteInformation is the number of subjects that have completed – final endpoint data is available.\n\n\n#Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\n✔\n✔\nThe number of subjects that have dropped out on each arm at each visit\n\n\nMean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe true mean response of each treatment arm for this simulation.\n\n\nTrue SD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe true SD of the response of each treatment arm for this simulation\n\n\nSigma &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe pooled estimate of the SD of the response across all the treatment arms\n\n\nSD Sigma &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe SD of the pooled estimate of the SD of the response across all the treatment arms\n\n\nBeta &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe estimate of Beta the baseline adjustment coefficient, if baseline adjustment is being used.\n\n\nSD Beta &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe SD of the estimate of Beta the baseline adjustment coefficient.\n\n\nMean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe estimate of the mean baseline score.\n\n\nSE Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe standard error of the estimate of the mean baseline score.\n\n\nSD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe estimate of the SD of the baseline score.\n\n\nTrue Mean Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe true mean of the simulated baseline score\n\n\nTrue SD Baseline &lt;Endpoint&gt;\n1 per continuous endpoint\n✔\n✔\nThe true SD of the simulated baseline score\n\n\nMean Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe mean estimate of the utility of this endpoint\n\n\nSD Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe SD of the estimate of utility of this endpoint\n\n\nMean Raw Response &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe mean raw response based solely on the observed data on each dose.\n\n\nSE mean Raw Response &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe SE of the estimate of the mean raw response on each dose.\n\n\nComplete &lt;Endpoint&gt; &lt;Dose&gt;\nD per continuous endpoint\n✔\n✔\nThe number of subjects with final data on this endpoint on each arm at the end of the trial / time of the interim.\n\n\nDR Param &lt;Endpoint&gt; &lt;Param&gt;\n10 per continuous endpoint\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 10 parameters, most less. The model parameters are labeled α1, α2, .. the subscripts corresponding to the column index &lt;Param&gt;.\n\n\nSd DR Param &lt;Endpoint&gt; &lt;Param&gt;\n10 per continuous endpoint\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod &lt;Endpoint&gt; &lt;Model&gt; &lt;Param&gt; &lt;Visit&gt;\nLM*LMP*V per continuous endpoint\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of model instances the user has specified.\nFor linear regression the parameters reported are:\n\nAlpha – per visit – the mean estimate of the constant offset in the change in response from this visit to the final visit\nBeta – per visit – the mean estimate of the coefficient of change in response from this visit to the final visit\nLambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\n\nFor time course hierarchical the parameters reported are:\n\nAlpha – per visit – the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit..\nLambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\nTau – the mean estimate of the SD of the per subject random effect\n\nFor ITP the parameters are:\n\nK – per model – the mean estimate of the ITP shape parameter\nTau – per model - the mean estimate of the SD of the per subject random effect\nLambda – per model – the mean estimate of the Sd of the residual error.\nOmega – per treatment arm – the mean estimate of the mean treatment arm effect.\n\nFor LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp &lt;Endpoint&gt; &lt;Dose&gt; &lt;Visit&gt;\nD*V per continuous endpoint\n✔\n✔\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean response for a particular dose at a particular visit.\n\n\nMean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nMean resp (loser CI) &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe lower bound of the 95% Credible Interval of the estimate of response for each treatment arm.\n\n\nMean resp (upper CI) &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe upper bound of the 95% Credible Interval of the estimate of response for each treatment arm.\n\n\nTrue Mean resp &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe true response rate of each treatment arm for this simulation.\n\n\nMean Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe mean estimate of the utility of this endpoint\n\n\nSD Utility &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe SD of the estimate of utility of this endpoint\n\n\nMean Raw Response &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe mean raw response based solely on the observed data on each dose.\n\n\nComplete &lt;Endpoint&gt; &lt;Dose&gt;\nD per dichotomous endpoint\n✔\n✔\nThe number of subjects with final data on each arm at the end of the trial / time of the interim.\n\n\nDR Param &lt;Endpoint&gt; &lt;Param&gt;\n4 per dichotomous endpoint\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled α1, α2, .. the subscripts corresponding to the column their value appears in here.\n\n\nSd DR Param &lt;Endpoint&gt; &lt;Param&gt;\n4 per dichotomous endpoint\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod &lt;Endpoint&gt; &lt;Model&gt; &lt;Visit&gt;\nLM*LMP*V per dichotomous endpoint\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of model instances the user has specified.\nFor Beta-Binomial the parameters reported are:\n\nAlpha0 & Beta 0 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 0\nProb01 – the mean probability of the Beta(Alpha0, Beta0) – the probability of 1 being the final result if the result at the visit is 0\nAlpha1 & Beta 1 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 1\nProb11 – the mean probability of the Beta(Alpha1, Beta1) – the probability of 1 being the final result if the result at the visit is 1\n\nFor logistic regression the parameters reported are:\n\nProb11 – the probability of 1 being the final result if the result at the visit is 1and\nProb01 – the probability of 1 being the final result if the result at the visit is 0\n\nFor restricted Markov the parameters reported are:\n\nAlpha0 – Alpha for state 0\nAlphaS – Alpha for stable state\nAlpha1 – Alpha for state 1\nProb0 – Transition probability to state 0\nProbS – Probability of remaining stable\nProb1 – Transition probability to state 1\n\n(values are for the transition to the next visit, so there are no values for the final visit)\n\n\nBAC Mean &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Tau &lt;Endpoint&gt;\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;Dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n\n\n\n\n\n\nThe QOI Columns depend on the QOIs that have been defined for this design. The columns are grouped, so that first all of the Utility QOIs are listed, then all of the Endpoint 1 QOIs are listed, and so on. Within the utility or an endpoint group, the QOIs are arranged in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-simulations_freq_missingness_endpoint.csv-weeks_freq_missingness_endpoint.csv",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-simulations_freq_missingness_endpoint.csv-weeks_freq_missingness_endpoint.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of Simulations_freq_<missingness>_<Endpoint>.csv, Weeks_freq_<missingness>_<Endpoint>.csv",
    "text": "Contents of Simulations_freq_&lt;missingness&gt;_&lt;Endpoint&gt;.csv, Weeks_freq_&lt;missingness&gt;_&lt;Endpoint&gt;.csv\nThese file contain the frequentist results after an interim analysis (weeks file) or final analysis (simulations file), apart for the first column, the columns are the same in the two types of file, and the same across the different methods for treating missing data and different endpoints.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation (simulations_freq_…csv only)\n\n\n# Weeks\n1\nThe week of the interim (weeks_freq_ only)\n\n\nTheta\nD\nUnadjusted estimate of response per treatment arm\n\n\nTheta SD\nD\nThe SD of the estimate of response per treatment arm\n\n\nTrt Effect\nD\nThe relative size of the treatment effect compared to control per arm (treatment effect for control is always 0). The direction of the comparison takes into account whether a response represents a subject’s condition improving or worsening, so that a positive treatment effect shows the response on the dose is better than on control.\n\n\nt-Stat\nD\nThe t-test statistic per treatment arm (for control this is always 0)\n\n\np-Value\nD\nThe unadjusted t-test p-value per arm (for control this is always 0)\n\n\nlower CI\nD\nThe unadjusted lower bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nupper CI\nD\nThe unadjusted upper bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nMin p-Value\n1\nThe smallest of the t-test p-values\n\n\np-Value Dunnett\nD\nThe Dunnett adjusted t-test p-value per arm (for control this is always 0).\n\n\nlower CI Dunnett\nD\nThe Dunnett adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper CI Dunnett\nD\nThe Dunnett adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Dunnett\n1\nThe smallest of the Dunnett adjusted p-values.\n\n\np-Value Bonf\nD\nThe Bonferroni adjusted t-test p-value per arm.\n\n\nlower-CI Bonf\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper-CI Bonf\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Bonf\n1\nThe smallest of the Bonferroni adjusted t-test value per arm.\n\n\nt-Stat trend\n1\nThe t statistic from the trend test.\n\n\np-Value trend\n1\nThe p-value from the trend test."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-patientsnnnnn.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in\n\n\nDateInWeeks\n1\nThe date, in weeks, from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit Overall\n1\nThe time of the last observation of the subject – the index of the visit of the observation from the overall visit list. NOTE this is the index of the last visit using the overall visit index (index of all visits). E.g. for a patient with 6 visits (index: 1, 2, …6), and an endpoint observed on 3 visits: visits 2, 4 and 6 of the overall schedule, if that patients last observed data was the second time this endpoint was observed (last visit for that endpoint will be “2”), the last visit overall will be “4”.\n\n\nDropout\n1\nWhether the subject dropped out of the trial (1) or not (0) before reaching their final visit.\n\n\nLastVisit &lt;Endpoint&gt;\n1\nThe time of the last observation of this endpoint – the index of the visit of the observation from the visit list for the endpoint\n\n\nBaseline &lt;Endpoint&gt;\n1\nThe baseline score for the subject, if baseline is simulated. If no baseline, or the endpoint is dichotomous, then this column is -9999.\n\n\nResponse &lt;Endpoint&gt; &lt;Visit&gt;\nV\nOne column per visit from the visit list from the endpoint, recording the observed endpoint value at that visit."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v72/userguides/core/simulation/multendpt.html#contents-of-mcmcnnnnn.csv",
    "title": "Multiple Endpoint Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file, if requested for output by the user, contains all the MCMC samples for the estimated responses for each arm on each endpoint. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The next set of columns are the MCMC samples for the first endpoint. This includes response rate estimates and, if the endpoint is continuous, a sigma. Then, various longitudinal model parameters are reported if used for that endpoint.\nThen second endpoint’s MCMC columns begin, again providing response estimates with or without sigma and longitudinal model parameters (if used). The other endpoints proceed in kind.\nThe first two columns are:\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\n\nThen, the endpoint specific output is provided for each endpoint. For a particular endpoint index, &lt;Endpoint&gt;, the columns output are:\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nTheta_&lt;Endpoint&gt; &lt;Dose&gt;\nD\nThe estimate of the mean response for each dose, based on the dose response model fitted.\n\n\nSigma_&lt;Endpoint&gt;\n1\nThe estimate of the SD of the endpoint.\n\n\nBeta_&lt;Endpoint&gt;\n1\nIf baseline is included in the simulated data and using the “Baseline Adjusted Model”, this is the value of “Beta”, the baseline coefficient.\n\n\nA&lt;1-8&gt;_&lt;Endpoint&gt; / Tau_&lt;Endpoint&gt;\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod_&lt;Endpoint&gt; &lt;model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\n\n\nIf using a dichotomized continuous variable, then the parameters for the LM are the same as for the continuous model MCMC output listed in the Continuous tab.\nIf using a dichotomous longitudinal model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nPi_&lt;Endpoint&gt; &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA&lt;1-8&gt;_&lt;Endpoint&gt; / Tau_&lt;Endpoint&gt;\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod_&lt;Endpoint&gt; &lt;model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\nIf using a restricted markov model for the endpoint, then the following columns are output in the MCMC file for that endpoint.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nPi_&lt;Endpoint&gt; &lt;Dose&gt;\nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA&lt;1-8&gt;&lt;Endpoint&gt; / Tau&lt;Endpoint&gt;\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod_&lt;Endpoint&gt; &lt;model&gt; &lt;Param&gt; &lt;Visit&gt;\nL*P*V\nCurrently the values of Alpha0, Alpha1, AlphaS (the parameters to the Dirichlet distribution) are (unnecessarily – these values are not sampled) output, along with the values for P0, P1 and PS the transition probabilities of each model at each visit."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html",
    "href": "documentation/v72/userguides/core/simulation/index.html",
    "title": "Simulation",
    "section": "",
    "text": "The Simulation tab allows the user to execute simulations for each of the scenarios specified for the study. The user may choose the number of simulations, whether to execute locally or on the Grid, and modify the random number seeds.\nIn the Simulation tab the user can provide simulation configuration parameters like the number of simulations to run, whether the simulations can be run on the Grid, the parallelization strategy, the random number seed used in the simulations, and the number of certain output files that should be kept during the simulation execution."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#number-of-simulations",
    "href": "documentation/v72/userguides/core/simulation/index.html#number-of-simulations",
    "title": "Simulation",
    "section": "Number of simulations",
    "text": "Number of simulations\nThis box allows the user to enter the number of simulations that they would like FACTS to run for each scenario listed in the table at the bottom of the simulation tab. There is no set number of simulations that is always appropriate.\n\n10 simulations\n\nYou might want to run 10 simulations if you just want to look at a few simulated trials and assess how the decision rules work and if FACTS is simulating what you expected based on what you input on the previous tabs. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\n\n100 simulations\n\nYou might want to run 100 simulations if you want to look at many individual trials to make sure that what you want to happen is nearly always happening. You can also start to get a very loose idea about operating characteristics like power based on 100 sims. 100 simulations is also usually sufficient to spot big problems with the data analysis such as poor model fits or significant bias in the posterior estimates.\n\n1,000 simulations\n\nYou might want to run 1,000 simulations if you want estimates of operating characteristics like power, sample size, and Type I error for internal use or while iterating the design. This generally isn’t considered enough simulations for something like a regulatory submission. With 1,000 simulations the standard error for a typical type I error calculation is on the order of \\(0.005\\).\n\n10,000 simulations\n\nYou might want to run 10,000 simulations per scenario if you are finalizing a design and are preparing a report. This is generally enough simulations for a regulatory submission, especially in non-null simulation scenarios. The standard error for a typical type I error calculation using 10,000 simulations is on the order of \\(0.0015\\).\n\n&gt; 10,000\n\nYou might want to run more than 10,000 simulations if you want to be very certain of an operating characteristic’s value - like Type I error. And plan to use the measurement of the quantity for something important like a regulatory submission. The standard error of a Type I error calculation with 100,000 simulations, e.g., is on the order of \\(0.0005\\).\n\n&gt; 100,000\n\nYou probably don’t want to run more than 100,000 simulations per scenario. Maybe your finger slipped an hit an extra 0, or you thought there were 5 zeroes in that number when there were actually 6. If the simulated trial is adaptive, this is going to take a while.\n\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. Not all scenarios must be run with the same number of simulations. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#start-at-simulation",
    "href": "documentation/v72/userguides/core/simulation/index.html#start-at-simulation",
    "title": "Simulation",
    "section": "Start at Simulation",
    "text": "Start at Simulation\nThe “Start at simulation” option allows for the simulation of a particular trial seen in a previous set of simulations without having to simulate all of the previous trials in that previous set to get to it.\nThe initial random seed for FACTS simulations is set in the simulation tab. The first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. Thus, it is possible to re-run a specific simulation out of a large set without re-running all of them. For example, say the 999th simulation out of a set displayed some unusual behavior, in order to understand why, one might want to see the individual interim analyses for that simulation (the “weeks” file), the sampled subject results for that simulation (the “Subjects” files) and possibly even the MCMC samples from the analyses in that simulation. You can save the .facts file with a slightly different name (to preserve the existing simulation results), then run 1 simulation of the specific scenario, specifying that the simulations start at simulation 999 and that at least 1 weeks file, 1 subjects file and the MCMC samples file (see the “MCMC settings” dialog) are output."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#parallelization-packet-size",
    "href": "documentation/v72/userguides/core/simulation/index.html#parallelization-packet-size",
    "title": "Simulation",
    "section": "Parallelization Packet Size",
    "text": "Parallelization Packet Size\nThe parallelization packet size option allows simulation jobs to be split into runs of no-more than the specified number of trials that are run in parallel. If more simulations of a scenario are requested than can be done in one packet, the simulations are broken into the requisite number of packets, run, and combined and summarized when they are all complete. The final results files will look just as though all the simulations were run as one job or packet.\nThe packet size must be a perfect divisor of the number of simulations. This is usually easy since common numbers of simulations are multiples of 100, but don’t try to use a prime number for Number of Simulations or you’re stuck with only 2 packet size options.\nBy default (if the check box with Choose Parallel Packet Size is not checked) the number of simulations per packet depends on the number of simulations per scenario. If the number of simulations is less than 1000, then each scenario is packaged as a single packed and simulated. If the number of simulations per scenario is greater than or equal to 1000, the default packet size is 10 and all simulations are decomposed into packets of size 10.\n\n\n\n\n\n\nPacketization with VSRs using .mvsr files\n\n\n\nIf an external file is used to create explicit VSRs (a .mvsr file), then the packet size should be a multiple of the number of rows in that MVSR file. Each packet will get passed the entire .mvsr file to run. If there are multiple .mvsr files with differing numbers of lines then only the VSR scenarios that have a .mvsr file that has a number of rows that is a divisor of the packet size will be run. The rest will error. The packet size can then be modified to get each of the .mvsr specified VSR files to be run.\nCare should be taken when packetizing a scenario that includes an external data file to supply the virtual subject responses; in this situation, a of copy of the external file is included in each packet which can cause the packetisation process to run out of memory as the packets are being created. In this case, use a smaller number of larger packets, such as packets that are 1/10th of the total number of simulations.\n\n\nWhen running simulations, FACTS will create and run as many packets in parallel as there are execution threads on the local machine. In general, the overhead of packetization is quite low, so a packet size of 10 to 100 can help speed up the overall simulation process. Threads used to simulate scenarios that finish quickly can pick up packets for scenarios that take longer. The progress bar updates as simulation packets complete, so the smaller the packet size, the more accurately FACTS can report the overall progress of the simulation execution."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#random-seed",
    "href": "documentation/v72/userguides/core/simulation/index.html#random-seed",
    "title": "Simulation",
    "section": "Random Seed",
    "text": "Random Seed\nRandom number generation plays a huge role in FACTS’s virtual patient generation and statistical analyses. In order to exactly reproduce a statistical set of results, it is necessary to start the random number generation process from an identical “Random Seed”. Using the same random seed in the same version of FACTS guarantees that simulated trials will always be reproducible. Changing the design parameters or the version of FACTS may or may not remove this reproducibility depending on the change.\nEven a small change in the random seed will produce very different simulation results.\nIn addition to setting the seed, the user can choose whether they want the “Same seed for all scenarios” or “Different seed” for different scenarios. If “Same seed for all scenarios” is selected, the subjects generated for each simulated trial will match for the different scenarios. This induces a correlation among the simulation output for different scenarios. This can be good if you’re trying to compare operating characteristics for different scenarios, but it can also be misleading. To disable this option select the “Different Seed” option. If “Different seed” is selected, then each scenario has its own seed that samples a different set of subjects than any other scenario. This uncorrelates the simulation output across scenarios, which can be advantageous if the absolute value of the operating characteristics are more valuable to you than the comparison of operating characteristics across scenarios."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#mcmc-settings",
    "href": "documentation/v72/userguides/core/simulation/index.html#mcmc-settings",
    "title": "Simulation",
    "section": "MCMC Settings",
    "text": "MCMC Settings\nTo set advanced settings for simulation, the user may click the “MCMC Settings” button, which will display a number of additional specifiable parameters for simulation in a separate window.\n\n\n\n\n\n\nFigure 2: Advanced MCMC settings.\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the MCMC chain to reach its stationary distribution. Burn-in samples are output in MCMC files if the files are output.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest.\n\nThe third parameter controls the number of MCMC samples taken after each imputation of missing data using the longitudinal model. The default value is 1. This parameter only has an effect if Bayesian imputation is being used to impute missing or partially observed data. Increasing the value of this parameter allows the parameter estimates to converge somewhat to a potentially new stationary distribution for each new set of imputed data. If the imputed data is only a small percentage of the overall data this is likely unnecessary. As a rough guide, if it at some early interims &gt; 5% of the data being analyzed will be imputed, a value in the range 2 to 10 is recommended to avoid underestimating the uncertainty. A higher number should be used the greater the proportion of imputed data.\nThe next parameter concerns the output of the MCMC samples to a file. It is possible to have the design engine output the sampled values from the MCMC in all of the interims of the first N simulated trials of each scenario by specifying the “Number of MCMC files to output” to be greater than 0. The resulting files, ‘mcmcNNNN.csv’, will be in the results directory with all the other results files for that scenario. These files include the burn-in samples from the MCMC chains.\nThe final parameter in MCMC Settings is the thinning parameter. This parameter will only keep every \\(N^{th}\\) sample taken during MCMC where \\(N\\) is the thinning parameter. Thinning MCMC samples can reduce the autocorrelation of consecutive MCMC iterations, which increases the effective samples per retained sample, but also results in needing many more MCMC iterations to reach the same number of retained samples. Generally, we do not recommend thinning for standard simulation runs.\n\n\n\n\n\n\nWarning about thinning\n\n\n\nUnlike other software that performs MCMC, when you choose to thin by a value, FACTS does not increase the number of MCMC iterations it performs in order to retain the value specified in “Number of Samples”. So if you leave “Number of Samples” at its default value, \\(2500\\), and thin by \\(10\\), you will be left with \\(250\\) retained samples. You should adjust for this by increasing the “Number of Samples” if you choose to thin."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#results-output",
    "href": "documentation/v72/userguides/core/simulation/index.html#results-output",
    "title": "Simulation",
    "section": "Results Output",
    "text": "Results Output\nThe results output section of the Simulation tab allows for the specification of how many output files should be generated for files that are individually created for each simulation. Summary files (summary.csv) that have 1 line per scenario are always created. Simulations files (simulations.csv) that have 1 line per simulation are always created. Weeks files (weeksXXXXX.csv), patients files (patientsXXXXX.csv), and frequentist weeks files (weeks_freq_{missingness}_XXXXX.csv) are not created for every single simulation. Instead, the number of simulation specific output files can be set per type. This limits the amount of output files that FACTS will save.\nSee the endpoint specific descriptions of the output files for descriptions of what the previously mentioned output files report. See here for core continuous or dichotomous output files, see here for core time-to-event output files, and see here for ordinal output files.\nSome plots in FACTS that are created based on weeks files, and if very few weeks files are saved, the plots will not be as accurate or descriptive."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#facts-grid-simulation-settings",
    "href": "documentation/v72/userguides/core/simulation/index.html#facts-grid-simulation-settings",
    "title": "Simulation",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nA user with access to a computational grid may choose to run simulations on the grid instead of running them locally. This frees the user’s computer from the computationally intensive task of simulating so that they can continue other work or even shutdown their PC or laptop. In order to run simulations on the grid, it must first be configured. This is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#right-click-menu",
    "href": "documentation/v72/userguides/core/simulation/index.html#right-click-menu",
    "title": "Simulation",
    "section": "Right Click Menu",
    "text": "Right Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 4: The menu that appears when you right click on the table within the simulation tab.\n\n\n\nThese will respectively:\n\nOpen a new Windows directory browser window showing the contents of the simulation results for that scenario.\nOpen a window that displays the individual simulation results for that scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results (see below) the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of the Simulation Results highlights window.\nOpen a window that displays the frequentist analysis summary results. This option is only available if one or more frequentist analyses have been selected on the Design &gt; Frequentist Analysis tab. (If more than one analysis has been requested – using different treatments of missing data there will be separate options in the menu to display each summary).\nOpen R loading in the result files for that scenario as separate dataframes.\nOpens the FACTS graph control displaying the graphs for that scenario.\nOpens the FACTS graph control that displays the trellis plot of graphs of selected scenarios for selected design variants."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#open-in-r",
    "href": "documentation/v72/userguides/core/simulation/index.html#open-in-r",
    "title": "Simulation",
    "section": "Open in R",
    "text": "Open in R\nThe “Open in R” button allows for the creation of an R script that has pre-populated code for loading in output files created by the FACTS simulations.\nBy default, any/all of the simulation output files can be included in the created script. If “Aggregation” (see below) has been performed, then only the aggregated files will be available for being loaded in R.\nWhen the button is clicked, FACTS will create an R script with the correct file paths to load in the data, as well as creating a function that will read the files in correctly. The file is then opened in the default R editor for the user. If there is no default program for opening a .R file, your operating system should ask how you want to open the file."
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#aggregation",
    "href": "documentation/v72/userguides/core/simulation/index.html#aggregation",
    "title": "Simulation",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 5: Window that appears when aggregating simulation results.\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nValues in columns that are independent of dose will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. WeeksNNNNN.csv files are aggregated into a single agg_[pivot_]weeks.csv file. PatientsNNNNN.csv files are aggregated into a single agg_patients.csv file, but they are never pivoted because each row already refers to a single dose. Similarly the various frequentist results at the summary, simulation and weeks level are aggregated (if they’ve been output).\nRegionIndex.csv is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nRecruitment Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g. External Subjects Profile if there are no external scenarios)\n\n\nDropouts Profile\n\n\n\nLongitudinal Rates Profile\n\n\n\nDose Response Profile\n\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nP(TS)\nProportion of trial success (early success + late success)\n\n\nP(TF)\nProportion of trial futility (early futility + late futility)\n\n\nSim\nSimulation number. Only present in weeks and patients files.\n\n\nDose\nOnly present if pivoted"
  },
  {
    "objectID": "documentation/v72/userguides/core/simulation/index.html#design-report",
    "href": "documentation/v72/userguides/core/simulation/index.html#design-report",
    "title": "Simulation",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found."
  },
  {
    "objectID": "documentation/v72/userguides/core/index.html",
    "href": "documentation/v72/userguides/core/index.html",
    "title": "Phase II & III Designs",
    "section": "",
    "text": "This document covers the design options that are common across the five FACTS Core Design Engines: Continuous, Dichotomous, Time-to-Event, Ordinal, and Multiple Endpoint. Some design elements are shared across design engines, in which case there is only a single description of them. Others differ based on the endpoint used, in which case separate pages have been created to describe each.\nThe screenshots provided are specific to a particular installation and  may not reflect the exact layout Screenshots from earlier versions of FACTS 6 are still used only when the tabs they show are unchanged in FACTS 7.1.  of the information seen by any particular user. They were taken from FACTS V7 & V6 installed on Windows 10 or 11. If FACTS is installed on different versions of Windows, or with different Windows ‘themes’ there will be some differences in appearance introduced by Windows. The contents of each tab, however, will generally be consistent with the screenshots in this document.\n\n\nThis is the version of the user guide for inclusion with the FACTS 7.1 release.\n\n\n\nPlease cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/core/index.html#facts-version",
    "href": "documentation/v72/userguides/core/index.html#facts-version",
    "title": "Phase II & III Designs",
    "section": "",
    "text": "This is the version of the user guide for inclusion with the FACTS 7.1 release."
  },
  {
    "objectID": "documentation/v72/userguides/core/index.html#citing-facts",
    "href": "documentation/v72/userguides/core/index.html#citing-facts",
    "title": "Phase II & III Designs",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/multendpt.html",
    "href": "documentation/v72/userguides/core/qois/multendpt.html",
    "title": "Multiple Endpoint QOIs",
    "section": "",
    "text": "The multiple endpoint engine allows the user to specify the quantities of interest for each endpoint in the same way that the continuous and dichotomous engines allow for specifications of QOIs.\nThe QOI tab in the multiple endpoint engine has a set of sub-tabs that are not present in the single endpoint engines. All of the sub-tabs except the last correspond to one of the endpoints. These tabs create QOIs that are only calculated for a single endpoint.\nThe last sub-tab on the multiple endpoint QOI page is the “Utility” tab. The utility tab exists so that decision making quantities can be calculated base on the distribution of the overall utility.\n\n\n\n\n\n\nFigure 1: Execution &gt; Quantities of Interest &gt; Utility tab for multiple endpoints.\n\n\n\n\nUtility QOIs\nSince the utility of an arm has a distribution, as discussed here, FACTS can calculate probabilistic quantities based on that utility. Examples of this include (among others):\n\nPr(U_d &gt; 0) - The probability that an arm’s combined utility is greater than 0\nPr(U_d - U_(Control) &gt; 1) - The probability that an arm’s conbined utility is greater than the control arm’s combined utility by at least 1 point.\nPr(UMax) - The probability that each arm has the highest utility among all arms.\nPr(UMin relative to Control: Delta=-1) - The probability that each dose is the dose with the smallest utility that is greater than the control utility minus 1.\n\nPredictive probabilities and p-values based on utilities are not supported.\nThe Decision Quantities on the “Utility” tab operate in the same way as they do in the other core engines. In order to get a single value of a QOI, rather than a vector of QOIs, you must specify a decision QOI, which includes the method of selecting a scalar value from the vector QOI.\nThe standard evaluation variables section of the QOI tab only contains an entry for the Clinically significant minimum utility (CSMU). This value only changes the value of the default posterior probability utility QOI that is created automatically."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/index.html",
    "href": "documentation/v72/userguides/core/qois/index.html",
    "title": "Quantities of Interest",
    "section": "",
    "text": "The QOI (Quantities Of Interest) tab allows the user to specify the Bayesian posterior quantities and frequentist p-values that are eligible to be used to make decisions in the trial as well as being output to the simulations results files.\nThere are 3 classes of QOI:\nTrial decisions at interim analyses or the final analysis are made based on decision quantities, but adaptations like adaptive allocation can be based on posterior probabilities, predictive probabilities, and target probabilities.\nNote that to creating a QOI for early stopping or final evaluation decisions will involve using 2 or 3 QOIs:\nThere are a number of pre-defined, default QOIs which simplifies the specification of the most commonly used decision quantities, and the importation of past FACTS designs. These are:"
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/index.html#posterior-probabilities",
    "href": "documentation/v72/userguides/core/qois/index.html#posterior-probabilities",
    "title": "Quantities of Interest",
    "section": "Posterior Probabilities",
    "text": "Posterior Probabilities\nThese are Bayesian quantities to be calculated at each interim and at the final analysis.\n\n\n\n\n\n\nFigure 2: Add dichotomous posterior probabilities page.\n\n\n\nA Posterior Probability is specified by providing a value for each of the following:\n\nCompare:\n\nContinuous: Means\nDichotomous: Rates or Log-odds\nTime-to-Event: Hazard Ratio or Hazard Rates.\n\nCondition: “&gt;” or “&lt;” a comparison value.\nRelative to an absolute value or relative to the response on a specific dose.\nThe comparison can include a delta, which is the absolute value to be compared against if t the difference relative to the comparison arm is compared to. he comparison is absolute, or a value that\n\nThe QOI will be given a “name” derived from these details and a short or Alternative name that will be used in the output files to allow easier access from other software, e.g. from within R.\nIf the endpoint is TTE and the design includes a predictor endpoint, then the definition of the QOI includes the specification of which endpoint it applies to.\n\n\n\n\n\n\nSpecial Qualities of Posterior Probabilities in Time-to-Event\n\n\n\n\n\nThere are a couple of aspects of posterior probabilities that are slightly different for posterior probability QOIs.\nFirst, in FACTS Core TTE a Posterior Probability QOI (a Bayesian comparison of estimates of response) can be used to compare Hazard Ratios or Hazard Rates. In their descriptions Hazard Ratio QOIs use “HR” (e.g. “HRd &lt; 1”) and Hazard Rate QOIs use \\(\\lambda\\) (e.g. “\\(\\lambda_d\\) &lt; \\(\\lambda_{control}\\)”). If there are hazard rate QOIs defined, then FACTS restricts the hazard model (defined on the Design &gt; Hazard Model tab) to use just a single segment (so that there is just one lambda to compare). This is set in the “Com are” box of the QOI definition window:\n\n\n\n\n\n\nFigure 3: Specifying a posterior probability QOI based on hazard rate.\n\n\n\nAdditionally, in FACTS Core TTE when defining a Posterior Probability QOI (a Bayesian comparison of estimates of response) or a Target QOI (a Bayesian assessment of which treatment arm is most likely to fulfill a ‘target’ criteria) – the user can select whether the evaluation uses the Final Event endpoint, or the Predictor.\n\n\n\n\n\n\nFigure 4: Specifying a posterior probability QOI based on the predictor endpoint.\n\n\n\n\n\n\n\nNotes on setting Deltas\nFor the three endpoint types, delta’s are defined as:\n\nContinuous\n\nA CSD (Clinically Significant Difference) in the estimates of the mean response.\n\nDichotomous\n\nA CSD in the estimate of the response rates if Rates is selected in the QOI, and of log-odds of the response rate if Log-odds is selected in the QOI.\n\nTime-to-Event\n\nA CSHRD (Clinically Significant Hazard Ratio Difference) in the estimate of the Hazard Ratio\n\n\nA standard hypothesis test for demonstrating superiority to control uses a delta of 0. Testing for superiority with a non-zero delta is different, and the implications need to be  carefully understood Simulating its use in FACTS is a good way to achieve that understanding! . Where the term CSD is used in this document, it should be taken to refer to CSHRD as well unless it is specifically stated otherwise.\nWhen setting a target treatment difference from control for the study to beat, it is also common to require a degree of confidence that the target has been beaten. To achieve posterior probabilities of &gt;50% that the target has been beaten, the estimated mean difference will have to be greater than the target difference.\nThus, when setting a delta, we are setting up two hurdles the study drug must beat, first the delta and then on top of that an additional margin to give a &gt;50% confidence that the margin has been beaten. Thus, it is necessary to avoid setting the target delta too large. A common mistake is to set the delta to the ‘expected difference’ (the value that might have been used in a conventional sample size calculation). In scenarios where the simulated response is equal to the ‘expected difference’ and hence the CSD this will give probabilities of being better than control by the delta of 50% on average, regardless of the sample size.\nIt is inadvisable to require a posterior probability of 50% that the response is better than the Control by the delta margin as this turns the test into one that simply depends on whether the point estimate of the response is better.\nIt is inadvisable to require posterior probability of less than 50% for success, as such criteria have the undesirable characteristic that they can be met in circumstances where it can be seen that if further data was gathered consistent with what has already been seen, it would lead the threshold no longer being met! The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a delta that is less than the expected difference that it is hoped to achieve, somewhat like having a non-inferiority margin around the target. So, when the design is simulated with a response at the expected difference the target will be clearly exceeded. A useful default value to use for a delta is half the ‘expected difference’. This usually yields “comprehensible” probability thresholds for both success and futility.\nUsing a delta has benefits however – simply being better than control but for very small difference can be established simply by having large sample sizes. Being better than control by a delta establishes some confidence that there is some patient benefit. Use of a delta is also useful if otherwise success thresholds “well out in the tail” (e.g. &gt; 0.99) are required such as the equivalent of an early look in a conservative Group Sequential design. Though the use of a delta does mean that there is a less direct equivalence to a p-value.\n\n\nP-value Delta’s\nSeparately from the CSD/CSHRD delta, with a continuous or dichotomous endpoint (but not time-to-event), a frequentist super-superiority/non-inferiority delta can be specified.\nThese use the same selection of super-superiority/non-inferiority as the CSD\n\n\n\n\n\n\nFigure 5: The “Standard Evaluation Variables” options at the bottom of the QOI page.\n\n\n\nCurrently, unlike the CSD delta that is only applied to the default posterior probability QOI and MED target QOI, the p-value delta is applied to all the p-value QOIs and it cannot be overridden.\nThe value of the p-value delta is constrained to be 0 or positive. The ‘direction’ of the delta depends then on the direction of the endpoint (whether “higher/lower is better” or “a response is a positive/negative outcome”).\n\n\n\nSuper superiority and non-inferiority margin directions.\n\n\n\n\n\n\n\n\n\n\n\n\nHigher is better / Response is positive\n\n\nLower is better / Response is negative\n\n\n\n\n\n\nSuper-Superiority\n\n\nTrt – Control &gt; delta\n\n\nTrt – Control &lt; -delta\n\n\n\n\nNon-inferiority\n\n\nTrt – Control &gt; -delta\n\n\nTrt – Control &lt; delta\n\n\n\n\n\n\nP-value Comparisons with No Control Arm\nIf no control arm is included p-values, with a continuous or dichotomous endpoint, the treatment response is compared to a fixed value. The fixed value is specified as “Frequentist response/rate to compare to for p-value QOIs:” in the Standard Evaluation Variables section at the bottom of the QOIs tab. It is not currently possible to compare different p-value QOIs to different fixed responses or rates."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/index.html#predictive-probabilities",
    "href": "documentation/v72/userguides/core/qois/index.html#predictive-probabilities",
    "title": "Quantities of Interest",
    "section": "Predictive Probabilities",
    "text": "Predictive Probabilities\nThere are two types of predictive probabilities –\n\nBayesian predictive probabilities, which are Bayesian predictions of frequentist outcomes and take into account the uncertainty around a set of parameters, and\nConditional Power, which aims to calculate the frequentist probability of success assuming a set of parameters to be true.\n\nThe primary difference between the Bayesian predictive probabilities and the conditional power calculations is that the Bayesian predictive probabilities are calculated taking the variability around the estimated treatment effect into account, while the conditional power calculations assume that the observed test statistic of past data is the exact effect that future data will be generated from.\nFor both Bayesian predictive probabilities and conditional power, we differentiate two types, predicting the outcome in the current trial and predicting the outcome in a future trial.\n\nBayesian predictive probabilities\n\nCurrent Trial Bayesian Predictive Probabilities\nIn the current trial, the outcome can be predicted under one of two assumptions:\n\nThat no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nThat the trial continues recruiting using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until complete.\n\nPredictive probabilities currently can only predict outcomes based on p-values. Note that since p-values are only calculated as comparisons against a control arm, predictive probabilities of success in the current trial are only available if the current trial includes a control arm.\n\n\n\n\n\n\nFigure 6: Add dichotomous predictive probability of current trial.\n\n\n\nThe user specifies how missingness is handled in the final analysis, the p-value test type – unadjusted, Bonferroni or Dunnett’s (Dunnett 1955) and the (one sided) alpha level for the significance test. The QOI assumes that the specified default p-value delta is used in the final test.\nThe predictive probability of the current trial at the maximum sample size is only available:\n\nIf the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation.\n\nThe predictive probability is calculated by simulating the remaining subjects, assuming their allocated at the current probabilities of allocation and simulating their final response based on the rate of response or (Normal) distribution of responses observed so far for each arm.\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\nThere is one simulation of the remaining data per MCMC loop, potentially significantly increasing the time to perform the overall simulation of the trial.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still perform the simulation of future subject outcomes based on posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\nCurrent Trial Bayesian Predictive Probabilities – Time-to-Event\nUnlike the Continuous and Dichotomous endpoints, to predict the probability of success at full enrollment with a Time-to-Event endpoint, it is also necessary to simulate the accrual rate (and hence how many events might be observed).\n\n\n\n\n\n\nFigure 7: Add time-to-event predictive probability of current trial.\n\n\n\nFor TTE, for a Predictive Probability of Success at Full Enrollment, there are new parameters to determine how accrual is modeled. There are 3 models for accrual\n\nFixed Rate, the parameters for this are:\n\nThe fixed (mean) accrual rate per week to simulate.\n\nEstimated from the Last ‘W’ Weeks accrual data, using a Poisson distribution, with a Gamma prior. The parameters for this are\n\nThe number of past weeks W to use the accrual data from.\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\nEstimated from all the accrual data from the start of the trial, using a Poisson distribution, with a Gamma prior, the parameters for this are:\n\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\n\n\n\nFuture Trial Bayesian Predictive Probabilities\nFor predictive probabilities for a future trial, a predictive probability of success in a subsequent phase 3 trial is estimated during the current trial. The probability of success in phase 3 is calculated for each study arm, based on the specification of the phase 3 trial given here of:\n\nwhether the aim is to show superiority or non-inferiority,\nthe sample size per arm,\nthe required one-sided alpha,\nand the super-superiority margin or non-inferiority margin (if any).\n\nGiven these criteria, FACTS calculates the predicted probability of success of the subsequent trial for that endpoint assuming the estimate of the response on that endpoint, integrated over the uncertainty in those estimates. The conventional expected power of the specified future trial is calculated for the treatment effect in each MCMC sample and then averaged. This predicted probability of success can then be used in the stopping criteria and final evaluation criteria for the current trial.\nThis QOI is an extension of the “probability of success in phase 3” in earlier versions of FACTS – the difference now is that multiple different possible future trials can be specified in different QOIs and used for different decisions.\n\n\n\n\n\n\nFigure 8: Add predictive probability of future trial non-inferiority.\n\n\n\nThis predictive probability has the following parameters that must be specified:\n\nWhether the future trial will be for Superiority or Non-inferiority.\nThe size of the future trial in terms of the number of subjects on each arm.\nThe (one sided) alpha level that will be used to determine the significance of the trial.\nThe Super-superiority margin (if any) or the non-inferiority margin. The default p-value delta is not used for this QOI it is specified as part of the QOI and can be different from the default.\n\nAs with all QOIs, the future trial predictive probability QOI will be given an alternative shorter name that can be used when accessing the output files from other software such as R.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still calculate the predictive probability of the future trial being a success based on the estimated posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\n\nConditional Power\nConditional power is currently available for Continuous and Dichotomous endpoints in Core and Staged Designs.\nWhen creating a Conditional Power QOI, the first specification that must be made is whether the conditional power should be calculated for the current trial or for a hypothetical future trial. The two selections are fundamentally different.\nThe Current Trial conditional power takes the information collected in the trial so far, and assumes that for the remainder of the current trial the data collected has an effect equal to the point estimate of the already collected data. The previously collected trial data is then combined with hypothetical future data and weighted based on the amount of information that has been and will be observed by the end of the trial. The result is the probability that the current trial will reject the null hypothesis at a provided alpha level under the assumption that the future patients are generated from a population having a treatment effect exactly equal to the past data’s test statistic.\nThe Future Trial conditional power also uses the information accrued in the trial so far to calculate the assumed treatment effect moving forward, but assumes that a new study would be started that does not use the data from the current trial in its analysis. The new study is assumed to be a 1:1 randomized study if there is a control arm, and a single arm study if no control, with a sample size per arm, objective, and alpha level specified at the time of QOI creation.\nIf a conditional power is ever calculated for an arm with no final response data or against a control arm with no final response data, the conditional power will be called missing (-9999 in FACTS).\n\nCurrent Trial Conditional Power\nWhen creating a current trial conditional power, the missingness strategy, multiplicity adjustment, final sample size, and alpha level for the significance test must be specified.\n\n\n\n\n\n\nFigure 9: Add a conditional power of the current trial QOI.\n\n\n\n\nHandle missingness using:\nMissingness handling for a continuous endpoint can be specified as:\n\nIgnore: subjects that are known dropouts are not included in the current or final sample size, and are not used to estimate the treatment effect.\nLast Observation Carried Forward (LOCF): subjects that are known dropouts have their last observed endpoint value assumed to be their final endpoint. Subjects with no early observed data are ignored. Subjects that have data to carry forward are considered complete, and are used to estimate the current treatment effect.\nBaseline Observation Carried Forward (BOCF): subjects that are known dropouts have their baseline value assumed to be their final endpoint. They are included in the current information as complete subjects and are used to estimate the current treatment effect. BOCF is only available for the continuous endpoint and when simulating baseline for subjects.\nFailure: subjects that are known dropouts are assumed to have a negative outcome at their final visit. If responses are good, the subject is a non-response, and if responses are bad, the subject is a responder. Failure is only available for dichotomous endpoints.\n\n\n\nTest Type\nThe test type can either be Unadjusted or Bonferroni. An unadjusted test will always use the specified alpha level in the significance test. The Bonferroni adjustment will divide the specified alpha value by the number of non-control arms enrolling in the study in the enrolment period leading up to the current analysis time.\n\n\nSample Size:\nThe current trial conditional power can be calculated at two different future time points.\n\nCurrent Enrollment: That no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nTrial Maximum: That the trial would continue recruiting subjects using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until they have the opportunity to complete.\n\n\n\nOne-sided Alpha\nThe threshold that the current trial would have to be less than in order to be considered a success. The success/futility values specified in the design tab are not used in the conditional power calculation. This value may be adjusted by the “Test type:” input.\n\n\nSuper-Superiority (Non-inferiority) margin for p-value:\nThis value cannot be changed on the QOI pop-up. It’s instead modified at the bottom of the QOI page. The objective of the current trial cannot be different for each individual current trial conditional power QOIs.\n\n\nAdditional Notes\nCurrently, conditional power will always assume that there is no correction applied to combine the different p-values from the analysis timepoints, i.e. no combination test is used.\nThe conditional power of the current trial at the maximum sample size is only available if the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation or deterministic allocation.\nConditional power for the current trial is calculated\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\n\n\n\nFuture Trial Conditional Power\nConditional power of a future trial is reduced to a power calculation for a future trial, given the current frequentist estimates of treatment effect and standard deviation and a set of assumptions for the future trial, such as sample sizes, hypothesis to be tested, and significance levels to be used.\nThe test type of the future trial can be set to Superiority or Non-inferiority. If the superiority or non-inferiority margin is set to 0, then both types of tests simplify to traditional tests of superiority with no margin. If a non-zero margin is set, then the future trial must meet the success criteria indicated by the test type and the margin. The margin cannot be negative, but a super superiority test with a margin of -0.5 is equivalent to a non-inferiority test with a margin of 0.5.\nThe subjects per arm dictates how many subjects would be enrolled on each arm in the future study. If there is a control arm in the current trial, then the future trial is assumed to be randomized 1:1 between the active arm and the control. If there is no control arm in the current study, then the future trial is a single arm trial testing against the performance goal specified in the Freq. Comparison Response box in the QOI creation pop-up.\nThe One-sided Alpha is the significance level of the final analysis test of the future trial. The future trial is assumed to have 1 active arm, so there are no adjustments to the alpha level available.\nThe superiority margin or non-inferiority margin indicate the amount that the active arm must be better/not worse than the control arm by. If there is no control arm then this is the Freq Comparison response, which is the value that the active arms must be significantly better than in order to be declared a success. The frequentist margin for p-value QOIs on the QOI tab is not used for Future conditional power calculations – the future trial can have a different objective than the current trial.\n\n\n\n\n\n\nFigure 10: Add a conditional power of a future trial QOI.\n\n\n\n\n\nTechnical Aspects of Conditional Power Calculations\nThe conditional power calculations in FACTS are all calculated similarly to Jennison and Turnbull (2000).\nFor continuous endpoint conditional power calculations, the conditional power of the future trial is calculated using a pooled standard deviation from all arms. For dichotomous conditional power calculations, each arm has its own standard deviation based on the MLE estimate of its proportion. These decision result in the conditional power tests matching how standard p-value QOIs are calculated for continuous and dichotomous endpoints.\nThe following sections will provide formulae to calculate the conditional power in the case where there is a control arm. Simplifications for when the arm is being compared to a fixed value rather than a control arm are trivial: information fractions are only a function of the active arm standard deviation and the test statistic does not have a second sample mean subtracted from the active arm.\nThe value of \\(\\delta\\), which can be a non-inferiority margin or a super superiority margin, is always positive coming out of FACTS. To make the math more concise we can define a couple of coefficients for the \\(\\delta\\) term that allow it to be used without re-writing formulas for different hypothesis test setups. If high values of the endpoint are good, then \\(s_1 = 1\\), and if low values of the endpoint are good, then \\(s_1=−1\\). If the specified \\(\\delta\\) is a non-inferiority margin, then \\(s_2 = 1\\), and if it’s a super superiority margin then \\(s_2=-1\\).\n\nContinuous Conditional Power for the Current Trial\nLet t be the interim index that the conditional power is being computed at, and T be the time of the analysis that the conditional power is being computed for. Then \\(Z_k\\) is the test statistic of the data collected up to the current interim analysis in the study, \\(I_k\\) is the information level at the time of the interim analysis, and \\(I_K\\) is the information level at the end of the study that the conditional power is being calculated for.\nLet arm 1 be the control and arm 2 be the active arm, \\(\\bar{x_{it}}\\) be the sample mean of arm \\(i\\) at time \\(t\\), \\(\\widehat{\\sigma_{i}^{2}}\\) be the sample variance of arm \\(i\\) at time \\(t\\), \\(n_{it}\\) be the number of subjects with complete known final data on arm \\(i\\) at interim analysis \\(t\\), and \\(n_{iT}\\) be the number of subjects with complete known final data on arm \\(i\\) at the time that conditional power is being calculated for. The pooled variance estimate is \\(\\widehat{\\sigma^{2}} = \\sum_{d = 1}^{D}\\widehat{\\frac{\\sigma_{d}^{2}}{n_{dt}}}\\) where D is the total number of arms in the study.\nThen,\n\\[I_{t} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1t}} + \\widehat{\\frac{\\sigma^{2}}{n_{2t}}} \\right)^{-1}\\]\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1T}} + \\widehat{\\frac{\\sigma^{2}}{n_{2T}}} \\right)^{-1}\\]\n\\[Z_{t} = \\left( {\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{1}s_{2}\\delta \\right)\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the non-inferiority or super superiority margin.\nThen for a one-sided alpha level of \\(\\alpha\\), let \\(Z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Continuous Conditional Power for a Future Trial\nMost of the future trial conditional power calculation is the same as the above current trial conditional power. There are some simplifications.\n\\({\\overline{x}}_{it}\\) and \\(\\widehat{\\sigma_{i}^{2}}\\) are the same as in the current conditional power calculation. \\(I_t\\), the weight of the current trial Z-score, is set to 0. \\(I_T\\) is now the information at the end of the future trial, and is calculated as:\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{T}} + \\widehat{\\frac{\\sigma^{2}}{n_{T}}} \\right)^{- 1}\\]\nwhere \\(n_T\\) is the sample size per arm in the future trial and again \\(\\widehat{\\sigma^{2}}\\) is the pooled variance.\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for the Current Trial\nThe dichotomous conditional power calculations are similar in spirit to the continuous conditional power calculations. One substantial difference is in how the dichotomous conditional power tests handle a non-inferiority or super superiority margin, \\(\\delta\\). The method of calculating an appropriate test statistic for a frequentist hypothesis test is not obvious when the null scenario has a non-zero \\(\\delta\\).\nWhen there is no margin, the estimate for each treatment is simply based on the observed response proportion \\(\\widehat{p_{i}}\\) for arm \\(i\\), and the test statistic for a comparison of the control arm, \\(c\\), with dose \\(d\\) is the usual Wald test\n\\[Z_{d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}}}{\\sqrt{\\frac{\\widehat{p_{d}}(1 - \\widehat{p_{d}})}{n_{d}} + \\frac{\\widehat{p_{c}}(1 - \\widehat{p_{c}})}{n_{c}}}}\\]\nWhen there is a margin, FACTS uses the Farrington-Manning Likelihood Score test statistic to estimate quantities \\(\\widetilde{p_{d}}\\) and \\(\\widetilde{p_{c}}\\) based on the MLEs of the arm proportions governed by the constraint that \\(\\widetilde{p_{d}} - \\widetilde{p_{c}} = - s_{1}s_{2}\\delta\\). These constrained MLE estimates are used in the standard error of the test statistic. The FM test statistic is then,\n\\[Z_{FM,d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}} + s_{1}s_{2}\\delta}{\\sqrt{\\frac{\\widetilde{p_{d}}(1 - \\widetilde{p_{d}})}{n_{d}} + \\frac{\\widetilde{p_{c}}(1 - \\widetilde{p_{c}})}{n_{c}}}}\\]\nSee the PASS documentation (NCSS 2025) or the SAS documentation (SAS Institute 2016) for a complete description of the calculations that go into the FM test. In the SAS documentation, note that the FM test is the same as the Miettinen-Nurminen test without including the \\(\\frac{n}{n - 1}\\) variance correction. The FM test was used rather than the MN test because as \\(\\delta \\rightarrow 0\\), the FM test converges to the simple Wald test.\nOnce the test statistic has been resolved, the dichotomous conditional power for the current trial is calculated as follows. For calculating the conditional power for arm 2 compared to the control arm, called arm 1 without loss of generality, let \\(I_t\\) be the current information amount and \\(I_T\\) be the amount of information that the conditional power is being calculated for. Then,\n\\[I_{t} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1t}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2t}} \\right)^{- 1}\\]\n\\[I_{T} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1T}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2T}} \\right)^{- 1}\\]\n\\[Z_{t} = \\left( \\widehat{p_{2}} - \\widehat{p_{1}} + s_{1}s_{2}\\delta \\right)*\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the super superiority or non-inferiority margin, and \\(n_{1t}\\) and \\(n_{2t}\\) are current number of completers on the control and active arm, and \\(n_{1T}\\) and \\(n_{2T}\\) are the number of completers that will be on the control and active arm at time that the conditional power is being calculated for. Additionally, if there is no non-inferiority or super superiority margin \\(\\delta\\), then all \\(\\widetilde{p_{*}}\\) values are equal to their corresponding \\(\\widehat{p_{*}}\\) values.\nFor a one-sided alpha level of \\(\\alpha\\), let \\(z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for a Future Trial\nMost of the calculation quantities for the future trial conditional power are the same as the current trial conditional power. The only difference is that for the future trial conditional power we discard the influence of the current trial test statistic on the final analysis, so \\(I_t=0\\). Then the conditional power calculations become:\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]"
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/index.html#p-values",
    "href": "documentation/v72/userguides/core/qois/index.html#p-values",
    "title": "Quantities of Interest",
    "section": "P-values",
    "text": "P-values\nA p-value QOI simply allows the user to specify which test type to be used (unadjusted, Bonferroni, Dunnett’s (Dunnett 1955), or Trend Test), and how missing data is to be handled (ignored, LOCF,  BOCF Only if continuous endpoint and baseline is being simulated. , and  missing is failure Dichotomous endpoint only. ). If a control arm is present, p-values are comparisons against the control arm. If there is no control arm present, p-value QOIs are comparisons against a fixed response or fixed rate that is specified at the bottom of the QOI tab (it cannot be set differently for separate QOIs).\nNote that for dichotomous endpoints p-value are calculated using the Test of Proportions, this test statistic asymptotically approaches a normal distribution, and at least  5 success and 5 failures Sometimes this is said to be 10 and 10, or 15 and 15. Your rule of thumb can be based on your comfort level for allowing CLT to kick in.  should be observed for this to be reasonable.\nThe p-values are calculated including the default frequentist delta that has been specified at the bottom of the screen. The delta margin cannot be modified as part of the QOI, the value of the delta and nature of the test is the same for all the p-value QOIs.\nIn a TTE design with a predictor, the p-values are only calculated for the final event endpoint, not the predictors.\n\n\n\n\n\n\nFigure 11: Add a p-value QOI.\n\n\n\n\nP-values when there is no control arm\nIf there is no control arm (not currently an option in Time-to-Event), the p-value delta is instead interpreted as the objective response or response rate to compare to. The option to set the trial type is also disabled (“Superiority” is selected and the “non-inferiority option is greyed out). For a non-inferiority trial comparing to an objective response or response rate simply use the objective rate minus the non-inferiority delta (if a higher response is better) or the objective rate plus the non-inferiority delta (if a lower response is better).\nIt is currently only possible to have one objective rate to compare against.\nThe same objective rate will be used for the target p-value test in the predictive probabilities.\n\n\n\n\n\n\nFigure 12: The Core Dichotomous engine QOI tab when no control arm is included.\n\n\n\n\n\nFisher-Exact Test\nWhen specifying the QOIs for a dichotomous endpoint in a trial with a control arm, the bottom of the QOI tab allows the user to specify what statistical test to use when calculating p-values. Options are “Normal approximation” (default) and “Fisher exact test”. “Normal approximation” uses a t-test and should yield similar results to a Chi-Square test without continuity correction, while “Fisher exact test” uses an exact Fisher’s exact test for 2x2 tables.\nIf “Fisher exact test” is chosen as the test type, all p-value QOIs will use a Fisher’s exact test, except for future trial predictive probabilities, which will still use a t-test.\nIf “Fisher exact test” is chosen as the test type, only “Bonferroni” and “Unadjusted” are available as multiplicity corrections and any QOIs previously created using different multiplicity corrections will be deleted.\n“Fisher exact test” is not available for non-inferiority comparisons.\n\n\n\n\n\n\nFigure 13: The Core Dichotomous engine QOI tab showing the Fisher exact test selected on the bottom."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/index.html#target-doses",
    "href": "documentation/v72/userguides/core/qois/index.html#target-doses",
    "title": "Quantities of Interest",
    "section": "Target Doses",
    "text": "Target Doses\nThe target dose QOIs are Bayesian posterior probabilities based on MCMC sampling of how often each dose meets the specified target criteria. There are 3 types of target specifiable\n\nMax – the dose with the maximum response, this has no parameters to specify, so is available as a pre-defined default QOI.\nMED – a Minimum Effective Dose, the lowest dose that has a response better than an absolute or relative target.\nEDq – an effective dose, the dose that achieves a specified proportion (quantile \\(q\\)) of the maximum improvement in response relative to an absolute value or the performance of a specific treatment arm.\n\n\n\n\n\n\n\nFigure 14: Add a probability of being target QOI."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/index.html#decision-quantities",
    "href": "documentation/v72/userguides/core/qois/index.html#decision-quantities",
    "title": "Quantities of Interest",
    "section": "Decision Quantities",
    "text": "Decision Quantities\nThe QOIs described so far have defined values to be calculated across all doses. For a Success/Futility decision to be made it is necessary to specify the treatment arm whose QOI value is to be used in comparison to the success and/or futility criteria. This selection can be done by specifying a specific arm, but normally it is specified by using a “Probability of being Target” selection criteria. A simple and common example is “The probability of the response being better than that of the Control of the Treatment arm with the maximum response”:\n\n\n\n\n\n\nFigure 15: Add a decision quantity QOI for the posterior probability of benefit over the control arm.\n\n\n\n\n\n\n\n\n\nFigure 16: Add a decision quantity QOI for the minimum effective dose.\n\n\n\nA decision QOI consists of (1) a QOI that has been calculated for each dose, and (2) a method of choosing a dose to use the QOI value of. Choosing the dose can be done either by using a target Dose QOI like Pr(Max), Pr(EDq…), etc, by choosing the dose with the highest or lowest value of a QOI, or by explicitly choosing a dose level in advance.\nAs an example using a target QOI, you can imagine evaluating a decision QOI that is specified to choose the probability of being better than Control by 2 units Pr(\\(\\theta_d - \\theta_0 &gt; 2\\)) based on the arm with the highest ED90 EDq relative to control; Quantile 0.9.\nInstead of a Target Dose QOI, it is also possible to specify a specific dose, or to use the terms “Max probability over all doses” and “Min probability over all doses” which mean that the dose with the minimum or maximum value of the per dose QOI is used. This enables:\n\nDecisions QOIs testing specific doses, for example by defining a Posterior probability QOI that compares all doses against the lowest dose, and then a Decision QOI using that posterior probability with the highest dose selected as the target dose it is possible to based decisions on the posterior probability that the highest dose is better than the lowest dose.\nA Decision QOI using “Max probability over all doses” allows a decision to be based on whether a probability or p-value is greater than a threshold at any dose, or less than a threshold at all doses.\nA Decision QOI using “Min probability over all doses” allows a decision to be based on whether a probability or p-value is less than a threshold for any dose or greater than a threshold at all doses.\n\nThere is one special case: if the QOI chosen is the Trend Test p-value, then because this is not actually a per-dose value, only one “Target dose” can be selected: “Overall Significance”."
  },
  {
    "objectID": "documentation/v72/userguides/core/qois/index.html#standard-evaluation-variables",
    "href": "documentation/v72/userguides/core/qois/index.html#standard-evaluation-variables",
    "title": "Quantities of Interest",
    "section": "Standard Evaluation Variables",
    "text": "Standard Evaluation Variables\nThese 2 parameters are used across some of the default QOIs and hence specified outside the normal QOI dialog boxes.\n\nThe CSD value\nand whether absolute or relative to the Control arm\n\nthese are used in both the default “Pr(CSD)” and the “MED relative to Control” QOIs.\n\n\n\n\n\n\nFigure 17: Standard Evaluation Variables for specifying the clinically significant difference (CSD) and the default QOI comparison type.\n\n\n\nNote that the CSD value here is designed to be usually entered as a positive value, as in earlier versions of FACTS. Its sign is automatically adjusted if “lower score means subject improvement” or if the trial is a non-inferiority trial.\nThese adjustments are not made for other user entered QOIs. The directions of comparison both for the definition of the probability to be calculated and the comparison of the resulting probability with a threshold are under the user’s control, as are whether delta’s are negative or positive. This allows the user to define QOIs in whatever fashion is natural to them and their team.\n\nThe direction of comparison for default QOIs\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the NIM must always be a positive value. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD will need to be subtracted from the control score before comparing with the estimate of response on a treatment arm)."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html",
    "href": "documentation/v72/userguides/flfll.html",
    "title": "FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems.\n\n\n\nFLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#purpose-and-scope-of-this-document",
    "href": "documentation/v72/userguides/flfll.html#purpose-and-scope-of-this-document",
    "title": "FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#overview",
    "href": "documentation/v72/userguides/flfll.html#overview",
    "title": "FLFLL",
    "section": "",
    "text": "FLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#linux",
    "href": "documentation/v72/userguides/flfll.html#linux",
    "title": "FLFLL",
    "section": "Linux",
    "text": "Linux\n\nInstall Mono version 6.12 or later from https://www.mono-project.com/docs/about-mono/releases onto the target machine/server running FLFLL and ensure that all users of FLFLL can run Mono. A simple test would be to ask FLFLL users to run “mono –version”.\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” (via Mono) present in the application folder.\nRetrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server.\nWithin the FLFLL application folder, go to the “bin” folder (which contains the Linux engines used by FLFLL) and elevate the Linux engine permissions to being executable by running “chmod +x [name of linux engine executable here]” for each of the Linux engines. If you do not have permission to do so, please ask your IT administrator to run this command."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#windows",
    "href": "documentation/v72/userguides/flfll.html#windows",
    "title": "FLFLL",
    "section": "Windows",
    "text": "Windows\n\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL. We recommend using 7Zip (https://www.7-zip.org/) to perform the unzipping rather than the Windows in-built unzipping tool: the latter can result in the corruption of the FLFLL application as a security precaution.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” present in the application folder.\nIf the machine/server running FLFLL already has a licensed version of FACTS installed on it, the following step can be skipped. Otherwise, retrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#linux-1",
    "href": "documentation/v72/userguides/flfll.html#linux-1",
    "title": "FLFLL",
    "section": "Linux",
    "text": "Linux\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\nmono \"FLFLL.exe\" -file \"home/mono/FLFLL/Input/myfile.facts\" -nSim 1\n-seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n\"home/mono/FLFLL/Output\" -logPath “/home/mono/Log”\nRun FLFLL to generate parameter files only for multiple FACTS project files in a single directory:\nmono \"FLFLL.exe\" -file \"home/mono/FLFLL/Input \" -nSim 1 -seed 3500\n-nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n\"home/mono/FLFLL/Output\" -logPath “/home/mono/Log”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS files older than FACTS 6.2."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#windows-1",
    "href": "documentation/v72/userguides/flfll.html#windows-1",
    "title": "FLFLL",
    "section": "Windows",
    "text": "Windows\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\nFLFLL.exe -file “C:\\MyDocuments\\FLFLL\\Input\\myfile.facts” -nSim 1\n-seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n“C:\\MyDocuments\\FLFLL\\Output\" -logPath “C:\\MyDocuments\\Log”\nRun FLFLL to generate parameter files only for multiple FACTS project files:\nFLFLL.exe -file “C:\\MyDocuments\\FLFLL\\Input” -nSim 1 -seed 3500\n-nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n“C:\\MyDocuments\\FLFLL\\Output\" -logPath “C:\\MyDocuments\\Log”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS file older than FACTS 6.2."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#overview-1",
    "href": "documentation/v72/userguides/flfll.html#overview-1",
    "title": "FLFLL",
    "section": "Overview",
    "text": "Overview\nUsage: FLFLL.exe [options] where [options] are:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n-[h|help]\nDisplay the help menu. Default (False).\n\n\n-[f|file]\nSpecifies the file or top-level directory to open.\n\n\n-[n|nSim]\nNumber of simulations to run. Default (5).\n\n\n-[p|packet]\nPacket size for parallelization. Default (1000).\n\n\n-[g|grid]\nFlag indicating sims to run on grid. Default (False).\n\n\n-[a|agg]\nAggregation mode. Default (None).\n\n\n-[aggPrefix]\nPrefix for aggregated files. Default (agg).\n\n\n-[nBurn]\nNumber of MCMC burn-in iterations. Default (1000).\n\n\n-[nMCMC]\nNumber of MCMC sample iterations. Default (2500).\n\n\n-[nWeeksFiles]\nNumber of weeks files to generate. Default (100).\n\n\n-[nSubjectFiles]\nNumber of subjects files to generate. Default (1).\n\n\n-[nMCMCFiles]\nNumber of MCMC output files to generate. Default (0).\n\n\n-[nMCMCThin]\nMCMC thinning parameter. Default (0).\n\n\n-[nMCMCImpute]\nMCMC length per imputation parameter. Default (1).\n\n\n-[seed]\nSet the random number seed. Default (3500).\n\n\n-[logPath]\nIf provided, specifies a directory where a log file is generated.\n\n\n-[outputPath]\nSpecifies the directory where output will be generated. Default (“out”).\n\n\n-[endToEndRun]\nFlag indicating if simulations should be run.\n\n\n-[skipMissingParamsCheck]\nFlag indicating to skip checking for missing parameters.\n\n\n-[scenarios]\nFlag indicating which scenarios should be processed.\n\n\n-[useDifferentSeedPerScenario]\nFlag indicating whether to use a different seed for each simulated scenario. Default (False).\n\n\n-[useDifferentSeedPerDesign]\nFlag indicating whether to use a different seed for each simulated design. Default (False)."
  },
  {
    "objectID": "documentation/v72/userguides/flfll.html#arguments",
    "href": "documentation/v72/userguides/flfll.html#arguments",
    "title": "FLFLL",
    "section": "Arguments",
    "text": "Arguments\n\n–[h | help] (Help)\nThe –h command line option displays the command line help options in the terminal. No simulations will be performed when the –h option is specified.\n\n\n–[v | version] (Version)\nThe –v command line option displays the FLFLL version in the terminal. No simulations will be performed when the –v option is specified.\n\n\n–[f | file] FILE (Run a specific .facts file)\nThe –f command line option is used to specify the “.facts” file to process. The –f option must be followed by a valid path to an existing “.facts” file or directory containing one or more “.facts” files. Hint: Remember to use quotes around the path if it includes spaces.\nIf the supplied file name is a directory, then FACTS will process each “.facts” file in the directory in turn. As this only starts FACTS once, this can be quite a bit quicker than using a batch file or script that loops and starts FACTS separately for each “.facts” file.\nFurthermore, if the supplied file name is a directory then FACTS also automatically recurses through every sub-directory processing every “.facts” file it finds.\n\n\n–[n | nSim] N (Run N simulations for each scenario)\nThe –n command line option is used to specify the number of simulations to run. The –n option must be followed by an integer value greater than 0. For each scenario in the FACTS project file, the application will run N simulations. Defaults to 5 if unspecified.\n\n\n–[p | packet] N (Set the packet size for simulations)\nThe –p command line option is used to specify the packet size for parallelization of simulations. The –p option must be followed by an integer value greater than 0. When using the –g option to run on a grid, or when running on a multicore machine it is often beneficial to parallelize simulations using the packetization process (see grid documentation for more information on packetization). The packet size must be greater than zero, but as in the GUI, there is no restriction that it be less than the number of simulations. If it is greater than the number of simulations, the simulations will not be packetized. Defaults to 1000 if unspecified.\n\n\n–[g | grid] (Run on grid)\nThe –g command line option instructs the application to send simulations to the grid (assumes that the grid is correctly configured). When running on the grid, the action is still performed synchronously (i.e. FACTS will wait while the simulations run and collect the results before exiting). This option is useful to parallelize long running simulations more than they can be parallelized locally. Defaults to run locally if unspecified.\n\n\n–[a | agg] Mode (Aggregation Mode)\nThe –a command line option specifies the aggregation action to take for completed simulation results. The available modes for this option are:\n\nNone – no aggregation will be performed\nNoPivot – Only standard aggregation will be performed\nPivot – Both standard and pivoted aggregation will be performed.\nDefault, if unspecified, is None.\n\n\n\n–aggPrefix prefix (Prefix for aggregation files)\nThe –aggPrefix command line option specifies the prefix to use when naming aggregated files and must be followed by a valid file prefix. This option is only used when –a is set to NoPivot or Pivot. The aggregation files produced by aggregating across all scenarios will be named using the prefix&lt;_pivot&gt;_(filename).csv pattern, where &lt;_pivot&gt; is included for pivoted files only, and (filename) is replaced by the name of the file being aggregated. Defaults to “agg” if unspecified.\n\n\n–nBurn N (Number of MCMC burn-in iteractions)\nThe –nBurn command line option specifies the number of burn-in MCMC iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 1000 if unspecified.\n\n\n–nMCMC N (Number of MCMC sample iterations)\nThe –nMCMC command line option specifies the number of MCMC sampling iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 2500 if unspecified.\n\n\n–nWeeksFiles N (Number of weeks files to output)\nThe –nWeeksFiles command line option specifies the number of weeks files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 100 if unspecified.\n\n\n–nSubjectFiles N (Number of subjects files to output)\nThe –nSubjectFiles command line option specifies the number of subject files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 1 if unspecified.\n\n\n–nMCMCFiles N (Number of MCMC files to output)\nThe –nMCMCFiles command line option specifies the number of MCMC sample files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 0 if unspecified.\nNote: This option potentially produces a very large amount of output data and may fail if sufficient disk space is not available.\n\n\n–nMCMCThin N (MCMC output thinning value)\nThe –nMCMCThin command line option specifies the MCMC thinning value to apply to the MCMC output and must be followed by a valid integer value at least &lt;x&gt;. The thinning parameter applies only to the MCMC output, all MCMC samples are used for analysis. Defaults to &lt;x&gt; if unspecified.\n\n\n–nMCMCImpute N (MCMC length per imputation value)\nThe –nMCMCImpute command line option specifies the number of MCMC sampling iterations to use in the simulation for each imputation. Defaults to 1 if unspecified.\n\n\n–seed (Random number seed)\nThe –seed command line option sets the random number generator seed value. The default value (3500) is the same as that used in the GUI. It can be set to any positive integer.\n\n\n-logPath Path (Path where the optional log file is placed)\nThe –logPath command line option (if given) is used to specify the directory where a log file is generated. If not specified, a log file will not be generated. The –logPath option must be followed by a valid path. If the path does not exist, it will be created.\nNote: Remember to use quotes around the path if it includes spaces.\n\n\n-outputPath Path (Path where output is generated)\nThe –outputPath command line option (if given) is used to specify the directory where parameter files and optional simulation files are placed. If not specified, the output path will be the directory in which the .facts file is present. The –outputPath option must be followed by a valid path. If the path does not exist, it will be created.\n\n\n-endToEndRun (Flag to indicate if simulations should be run also)\nThe –endToEndRun command line instructs FLFLL to run full simulations. If unspecified, only parameter files will be generated.\n\n\n-skipMissingParamsCheck (Flag to indicate to skip checking of missing parameters)\nThe –skipMissingParamsCheck command line instructs FLFLL to skip the process of checking for missing parameters in legacy FACTS project files (.facts). If not specified, when running FACTS projects files prior to version 6.2, errors will prevent FLFLL from completing.\n\n\n-scenarios (Flag indicating which scenarios should be processed)\nThe –scenarios command line instructs FLFLL to run the specified scenarios by name. The names of the scenarios to run should be provided as a comma separated list. If not specified, all scenarios will be processed.\n\n\n-useDifferentSeedPerScenario (Flag indicating whether to use a different seed for each simulated scenario)\nThe –useDifferentSeedPerScenario command line option provides FLFLL with an option to set a different random number seed for each of the scenarios that are being simulated. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated scenario and then adds 1111 to the random number seed of any additional scenarios being simulated. For example, if three scenarios are being simulated and the -seed flag is set to 3500, the first scenario will have a random number seed of 3500, the second scenario a random number of 4611 and the third scenario a random number seed of 5722. This deterministic way of setting different random number seeds allows for reproducible simulation results.\n\n\n-useDifferentSeedPerDesign (Flag indicating whether to use a different seed for each simulated design)\nThe –useDifferentSeedPerDesign command line option provides FLFLL with an option to set a different base random number seed for each of the designs that are being simulated. This option is only used when a directory containing multiple FACTS designs is passed as an argument to the FLFLL command line, rather than a single design. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated design and then adds 1234 to the random number seed of any additional designs being simulated. For example, if three designs are being simulated and the -seed flag is set to 3500, the first design will have a base random number seed of 3500, the second scenario a random number of 4734 and the third design a base random number seed of 5968. This deterministic way of setting different random number seeds allows for reproducible simulation results."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html",
    "href": "documentation/v72/userguides/staged.html",
    "title": "Seamless Trial User Guide",
    "section": "",
    "text": "FACTS seamless designs, simulated using the Staged Design Engine, is for simulating trials where there are two stages. The first stage can be a full adaptive trial, the second stage can be a full adaptive trial, and there is a transition that allows for specifying how to transfer from the first stage to the second.\nFACTS Staged Design can thus be used to simulate:\nFACTS Staged Design can simulate these successive “stages” as two separate and distinct trials, as a single trial with a significant adaptation at some point, or somewhere on the spectrum between these two where two stages of clinical development are linked sequentially.\nThe structure of a Staged Design trial is very similar to that of a FACTS Core trial. If you are not familiar with using FACTS Core you should read the FACTS Core User Guide and acquaint yourself with some FACTS Core tutorials. In fact it is recommended you first design the first stage in FACTS Core, then import it into FACTS Staged Design.\nThe principal difference from a simple Core design is that a Staged Design has, or can have:\nAs a result, Staged Designs have more output than a Core Design – there are outputs for both the first and second stage. There can be two sets of outputs for the first stage – one at the time of the arm selection for Stage 2 (or the decision not to graduate), and one at the time of the complete data for the first stage. This is necessary because Staged Designs encompass a range of settings: from simulating what are essentially two separate trials, where the second depends on the first; to simulating a single trial with an interim where there is a trimming of the number of arms being studied.\nIf the patient data files are written out for the simulation, then there are patient data files for the first stage, second stage and overall."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#study-info",
    "href": "documentation/v72/userguides/staged.html#study-info",
    "title": "Seamless Trial User Guide",
    "section": "Study Info",
    "text": "Study Info\n\nContinuous/DichotomousMultiple EndpointTime-to-Event\n\n\n\n\n\n\n\n\nFigure 2: The Staged design Study &gt; Study Info tab for a continuous endpoint.\n\n\n\n\nDesign Options\nIn a staged design the “Enable adaptive features” option can be specified separately for Stage 1 and Stage 2. If either stage is not adaptive, then tabs such as interims, early stopping criteria, and adaptive allocation options are not available in that stage.\nThe “Use Longitudinal Modeling” option works the same as in core.\nThe “Include simulation of baseline” and “Special longitudinal models” are the same as in FACTS core.\n\n\nStudy Information\n\nSimulate Stage 1 Accrual\nIn the Staged design engine, subject recruitment for Stage 1 can be simulated continuously or deterministically. If recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab. Specifying deterministically allows the user to specify a file with subject recruitment dates that will be used for all simulations. Stage 2 accrual can only be continuous.\n\n\nMaximum Sample Sizes\nIn staged design, the maximum number of subjects is more complicated than in core designs. Rather than specifying a single maximum, the maximum number of subjects can be specified by stage.\nYou must either specify the maximum number of subjects combined across both stages, or specify the maximum number of subjects allowed in both Stage 1 and Stage 2. If the maximum number of subjects for a stage is left blank, then there is no maximum for that stage and it is allowed to use all remaining subjects up to the overall maximum.\nIf the “Maximum number of subjects” is ever reached in a simulation at any stage, then accrual stops. Interim analyses may still be triggered. Existing subjects will be followed up and a final analysis will be performed.\nIf the “Maximum number of subjects Stage 1” is reached during stage 1, then accrual stops, interim analyses may still be triggered, subjects are followed up, and a decision based on the “Transition” to stage 2 is made.\nThe “Maximum number of subjects Stage 2” may either be specified as a fixed value, or can “depend on number of treatment arms in Stage 2.” If the “Maximum number of subjects Stage 2” is set, then if the number of subjects accrued in Stage 2 gets to the specified maximum accrual will stop, subjects will be followed up, and the final analysis will be performed. If the maximum number of subjects in Stage 2 is not specified explicitly, but is chosen to depend on the number of arms in stage 2, then the Stage 2 sample size maximum is specified in the Stage 2 Design &gt; Allocation tab.\n\n\n\n\n\n\nNotes on Staged Design Sample Sizes\n\n\n\nIt is possible to specify rules that allow Stage 1 to use all the allowed subjects. If Stage 2 cannot accrue any subjects and Stage 2 analysis does not include Stage 1 data then it is considered a ‘Null Stage 2’. If some or all Stage 1 data is included in the Stage 2 analysis then Stage 2 immediately completes (after any specified operational delay) with a final analysis just using the Stage 1 data included in Stage2.\nThe fact that the Stage 2 analysis will be carried out even if Stage 2 has no sample size (if Stage 1 data has been included in Stage 2) creates a useful FACTS trick using staged design. Giving Stage 2 a size of 0 and carrying all Stage 1 data into Stage 2 allows FACTS Staged Design to be used to simulate a FACTS Core design where a different analysis model is to be used for the final analysis (e.g. with a different prior or not using longitudinal modeling).\n\n\n\n\nMaximum time for Stage 1 accrual\nThe “Maximum time for Stage 1 accrual” can be set instead of or in addition to a Stage 1 maximum sample size. If this option is specified and if, at the specified time Stage 1 has not stopped accruing, then accrual stops, any specified interims continue, and, if specified, subjects are followed to completion.\nIf the user specifies both a “Maximum number of subjects in Stage 1” and a “Maximum time for Stage 1 accrual” then whichever occurs first triggers the stopping of Stage 1 accrual. If Stage 1 does not stop or graduate at an interim, then after stopping accrual, Stage 1 subjects are followed up and only when all follow-up is complete does the Stage 1 final analysis occur.\n\n\nResponse\nThe response option for continuous, dichotomous, time-to-event, and multiple endpoint are the same as in FACTS Core.\n\n\n\nSchedule of Post-Baseline Visits\nIf “Use longitudinal modeling” is selected in the Design Options section, then the visit schedule for a patient should be specified in the “Schedule of Post-Baseline Visits” section in the same way as in FACTS Core.\n\n\nStage 2 Delay\nThe value provided in this box creates an operational delay between when arms are selected for inclusion in Stage 2 and the start of accrual in Stage 2.\n\n\n\nAs in the Core engines, many of the options available for a single endpoint on the Study &gt; Study Info tab have been moved to the Study &gt; Endpoints tab in the multiple endpoints engine.\n\nDesign Options\nIn a staged design the “Enable adaptive features” option can be specified separately for Stage 1 and Stage 2. If either stage is not adaptive, then tabs such as interims, early stopping criteria, and adaptive allocation options are not available in that stage.\nThe special endpoint options are located in the Study &gt; Endpoints tab.\n\n\nStudy Information\n\nSimulate Stage 1 Accrual\nIn the Staged design engine, subject recruitment for Stage 1 can be simulated continuously or deterministically. If recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab. Specifying deterministically allows the user to specify a file with subject recruitment dates that will be used for all simulations. Stage 2 accrual can only be continuous.\n\n\nMaximum Sample Sizes\nIn staged design, the maximum number of subjects is more complicated than in core designs. Rather than specifying a single maximum, the maximum number of subjects can be specified by stage.\nYou must either specify the maximum number of subjects combined across both stages, or specify the maximum number of subjects allowed in both Stage 1 and Stage 2. If the maximum number of subjects for a stage is left blank, then there is no maximum for that stage and it is allowed to use all remaining subjects up to the overall maximum.\nIf the “Maximum number of subjects” is ever reached in a simulation at any stage, then accrual stops. Interim analyses may still be triggered. Existing subjects will be followed up and a final analysis will be performed.\nIf the “Maximum number of subjects Stage 1” is reached during stage 1, then accrual stops, interim analyses may still be triggered, subjects are followed up, and a decision based on the “Transition” to stage 2 is made.\nThe “Maximum number of subjects Stage 2” may either be specified as a fixed value, or can “depend on number of treatment arms in Stage 2.” If the “Maximum number of subjects Stage 2” is set, then if the number of subjects accrued in Stage 2 gets to the specified maximum accrual will stop, subjects will be followed up, and the final analysis will be performed. If the maximum number of subjects in Stage 2 is not specified explicitly, but is chosen to depend on the number of arms in stage 2, then the Stage 2 sample size maximum is specified in the Stage 2 Design &gt; Allocation tab.\n\n\n\n\n\n\nNotes on Staged Design Sample Sizes\n\n\n\nIt is possible to specify rules that allow Stage 1 to use all the allowed subjects. If Stage 2 cannot accrue any subjects and Stage 2 analysis does not include Stage 1 data then it is considered a ‘Null Stage 2’. If some or all Stage 1 data is included in the Stage 2 analysis then Stage 2 immediately completes (after any specified operational delay) with a final analysis just using the Stage 1 data included in Stage2.\nThe fact that the Stage 2 analysis will be carried out even if Stage 2 has no sample size (if Stage 1 data has been included in Stage 2) creates a useful FACTS trick using staged design. Giving Stage 2 a size of 0 and carrying all Stage 1 data into Stage 2 allows FACTS Staged Design to be used to simulate a FACTS Core design where a different analysis model is to be used for the final analysis (e.g. with a different prior or not using longitudinal modeling).\n\n\n\n\nMaximum time for Stage 1 accrual\nThe “Maximum time for Stage 1 accrual” can be set instead of or in addition to a Stage 1 maximum sample size. If this option is specified and if, at the specified time Stage 1 has not stopped accruing, then accrual stops, any specified interims continue, and, if specified, subjects are followed to completion.\nIf the user specifies both a “Maximum number of subjects in Stage 1” and a “Maximum time for Stage 1 accrual” then whichever occurs first triggers the stopping of Stage 1 accrual. If Stage 1 does not stop or graduate at an interim, then after stopping accrual, Stage 1 subjects are followed up and only when all follow-up is complete does the Stage 1 final analysis occur.\n\n\n\nOverall Schedule of Post-Baseline Visits\nAs in the FACTS Core Multiple Endpoint Study Info tab, the “Overall Schedule of Post-Baseline Visits” tab creates the set of possible visits for observation of any of the endpoints. Each endpoint can be observed only at specific visits based on inputs in the Study &gt; Endpoints tab.\n\n\nStage 2 Delay\nThe value provided in this box creates an operational delay between when arms are selected for inclusion in Stage 2 and the start of accrual in Stage 2.\n\n\n\n\n\n\n\n\n\nFigure 3: The Staged design Study &gt; Study Info tab for a time-to-event endpoint.\n\n\n\n\nDesign Options\nThe staged design the “Enable adaptive features” option can be specified separately for Stage 1 and Stage 2. If either stage is not adaptive, then tabs such as interims, early stopping criteria, and adaptive allocation options are not available in that stage.\nThe “Enable predictor modeling” option in Staged TTE trials works the same as the Core predictor specification.\n\n\nStudy Information\n\nStudy and Events\n\nMaximum Sample Sizes\nIn staged design, the maximum number of subjects is more complicated than in core designs. Rather than specifying a single maximum, the maximum number of subjects can be specified by stage.\nYou must either specify the maximum number of subjects combined across both stages, or specify the maximum number of subjects allowed in both Stage 1 and Stage 2. If the maximum number of subjects for a stage is left blank, then there is no maximum for that stage and it is allowed to use all remaining subjects up to the overall maximum.\nIf the “Maximum number of subjects” is ever reached in a simulation at any stage, then accrual stops. Interim analyses may still be triggered. Existing subjects will be followed up and a final analysis will be performed.\nIf the “Maximum number of subjects Stage 1” is reached during stage 1, then accrual stops, interim analyses may still be triggered, subjects are followed up, and a decision based on the “Transition” to stage 2 is made.\nThe “Maximum number of subjects Stage 2” may either be specified as a fixed value, or can “depend on number of treatment arms in Stage 2.” If the “Maximum number of subjects Stage 2” is set, then if the number of subjects accrued in Stage 2 gets to the specified maximum accrual will stop, subjects will be followed up, and the final analysis will be performed. If the maximum number of subjects in Stage 2 is not specified explicitly, but is chosen to depend on the number of arms in stage 2, then the Stage 2 sample size maximum is specified in the Stage 2 Design &gt; Allocation tab.\n\n\n\n\n\n\nNotes on Staged Design Sample Sizes\n\n\n\nIt is possible to specify rules that allow Stage 1 to use all the allowed subjects. If Stage 2 cannot accrue any subjects and Stage 2 analysis does not include Stage 1 data then it is considered a ‘Null Stage 2’. If some or all Stage 1 data is included in the Stage 2 analysis then Stage 2 immediately completes (after any specified operational delay) with a final analysis just using the Stage 1 data included in Stage2.\nThe fact that the Stage 2 analysis will be carried out even if Stage 2 has no sample size (if Stage 1 data has been included in Stage 2) creates a useful FACTS trick using staged design. Giving Stage 2 a size of 0 and carrying all Stage 1 data into Stage 2 allows FACTS Staged Design to be used to simulate a FACTS Core design where a different analysis model is to be used for the final analysis (e.g. with a different prior or not using longitudinal modeling).\n\n\n\n\nMaximum Number of Events\nA Staged Design using a Time-to-Event endpoint can also define study size and stage lengths in terms of the number of events. This can be specified either by overall events, or events by stage. If there is a time-to-event predictor, then the maximum number of events can apply to the Final events or the Predictor events.\nThe event limits are specified, and work, in largely the same way as the sample size caps specified above.\nAgain, either the overall max must be set, or both a Stage 1 and a Stage 2 max must be set. If an overall max is set, Stage 1 and/or Stage 2 maxes may also be set.\nIf a cap is on events in S1, the events cap is respected for Stage 1 and the s1-patients files, but the cap on S1 events can be exceeded on S1 subjects during Stage 2. This will be reflected in the Stage 2 results, the master-patients and s2-patients data files.\nIf the cap is on overall events and there is no cap on S1, then S1 may complete when the overall event cap is reached, so there is no further data to collect in Stage 2.\n\nIf Stage 2 does not include Stage 1 data (specified on the Transition &gt; Data Inclusion tab) and there is no data to accrue in Stage 2 and it is a null stage.\nIf Stage 2 does include Stage 1 data then a Stage 2 analysis is performed immediately (after any operational delay) on the Stage 1 data. If there is an operational delay and not all S1 subjects are complete then they are followed up until the Stage 2 analysis. This is the one circumstance in which the overall event cap can be exceeded. This will be reflected in the Stage 2 results, the master-patients, and s2-patients data files.\n\n\n\n\nTiming\n\nMaximum time for Stage 1 accrual\nThe “Maximum time for Stage 1 accrual” can be set instead of or in addition to a Stage 1 maximum sample size. If this option is specified and if, at the specified time Stage 1 has not stopped accruing, then accrual stops, any specified interims continue, and, if specified, subjects are followed to completion.\nIf the user specifies both a “Maximum number of subjects in Stage 1”/“Maximum number of events Stage 1” and a “Maximum time for Stage 1 accrual” then whichever occurs first triggers the stopping of Stage 1 accrual. If Stage 1 does not stop or graduate at an interim, then after stopping accrual, Stage 1 subjects are followed up and only when all follow-up is complete does the Stage 1 final analysis occur.\n\n\nStage 1 follow-up\nMaximum subject follow-up is specified in terms of either the maximum follow-up for each subject, or the maximum follow-up beyond the full accrual of the stage.\nIf the Stage 1 “Max follow-up per subject” is selected and specified, then no subject accrued during S1 will be followed for longer than this. If Stage 1 does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event. With this option all Stage 1 subjects have the same maximum follow-up.\nIf “Follow-up after Stage 1 full accrual” is selected and specified, then if Stage 1 does not stop early or transition to stage 2 early, then it will stop the specified time after full enrolment or when every subject has had an event. With this option subjects recruited earlier may be follow-up for longer than subjects recruited later in the trial and the overall amount of exposure is maximized.\n\n\nStage 2 follow-up\nThe stage 2 follow-up rules work in the same way as the stage 1 rules. Max follow-up per subjects constrains follow-up so that every subjects has a maximum of the specified amount of exposure. The follow-up after Stage 2 full accrual option allows earlier subjects to be followed for longer, but also guarantees that ever subjects has at least a certain amount of follow-up if Stage 2 does not stop early.\nIf the maximum follow-up is specified as maximum follow-up beyond full accrual in both stages then it is also possible to specify that Stage 1 subjects are further followed into Stage 2:\n\nif this is not checked then Stage 1 subjects are not followed up after Stage 1;\nif this is checked then Stage 1 subjects included in Stage 2 are followed up using Stage 2 follow-up rules. Note that if Stage 1 completes and graduates to stage 2 at the final evaluation (after all S1 subjects have been fully followed up) then all Stage 1 subjects are complete and are not be further followed up in Stage 2, even if they have not yet had an event.\n\nMaximum follow-up is specified separately for each stage and can use different rules and durations.\n\n\n\n\n\n\n\nVisits (Time-To-Event Only)\nThe details of specifying a visit schedule for a staged time-to-event trial are identical to the same tab in the FACTS Core User Guide. The visit schedule specified here is shared across both stages.\n\n\nTreatment Arms\nTreatment arms in staged designs are specified in the same way as in the FACTS Core engine.\nIn the stage design engine it is common to make decisions leading to not randomizing subjects to particular arms in Stage 2, so it is worth noting that all treatment arms are included in analysis models in both stages of the trial regardless of whether they have been allocated to or not.\nThis results in treatment arms that are not accruing subjects having estimated QOI values. It can even lead to doses with no subjects allocated to them have a high probability of being the dose with the maximum response or being a minimally efficacious dose.\nIn Stage 2 there is an option (on the Transition &gt; Data Inclusion tab) to restrict the estimation of Target QOIs (such as Pr(Max) and Pr(MED)) to those arms that were selected for inclusion in Stage 2.\n\n\n\n\n\n\nEstimation of Dose Response models on doses without subjects\n\n\n\nNote that doses that have not been allocated to will have an estimated response based on the prior and the dose response model being used.\n\nFor dose response models with parametric relationships on the dose strength, like Hierarchical Logistic or Sigmoid/E-max models, the estimates may still be relatively precise depending on how well the curve can be estimated from the doses for which data has been gathered.\nFor dose response models that are not parameterized based on the effective doses strength, like the independent dose model, the estimate of response are likely to have high degrees of uncertainty.\nFor the complex shape models like “Plateau” and “Inverted U”, even though these are parameterized with respect to the effective dose strength, different parameters estimate the curve for different dose zones, and little information is shared across the dose zones. So, certain parameters may exibit high uncertainty depending on the doses that have not been allocated to. If only a small proportion of the doses have not been allocated to, then these models may function like the parametric response models.\n\n\n\nIf only a few doses are being allocated to in Stage 1, or there is a need to estimate the Target QOIs over the whole dose range in Stage 2. It is worth considering creating decision QOI’s based on specific doses that the design guarantees subjects have been allocated to. (E.g. if in Stage 1 subjects are only allocated to Control and the Top Dose, then create a decision QOI where the evaluation dose is explicitly the top dose for making end of Stage 1 decisions)."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#variants",
    "href": "documentation/v72/userguides/staged.html#variants",
    "title": "Seamless Trial User Guide",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Each variant is a slightly different design that can be simulated on all created simulation assumption scenarios.\nFor Staged Continuous, Dichotomous, or Multiple Endpoint trials, the only design feature that can be changed is the sample size (maximum number of subjects). The overall maximum, maximum for stage 1, and maximum for stage 2 can be varied, and the same rules as on the Study &gt; Study Info tab apply in the variants.\nIn the Staged Time-to-Event engine, the maximum umber of subjects can be set, but the maximum number of events can be set as well. Again, the maximum number of events can be set overall or for one of/both stages of the design. The same rules as on the Study &gt; Study Info tab apply in the variants.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Maximum Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\n\n\n\n\n\n\nFigure 4: The Variants tab for a Continuous, Dichotomous, or Multiple Endpoint staged design."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#endpoints",
    "href": "documentation/v72/userguides/staged.html#endpoints",
    "title": "Seamless Trial User Guide",
    "section": "Endpoints",
    "text": "Endpoints\nAs with the FACTS Core Multiple Endpoint design Endpoints tab, the staged design Study &gt; Endpoints tab allows for specification of the number, type, and individual properties of the different endpoints. The details of this tab are identical to the Core design, except it is now possible to define the utility function per stage.\nThe Component utility combination method now has a drop down for each stage to specify how the endpoint utilities are to be combined to create the component utility. The Utility Function section has a Stage 1 tab and a Stage 2 tab with identical tables to enter the parameter coefficients for the endpoint utility calculation. Clicking the “Mirror Stage 1 data in Stage 2” option deactivates the Stage 2 utility table for the endpoint and will use whatever is entered on the Stage 1 utility tab for Stage 2."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#accrual",
    "href": "documentation/v72/userguides/staged.html#accrual",
    "title": "Seamless Trial User Guide",
    "section": "Accrual",
    "text": "Accrual\nThe simulation of “Continuous” accrual and “Deterministic” accrual are done in the same way as in FACTS Core, but there is a little bit more to specify in the Staged design engine.\nAccrual can be specified in three ways:\n\nWith a single, common accrual profile that applies across both stages. In this situation, there is no break in recruitment between stages and regions do not ramp up a second time in the second stage. Regions’ start dates can be delayed so that they are likely to only participate in the second stage but, given the stochastic simulation of the recruitment and possibly uncertain timing of the end of the first stage if it is adaptive, under this option there is no certain linkage between the accrual rate and which stage the trial is in. This option is best used when the accrual is independent of the stage.\n\n\n\n\n\n\n\nFigure 5: Specify accrual rates with a common profile between stages.\n\n\n\n\nWith two separate profiles - one for each stage. If the profiles are similar, there is a control that can be used to copy the details of Stage 1 to Stage 2. Unlike the “mirroring” option on some of the design tabs, this does not link the Stage 2 profile to the Stage 1 profile, it simply replaces the current Stage 2 profile with a copy of the curernt Stage 1 profile.\nWith this option, the accrual in Stage 2 starts again from the beginning of the Stage 2 profile as if the start of Stage 2 is time 0 (for ramp-up, for example). All dates in the Stage 2 profile are relative to the start of Stage 2.\n\n\n\n\n\n\n\nFigure 6: Specify different accrual profiles between stages.\n\n\n\n\nIf “Determinisically” was selected for “Simulate Stage 1 Accrual:” on the Study &gt; Study Info tab, then the Stage 1 accrual gets its own tab that is exactly like the Core Design Deterministic Accrual tab for the Stage 1 accrual rate. The Stage 2 accrual profile is always Continuous, and is specified in the normal manner."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#dropout",
    "href": "documentation/v72/userguides/staged.html#dropout",
    "title": "Seamless Trial User Guide",
    "section": "Dropout",
    "text": "Dropout\nDropouts are specified in Staged Design in exactly the same manner, and with the same options as in FACTS Core Design for the same endpoint.\nNote that dropouts can either be simulated identically for both stages, or separate drop out patterns can be specified for each stage:\n\n\n\n\n\n\nFigure 7: The Execution &gt; Dropout Rate tab for a continuous endpoint, showing different dropout profiles by stage.\n\n\n\nA subject’s drop out probabilities are determined by the Stage they are recruited in, thus a subject recruited in Stage 1 but followed up in Stage 2 will continue to have the Stage 1 dropout probabilities even though Stage 2 has now started.\nIn case a complex dropout pattern has been specified (i.e. with per arm per visit probabilities) there is an option to copy the Stage 1 dropout parameters to Stage 2. This does not link the inputs, just copies the Stage 1 values into the Stage 2 entries."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#mirroring-functionality",
    "href": "documentation/v72/userguides/staged.html#mirroring-functionality",
    "title": "Seamless Trial User Guide",
    "section": "Mirroring functionality",
    "text": "Mirroring functionality\nNew checkboxes are available to the analysis model tabs: “Dose Response” and “Longitudinal Model” - to allow Stage 1 design data to be ‘mirrored’ in Stage 2. When mirrored, the Stage 2 data is ‘locked’ to that of Stage 1 – i.e. it is disabled and is synched to be an exact copy of Stage 1 data - updating automatically when this data changes. If the mirror is then switched off, the Stage 2 data becomes a copy of the Stage 1 data, but can now be edited to change it specifically for Stage 2.\nIn Staged Design with a Time-to-Event endpoint, the option to mirror the Stage 1 design in Stage 2 is also present on the TTE specific tabs: “Predictor Model &gt; Dose Response”, “Predictor Model &gt; Relationship to Endpoint” and “Hazard Model”.\nNote, mirroring of Dose Response and Hazard Model data which has hierarchical priors defined, will also lead to the related hierarchical priors data being mirrored (i.e. on the separate “Hierarchical Priors” tab)."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#stage-1-interims",
    "href": "documentation/v72/userguides/staged.html#stage-1-interims",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 Interims",
    "text": "Stage 1 Interims\nThe Stage 1 interims have the same facilities (in “Interim Analysis Frequency”) for defining interims as FACTS Core.\n\n\n\n\n\n\nFigure 8: The Interims tab for the Stage 1 design.\n\n\n\nThe “Subject Followup Options” has been renamed to “Subject Follow-up Options After Early Decision” and has 1 new option. Otherwise it is the same as FACTS Core. The new option is whether or not to “continue follow-up after interim graduation”. If selected, this option says that if Stage 1 “graduates” (passes the decision criteria to go to Stage 2) at an interim (not the final evaluation when every subject would be complete), then we continue to follow-up the Stage 1 subjects who are not complete. Checking this option enables three further options:\n\nSpecify a delay between graduating at an interim and making the arm selection for Stage 2 and starting accrual. The options are:\n\nWait until all Stage 1 subjects have completed.\nWait a fixed additional time (or until all Stage 1 subjects have completed, whichever occurs first).\nNo delay\n\nRequest an additional analysis of the Stage 1 data after all Stage 1 subjects are complete. This is referred to as a Complete Data Analysis (CDA). This CDA would be assessed using the final evaluation criteria.\n\nIf a CDA is requested, by default the output of the Stage 1 patients data will be as at the Stage 1 CDA. If instead you want the patients data to be output as it was at the Stage 2 arm selection analysis, select the “Censor s1-patientsXXXX data as at arm selection” option.\n\nOn the “Transitions &gt; Data Inclusion” tab it will be possible to select from various options to use Stage 1 data in the Stage 1 analysis. If “continue follow-up after interim graduation” is not checked only the “not used” option will be available."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#stage-1-allocation",
    "href": "documentation/v72/userguides/staged.html#stage-1-allocation",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 Allocation",
    "text": "Stage 1 Allocation\nSpecification of Stage 1 allocation is identical to FACTS Core, except that using Arm Dropping in Stage 1 has some consequences that are unique to Staged Design:\n\nAt Stage 2 Arm selection, regardless of the arm selection rules, an arm dropped in Stage 1 cannot be selected for Stage 2.\nAs a dropped arm cannot be selected for Stage 2, if the data inclusion option only includes subjects on arms kept in Stage 2 then the subjects on a dropped arm will not be included in Stage 2... However if the Stage 1 data is included in full, or included in full and pooled then data from completed subjects on a dropped arm are included in Stage 2.\nIf a Subject is on an arm that is dropped, and then the Study stops for Success/Futility the subject only continues to be followed up if both “Continue follow up if arm dropped” has been selected and the appropriate “Continue follow-up if study stopped for success/futility” has been checked.\n\nAs noted in the Stage 2 section on Allocation below, Arm Dropping is not available as an allocation adaptation in Stage 2."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#stage-1-successfutilitygraduation-criteria",
    "href": "documentation/v72/userguides/staged.html#stage-1-successfutilitygraduation-criteria",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 Success/Futility/Graduation Criteria",
    "text": "Stage 1 Success/Futility/Graduation Criteria\nThe Success/Futility/Graduation tab works in the same way as the Success/Futility tab in FACTS Core. For adaptive designs, the user can specify criteria for stopping in Stage 1 at an interim, and can define different criteria for different interims. Interim’s are indexed 1, 2, 3 etc. If stopping criteria are specified for an interim, they apply to all subsequent interims until there is an interim with new criteria specified. For example if stopping criteria are specified for Interim 1 and Interim 4, then at Interims 2 & 3, the stopping conditions for Interim 1 are used. The decision criteria for the final evaluation are always specified separately.\nIn a Staged Design, in additional to stopping Stage 1 for Success or Futility, it is possible to decide to stop Stage 1 in order to graduate to Stage 2. The decision criteria are checked in the order: ‘Futility’, ‘Success’, and then ‘Graduation’. The first criteria that are met are acted on. Note that deciding that Stage 1 is a Success or is Futile prevents Stage 2 from running, Success or Futility is a trial level decision.\n\nStage 1 Decisions\nThe possible outcomes for Stage 1, called “Outcome” in the Stage 1 output files, can be divided into 13 possible distinct decisions. Each simulated staged design makes a stage 1 decision that falls into exactly one of these categories. The outcomes reported for stage 1 do not depend on the decision made in Stage 2. They are fixed an known at the time of initiation of the second stage of the trial. The possible decisions are as follows:\n\nTrials that stopped in Stage 1\n\n1. Stage 1 Early Success\n\nStage 1 Early Success is achieved if and only if the trial meets the success condition at an interim analysis during Stage 1, and does not meet the futility criteria at the Stage 1 final analysis. The final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 1.\n\n2. Stage 1 Late Success\n\nStage 1 Late Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis success criteria.\n\n3. Stage 1 Late Futility\n\nStage 1 Late Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis futility criteria. Stage 1 Late Futility is not the complement of Stage 1 Late Success since the decision made if neither Late Success nor Late Futility are achieved at the Stage 1 final analysis is to graduate to Stage 2.\n\n4. Stage 1 Early Futility\n\nStage 1 Early Futility is achieved if and only if the trial meets the futility condition at an interim analysis during Stage 1, and does not meet the success criteria at the Stage 1 final analysis. The final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 1.\n\n5. Stage 1 Success to Futility Flip-Flop\n\nStage 1 Success to Futility Flip-Flop is achieved if and only if the trial meets the success condition at an interim analysis in Stage 1, but meets the futility condition at the Stage 1 final analysis. Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 1.\n\n6. Stage 1 Futility to Success Flip-Flop\n\nStage 1 Futility to Success Flip-Flop is achieved if and only if the trial meets the futility condition at an interim analysis in Stage 1, but meets the success condition at the Stage 1 final analysis. Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 1.\n\n\n\n\nTrials that graduated in Stage 1\n\n8. Early Graduation\n\nEarly Graduation is achieved if and only if the trial graduates at an interim analysis in Stage 1 and successfully chooses at least 1 active arm to carry forward into Stage 2.\n\n9. Late Graduation\n\nLate Graduation is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, and successfully chooses at least 1 active arm to carry forward into Stage 2.\n\n10. Early Graduation, None Selected\n\nEarly Graduation, None Selected is achieved if and only if the trial graduates at an interim analysis in Stage 1, but no active arms meet the criteria for moving to Stage 2. In this case, Stage 2 is considered “Null” and is not performed.\n\n11. Late Graduation, None Selected\n\nEarly Graduation, None Selected is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, but no active arms meet the criteria for moving to Stage 2. In this case, Stage 2 is considered “Null” and is not performed.\n\n\n\n\nOutcomes Specific to the Complete Data Analysis\nIf the option to “Perform Stage 1 complete data analysis” is checked in the Stage 1 Design &gt; Interims tab, then after an early graduation decision is made in Stage 1, the complete data analysis is conducted after full information is collected on all subjects enrolled during Stage 1. The complete data analysis row is recorded in the Stage 1 weeks files in the row with Interim Number equal to 1000. This row is distinct from the rest of the trial operation, and is not truly an interim analysis at which a trial decision can be made.\nThe special decisions, beyond the first 6 decisions that can be made for a trial that stops early in Stage 1, are:\n\n15. Early Graduation, Complete Data Analysis Futility\n\nEarly Graduation, Complete Data Analysis Futility is achieved if and only if the trial graduates at an interim analysis in Stage 1, and meets Stage 1 final futility criteria at the complete data analysis after full follow-up on subjects accrued during Stage 1 is collected.\n\n16. Early Graduation, Complete Data Analysis Success\n\nEarly Graduation, Complete Data Analysis Success is achieved if and only if the trial graduates at an interim analysis in Stage 1, and meets Stage 1 final futility criteria at the complete data analysis after full follow-up on subjects accrued during Stage 1 is collected.\n\n17. Early Graduation, Complete Data Analysis Inconclusive\n\nEarly Graduation, Complete Data Analysis Inconclusive is achieved if and only if the trial graduates at an interim analysis in Stage 1, and does not meet the Stage 1 final futility or Stage 1 final success criteria at the complete data analysis after full follow-up on subjects accrued during Stage 1 is collected.\n\n\nDecisions 15, 16, and 17 are unique to the complete data analysis, and cannot occur at a regular interim analysis or a final analysis.\n\n\n\nCreating Decision Criteria\nAs in FACTS Core, each decision can be specified in the following manner:\n\nFirst the decision is enabled / disabled if that outcome can / cannot be determined at that analysis.\nSecondly the “Decision QoI” to base the decision on, the direction of comparison, and the threshold to compare against are specified.\nMultiple “Decision QoI”s with their own comparisons may be specified as well as whether the test is for them all to be met (the criteria combined by AND) or for any one of them to be met (the criteria combined by OR).\nFinally, a minimum amount of information can be specified – either overall or on a specific arm. The nature of the information – Subjects Enrolled, Subjects Complete, or Subjects with the Opportunity to Complete is the same as has been selected on the Interim tab to define interim timings.\n\n\n\n\n\n\n\nFigure 9: The Success/Futility/Graduation Criteria tab at an interim analysis. Both early futility and early graduation can be achieved at this interim.\n\n\n\nSpecifying the Stage 1 Design’s Final Evaluation criteria is slightly different in staged than the Final Evaluation tab in Core:\n\nThere are no minimum information criteria, because it’s the final evaluation no further information will be gathered.\nThere are no graduation criteria. Graduation is the automatic decision if neither te Final Success Criteria or Final Futility Criteria are met.\n\n\n\n\n\n\n\nFigure 10: The Success/Futility/Graduation Criteria tab at the Stage 1 Final Evaluation. No Success Criteria has been provided, so any trial that does not meet the Futility Criteria will graduate to Stage 2.\n\n\n\n\n\n\n\n\n\nDesigning a Seamless Phase II/III Trial\n\n\n\nThere is a pit-fall that catches some users off guard when they are trying to design an inferentially seamless trial in the Staged design engine. A common staged design is one in which you would like to enroll a set number of subjects in Stage 1, and then transition to Stage 2 and seamlessly start enrolling Stage 2 with no pause. It seems logical that you could just not specify any interim graduation rules, and let the trial graduate at the final evaluation and start Stage 2. This would even seem to work on first glance.\nThe commonly unexpected issue is that the Final Evaluation Success/Futility/Graduation Criteria are not assessed until all accrued Stage 1 subjects have had the chance to observe their full follow-up. This means that if there is a long time to endpoint, that the trial will sit still and not accrue any new subjects for the length of the endpoint time. Additionally, when the study does transition to stage 2, it will have complete data on all Stage 1 subjects, so an early adaptation in Stage 2 may have more data to work with than intended.\nTo design an inferentially seamless trial, you must conduct an interim analysis at the time that the final Stage 1 subject is accrued, and you should set up the graduation rule at that interim analysis so that it is always met.\nEarly graduation at an interim analysis always transition seamlessly to the next stage as long as “Stage 2 Delay” is 0 on the Study &gt; Study Info tab, and the wait time before Stage 2 dose selection is set to “No Delay” on the Stage 1 Design &gt; Interims tab."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#dose-selection",
    "href": "documentation/v72/userguides/staged.html#dose-selection",
    "title": "Seamless Trial User Guide",
    "section": "Dose selection",
    "text": "Dose selection\nThere are two different forms of arm selection available: “Standard Selection Logic” and “Representative Arm Logic”. Independently of these, there are simple selections as to whether the Control and Active Comparator (if present) arms are to be included in Stage 2.\nThe same set of treatment arms are available in Stage 1 and Stage 2: these are the arms defined on the Study &gt; Treatment Arms tab. Usually all arms are available in Stage 1, but, if using fixed allocation in Stage 1, it is possible to assign zero subjects to an arm. This arm will still be included in analysis, however.\nIt is quite possible that, after a successful graduation decision (either at an interim in Stage 1 or at Stage 1 final analysis), the defined arm selection rules select no arms. This leads to a ’Null” Stage 2, i.e. Stage 2 does not run and overall the result of the combined stages is futility.\nIn outline, the two arm selection methods are as follows:\n\nStandard Selection Logic: This provides three levels of decision logic that are applied in turn to select:\n\nindividual specific arms to be kept or dropped;\narms identified as specific “Target” arms are kept;\narms that have scored highest or lowest on specified QoIs to be kept or dropped, with a specified minimum or maximum number of arms to be kept or dropped.\n\nRepresentative Arm Logic: This provides the ability to place the different treatment arms in different groups and then apply one of three forms of decision logic:\n\nKeep one from each group\nKeep groups based on the performance of each of the ‘representative arms’ from each group\nKeep just one group based on the performance of the ‘representative arm’ from each group.\n\n\n\nStandard Selection Logic\nWhen using “Standard Selection Logic” for the arm selection criteria, there are 3 levels of rules that are applied in order. It is not necessary to specify rules at each level; indeed if the selection you require can be achieved by a single rule at one particular level, that is usually the best way to specify it.\n\nIndividual Dose Decisions\nThe Individual Dose Decisions level is applied first. At this level the rule can be very simple: select a specific treatment arm, and specify a rule that decides if it should be kept or dropped. The rule can also be made dependent on a QoI achieving a particular threshold. Use this level of rules when you want to keep or drop specific arms.\n\n\n\n\n\n\nFigure 11: The Transition &gt; Dose Selection &gt; Individual Dose Decisions tab with rules specified for Dose 6 and Dose 3.\n\n\n\nWhen adding an Individual Dose Decision, the rule is made up of:\n\nThe specific arm the decision applies to.\nWhether the decision is to Keep or Drop the arm.\nThe decision criterion to use. Note this is optional and used to build more complex rules. Multiple criteria can be specified for each arm by adding new rules targeting the same arm. Each criterion is made of:\n\nThe “Decision Quantity” QoI to be tested. Often this will require a decision QoI to be created that is the evaluation of a QoI at the arm in question (but it could be a QoI at a different arm– e.g. keep the top dose if the middle dose has not met its criteria to be kept).\nThe direction of comparison.\nThe threshold.\nWhether, if additional criteria are specified, they are combined with a logical “AND” or a logical “OR”. This has to be specified on the first criteria for a particular arm, even if no other criteria are specified. When subsequent criteria are specified for the arm the join condition is displayed, but cannot be changed. To change the criteria, edit the first criteria for the arm.\n\n\n\n\n\n\n\n\nFigure 12: Add an Individual Dose Decision to drop Dose 1.\n\n\n\n\n\nTarget Dose Decisions\nThe second level of rules to be applied is “Target Dose Decisions”. The rules defined at this level are a simple selection from the list of available “Probability of being Target” QoIs (such as Pr(Max), Pr(MED), etc), and the arm that has the highest probability of meeting that target criteria is kept.\n\n\n\n\n\n\nFigure 13: The Transition &gt; Dose Selection &gt; Target Dose Decisions tab that keeps the dose with the highest Pr(Max).\n\n\n\n\n\nAll Dose Decisions\nThe final level of rules to be applied is “All Dose Decisions”, so called because the rule is applied to all the arms.\n\n\n\n\n\n\nFigure 14: The Transition &gt; Dose Selection &gt; All Dose Decisions tab that keeps 1 or 2 doses depending on how many have \\(\\Pr(\\theta_d - \\theta_{Control} &gt; 1)\\).\n\n\n\nThese rules comprise:\n\nA selection of whether these are rules to Keep or Drop arms.\nThe Minimum and the Maximum number of arms to Keep or Drop. Note that this minimum and maximum takes into account the number of arms kept or dropped by the rules at the earlier levels, and does not overrule them. For example: if the level 1 & 2 rules have caused 3 arms to be kept and the maximum is specified here to be 2, the number of arms kept is not reduced to 2, but no further arms, no matter how many meet this rule will be kept. Conversely if the previous levels have not caused any arms to be kept and the minimum is specified here to be 1, then 1 arm will be selected for keeping even if it does not fully meet the criteria.\nThe QoI to be used for the sort criterion is specified and the sort priority. The arms will be sorted by their value of this QoI when being tested against the additional criteria below.\nThere is no way to not specify a sort criterion, if the desire is that this rule should not be used to keep or drop doses, then the “minimum to keep” and “maximum to keep” should be set to 0.\nA filtering criteria can be specified using an “All Dose QOI”, condition and threshold. All arms will be judged using these criteria, and only those meeting the criteria will be selected as candidates. Multiple criteria may be specified, and the user selects whether these are combined with a logical ’AND” or a logical ’OR”. If no filtering criteria are specified then all the undecided arms are candidates.\nIf the already decided arms plus the candidates sum to more than the maximum arms being selected, then the candidate arms will be selected in order using the sort criteria up to the maximum.\nIf the already decided plus the candidate sum to less than the minimum required arms being selected, then additional arms will be selected from the remaining non-candidate arms in order, using the sort criteria, up to the specified minimum.\nIf the sum of the already decided arms plus the candidates lies between the specified minimum and maximum (inclusive) then no additional arms are selected, and the sort order is not required.\nIf no criteria are specified, then arms are simply selected in sort order.\n\n\n\nDecision Summary Tab\nTo simply the task of checking the rules that have been specified, there is a summary tab which describes the Selection Logic that has been specified.\n\n\n\n\n\n\nFigure 15: The Summary tab for the complicated set of dose selection rules provided in the screenshots above.\n\n\n\n\n\n\nRepresentative Arm Logic\nThe Representative Arm Logic method of specifying the arms to be selected for Stage 2 can be used when the selection needs to treat the arms as belonging to different groups, for example high and low doses or the treatment given in isolation or given in combination.\nThe tab allows the user to specify sub-groupings of the arms:\n\nThe number of groups is specified, initially with all the arms placed in “Group A”. Incrementing the “Group Count” parameter will create some additional empty groups – up to a maximum of four groups:\n\n\n\n\n\n\n\nFigure 16: The Group Builder’s default state when the Group Count is incremented to 2. All treatments are in the first group.\n\n\n\n\nThe treatment arms can then be moved between groups by simply dragging them, and the arms can be ordered within groups again by dragging them:\n\n\n\n\n\n\n\nFigure 17: The Group Builder with Dose 6, 5, and 4 dragged to the second group.\n\n\n\n\nThe top arm within each group is the default “representative arm” for that group for decision purposes (referred to below as the “Pre-selected Top Dose”). The arms can be dragged and reordered within groups to change the default representative.\n\n\n\n\n\n\n\nBreaking of Ties when Selecting Representative Arms\n\n\n\nWhen selecting doses between doses that have the same value for the QOI (which is highly unlikely except in circumstances where the value is ‘1’) in the Representative Arm logic, as elsewhere in FACTS, the lowest dose ‘wins’. Thus, if “keep one from each group” is being used and all the doses in a group are tied on the QOI criterion, the lowest dose in the group will be selected. If “keep one group” is being used and the representative doses of the groups are tied on the QOI criterion, then the group with the lowest representative dose will be selected.\n\n\nThere are three different “Representative Arm Logic” decision methods that can be used – as described in the following subsections.\n\nKeep one from each group\nWhen using Keep One From Each Group logic, the sub-groups are arranged with the intention to keep one arm from each group – selected according to specified criteria.\nAt least one criterion, comprising of an “All Dose” QoI with a condition and a threshold, is specified; this is taken as the primary decision to filter on and is used to sort the arms in each group. The direction of the condition determines the priority order of the QoI. Note the threshold can be set to 0 or 1 if no criterion is required. Additional criteria can be defined to refine the selection filter – and these are combined using a logical ‘AND’ or an ‘OR’. For each group, the arms that meet the criteria are selected, and (if there are multiple) the arm with the greatest (if the Condition is ‘&gt;’) or lowest (if the Condition is ‘&lt;’) value from the primary (first) criteria is selected as the arm from that group.\nIf no arms meet the criteria, then the arm with greatest/lowest value from the primary QoI is selected.\n\n\n\n\n\n\nFigure 18: The transition tab when using Representative Arm Decisions and keeping one dose from each group.\n\n\n\n\n\nKeep group based on representative\nWhen using Keep Group Based on Representative Arm logic, the user specifies how a representative arm in each group is selected and criteria to filter these representative arms; then all the arms in groups whose representative arm satisfies this filter are selected.\nThe user first specifies how the representative arm in each group is selected, this can either be:\n\nThe Pre-selected Top Dose: the arm that the user has moved to the top of the list, per group, in the Group Builder panel.\nAny Target QoI (such as Pr(Max)), in which case the arm in the group that has the greatest probability of being that target arm is the representative for that group.\n\nCriteria are then specified in the same manner as for the “Keep One From Each Group” logic. These criteria are applied to the representative arm of each group. If the criteria are met, all arms for that group are retained.\n\n\n\n\n\n\nFigure 19: The transition tab when using Representative Arm Decisions and keeping a group based on the representative.\n\n\n\n\n\nKeep one group\nWhen using Keep One Group logic, the user first specifies how the representative arm in each group is selected, this can either be:\n\nThe Pre-selected Top Dose: the arm that the user has moved to the top of the list, per group, in the Group Builder panel.\nThe Sum: the greatest summed value of an “All Dose” QoI for all the specific arms in the group (e.g. pick the group with the overall greatest probability of having the maximum response, rather than the arm with the probability of having the maximum response).\nAny Target QoI (such as Pr(Max)): the arm in the group that has the greatest probability of being that target arm is the representative for that group.\n\nThe user then specifies the selection QoI, which can be any of the “All Dose” QoIs (excluding the target QoIs – unless using Sum). The user then specifies the Priority Order as “Least” or “Greatest”. The group selected will be the group where the representative arm has the Greatest / Least value for the specified QoI.\n\n\n\n\n\n\nFigure 20: The transition tab when using Representative Arm Decisions and keeping one entire group."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#data-inclusion",
    "href": "documentation/v72/userguides/staged.html#data-inclusion",
    "title": "Seamless Trial User Guide",
    "section": "Data Inclusion",
    "text": "Data Inclusion\nOn the Data inclusion tab, the user specifies how much Stage 1 data is used in the Stage 2 analysis. The options are:\n\nnot used: Stage 1 data is not used at all in Stage 2. The two stages are effectively separate trials.\nincluded in full: All Stage 1 data is used in Stage 2. The two stages are effectively 2 parts of a single trial.\nincluded where the subjects are on arms that are kept in Stage 2: Stage 1 data is used in Stage 2, but just on the arms selected for Stage 2. This could be for example for an inferentially seamless phase 2/3 trial.\nincluded in full and pooled with the one Stage 2 treatment arm: Stage 1 data is used in full, but pooled onto the one treatment arm selected for Stage 2. This option is only available when the selection criteria can clearly only take one arm into Stage 2.\nrestricted to only those subjects that did not complete in Stage 1 and are on arms that are kept in Stage 2: Only incomplete Stage 1 subjects on arms that are selected for Stage 2 are used in the Stage 2 analysis. The user specifies what is the latest visit the user can have been observed at and still count as “incomplete”. Whether the visit was observed in stage 1 is judged at the Stage 1 Dose Selection analysis. This option is not available for designs using a TTE endpoint.\n\nThe user also specifies whether in Stage 2 “Target QoIs” are evaluated over all the arms in the trial or just over the arms selected for Stage 2. “Target QoIs” are QoIs such as Pr(Max) that calculate for each arm the probability that it meets some target criteria.\n\n\n\n\n\n\nFigure 21: The Transition &gt; Data Inclusion tab for a trial that doesn’t use the Stage 1 data in Stage 2.\n\n\n\nRegardless of whether and how Stage 1 data is used in Stage 2, all Stage 1 data is always used in the Stage 1 analysis."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#dose-response-modeling",
    "href": "documentation/v72/userguides/staged.html#dose-response-modeling",
    "title": "Seamless Trial User Guide",
    "section": "Dose Response Modeling",
    "text": "Dose Response Modeling\nThe dose response modeling in Stage 2 is applied to all doses, regardless of whether they have been selected for inclusion in Stage 2 or not. Unless all Stage 1 data is included in Stage 2, if only a small portion of all doses are included in Stage 2, then the Independent Dose Model should be considered for the dose response analysis, along with the option on the Data Inclusion tab to restrict the estimate of Target QOIs to only the arms selected for use in Stage 2.\nFor more on this discussion, see above."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#allocation",
    "href": "documentation/v72/userguides/staged.html#allocation",
    "title": "Seamless Trial User Guide",
    "section": "Allocation",
    "text": "Allocation\nIn Stage 2 of a staged design, there are only 2 allocation options: fixed and adaptive. There is no “arm dropping” option, because of the difficulty of specifying arm dropping rules when the number of arms starting out can vary.\n\nFixed Allocation\nFixed allocation in Stage 2 is specified in the same way as FACTS Core and Stage 1: allocation is blocked, and the user specifies the number of times each treatment occurs in the block (and hence the block size). The difference in Stage 2 is that the exact number of treatment arms may be uncertain, depending on the arm selection rules and the data available at the time of arm selection.\nThus, in Stage 2, rather than a single set of randomization weightings, fixed allocation is defined for each of the possible numbers of treatment arms taken to Stage 2 (but not on exactly which arms are taken, only the number taken).\nFACTS displays a table with a row for each potential number of arms in Stage 2 and the user can assign weightings to each one. Arms are identified in terms of their relative “Effective Dose Strength”. Trt 1 refers to the treatment arm selected for Stage 2 with the lowest effective strength, Trt 2 the next lowest, etc. Effective dose strength for each arm can be entered on the “Study &gt; Treatment Arms” tab. Hence if the 2nd, 3rd and 5th arms are selected they will receive the allocation of “Trt 1”, “Trt2” and “Trt 3”, on the “possible # treatments = 3” row.\n\n\n\n\n\n\nFigure 22: Stage 2 Fixed allocation when there could be 1, 2, or 3 treatments in Stage 2.\n\n\n\nIn the example above, the assignment to control has been changed from the default of ‘1’ to maintain a 50% allocation to Control in each of the possible settings, of whether 1, 2, or 3 treatment arms were selected for Stage 2.\nNote, the number of arms available to allocate to (i.e. the number of rows in the table) is dictated by the settings on the “Arm selection” tab – e.g. if using Standard Selection Logic with a min and max number of arms to keep of ‘3’ (and more than three arms are defined), there will be three rows in this table.\n\n\nAdaptive Allocation\nIf using adaptive allocation in Stage 2, there are two sets of allocation data to define:\n\nThe lower table in the tab defines the fixed allocation that occurs prior to the first Stage 2 interim analysis. This is specified in the same way as for fixed allocation (see the section above).\nThe upper table in the tab specifies which arms are adaptively allocated to after the first Stage 2 interim, how many slots there are in the randomization block, which arms are allocated with a fixed number of slots in the block, and hence how many slots are left over to be allocated adaptively. To allocate to an arm adaptively, no value should be entered in the cell, and the cell will turn orange. Note, the first row must have all values defined, whereas other rows, if they are to be specified adaptively, must have at least two empty cells. The table is laid out in the same way as the table for fixed allocation, with a row for each number of treatments that might be selected for Stage 2, and then the columns for the arms in Stage 2 ordered by their effective strength.\n\n\n\n\n\n\n\nFigure 23: The Stage 2 adaptive allocation tab when 1 to 3 arms could be carried forwards into Stage 2. Response adaptive randomization is configured to be based on the probability that each dose is better than control.\n\n\n\nThe adaptive allocation targets section is completed in the same way as in the FACTS Core design Adaptive Allocation tab. If static weights are assigned to arms, then the weighting is normalised across just those arms that have been selected for Stage 2 and are being adaptively allocated. To put it another way, arms that have not been selected for Stage 2 or are selected but have been given a fixed allocation are excluded from the static weighting scheme, the weighting is then re-normalized across those doses that are left.\nNote that, because in Stage 2 it is usually not known at design time which doses will be in Stage 2, nor often how many, the static weight table does not disable entries for doses that may not be present in Stage 2.\n\n\n\n\n\n\nFigure 24: The adaptive allocation targets panel in Stage 2 allocation with static weights being used in the RAR calculation."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#interims",
    "href": "documentation/v72/userguides/staged.html#interims",
    "title": "Seamless Trial User Guide",
    "section": "Interims",
    "text": "Interims\nInterims are specified in the Stage 2 design in the same way as FACTS Core and Stage 1, except in Stage 2 there is an option to specify whether the Stage 2 information requirements are based solely on Stage 2 data, or on the Stage 2 plus Stage 1.\n\n\n\n\n\n\nFigure 25: Stage 2 design Interims tab with the new prompt at the bottom of the left hand section.\n\n\n\nNote that the interim numbers in Stage 2 “start over” after the transition. So the interim numbers written to the stage 2 “weeks” file will start at 1, and will correspond to the interim decisions specified on the Success/Futility Criteria tab.\nInterim timing in the second stage may be affected by the presence of first stage data, if the option is selected to base interim information on both the first and second stage data. In this case, interims are handled as follows:\n\nIf interims are specified by time, then the “first interim” may be skipped if the information carried forward from the first stage exceeds the amount of information specified for the “first interim.” The “Interim 1” success/futility criteria are then ignored, and would be superceded by the “Inteirm 2” success futility criteria, if they exist. The initial analysis is performed at X weeks into the second stage, according to the first “Time: every X weeks” specification that occurs after Stage 2 starts.\nIf interims are specified by time, and the first interim is by number of subjects enrolled the “first interim” condition may never be reached (particularly in a Stage 2 where the amount of information can be very dependent on when Stage 1 graduates to Stage 2). In this instance the timed interims start at full accrual.\nIf interims are specified by information, then the first interim performed is the first interim that has information greater than the information available from the first stage data. Interim numbering still corresponds to the rows of the “Information at” specifications."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#successfutility-criteria",
    "href": "documentation/v72/userguides/staged.html#successfutility-criteria",
    "title": "Seamless Trial User Guide",
    "section": "Success/Futility Criteria",
    "text": "Success/Futility Criteria\nIn Stage 2 the “Success/Futility Criteria” tab is laid out as in FACTS Core. It differs from the Stage 1 Design &gt; Success/Futility/Graduation Criteria tab in not having any graduation criteria conditions.\nSee the Stage 2 Interims tab description for specific details on Success/Futility rule assessment if interims are unattainable or already attained at the start of Stage 2.\n\nThe Staged Design Decisions for Stage 1 and Stage 2 Combined\nThe decisions that can be made in staged designs, called Comb. Outcome in the output files, are expanded to accomodate the two stages. Instead of the 7 possible outcomes in a single stage FACTS Core design, there are more available in the staged engine. Every possible combination decision that can be made in a FACTS Staged designs falls into exactly one of 23 possible decisions. They are as follows:\n\nTrials that stopped in Stage 1\n\n1. Stage 1 Early Success\n\nStage 1 Early Success is achieved if and only if the trial meets the success condition at an interim analysis during Stage 1, and does not meet the futility criteria at the Stage 1 final analysis. The final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 1.\n\n2. Stage 1 Late Success\n\nStage 1 Late Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis success criteria.\n\n3. Stage 1 Late Futility\n\nStage 1 Late Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on Stage 1 subjects, and then meets the Stage 1 final analysis futility criteria. Stage 1 Late Futility is not the complement of Stage 1 Late Success since the decision made if neither Late Success nor Late Futility are achieved at the Stage 1 final analysis is to graduate to Stage 2.\n\n4. Stage 1 Early Futility\n\nStage 1 Early Futility is achieved if and only if the trial meets the futility condition at an interim analysis during Stage 1, and does not meet the success criteria at the Stage 1 final analysis. The final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 1.\n\n5. Stage 1 Success to Futility Flip-Flop\n\nStage 1 Success to Futility Flip-Flop is achieved if and only if the trial meets the success condition at an interim analysis in Stage 1, but meets the futility condition at the Stage 1 final analysis. Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 1.\n\n6. Stage 1 Futility to Success Flip-Flop\n\nStage 1 Futility to Success Flip-Flop is achieved if and only if the trial meets the futility condition at an interim analysis in Stage 1, but meets the success condition at the Stage 1 final analysis. Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 1.\n\n\n\n\nTrials that graduated early from Stage 1\n\n21. Stage 1 Early Graduation, Stage 2 Early Success\n\nStage 1 Early Graduation, Stage 2 Early Success is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the success criteria at an interim analysis during Stage 2, and does not meet the futility criteria at the Stage 2 final analysis. The Stage 2 final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 2.\n\n22. Stage 1 Early Graduation, Stage 2 Late Success\n\nStage 1 Early Graduation, Stage 2 Late Success is achieved if and only if the trial graduates at an interim analysis in Stage 1, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis success criteria.\n\n23. Stage 1 Early Graduation, Stage 2 Late Futility\n\nStage 1 Early Graduation, Stage 2 Late Futility is achieved if and only if the trial graduates at an interim analysis in Stage 1, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis futility criteria. Stage 2 Late futility is not automatically the complement of Stage 2 Late Success; the Stage 2 final analysis futility rule must be specified as the complement of the success rule to make it true.\n\n24. Stage 1 Early Graduation, Stage 2 Early Futility\n\nStage 1 Early Graduation, Stage 2 Early Futility is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the futility criteria at an interim analysis during Stage 2, and does not meet the success criteria at the Stage 2 final analysis. The Stage 2 final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 2.\n\n25. Stage 1 Early Graduation, Stage 2 Success to Futility Flip-Flop\n\nStage 1 Early Graduation, Stage 2 Success to Futility Flip-Flop is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the success condition at an interim analysis in Stage 2, but meets the futility condition at the Stage 2 final analysis. Stage 2 Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 2.\n\n26. Stage 1 Early Graduation, Stage 2 Futility to Success Flip-Flop\n\nStage 1 Early Graduation, Stage 2 Futility to Success Flip-Flop is achieved if and only if the trial graduates at an interim analysis in Stage 1, meets the futility condition at an interim analysis in Stage 2, but meets the success condition at the Stage 2 final analysis. Stage 2 Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 2.\n\n27. Stage 1 Early Graduation, Stage 2 Inconclusive\n\nStage 1 Early Graduation, Stage 2 Inconclusive is achieved if and only if the trial graduates at an interim analysis in Stage 1, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then does not meet the Stage 2 final success or final futility criteria.\n\n\n\n\nTrials that graduated at the Stage 1 final analysis\n\n31. Stage 1 Late Graduation, Stage 2 Early Success\n\nStage 1 Late Graduation, Stage 2 Early Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the success criteria at an interim analysis during Stage 2, and does not meet the futility criteria at the Stage 2 final analysis. The Stage 2 final analysis futility criteria must not be met whether or not subjects were selected to follow-up after an early success decision in Stage 2.\n\n32. Stage 1 Late Graduation, Stage 2 Late Success\n\nStage 1 Late Graduation, Stage 2 Late Success is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis success criteria.\n\n33. Stage 1 Late Graduation, Stage 2 Late Futility\n\nStage 1 Late Graduation, Stage 2 Late Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then meets the Stage 2 final analysis futility criteria. Stage 2 Late futility is not automatically the complement of Stage 2 Late Success; the Stage 2 final analysis futility rule must be specified as the complement of the success rule to make it true.\n\n34. Stage 1 Late Graduation, Stage 2 Early Futility\n\nStage 1 Late Graduation, Stage 2 Early Futility is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the futility criteria at an interim analysis during Stage 2, and does not meet the success criteria at the Stage 2 final analysis. The Stage 2 final analysis success criteria must not be met whether or not subjects were selected to follow-up after an early futility decision in Stage 2.\n\n35. Stage 1 Late Graduation, Stage 2 Success to Futility Flip-Flop\n\nStage 1 Late Graduation, Stage 2 Success to Futility Flip-Flop is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the success condition at an interim analysis in Stage 2, but meets the futility condition at the Stage 2 final analysis. Stage 2 Success to Futility Flip-Flops can be achieved whether or not subjects are followed up after the early success decision in Stage 2.\n\n36. Stage 1 Late Graduation, Stage 2 Futility to Success Flip-Flop\n\nStage 1 Late Graduation, Stage 2 Futility to Success Flip-Flop is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, meets the futility condition at an interim analysis in Stage 2, but meets the success condition at the Stage 2 final analysis. Stage 2 Futility to Success Flip-Flops can be achieved whether or not subjects are followed up after the early futility decision in Stage 2.\n\n37. Stage 1 Late Graduation, Stage 2 Inconclusive\n\nStage 1 Late Graduation, Stage 2 Inconclusive is achieved if and only if the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, enrolls to the maximum allowed Stage 2 sample size, collects full follow-up on all Stage 2 subjects, and then does not meet the Stage 2 final success or final futility criteria.\n\n\n\n\nOther\n\n10. Stage 1 Early Graduation, Dose Selection Unsuccessful\n\nStage 1 Early Graduation, Dose Selection Unsuccessful is achieved if and only if the trial graduates at an interim analysis in Stage 1, but when selecting treatments to carry forward into Stage 2, no treatment arms satisfy the required Transition criteria. In this case, Stage 2 is considered “Null” and is not performed.\n\n11. Stage 1 Late Graduation, Dose Selection Unsuccessful\n\nStage 1 Early Graduation, Dose Selection Unsuccessful is achieved if and only if the the trial enrolls to the maximum Stage 1 sample size, collects full follow-up on all Stage 1 subjects, graduates at the Stage 1 final analysis, but when selecting treatments to carry forward into Stage 2, no treatment arms satisfy the required Transition criteria. In this case, Stage 2 is considered “Null” and is not performed.\n\n40. Stage 1 Graduation, Max sample size reached\n\nStage 1 Graduation, Max sample size reached is achieved if and only if the trial graduates at a Stage 1 analysis, but the calculated Stage 2 sample size is 0. In this case, Stage 2 is considered “Null” and is not performed."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#simulation-results",
    "href": "documentation/v72/userguides/staged.html#simulation-results",
    "title": "Seamless Trial User Guide",
    "section": "Simulation Results",
    "text": "Simulation Results\nIn the center of the simulation tab, the summary simulation results are displayed. There are many columns that are output by FACTS. Only the columns considered “Highlights” are displayed by default. Other groups of output columns can be displayed by clicking on the “Show More Columns” button.\nThese windows will show:\n\n\n\n\n\n\nFigure 26: The options available in “Show More Columns” for a staged design.\n\n\n\n\n\n\n\n\n\n\nName\nColumn Description\n\n\n\n\nAll\nAll summary columns\n\n\nHighlights\nOnly the columns shown on the main tab\n\n\nStage 2 Highlights\nThe columns from the main tab that pertain to Stage 2\n\n\nAllocation\nThe columns that report on participant recruitment and allocation\n\n\nResponse\nThe columns that report that estimate treatment response, the SD of the estimate, the estimate of the SD of the response, the true treatment response and the true SD of the response.\n\n\nObserved\nThe raw endpoint output and the dropout rates by arm and visit\n\n\nProbabilities\nThe final estimates for the QOIs that were computed for the trial.\n\n\nTiming\nThe duration to the end of the trial under certain conditions.\n\n\nModel Parameters\nThe columns that report the estimates of the values of the model parameters.\n\n\nStage 2 Simulation Results\nA window that displays the individual Stage 2 simulation results for the currently selected scenario. The results initially displayed are the Stage 2 ‘highlights’ columns. Other groups of these simulation results can be opened from the Right Click menu of this window.\n\n\nStage 1 Summary\nA window that displays the end of Stage 1 Summary results similar to FACTS Core summary\n\n\nStage 1 Simulation Results\nA window that displays the individual Stage 1 simulation results for the currently selected scenario. The results initially displayed are the Stage 1 ‘highlights’ columns. Other groups of these simulation results can be opened from the Right Click menu of this window.\n\n\nStage 1 DS Summary\nA window that displays a summary of the end of stage 1 arm selection results\n\n\nFrequentist results\nIf frequentist analysis is enabled, the summary results can be viewed, these are grouped by how missing data has been treated: Last Observation Carried Forward (LOCF), Baseline Observation Carried Forward (BOCF – if baseline has been simulated), Per-Protocol (PP).\n\n\nSimulation Duration\nA window that displays how long each scenario took to simulate"
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#right-click-menu",
    "href": "documentation/v72/userguides/staged.html#right-click-menu",
    "title": "Seamless Trial User Guide",
    "section": "Right Click Menu",
    "text": "Right Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 27: The right click menu on the Staged design simulations tab.\n\n\n\nThese will respectively:\n\nOpen results folder\n\nOpens a new Windows directory browser window showing the contents of the simulation results for that scenario.\n\nStage 2 Simulation Results\n\nOpens a window that displays the individual Stage 2 simulation results for the currently selected scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of this window.\n\nStage 1 Summary\n\nOpens a window that displays the end of Stage 1 Summary results similar to FACTS Core summary\n\nStage 1 Simulation Results\n\nOpens a window that displays the individual Stage 1 simulation results for the currently selected scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of this window.\n\nStage 1 DS Summary\n\nOpens a window that displays a summary of the end of stage 1 arm selection results\n\nFrequentist Stage 2 Results\n\nIf frequentist analysis is enabled, this opens a window that displays the summary frequentist results grouped by how missing data has been treated: Last Observation Carried Forward (LOCF), Baseline Observation Carried Forward (BOCF – if baseline has been simulated), Per-Protocol (PP).\n\nOpen R\n\nStarts R, loading in the result files for that scenario as separate dataframes.\n\nShow ... Graphs\n\nOpens the FACTS results graph control displaying the corresponding graphs for that scenario. See below for a description of the different groups of graphs."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#graphs-of-simulation-results",
    "href": "documentation/v72/userguides/staged.html#graphs-of-simulation-results",
    "title": "Seamless Trial User Guide",
    "section": "Graphs of Simulation Results",
    "text": "Graphs of Simulation Results\nThe graphs produced in the staged engine are usually very similar to the graphs produced in the FACTS Core engine. It is certainly worth familiarizing yourself with those graphs when using Staged design.\nGraphs available for continuous or dichotomous core designs\nGraphs available for time-to-event core designs\nGraphs available for multiple endpoint core designs\n\nPer Scenario Graphs\nThe staged design graphs include the FACTS Core graphs, broken out by stage, the complete data analysis graphs (if performed), the arm selection graphs, and overall graphs, which are combined across stages.\n\nStage 1 Graphs\n\nThese are the same graphs as are available in FACTS Core, based on the analysis in Stage 1 when the Futility / Graduation / Success decision was made. There are 3 additional graphs not in FACTS Core: Time Course for Stopping/Graduation, Proportion Times Arms Selected, and Numbers of Arms Selected\n\nStage 1 CD Graphs\n\nThese are the same graphs as are available in FACTS Core, based on the Stage 1 Complete Data Analysis, (if the “Perform Stage 1 complete data analysis” option was de-selected then these graphs are not available). It does not include “interim” based graphs or arm selection graphs.\n\nStage 1 DS Graphs\n\nThese are the same graphs as are available in FACTS Core, based on the Stage 1 Arm selection data analysis. It does not include “interim” based graphs.\n\nStage 2 Graphs\n\nThese are mostly the same graphs as are available in FACTS Core, based on Stage 2. There are 3 additional graphs not in FACTS Core: Time Course for Stopping, Outcomes by Arms Selected, and Outcomes by Number of Arms Selected\n\nOverall Graphs\n\nThese graphs are based on the combined Stage 1/Stage 2. They comprise:\n\n\n\nCombined Allocation Boxplot\n\nSimilar to this\n\nTarget Response Scatter plot Combined\n\nSimilar to this\n\nCombined Time Course for Stopping\n\nSimilar to this\n\nSample Size/Duration by Arm Selected\n\nSee below\n\nSample Size/Duration by Num Arms Selected\n\nSee below\n\n\n\nTime Course Graphs\nSome of the most useful graphs for getting an overview of the results of Staged Design simulations of a scenario are the time course graphs. In particular the graph in the “Overall Graph” set: Combined Time Course for Stopping\n\n\n\n\n\n\nFigure 28: The Time Course for Stopping Graph from the “Overall” graph set with time on the x-axis.\n\n\n\nThis graph shows a stacked histogram for the state of all the simulations at any given week / sample size. The user can select whether the x-axis is time of number of subjects enrolled.\nAt each time point / number of subjects the column of the graph above that point is coloured reflecting the state of the simulations – if they have not yet achieved their final outcome they are shown grey for “Continuing / Inconclusive”. Otherwise they are coloured according to final outcome. Once a simulation is in a final outcomes state it never changes, so the grey area is the only area that reduces as the graph is read from left to right.\n\n\n\n\n\n\nFigure 29: The Time Course for Stopping Graph from the “Overall” graph set with number of subjects randomized on the x-axis.\n\n\n\nThere is also a time course graph in the Stage 1 graphs group that shows the time to make a Stage 1 decision, and ignores Stage 2. It’s called Time Course for Stopping/Graduation.\n\n\n\n\n\n\nFigure 30: The Time Course for Stopping/Graduation Graph from the “Stage 1” graph set with time on the x-axis.\n\n\n\nThe time course plot in the Stage 2 graphs group shows the decisions made in Stage 2, using only trials that actually moved on the Stage 2. The timing and number of subjects resets at 0 for the start of Stage 2 in these graphs.\n\n\n\nThe Time Course for Stopping Graph from the “Stage 2” graph set with time since the start of Stage 2 on the x-axis.\n\n\nThe time course plot in the Stage 1 DS section only includes simulations that graduated and moved to dose selection.\nThe time course plot in the Stage 1 CD section only includes the assessment of the final analysis after full follow-up of all Stage 1 subjects. It ignores the Stage 1 decision made by the trial.\n\n\nStage 1 - Proportion of Times Arms Selected\nThis is a histogram of the proportion of times each arm was selected to be used in Stage 2. Notice in the example image below that Control does not achieve a proportion of 1 despite being automatically selected for Stage 2: the proportion is across all simulations of the scenario. Including those that did not graduate to Stage 2. In this example, only one arm as well as control was selected for Stage 2, so the sum of the proportion of times the other arms were selected equals the proportion of times Control was selected, which is also the proportion of trials that graduated. If more than one arm would have been selected, these columns would have been higher.\n\n\n\n\n\n\nFigure 31: The proportion of simulated trials in which each arm was selected for inclusion in Stage 2.\n\n\n\n\n\nStage 1 - Number of Arms Selected\nThis is a straightforward histogram of the proportion of times different numbers of treatment arms (excluding Control and Active Comparator) were selected to go forward to the Second Stage.\n\n\n\n\n\n\nFigure 32: Plot of how many active treatment arms were carried forward into Stage 2.\n\n\n\n\n\nStage 2 - Outcomes by Arms Selected\nThis graph shows a stacked bar chart of trial outcomes for all trials in which the x-axis arm was included in Stage 2. The higher the bar, the more often the arm was included in Stage 2.\n\n\n\n\n\n\nFigure 33: Plot of the outcomes for trials that included each arm in Stage 2.\n\n\n\n\n\nStage 2 - Outcomes by Number of Arms Selected\nThis graph shows a stacked bar chart of trial outcomes separated by the number of arms that were carried forwards into Stage 2 (not including the control or active comparator). The higher the bar, the more often that number of arms were carried forwards. Which arms were included in stage 2 is irrelevant in this plot.\n\n\n\n\n\n\nFigure 34: Plot of the outcomes for trials based on the number of arms included in Stage 2.\n\n\n\n\n\nOverall - Sample Size/Duration by Arm Selected\nThis graph shows a box plot of either the total sample size or the total duration combined across the two stages conditioning on trials in which each arm was included. Each simulation counts towards all arms that were included in Stage 2, so a trial that graduated and had Control, Dose 2, and Dose 4 move into Stage 2 would contribute to the box plots of the Control, Dose 2, and Dose 4.\n\n\n\n\n\n\nFigure 35: Boxplot of the Combined Sample Size of Stages 1 and 2 for trials that included each arm in thier second stage.\n\n\n\n\n\nOverall - Sample Size/Duration by Num Arms Selected\nThis graph shows a box plot of either the total sample size or the total duration combined across the two stages separating out trials by the number of arms that moved forward into Stage 2. Each simulation counts towards exactly one boxplot in this graph.\n\n\n\n\n\n\nFigure 36: Boxplot of the Combined Duration of Stages 1 and 2 for trials that had a specific number of arms move forwards into Stage 2.\n\n\n\n\n\n\nAcross Scenario Graphs\nSimilar to the Across Scenario Graphs in FACTS Core these show trellis plots of graphs for the operating characteristics that are most often compared across different designs. For most of these, there are different versions of the graph for the two stages:\n\nArm selection Stage 1\n\nSee below\n\nArm selection Stage 2\n\nSimilar to this\n\nEstimates of QOIs (Stage 1 and Stage 2)\n\nSimilar to this\n\nOverall ppn of success\n\nSimilar to this\n\nSubject allocation (Stage 1 and Combined)\n\nSimilar to this\n\nOverall sample size (Stage 1 and Combined)\n\nSimilar to this\n\nResponse (Stage 1 and Stage 2)\n\nSimilar to this\n\n\n\nStage 1 - Selected Arms\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. This is the one Across Scenarios graph that differs from its FACTS Core counterpart.\nEach bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” – the arm was correctly selected (it was selected and marked as “Should succeed” on the VSR tab) and the trial was successful at the end of Stage 1.\n“Should not succeed” – the arm was incorrectly selected (it was selected but not marked as “Should succeed” on the VSR tab) and the trial was successful at the end of Stage 1.\n“Selected” – the arm was correctly selected for graduation (it was selected and marked as “Should succeed” on the VSR tab) and the trial graduated to Stage 2.\n“Mis-selected” – the arm was incorrectly selected for graduation (it was selected but not marked as “Should succeed” on the VSR tab) and the trial graduated to Stage 2\nUnsuccessful – the arm was selected and the trial failed.\n\n\n\n\n\n\n\nFigure 37: The Across Scenarios Arm Selection graph for Stage 1."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#viewing-simulation-results-in-the-simulations-tab",
    "href": "documentation/v72/userguides/staged.html#viewing-simulation-results-in-the-simulations-tab",
    "title": "Seamless Trial User Guide",
    "section": "Viewing Simulation Results in the Simulations Tab",
    "text": "Viewing Simulation Results in the Simulations Tab\n\nSummary Per Scenario\nMost of the summary results are as for FACTS CORE. The main differences are in the Highlights – which summarise over both stages, and a Staged Design specific results set “Stage 1 DS Summary”.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, they can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nExternal Data File\nOptional\nThis column is visible if any of the scenarios use an external data file to define the subject responses on each treatment arm, specified on the ‘Virtual Subject Response &gt; External Files’ tab. For those scenarios this gives the name of the external data file, otherwise it is blank.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nComb. Mean Subj.\n1\nThis is the mean (over the simulations) of the combined number of subjects recruited across both stages in this scenario.\n\n\nSD Comb. Subj.\n1\nThis is the standard deviation across the simulations of the combined number of subjects recruited across both stages in this scenario.\n\n\nComb. Subj 80%\n1\nThis is the eightieth percentile across the simulations of the combined number of subjects recruited across both stages in this scenario\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that were successful – whether that was early success in Stage 1, late success in Stage 1, early success in Stage 2 or late success in Stage 2.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that were futile – whether that was early futility in Stage 1, late futility in Stage 1, early futility in Stage 2 or late futility in Stage 2.\n\n\n\nThere are then columns that report the type of outcome depending on whether that outcome happened in Stage 1 (S1), in Stage 2 after graduating early (at a Stage 1 interim) from Stage 1 (S2EG) or in Stage 2 after graduating late (at the Stage 1 final analysis) from Stage 1 (S2LG)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Suc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Fut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile nor (Stage 1 only) graduated in the final analysis.\n\n\nPpn Early Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at a Stage 1 interim, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn Late Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at the Stage 1 final analysis, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn NoN Stage 2\n1\nThis is the proportion of simulations where Stage 2 was due to be able to run but was unable to because the overall maximum sample size had been reached.\n\n\nMean Comb Alloc &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the combined allocation of subjects to this arm over both stages.\n\n\nSD Comb Alloc &lt;Dose&gt;\nOne per arm\nThis is the SD (over the simulations) of the combined allocation of subjects to this arm over both stages.\n\n\nMean Comb. Duration\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit in Stage 1 to last patient last visit in Stage 2.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#stage-1-ds-summary",
    "href": "documentation/v72/userguides/staged.html#stage-1-ds-summary",
    "title": "Seamless Trial User Guide",
    "section": "Stage 1 DS Summary",
    "text": "Stage 1 DS Summary\nThese columns summarise the Stage 1 transition to Stage 2 after graduation.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nExternal Data File\nOptional\nThis column is visible if any of the scenarios use an external data file to define the subject responses on each treatment arm, specified on the ‘Virtual Subject Response &gt; External Files’ tab. For those scenarios this gives the name of the external data file, otherwise it is blank.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in Stage 1 in this scenario.\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success in Stage 1 (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early in Stage 1 but were successful in the final analysis of Stage 1.\n\n\nPpn Early Graduation\n1\nThis is the proportion of simulations that stopped early in Stage 1 for graduation to Stage 2.\n\n\nPpn Late Graduation\n1\nThis is the proportion of simulations that did not stop early in Stage 1 and graduated to Stage 2 at the final Stage 1 analysis.\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early in Stage 1 but were futile in the final analysis of Stage 1.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility in Stage 1 (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Suc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success in Stage 1 but regressed to futility in the final analysis.\n\n\nPpn Fut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility in Stage 1 but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile nor graduated in the final analysis of Stage 1.\n\n\nPpn Early Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at a Stage 1 interim, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn Late Non-Graduation\n1\nThis is the proportion of simulations that met graduation conditions at the Stage 1 final analysis, but then at arm selection were unable to find any arms that met the arm selection rules, so Stage 2 was not run.\n\n\nPpn Selected &lt;Dose&gt;\nOne per arm\nThis is the proportion of simulations that graduated to Stage 2 and selected this arm for inclusion in Stage 2.\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the mean response on this arm at the ‘decision analysis’ in Stage 1.\n\n\nPpn Arms Drop: &lt;Dose&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped during Stage1 by the Arm Dropping rules. Not selecting the arm for inclusion in Stage 2 is not counted as “dropping the arm” for the purposes of this count.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to the enrollment of the Last Patient (LP) in Stage 1 (i.e. the duration of accrual).\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#notes-on-the-differences-in-the-sd-output-files-from-facts-core",
    "href": "documentation/v72/userguides/staged.html#notes-on-the-differences-in-the-sd-output-files-from-facts-core",
    "title": "Seamless Trial User Guide",
    "section": "Notes on the differences in the SD output files from FACTS Core",
    "text": "Notes on the differences in the SD output files from FACTS Core\nSee the FACTS Core User Guide for the appropriate endpoint for a detailed listing of the columns in each type of output file.\n\nInterims in the weeks files\nIn the Stage 1 “weeks” files that report the results of each analysis in a simulation, there are two “special” analyses that are numbered 999 and 1000, respectively. The 999 interim is the Arm selection analysis after the Stage 1 outcome decision was made, and the 1000 analysis is the “Complete Data Analysis” which may or may not be enabled if early stopping in Stage 1 is possible, and subjects are followed-up after the stopping decision.\nThe s1-simulations.csv file contains the 999 rows of each s1-weeksXXXXX.csv, while s1-cd-simulations.csv data corresponds to the 1000 row.\nThe 999 row is always included (even if no graduation and hence no Arm selection is actually occurring). If no arm selection is taking place or if there is no delay in performing a Arm selection following the decision to graduate, then the analysis is of exactly the same data as that of the analysis before it, the estimates will differ however from the preceding analysis just due to MCMC sampling differences.\n\n\nAdditional columns in Stage 1 results files\nThe Stage 1 simulations, weeks, and summary files have some additional columns regarding new measurements relevant in a staged design. Besides these columns, the staged output files are similar to the FACTS Core output files:\n\nSimulations.csv and weeksNNNNN.csv for continuous/dichotomous, time-to-event, or multiple endpoints\nSummary.csv for continuous/dichotomous, time-to-event, or multiple endpoints\n\nThe new columns in the staged design output files are:\n\n\nNew s1-summary and s1-ds-summary file fields\nIn s1-summary and s1-ds-summary.csv:\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nP(EG)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2.\n\n\nP(LG)\n1\nThe proportion of simulations where there was successful late graduation (at the final analysis) to Stage 2\n\n\nP(ENG)\n1\nThe proportion of simulations where there was a decision to graduate early (at an interim) to Stage 2, but then Stage 2 could not be run because no treatment arms met the dose selection criteria. (ENG=Early Non-Graduation)\n\n\nP(LNG)\n1\nThe proportion of simulations where there was a decision to graduate late (at the final analysis) to Stage 2, but then Stage 2 could not be run because no treatment arms met the dose selection criteria. (LNG=Late Non-Graduation)\n\n\nNum Selected\n1\nIn s1-ds-summary this is the average over the simulations where stage 1 graduated, of the number of doses selected.\nIn s1-summary this is the average over all the simulations of the number of doses selected.\n\n\nPpn Selected &lt;dose&gt;\nD\nIn s1-ds-summary this is the proportion of the simulations where stage 1 graduated, where this dose was selected\nIn s1-summary this is the proportion of all the simulations where this dose was selected.\n\n\n\n\n\nNew s1-cd-summary file fields\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nUndec\n1\nThe proportion of simulations that reached final analysis and did not meet the Stage 1 final success or futility criteria (and thus graduated late to Stage 2).\n\n\nP(G2F)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2 but where the Stage 1 analysis after complete data met the Stage 1 futility criteria\n\n\nP(G2S)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2 but where the Stage 1 analysis after complete data met the Stage 1 success criteria\n\n\nP(G2I)\n1\nThe proportion of simulations where there was successful early graduation (at an interim) to Stage 2 but where the Stage 1 analysis after complete data did not meet the Stage 1 final success or futility criteria (and thus fall into the Stage 1 late graduation criteria)\n\n\n\n\n\nNew s1-weeks & s1-simulation file fields\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nOutcome\n1\nThis is the outcome status for the Stage 1 simulation – which has some additional possible outcome values compared to FACTS Core:\n\n8: Early Graduation (EG)\n9: Late Graduation (LG)\n10: Early Non-Graduation (ENG) – the graduation criteria were met but no treatment arms met the dose selection criteria, hence Stage 2 was ‘Null’.\n11: Late Non-Graduation (LNG) – the graduation criteria were met but no treatment arms met the dose selection criteria, or the overall max sample size was reached in Stage 1, hence Stage 2 was ‘Null’.\n15: Early graduation, and meets Stage 1 final futility criteria in the complete data analysis\n16: Early graduation, and meets Stage 1 final success criteria in the complete data analysis\n17: Early graduation, and does not meet Stage 1 final futility or success criteria in the complete data analysis\n\n\n\nNum Selected\n1\nThe number of treatment arms selected for inclusion in Stage 2\n\n\nSelect &lt;dose&gt;\nD\nA flag per arm (including Control) indicating whether it was selected for use in Stage 2\n\n\nGraduation &lt;QOI&gt;\n-\nA flag for each QOI in the Early Graduation criteria. In the weeks file on interim analysis rows these will be 1 if met and 0 if not met. However the row in simulations.csv corresponds to the 999 row (dose selection) where the EG criteria are not re-evaluated. Thus in s1-simulations these columns either have the value -1 not evaluated because stage 1 reached final analysis when early graduation criteria are not evaluated or -9999 because the values correspond to the dose selection analysis when no decision criteria are evaluated.\n\n\nGraduation Combined\n1\nA flag for whether the complete graduation criteria were met, including at the final analysis when graduation occurs if neither the final analysis Success or Futility criteria are met. If the trial stopped at an interim the value here will be -9999 as the row corresponds to the 999 row (dose selection) where the early decision criteria are not re-evaluated."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#additional-columns-in-stage-2-results-files",
    "href": "documentation/v72/userguides/staged.html#additional-columns-in-stage-2-results-files",
    "title": "Seamless Trial User Guide",
    "section": "Additional columns in Stage 2 results files",
    "text": "Additional columns in Stage 2 results files\n\nNew s2-summary file fields\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNtrials\n1\nThis is the number of trials simulated. (Whereas the column Nsims reports the number of Stage 2’s simulated).\n\n\nComb No. subj\n1\nThe average (over all the trial simulations) of the combined number of subjects in the trial from Stage 1 and Stage 2\n\n\nSE No. Subj\n1\nThe standard error of the combined number of subjects\n\n\nComb No.Subj 80%-ile\n1\nThe 80th percentile of the combined number of subjects\n\n\nP(ES1)\n1\nThe proportion of trials that ended early for success in Stage 1\n\n\nP(LS1)\n1\nThe proportion of trials that ended late for success in Stage 1\n\n\nP(LF1)\n1\nThe proportion of trials that ended late for futility in Stage 1\n\n\nP(EF1)\n1\nThe proportion of trials that ended early for futility in Stage 1\n\n\nSFFF1\n1\nThe proportion of trials that ended early for success in Stage 1 but “flip-flopped” to futility after completing follow-up\n\n\nFSFF1\n1\nThe proportion of trials that ended early for futility in Stage 1 but “flip-flopped” to success after completing follow-up\n\n\nUndec 1\n1\nThe proportion of trials that ended undecided at the end of Stage 1. (This will be 0. All trials that are undecided at the end of Stage 1 graduate - but they may fail dose selection, which in FACTS output is labeled “Non-Graduation”).\n\n\nP(ENG)\n1\nThe proportion of trials that graduated early from Stage 1 but Stage 2 was unable to run because no treatment arms were selected.\n\n\nP(LNG)\n1\nThe proportion of trials that graduated late from Stage 1 but Stage 2 was unable to run because no treatment arms were selected.\n\n\nP(ES2E)\n1\nThe proportion of trials that ended early for success in Stage 2 having graduated early from Stage 1\n\n\nP(LS2E)\n1\nThe proportion of trials that ended late for success in Stage 2 having graduated early from Stage 1\n\n\nP(LF2E)\n1\nThe proportion of trials that ended late for futility in Stage 2 having graduated early from Stage 1\n\n\nP(EF2E)\n1\nThe proportion of trials that ended early for futility in Stage 2 having graduated early from Stage 2 having graduated early from Stage 1\n\n\nSFFF2E\n1\nThe proportion of trials that ended early for success in Stage 2 but “flip-flopped” to futility after completing follow-up, having graduated early from Stage 1\n\n\nFSFF2E\n1\nThe proportion of trials that ended early for futility in Stage 2 but “flip-flopped” to success after completing follow-up, having graduated early from Stage 1\n\n\nUndec.2E\n1\nThe proportion of trials that ended undecided at the end of Stage 2 having graduated early from Stage 1.\n\n\nP(ES2L)\n1\nThe proportion of trials that ended early for success in Stage 2 having graduated late from Stage 1\n\n\nP(LS2L)\n1\nThe proportion of trials that ended late for success in Stage 2 having graduated late from Stage 1\n\n\nP(LF2L)\n1\nThe proportion of trials that ended late for futility in Stage 2 having graduated late from Stage 1\n\n\nP(EF2L)\n1\nThe proportion of trials that ended early for futility in Stage 2 having graduated late from Stage 2 having graduated early from Stage 1\n\n\nSFFF2L\n1\nThe proportion of trials that ended early for success in Stage 2 but “flip-flopped” to futility after completing follow-up, having graduated late from Stage 1,\n\n\nFSFF2L\n1\nThe proportion of trials that ended early for futility in Stage 2 but “flip-flopped” to success after completing follow-up, having graduated late from Stage 1\n\n\nUndec.2L\n1\nThe proportion of trials that ended undecided at the end of Stage 2 having graduated late from Stage 1.\n\n\nNoNSt2\n1\nThe proportion of trials where Stage 1 graduated to Stage 2 and dose selection was successful, but Stage 2 could not be run because the trial overall sample size had been reached in Stage 1 and Stage 1 data was not being used in Stage 2.\n\n\nMean Comb Alloc &lt;dose&gt;\nD\nFor each dose the mean of the combined number of subjects (over the simulations) allocated to that dose in both Stage 1 and Stage 2.\n\n\nSE Comb Alloc &lt;dose&gt;\nD\nThe standard error of the mean of the combined number of subjects allocated to each dose.\n\n\nComb Duration\n1\nThe mean (over the simulations total duration of the trial from the start of Stage 1 to the end of Stage 2, if a Stage 2 was run, otherwise the end of Stage 1.\n\n\nComb LPFV\n1\nThe mean (over the simulations) of the total accrual period from the start of Stage 1 to the Last Patient First Visit (whether that was in Stage 1 or Stage 2)\n\n\n\nThe remaining columns are as per the summary file for this endpoint in FACTS Core, but note that averages, proportions etc. are taken over only those Stage 2’s that ran.\nIn addition:\n\nThe summaries of mean treatment effect (and mean treatment effect SD) are done over the simulations in which S2 was run AND that treatment arm was kept in stage 2.\nIf on the Transition &gt; Data Inclusion tab, under the option “Min and Max Decision QOIs and Target QOIs are selectable from:”, “only arms selected for Stage 2” has been s elected, then these QOIs will be different simulation to simulation compared to a design where “all arms” was selected, and this will result in different values for the summary.\n\n\n\nNew s2-weeks and s2-simulation file fields\nThe preceding columns are as per the Weeks and Simulations files for this endpoint in FACTS Core (see the corresponding User Guide for detailed listings), but note that if the Stage 2 did not run these columns will contain -9999 in the Simulations file and the Weeks file are empty of data rows.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nComb Weeks\n1\nNumber of weeks into the trial from the start of Stage 1.\n\n\nComb Outcome\n1\nThis is the final outcome of both stages. If the outcome is in the range 1-6then the trial did not graduate to Stage 2 and the outcome of the trial is the outcome of Stage 1.\nOutcome 7 does not occur as failing to meet final success or futility in the final analysis of Stage 1 results in Graduation to Stage 2, not an inconclusive outcome.\nOutcomes:\n10: Early No Graduation – Stage 1 graduated early but no treatment arms were selected for Stage 2\n11: Late No Graduation – Stage 1 graduated late but no treatment arms were selected for Stage 2\n2N: Stage 1 graduated early, N will be in the range 1-7 and indicates the outcome of Stage 2 which is the outcome of the trial.\n3N: Stage 1 graduated late, N will be in the range 1-7 and indicates the outcome of Stage 2 which is the outcome of the trial.\n40: Stage 1 graduated to Stage 2 and dose selection was successful, but Stage 2 could not be run because the trial overall sample size had been reached in Stage 1 and Stage 1 data was not being used in Stage 2.\n\n\nComb # Subjects\n1\nThe total number of subjects enrolled in the trial from both Stage 1 and Stage 2.\n\n\nComb Alloc &lt;Dose&gt;\nD\nThe total number of subjects allocated to each arm from both Stage 1 and Stage 2\n\n\nComb Duration\n1\nThe total duration of the trial from the FPFV of Stage 1 to the final analysis of Stage 2.\n\n\nComb LPFV\n1\nThe total accrual period of the trial from the FPFV of Stage 1 to the LPFV of Stage 2\n\n\nNull Stage\n1\nA flag value 0 or 1. A value of 1 indicates that Stage 2 was not run."
  },
  {
    "objectID": "documentation/v72/userguides/staged.html#data-in-the-patients-files",
    "href": "documentation/v72/userguides/staged.html#data-in-the-patients-files",
    "title": "Seamless Trial User Guide",
    "section": "Data in the patients files",
    "text": "Data in the patients files\nThe data provided in the patients files is the same as the data provided in a patients file for FACTS Core. See the continuous/dichotomous, time-to-event, or multiple endpoint documentation.\nThe contents of the patients files reflect the following principles:\n\ns1-patients – contain the data at the furthest point in time that any analysis is done.  So if the CDA (Complete Data Analysis) option is checked, s1 always has the data associated with the 1000 line. If it isn't checked, it's whatever data is associated with the 999 line.\n\nUnless ‘no censoring is checked’ in which case s1-patients has the same data as if a CDA had not been carried out.\n\ns2-patients – contains the data associated with the Stage 2 999 analysis (i.e. with all appropriate Stage 1 inclusions, arm-relabelling, etc.)\n\nIf in Stage 1 there was follow up, but no CDA and Stage 1 patients’ data was censored then the s1 data in s2-patients will differ from that in s1-patients as it will reflect the full follow up of s1 subjects in S2.\n\nmaster-patients – contains the union of s1-patients and s2-patients. (Note: In TTE, if different follow-up rules are applied in Stages 1 and 2, and S1 subjects are followed into S2, the data that is recorded for these subjects is the data available after stage 2).\n\n\nS1-patients\n\nContents of the s1-patients file after early graduation based on three different inputs in the design.\n\n\n\n\n\n\n\n\nIs Follow up after Interim Graduation Checked?\nIs there a S1 Complete data analysis?\nIs S1 data censored at dose selection?\nContents of the s1-patients file\n\n\nN\n-\n-\nS1 data as at the last s1 analysis.\n\n\nY\nY\n-\nS1 data as at the complete data analysis (full follow up)\n\n\nY\nN\nN\nS1 data after full follow up.\n\n\nY\nN\nY\nS1 data as at the last S1 analysis.\n\n\n\n\nContents of the s1-patients file after a decision that is not early graduation.\n\n\n\n\n\n\nS1 data after …\nContents of the s1-patients file\n\n\nS1 Interim Success/Futility with no follow up checked\nS1 data as at last s1 analysis – the stopping interim\n\n\nS1 Interim Success/Futility with follow up checked\nS1 data as at last s1 analysis – the analysis after follow up\n\n\nS1 Final Success/Futility\nS1 data as at the s1 final analysis\n\n\nFinal Graduation\nS1 data as at the s1 final analysis.\n\n\nSuccess/Futility at Max Time\nS1 data as at the s1 final analysis.\n\n\nGraduation at Max Time\nS1 data as at the s1 final analysis.\n\n\n\n\n\nS2-patients and master-patients\n\nThe data lock timing for the s2-patients and master-patients file based on how stage 1 data is used in stage 2.\n\n\n\n\n\n\n\nS1 use in S2 method\nContents of s2-patients file\nContents of Master-patients file\n\n\nNot used\nS2 data as at last s2 analysis\nCombined s1-patients and s2 patients data\n\n\nUsed in full\nS2 data as at last s2 analysis hence including all s1 data\nCombined s1-patients and s2 patients data, with the s1 use in s2 set\n\n\nSubjects on arms kept in S2 are used\nS2 data as at last s2 analysis including those s1 subjects used in s2\nCombined s1-patients and s2 patients data, with the s1 use in s2 set for those s1 subjects on the arms kept in S2\n\n\nUsed in full and pooled onto the single arm in S2\nS2 data as at last s2 analysis, hence including the s1 data that was used in s2 but with the arm re-labeled\nCombined s1-patients and s2 patients data, with the s1 use in s2 set for those s1 subjects included and which arm in s2 the s1 subjects were pooled with.\n\n\nSubjects not complete in S1 are included in S2\nS2 data as at last s2 analysis, hence including the partial s1 data that was used in s2\nCombined s1-patients and s2 patients data, with the s1 use in s2 set for those s1 subjects included\n\n\n\nThere are two columns in the master-patients file that require explanation:\nStage2Use: This field is a flag indicating how and whether the data from a patient in stage 1 is used in stage 2, the values of the flag are:\n\nNot used in stage 2\n&lt;This value is not used.&gt;\nRetained in stage 2, treated as on the same arm as in stage 1.\nRetailed in stage 2, but pooled onto a different arm, the arm that was retained in stage 2.\n&lt;This value is not used.&gt;\nRetained in stage 2, the patient did not complete in stage 1, so was retained to complete in stage 2.\n\nStage2Arm: This field records which arm (if any) the patient’s data is treated as belonging to in stage 2. This will be same as the arm in stage 1 unless the stage 1 data has been pooled onto the 1 arm retained in stage 2. If the patient’s data is not used in stage 2 this field is set to -9999."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/tte.html",
    "href": "documentation/v72/userguides/enrichment/study/tte.html",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether an event represents patient improvement or deterioration, whether the trial is attempting to show the treatment’s superiority over the control, or its non-inferiority to it, and the length of time subjects are to be followed.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether an event is subject improvement or subject deterioration. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf an event indicates success (subject improvement), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) is greater than 1 by some Target Hazard Ratio Difference for Success (THRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is greater than 1 by more than the Target Hazard Ratio Difference for Futility (THRDF).\nIf an event indicates failure (subject condition worsening), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) to be less than 1 by some Target Hazard Ratio Difference for Success (TRDS). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is less than 1, by the Target Hazard Ratio Difference for Futility (THRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference (CSD), a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\nThe limit on subject follow up, this can be either:\n\nThe maximum follow-up time, no subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event.\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event.\n\n\nNote that as a result of being able to specify whether a higher or lower event rate represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a THRD of 0.3 for success corresponds to a hazard ratio of 0.7”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/tte.html#study-info",
    "href": "documentation/v72/userguides/enrichment/study/tte.html#study-info",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether an event represents patient improvement or deterioration, whether the trial is attempting to show the treatment’s superiority over the control, or its non-inferiority to it, and the length of time subjects are to be followed.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab.\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether an event is subject improvement or subject deterioration. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf an event indicates success (subject improvement), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) is greater than 1 by some Target Hazard Ratio Difference for Success (THRDS) (also called the Clinically Significant Difference (CSD)). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is greater than 1 by more than the Target Hazard Ratio Difference for Futility (THRDF).\nIf an event indicates failure (subject condition worsening), then success criteria will estimate the probability that the hazard ratio of the treatment arm to the control arm (or of a specified event rate based on historical data) to be less than 1 by some Target Hazard Ratio Difference for Success (TRDS). Futility criteria will estimate the probability that the hazard ratio of the treatment arm to the control is less than 1, by the Target Hazard Ratio Difference for Futility (THRDF).\nSee Section 2 for more information.\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference (CSD), a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be not worse than control by the specified margin.\nThe limit on subject follow up, this can be either:\n\nThe maximum follow-up time, no subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event.\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event.\n\n\nNote that as a result of being able to specify whether a higher or lower event rate represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the Margin will almost always be a positive value. The user may specify clinically significant differences, the engine will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a THRD of 0.3 for success corresponds to a hazard ratio of 0.7”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\n\n\n\n\n\n\nFigure 2: Defining Groups.\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nGroups can be named and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the maximum ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall cap (which to have any meaning, must be less than the sum of the individual group caps)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/tte.html#sec-groups",
    "href": "documentation/v72/userguides/enrichment/study/tte.html#sec-groups",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Group Info",
    "text": "Group Info\nOn this tab values to define the analysis for each group are specified.\nIn a trial to show superiority, for each group the Target Clinically Significant Difference can be set. Optionally the parameters for a phase 3 may be specified to allow the predictive probability of subsequent success in such a phase 3 trial to be used as decision criteria.\n\nWith a time-to-event endpoint the Clinically Significant Difference is in terms of Target Hazard Ratio Difference – that is a difference from 1 in the ratio of the hazard rate in the treatment arm and the hazard rate in the control arm. Separate differences can be set for determining success and futility, and separate differences can be set for each group and for the across groups analysis. Their use in practice is specified on the “Design &gt; Stopping Criteria” and “Design &gt; Evaluation Criteria” tabs.\nThe phase 3 success criteria are:\n\nPhase 3 total number of subjects per arm\nThe required one-sided alpha\nWhether the phase 3 is to use a test for superiority or non-inferiority (set independently from whether the ED trial is for superiority or non-inferiority)\nA super-superiority margin / non-inferiority margin (depending on whether the phase 3 trial is for superiority or non-inferiority), this margin is independent from any margins specified for the ED trial.\n\nGiven these criteria FACTS calculates the predicted probability of success in the subsequent phase 3, given the estimate of the hazard ratio integrated over the uncertainty in that estimate. The conventional expected power of the specified phase 3 is calculated for the treatment effect in each MCMC sample and then averaged. The resulting predicted probability of success in phase 3 can then be used in the stopping criteria and final evaluation criteria.\n\n\n\n\n\n\n\nFigure 3: The Group Info sub-tab.\n\n\n\nSeparate ‘Target Hazard Ratio Differences’ can be set for success and for futility, specifying the THRD for success and THRD for futility. We use the terms Target Hazard Rate Difference here as it makes it clearer that the value to be entered should be positive. Elsewhere (in column headings for instance) the more conventional term CSD may be used.\n\nIf used to decide to stop a group for success or judge whether the group was successful in the final evaluation, the criteria will test whether \\(PR(1 - HR &gt; CSHRD\\ for\\ success) &gt; Success\\ threshold\\ \\).\nThat is, whether the posterior probability that the hazard ratio is less than one by more than the target hazard ratio difference is greater than a specified threshold (which is set on the Design tabs).\nIf used to decide to stop a group for futility or judge whether the group was futile in the final evaluation, the criteria will test whether \\(\\Pr(1 - HR &gt; CSHRD\\ for\\ futility) &lt; Futility\\ threshold\\)\nThat is, whether he posterior probability that the hazard ratio is less than one by more than the target hazard ratio difference for futility is less than a specified threshold (which is set on the Design tabs).\nIf the endpoint is such that an event means subject improvement, then the comparison is reversed (the treatment difference becomes \\(HR - 1\\) and if the trial is non-inferiority then the test for “being greater than the CSD” is replaced with testing for “being less than the NIM” (Non-Inferiority Margin). Thus the meaning of the user specified Difference or Margin is interpreted taking into account both whether a response means ‘better’ or ‘worse’ and whether the trial aim is ‘superiority’ or ‘non-inferiority’. The result is that for normal usage the value entered will be +ve, as the following diagrams should make clear:\n\n\n\n\n\n\nFigure 4\n\n\n\n\nNote that in Superiority trials the required THRD for success will be greater than or equal to the THRD for futility, whereas in the Non-Inferiority trials the required NIM for futility will be greater than or equal to the NIM for success.\n\nNotes on setting Target Hazard Ratio Differences\nTarget Hazard Ratio Differences (THRD) are also referred to as Clinically Significant Differences (CSD). A “standard” hypothesis test for demonstrating superiority to control uses a CSD of 0. Testing with a non-zero CSD is different, and the implications need to be carefully understood.\nThe first mistake to avoid is setting the target hazard ratio difference for success too large. The decisions for success are in terms of the estimated posterior probability that the difference of the hazard ratio from 1 is greater than the target. If the CSD is set to what might be the treatment difference in the “alternate hypothesis” of a standard hypothesis test, we could only expect on average to have a posterior probability that the treatment difference is greater than the CSD of 50%.\nTo achieve posterior probabilities of &gt; 50%, the observed difference must be greater than the target difference. To achieve the desired power by lowering the required posterior probability would be a mistake, as posterior probability thresholds of &lt; 50% have the undesirable characteristic that the criteria can be met in circumstances where it can be seen that if further data was gathered consistent with what had been seen already, it would lead the threshold no longer being met. The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a target hazard ratio difference for success that, should the treatment have the value that it is hoped to achieve, we would expect to see some &gt;50% probability of being greater than it. Thus rather than using what might be termed ‘the target value’ for the THRDS, it is better to use ‘the minimum acceptable value’.\nThe same target difference can be used to decide futility, requiring a &lt;&lt; 50% confidence that the difference of the hazard ratio from 1 is greater than the target difference. However, particularly if there are other endpoints not being explored in the simulation, or other properties of the treatment (such as convenience, compliance, cost, tolerability etc.) that might justify continued development even if it is not an outright winner on the primary endpoint, it may be that a lower target hazard ration difference needs to be set as the threshold for determining futility – for example sufficient to demonstrate that development even on the basis of non-inferiority on the primary endpoint is likely to fail. Hence FACTS allows separate CSDs to be set for assessing success and futility.\n\n\nNon-inferiority\nIn a trial to show non-inferiority, the tab is the same except that ‘Target Hazard Ratio Differences’ are now ‘Margin deltas’ to be set.\n\n\n\n\n\n\nFigure 5: Group Info tab, in a non-inferiority design."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/tte.html#visits",
    "href": "documentation/v72/userguides/enrichment/study/tte.html#visits",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Visits",
    "text": "Visits\nIf events are observed at any time that they occur during follow up, then there is no need to specify a visit schedule, the length of follow up will be as it has been specified on the Study &gt; Study Info tab (above), leave the “Events Are Only Observed at Visits” check box unchecked:\n\n\n\n\n\n\nFigure 6: Events Observed When They Occur.\n\n\n\nIf observation of event is to be only at visits, the visit schedule needs to be specified. There is an additional observation at the end of follow-up. Visits can be specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 7: Observation of Events Restricted to Visits."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/tte.html#variants",
    "href": "documentation/v72/userguides/enrichment/study/tte.html#variants",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of subjects).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different maximum subjects for each group that has had a cap specified on the Study &gt; Study Info tab and maximum “Total Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\nIn FACTS Enrichment Designs, as well as trial Success and Failure rates, a major Operating Characteristic that we often wish to estimate is the ability of the design (if the trial is successful) to select the ‘right’ groups, depending of course on the scenario being simulated. To enable FACTS to report this the user must specify on the Virtual Subject Response &gt; Explicitly Defined &gt; Group Response profiles which of the groups “Should succeed”, that is, it would constitute a ‘correct selection’ by the design in that scenario.\n\n\n\n\n\n\nFigure 8: The Variants tab, specifying 3 variants."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/continuous.html",
    "href": "documentation/v72/userguides/enrichment/study/continuous.html",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included, the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether higher response or lower response is subject improvement. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a higher response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be greater than that of the subjects in the control arm (or of a specified mean change based on historical data), or greater than control by some Target mean difference for success (The Clinically Significant Difference, CSD, for success).\nIf a lower response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be less than that of the subjects in the control arm (or of a specified mean change based on historical data), or less than control by some Target Mean Difference for Futility (The CSD for futility).\nSee Section 2 for more information\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be “not worse than” control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSDs or the NI Margins will almost always be positive values. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 10 for success corresponds to lowering the mean by 10”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\nTo ‘Include baseline data’ or not. If baseline is included then additional options are enabled to model subjects’ baselines and their possible interaction with response. If baseline is included response may be:\n\nChange from baseline\nFinal endpoint value\n\nNote that this setting changes both how responses are simulated and how they are modelled, so for most purposes this choice of response merely changes how things are labelled. The exceptions are ITP and Baseline Carried Forward where the choice of ‘change from baseline’ or ‘final endpoint value’ does affect behavior.\n\n\n\n\n\n\n\n\nFigure 2: The Group Info sub-tab\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nThe groups’ names edited to something meaningful for the trial and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall study cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall study cap (which to have any effect, must be less than the sum of the individual group caps)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/continuous.html#study-info",
    "href": "documentation/v72/userguides/enrichment/study/continuous.html#study-info",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "The Study Info sub-tab provides parameters for specifying:\n\nStudy Information, selecting: whether there are control arms in each group as well as a treatment arm or only a treatment arm, whether a higher or lower response represents patient improvement and whether the trial attempting to show the treatment’s superiority over the control, or its non-inferiority to it.\nDesign Options, selecting: whether the design is adaptive and there are to be interims where the trial can be modified or not, and whether the design will look at subject’s longitudinal results at interim visits before their final visit.\nGroups, defining the different groups to be studied.\n\n\n\n\n\n\n\nFigure 1: The Study Info sub-tab\n\n\n\n\nIn ‘Study Information’ the following is specified:\n\nWhether there are control arms in each group as well as a treatment arm or only a treatment arm (the analysis is to “Compare to an objective control”).\n\nIf control arms are included, the analysis will be by the comparison of the results of the subjects on the treatment arm with that of the subjects on the control arm.\nIf an objective control is used, there will be no control arm in the trial and the analysis will be by the comparison of the results of the subjects on the treatment arm with an assumed ‘historic’ or ‘objective’ control response.\n\nWhether higher response or lower response is subject improvement. This determines the direction of the comparisons performed when making decisions about the trial’s success or futility.\n\nIf a higher response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be greater than that of the subjects in the control arm (or of a specified mean change based on historical data), or greater than control by some Target mean difference for success (The Clinically Significant Difference, CSD, for success).\nIf a lower response is subject improvement, then success criteria will look for the mean change from baseline of subjects in the treatment arm to be less than that of the subjects in the control arm (or of a specified mean change based on historical data), or less than control by some Target Mean Difference for Futility (The CSD for futility).\nSee Section 2 for more information\n\nWhether the aim of the study is to determine superiority or non-inferiority. If non-inferiority is specified, then on other tabs, instead of specifying a clinically significant difference, a non-inferiority margin is specified. Whereas in a superiority trial the target is to be ‘better’ than control by the specified margin, in a non-inferiority trial the target is to be “not worse than” control by the specified margin.\n\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSDs or the NI Margins will almost always be positive values. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD of 10 for success corresponds to lowering the mean by 10”)\nIn ‘Design Options’ the following is specified, where the simpler options are taken, this allows the GUI to reduce the number of options presented to the user.\n\nSpecify an Adaptive or Non-Adaptive design. If ‘Enable adaptive features’ is selected the ’Design &gt; Interims’ and ’Design Success/Futility Criteria &gt; Interims tabs are now visible having not been required for non-adaptive designs.\nSpecify whether the design will use ‘Longitudinal modelling’ or not. If longitudinal modelling is not selected then the ‘Virtual Subject Response &gt; Longitudinal’ and ‘Design &gt; Longitudinal’ tabs are hidden as they are not required.\nTo ‘Include baseline data’ or not. If baseline is included then additional options are enabled to model subjects’ baselines and their possible interaction with response. If baseline is included response may be:\n\nChange from baseline\nFinal endpoint value\n\nNote that this setting changes both how responses are simulated and how they are modelled, so for most purposes this choice of response merely changes how things are labelled. The exceptions are ITP and Baseline Carried Forward where the choice of ‘change from baseline’ or ‘final endpoint value’ does affect behavior.\n\n\n\n\n\n\n\n\nFigure 2: The Group Info sub-tab\n\n\n\n\nGroups:\n\nGroups can be added and deleted (clicking ‘Delete’ deletes the currently selected group).\nThe groups’ names edited to something meaningful for the trial and it is possible to specify whether there is a ‘cap’ on the number of subjects that can be recruited into that group, if so then the ‘Group size’ must be specified.\nIf any groups do not have an individual group cap, then an overall study cap (max sample size) must be specified. If all groups have a cap it is still possible to specify an overall study cap (which to have any effect, must be less than the sum of the individual group caps)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/continuous.html#sec-group-info",
    "href": "documentation/v72/userguides/enrichment/study/continuous.html#sec-group-info",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Group Info",
    "text": "Group Info\nOn this tab values that govern the analysis for each group are specified.\n\nTrials to show Superiority\nIn a trial to show superiority, for each group the Target mean difference for success and Target mean difference for futility may be specified, these are referred to as the CSDs (Clinically Significant Differences). Optionally the parameters for a phase 3 may be to allow the predictive probability of subsequent success in such a phase 3 trial to be used as decision criteria.\n\nWith a continuous endpoint the Clinically Significant Difference is in terms of Target Mean Difference – that is a difference between the mean change from baseline in the treatment arm and the mean changed from baseline in the control arm. Separate differences can be set for determining success and futility, and separate differences can be set for each group and for the across groups analysis. Their use in practice is specified on the “Design &gt; Stopping Criteria” and “Design &gt; Evaluation Criteria” tabs.\n\n\n\n\n\n\n\nFigure 3: The Group Info sub-tab\n\n\n\n\nThe phase 3 criteria are:\n\nPhase 3 total number of subjects per arm\nThe required one-sided alpha\nWhether the phase 3 is to use a test for superiority or non-inferiority (set independently from whether the ED trial is for superiority or non-inferiority)\nA super-superiority margin / non-inferiority margin (depending on whether the phase 3 trial is for superiority or non-inferiority), this margin is independent from any margins specified for the ED trial.\n\nGiven these criteria FACTS calculates the predicted probability of success in such a trial for each treatment arm given the estimate of the treatment difference, integrated over the uncertainty in that estimate. The conventional expected power of the specified phase 3 is calculated for the treatment effect in each MCMC sample and then averaged. The resulting predicted probability of success in phase 3 can then be used in the stopping criteria and final evaluation criteria.\n\nSeparate ‘Target Mean Differences’ can be set, specifying the TMD for success and TMD for futility. We use the terms Target Mean Difference here, as it makes it clearer that the value to be entered should be positive. Elsewhere (in column headings for instance) the more conventional term CSD is used.\n\nIf the “Posterior probability” criteria is used for stopping a group for success or judging if a group is successful in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\theta_{d} - \\theta_{0} &gt; CSD\\ for\\ success \\right) &gt; Success\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target mean difference for success is greater than a specified threshold (set on the Design tabs).\nIf the “Posterior probability” criteria is used for stopping a group for futility or judging if a group is futile in the final evaluation, the criteria will test whether \\(\\Pr\\left( \\theta_{d} - \\theta_{0} &gt; CSD\\ for\\ futility \\right) &lt; Futility\\ threshold\\)\nThat is whether the posterior probability that the mean treatment difference is greater than the target mean difference for futility by is less than a specified threshold (set on the Design tabs).\nIf the endpoint is such that a lower response means improvement, then the comparison is reversed (becomes \\(\\theta_{0} - \\theta_{d}\\)) and if the trial is non-inferiority then the test for “being greater than the CSD” is replaced with testing for “being less than the NIM” (Non-Inferiority Margin). Thus the meaning of the user specified Difference or Margin is interpreted taking into account both whether a higher response means ‘better’ or ‘worse’ and whether the trial aim is ‘superiority’ or ‘non-inferiority’. The result is that for normal usage the value entered will be +ve, as the following diagrams should make clear:\n\n\n\n\n\n\nFigure 4\n\n\n\n\nNote that in Superiority trials the required TMD for success will be greater than or equal to the TMD for futility, whereas in the Non-Inferiority trials the required NIM for futility will be greater than or equal to the NIM for success.\n\n\nNotes on setting Target Mean Differences\nA “standard” hypothesis test for demonstrating superiority to control uses an effective TMD, or CSD, of 0. Testing with a non-zero CSD is different, and the implications need to be carefully understood.\nThe first mistake to avoid is setting the target mean difference for success too large. The decisions for success are in terms of the estimated posterior probability that the mean difference between the response on the treatment arm and the response on the control arm is greater than the target. If the CSD is set to what might be the treatment difference in the “alternate hypothesis” in standard hypothesis test, we could only expect on average to have a posterior probability that the treatment difference is greater than the CSD of 50%.\nTo achieve posterior probabilities of &gt; 50%, we must set a CSD that we expect the treatment to exceed. To achieve the desired power by instead lowering the required posterior probability threshold would be a mistake, as posterior probability thresholds of &lt; 50% have the undesirable characteristic that the criteria can be met in circumstances where it can be seen that if further data was gathered consistent with what had been seen already, it would lead the threshold no longer being met. The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a target mean difference for success that, should the treatment have the value that it is hoped to achieve, we would expect to see some &gt;50% probability of being greater than it. Thus rather than using what might be termed ‘the target value’ for the CSD, it is better to use ‘the minimum acceptable value’.\nThe same target difference can be used to decide futility, requiring a &lt;&lt; 50% confidence that the mean difference of the response on the treatment arm from response on the control arm is greater than the target. However, particularly if there are other endpoints not being explored in the simulation, or other properties of the treatment (such as convenience, compliance, cost, tolerability etc.) that might justify continued development even if it is not an outright winner on the primary endpoint, it may be that a lower target mean difference needs to be set as the threshold for determining futility – for example sufficient to demonstrate that development even on the basis of non-inferiority on the primary endpoint is likely to fail. Hence FACTS allows separate CSDs to be set for assessing success and futility.\n\n\nTrials to show Non-inferiority\nIn a trial to show non-inferiority, the tab is the same except that ‘Target Mean Differences’ are now ‘Target Non-inferiority Margins’.\n\n\n\n\n\n\nFigure 5: Group Info tab, in a non-inferiority design\n\n\n\nAs with trials to show Superiority, optionally the phase 3 criteria can be set (see Section 2.1)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/continuous.html#visits",
    "href": "documentation/v72/userguides/enrichment/study/continuous.html#visits",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Visits",
    "text": "Visits\nIf longitudinal modeling is not being used, then simply the time to the final visit is specified:\n\n\n\n\n\n\nFigure 6: Time to endpoint\n\n\n\nOtherwise the visit schedule needs to be specified. The last visit in the schedule is taken to be when the final endpoint is observed. Visits can be specified one at a time by entering the week of the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week of the visit and the visit index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 7: Visit schedule"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/study/continuous.html#variants",
    "href": "documentation/v72/userguides/enrichment/study/continuous.html#variants",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of subjects).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with a different maximum number of subjects.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different maximum subjects for each group that has had a cap specified on the Study &gt; Study Info tab and maximum “Total Subjects” for each variant. On the simulations tab FACTS will then create a copy of all the scenarios to run with each variant.\nIn FACTS Enrichment Designs, as well as trial Success and Failure rates, a major Operating Characteristic that we often wish to estimate is the ability of the design (if the trial is successful) to select the ‘right’ groups, depending of course on the scenario being simulated. To enable FACTS to report this the user must specify on the Virtual Subject Response &gt; Explicitly Defined &gt; Group Response profiles which of the groups “Should succeed”, that is, it would constitute a ‘correct selection’ by the design in that scenario.\n\n\n\n\n\n\nFigure 8: The Variants tab, specifying 5 variants"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/dichotomous.html",
    "href": "documentation/v72/userguides/enrichment/vsr/dichotomous.html",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual subject response from externally simulated PK/PD data. When simulations are executed, they will be executed for each profile specified by the user."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/dichotomous.html#explicitly-defined",
    "href": "documentation/v72/userguides/enrichment/vsr/dichotomous.html#explicitly-defined",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\n\nGroup Response\nResponse profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 1 Response rate values are entered for the treatment and control arms (if present) for each group directly into the Treatment Response Rate and Control Response Rate columns of the table. The graphical representation of these values updates accordingly.\nIn addition it is possible for the user to specify if a group “Should Succeed” using the “Should succeed” checkbox on each row. This is then used in the summary of the simulation results to compute how often the simulated trial was successful and groups that ‘Should Succeed’ were successful (reported in the column “Ppn Correct Groups”) and how often the simulated trial was successful and groups that were not marked ‘Should Succeed’ were successful (reported in the column “Ppn Incorrect Groups”).\n\n\n\n\n\n\nFigure 1: Virtual Subject Response – Explicitly-Defined - Group Response sub-tab.\n\n\n\nThe graph that shows the response defined – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right-clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\n\n\nLoad Scenario Responses From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses.\nEach individual simulation uses one set of responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\n\n\n\n\n\nFigure 2: Virtual Subject Response - Loading scenario group response rates from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the individual mean responses and the overall mean response over all the VSRs.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. In the normal case the format is:\n\nIf a control arm is being used: Each line should contain columns [PT1, PT2, … , PTG, PCG, PC1, PC2, … , PCG] giving the true mean response probabilities (PTi) for the Treatment arm in each of the G groups, followed by the analogous parameters for the Control arms.\nWithout a control arm, using Objective Control: Each line should contain columns [PT1, PT2, … , PTG] giving the true mean response probabilities (PTi) for the Treatment arm in each of the G groups.\n\nWhen using the Restricted Markov Model the format is:\n\nIf a control arm is being used: Each line should contain columns [P1T1, P1T2, … , P1TG, P0T1, P0T2, … , P0TG, P1C1, P1C2, … , P1CG, P0C1, P0C2, … , P0CG] giving the true mean response probabilities (P1Ti) and true mean non-response probabilities (P0Ti) for the Treatment arm in each of the G groups, followed by the analogous parameters for the Control arms.\nWithout a control arm, using Objective Control: Each line should contain columns [P1T1, P1T2, … , P1TG, P0T1, P0T2, … , P0TG] giving the true mean response probabilities (P1Ti) and true mean non-response probabilities (P0Ti) for the Treatment arm in each of the G groups.\n\nFor example:\n# Dichotomous, 4 groups with control\n#T1 T2 T3 T4 C1 C2 C3 C4\n# 5 null cases\n0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23\n0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24\n0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25\n0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26\n0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27\n# 5 increasingly good\n0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25\n0.275, 0.31, 0.26, 0.30, 0.25, 0.25, 0.25, 0.25\n0.30, 0.37, 0.27, 0.35, 0.25, 0.25, 0.25, 0.25\n0.325, 0.43, 0.28, 0.40, 0.25, 0.25, 0.25, 0.25\n0.35, 0.49, 0.29, 0.45, 0.25, 0.25, 0.25, 0.25"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/dichotomous.html#longitudinal-models",
    "href": "documentation/v72/userguides/enrichment/vsr/dichotomous.html#longitudinal-models",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Longitudinal Models",
    "text": "Longitudinal Models\nIf ‘Use longitudinal modeling’ has been checked on the ‘Study Info’ tab, and explicitly defined group responses have been specified, then it will be necessary to specify how to simulate the subjects’ responses at intermediate visits. This is done by specifying a transition method that can be combined with any of the response profiles to generate the intermediate results for subjects on treatment or control in all the groups.\nThe methods are parameterized so that its inclusion does not affect the response rate of the final endpoint to be simulated, and they scale automatically to suit each group response profile.\nClick here for an overview of longitudinal models for dichotomous endpoints in the FACTS Core engine.\n\nEnter / Retain Method\nThe Enter/Retain method is parameterized by setting probabilities for transition from non-responder to responder, and for remaining a responder once observed for each visit.\n\n\n\n\n\n\nFigure 3: Simulated Longitudinal Results – enter/retain method.\n\n\n\nThe transition probabilities will be modified for each treatment arm in each group response profile, to preserve the specified rate of response to simulate. This is done by finding (using numerical iteration) for each response rate to be simulated, the offset which, when added to the log odds of the all the transition probabilities, results in transition probabilities that give the required response rate.\n\n\nRestricted Markov Model\nIf the restricted Markov model was selected on the Study Info tab, the subject simulation method is fully described by the overall probability of response and failure for each group.\n\n\n\n\n\n\nFigure 4: Simulated subject response - restricted Markov method.\n\n\n\nFor each group, a final probability of response and failure are specified. The Probability of final stability is then inferred as the probability that neither response nor failure is observed. The response and failure rate can then be calculated as:\n\\[\n\\lambda_{r} = \\frac{\\Pr(resp)\\left\\lbrack - \\ln{\\Pr(stable)} \\right\\rbrack}{\\nu_{T}\\left( 1 - Pr(stable) \\right)}\n\\]\n\\[\n\\lambda_{f} = \\frac{\\Pr(fail)\\left\\lbrack - \\ln{\\Pr(stable)} \\right\\rbrack}{\\nu_{T}\\left( 1 - Pr(stable) \\right)}\n\\]\nwhere \\(\\nu_{T}\\) is the time of the final visit. When a virtual subject is recruited into the trial, a response and failure time are simulated based on the response and failure rates. If both events (response and failure) occur after the final visit, the simulated subject is counted as a final stability and their outcome is based on whether final stabilities count as success or failure as specified on the Study &gt; Study Info tab. If one or both events occur before the final visit, the event that occurred first is taken as the subject’s final response because this is an absorbing state model. Once an outcome has been observed, that outcome remains for the rest of the treatment schedule.\nIf values are entered for the probability of response and failure that sum to greater than one, the probability of stability displays an error and the response rates are reported as NaN (not a number).\nIf values are entered for the probability of response and failure that sum to exactly one, the probability of stability is 0 and both response rates are effectively infinite. In this case, “**” is displayed as the response rate although these are valid settings."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/vsr/dichotomous.html#external",
    "href": "documentation/v72/userguides/enrichment/vsr/dichotomous.html#external",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subject response within FACTS they can be simulated externally, from a PK-PD model for instance, and imported into FACTS, and the supplied responses are sampled from (with replacement) to provide the subject responses in the simulation. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below.\nTo import an external file, the user must first add a profile to the table. After adding the profile, a file selector window is opened and the user must select the file of externally simulated data. To change the selection, click the ‘Browse’ button.\n\n\n\n\n\n\nFigure 5: External Data.\n\n\n\n\nRequired Format of Externally Simulated Data\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id (must be positive and change from subject to subject)\nGroup index (1, 2, 3,… )\nArm Index (1 = Control, 2 = Treatment)\nVisit Id (1, 2, 3, …)\nResponse (0, 1), if the Restricted Markov model is being used then possible Response values are (0, 1, -1 = stable).\n\nThe GUI requires that the file name has a “.dat” suffix.\nThe following shows values from an example file. Note that all visits for each subject must be grouped together. Thus all the data for the first subject comes before that of the second, and so on.\n#Patient ID, Group Index, Arm Index, Visit, Response\n1, 1, 1, 1, 0\n1, 1, 1, 2, 0\n1, 1, 1, 3, 1\n1, 1, 1, 4, 0\n2, 1, 2, 1, 0\n2, 1, 2, 2, 0\n2, 1, 2, 3, 0\n2, 1, 2, 4, 0\n3, 2, 1, 1, 0\n3, 2, 1, 2, 0\n3, 2, 1, 3, 1\n3, 2, 1, 4, 1\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/design.html",
    "href": "documentation/v72/userguides/enrichment/design.html",
    "title": "Design Overview",
    "section": "",
    "text": "The Design Tab allows the user to specify how control responses, treatment responses and longitudinal responses are modeled, how subjects are allocation, the timing of interims, and the criteria for stopping groups or the study for early success or futility, and the criteria for judging final success or futility of each group and the study. Wherever applicable, this user guide is separated by type of endpoint used (continuous/dichotomous or time-to-event)."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/design.html#allocation",
    "href": "documentation/v72/userguides/enrichment/design.html#allocation",
    "title": "Design Overview",
    "section": "Allocation",
    "text": "Allocation\nOn the allocation tab:\n\nIf there are control arms included in the trial - the user specifies the relative proportion of subjects allocated to each arm in each group. The proportions are only relative within a group, not across groups. The relative proportion of subjects between groups depends on the relative accrual rates into the different groups and the points at which each group stops accruing.\nIf no Control Arm is included in the trial this tab is not displayed.\n\n\n\n\n\n\n\nFigure 34: Design - Allocation tab\n\n\n\nIn the example screenshot above, 2:2 randomizations has been specified for all groups, giving them 1:1 allocation with a block size of 4."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html",
    "title": "Simulation",
    "section": "",
    "text": "The Simulation tab allows the user to execute simulations for each of the scenarios specified for the study. The user may choose the number of simulations, whether to execute locally or on the Grid, and modify the random number seeds.\nIn the Simulation tab the user can provide simulation configuration parameters like the number of simulations to run, whether the simulations can be run on the Grid, the parallelization strategy, the random number seed used in the simulations, and the number of certain output files that should be kept during the simulation execution.\nFACTS uses Markov Chain Monte Carlo (MCMC) methods in the generation of simulated patient response data and trial results. In order to exactly reproduce a statistical set of results, it is necessary to start the Markov Chain from an identical “Random Seed”. The initial random seed for FACTS simulations is set from the simulation tab, the first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. It is possible to re-run a specific simulation, for example to have more detailed output files generated, by specifying ‘start at simulation’."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#number-of-simulations",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#number-of-simulations",
    "title": "Simulation",
    "section": "Number of simulations",
    "text": "Number of simulations\nThis box allows the user to enter the number of simulations that they would like FACTS to run for each scenario listed in the table at the bottom of the simulation tab. There is no set number of simulations that is always appropriate.\n\n10 simulations\n\nYou might want to run 10 simulations if you just want to look at a few simulated trials and assess how the decision rules work and if FACTS is simulating what you expected based on what you input on the previous tabs. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\n\n100 simulations\n\nYou might want to run 100 simulations if you want to look at many individual trials to make sure that what you want to happen is nearly always happening. You can also start to get a very loose idea about operating characteristics like power based on 100 sims. 100 simulations is also usually sufficient to spot big problems with the data analysis such as poor model fits or significant bias in the posterior estimates.\n\n1,000 simulations\n\nYou might want to run 1,000 simulations if you want estimates of operating characteristics like power, sample size, and Type I error for internal use or while iterating the design. This generally isn’t considered enough simulations for something like a regulatory submission. With 1,000 simulations the standard error for a typical type I error calculation is on the order of \\(0.005\\).\n\n10,000 simulations\n\nYou might want to run 10,000 simulations per scenario if you are finalizing a design and are preparing a report. This is generally enough simulations for a regulatory submission, especially in non-null simulation scenarios. The standard error for a typical type I error calculation using 10,000 simulations is on the order of \\(0.0015\\).\n\n&gt; 10,000\n\nYou might want to run more than 10,000 simulations if you want to be very certain of an operating characteristic’s value - like Type I error. And plan to use the measurement of the quantity for something important like a regulatory submission. The standard error of a Type I error calculation with 100,000 simulations, e.g., is on the order of \\(0.0005\\).\n\n&gt; 100,000\n\nYou probably don’t want to run more than 100,000 simulations per scenario. Maybe your finger slipped an hit an extra 0, or you thought there were 5 zeroes in that number when there were actually 6. If the simulated trial is adaptive, this is going to take a while.\n\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. Not all scenarios must be run with the same number of simulations. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#sec-startatsimulation",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#sec-startatsimulation",
    "title": "Simulation",
    "section": "Start at Simulation",
    "text": "Start at Simulation\nThe “Start at simulation” option allows for the simulation of a particular trial seen in a previous set of simulations without having to simulate all of the previous trials in that previous set to get to it.\nThe initial random seed for FACTS simulations is set in the simulation tab. The first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. Thus, it is possible to re-run a specific simulation out of a large set without re-running all of them. For example, say the 999th simulation out of a set displayed some unusual behavior, in order to understand why, one might want to see the individual interim analyses for that simulation (the “weeks” file), the sampled subject results for that simulation (the “Subjects” files) and possibly even the MCMC samples from the analyses in that simulation. You can save the .facts file with a slightly different name (to preserve the existing simulation results), then run 1 simulation of the specific scenario, specifying that the simulations start at simulation 999 and that at least 1 weeks file, 1 subjects file and the MCMC samples file (see the “MCMC settings” dialog) are output."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#parallelization-packet-size",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#parallelization-packet-size",
    "title": "Simulation",
    "section": "Parallelization Packet Size",
    "text": "Parallelization Packet Size\nThe parallelization packet size option allows simulation jobs to be split into runs of no-more than the specified number of trials that are run in parallel. If more simulations of a scenario are requested than can be done in one packet, the simulations are broken into the requisite number of packets, run, and combined and summarized when they are all complete. The final results files will look just as though all the simulations were run as one job or packet.\nThe packet size must be a perfect divisor of the number of simulations. This is usually easy since common numbers of simulations are multiples of 100, but don’t try to use a prime number for Number of Simulations or you’re stuck with only 2 packet size options.\nBy default (if the check box with Choose Parallel Packet Size is not checked) the number of simulations per packet depends on the number of simulations per scenario. If the number of simulations is less than 1000, then each scenario is packaged as a single packed and simulated. If the number of simulations per scenario is greater than or equal to 1000, the default packet size is 10 and all simulations are decomposed into packets of size 10.\n\n\n\n\n\n\nPacketization with VSRs using .mvsr files\n\n\n\nIf an external file is used to create explicit VSRs (a .mvsr file), then the packet size should be a multiple of the number of rows in that MVSR file. Each packet will get passed the entire .mvsr file to run. If there are multiple .mvsr files with differing numbers of lines then only the VSR scenarios that have a .mvsr file that has a number of rows that is a divisor of the packet size will be run. The rest will error. The packet size can then be modified to get each of the .mvsr specified VSR files to be run.\nCare should be taken when packetizing a scenario that includes an external data file to supply the virtual subject responses; in this situation, a of copy of the external file is included in each packet which can cause the packetisation process to run out of memory as the packets are being created. In this case, use a smaller number of larger packets, such as packets that are 1/10th of the total number of simulations.\n\n\nWhen running simulations, FACTS will create and run as many packets in parallel as there are execution threads on the local machine. In general, the overhead of packetization is quite low, so a packet size of 10 to 100 can help speed up the overall simulation process. Threads used to simulate scenarios that finish quickly can pick up packets for scenarios that take longer. The progress bar updates as simulation packets complete, so the smaller the packet size, the more accurately FACTS can report the overall progress of the simulation execution."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#sec-randomseed",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#sec-randomseed",
    "title": "Simulation",
    "section": "Random Seed",
    "text": "Random Seed\nRandom number generation plays a huge role in FACTS’s virtual patient generation and statistical analyses. In order to exactly reproduce a statistical set of results, it is necessary to start the random number generation process from an identical “Random Seed”. Using the same random seed in the same version of FACTS guarantees that simulated trials will always be reproducible. Changing the design parameters or the version of FACTS may or may not remove this reproducibility depending on the change.\nEven a small change in the random seed will produce very different simulation results.\nIn addition to setting the seed, the user can choose whether they want the “Same seed for all scenarios” or “Different seed” for different scenarios. If “Same seed for all scenarios” is selected, the subjects generated for each simulated trial will match for the different scenarios. This induces a correlation among the simulation output for different scenarios. This can be good if you’re trying to compare operating characteristics for different scenarios, but it can also be misleading. To disable this option select the “Different Seed” option. If “Different seed” is selected, then each scenario has its own seed that samples a different set of subjects than any other scenario. This uncorrelates the simulation output across scenarios, which can be advantageous if the absolute value of the operating characteristics are more valuable to you than the comparison of operating characteristics across scenarios."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#mcmc-settings",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#mcmc-settings",
    "title": "Simulation",
    "section": "MCMC Settings",
    "text": "MCMC Settings\nTo set advanced settings for simulation, the user may click the “MCMC Settings” button, which will display a number of additional specifiable parameters for simulation in a separate window.\n\n\n\n\n\n\nFigure 2: Advanced MCMC settings.\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the MCMC chain to reach its stationary distribution. Burn-in samples are output in MCMC files if the files are output.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest.\n\nThe third parameter controls the number of MCMC samples taken after each imputation of missing data using the longitudinal model. The default value is 1. This parameter only has an effect if Bayesian imputation is being used to impute missing or partially observed data. Increasing the value of this parameter allows the parameter estimates to converge somewhat to a potentially new stationary distribution for each new set of imputed data. If the imputed data is only a small percentage of the overall data this is likely unnecessary. As a rough guide, if it at some early interims &gt; 5% of the data being analyzed will be imputed, a value in the range 2 to 10 is recommended to avoid underestimating the uncertainty. A higher number should be used the greater the proportion of imputed data.\nThe next parameter concerns the output of the MCMC samples to a file. It is possible to have the design engine output the sampled values from the MCMC in all of the interims of the first N simulated trials of each scenario by specifying the “Number of MCMC files to output” to be greater than 0. The resulting files, ‘mcmcNNNN.csv’, will be in the results directory with all the other results files for that scenario. These files include the burn-in samples from the MCMC chains.\nThe final parameter in MCMC Settings is the thinning parameter. This parameter will only keep every \\(N^{th}\\) sample taken during MCMC where \\(N\\) is the thinning parameter. Thinning MCMC samples can reduce the autocorrelation of consecutive MCMC iterations, which increases the effective samples per retained sample, but also results in needing many more MCMC iterations to reach the same number of retained samples. Generally, we do not recommend thinning for standard simulation runs.\n\n\n\n\n\n\nWarning about thinning\n\n\n\nUnlike other software that performs MCMC, when you choose to thin by a value, FACTS does not increase the number of MCMC iterations it performs in order to retain the value specified in “Number of Samples”. So if you leave “Number of Samples” at its default value, \\(2500\\), and thin by \\(10\\), you will be left with \\(250\\) retained samples. You should adjust for this by increasing the “Number of Samples” if you choose to thin."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#results-output",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#results-output",
    "title": "Simulation",
    "section": "Results Output",
    "text": "Results Output\nThe results output section of the Simulation tab allows for the specification of how many output files should be generated for files that are individually created for each simulation. Summary files (summary.csv) that have 1 line per scenario are always created. Simulations files (simulations.csv) that have 1 line per simulation are always created. Weeks files (weeksXXXXX.csv), patients files (patientsXXXXX.csv), and frequentist weeks files (weeks_freq_{missingness}_XXXXX.csv) are not created for every single simulation. Instead, the number of simulation specific output files can be set per type. This limits the amount of output files that FACTS will save.\nSee the endpoint specific descriptions of the output files for descriptions of what the previously mentioned output files report (continuous, dichotomous and time-to-event).\nSome plots in FACTS that are created based on weeks files, and if very few weeks files are saved, the plots will not be as accurate or descriptive."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#facts-grid-simulation-settings",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#facts-grid-simulation-settings",
    "title": "Simulation",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nA user with access to a computational grid may choose to run simulations on the grid instead of running them locally. This frees the user’s computer from the computationally intensive task of simulating so that they can continue other work or even shutdown their PC or laptop. In order to run simulations on the grid, it must first be configured. This is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#right-click-menu",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#right-click-menu",
    "title": "Simulation",
    "section": "Right Click Menu",
    "text": "Right Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 4: The menu that appears when you right click on the table within the simulation tab.\n\n\n\nThese will respectively:\n\nOpen a new Windows directory browser window showing the contents of the simulation results for that scenario.\nOpen a window that displays the individual simulation results for that scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results (see below) the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of the Simulation Results highlights window.\nOpen a window that displays the frequentist analysis summary results. This option is only available if one or more frequentist analyses have been selected on the Design &gt; Frequentist Analysis tab. (If more than one analysis has been requested – using different treatments of missing data there will be separate options in the menu to display each summary).\nOpen R loading in the result files for that scenario as separate dataframes.\nOpens the FACTS graph control displaying the graphs for that scenario.\nOpens the FACTS graph control that displays the trellis plot of graphs of selected scenarios for selected design variants."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#open-in-r",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#open-in-r",
    "title": "Simulation",
    "section": "Open in R",
    "text": "Open in R\nThe “Open in R” button allows for the creation of an R script that has pre-populated code for loading in output files created by the FACTS simulations.\nBy default, any/all of the simulation output files can be included in the created script. If “Aggregation” (see below) has been performed, then only the aggregated files will be available for being loaded in R.\nWhen the button is clicked, FACTS will create an R script with the correct file paths to load in the data, as well as creating a function that will read the files in correctly. The file is then opened in the default R editor for the user. If there is no default program for opening a .R file, your operating system should ask how you want to open the file."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#aggregation",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#aggregation",
    "title": "Simulation",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 5: Window that appears when aggregating simulation results.\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per group, plus an extra across groups row.\n\nWhere there is a group of columns for each group, they will be turned into a single column with each value on a new row.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. WeeksNNNNN.csv files are aggregated into a single agg_[pivot_]weeks.csv file. PatientsNNNNN.csv files are aggregated into a single agg_patients.csv file, but they are never pivoted because each row already refers to a single group. Similarly the various frequentist results at the summary, simulation and weeks level are aggregated (if they’ve been output).\nRegionIndex.csv is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nRecruitment Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g. External Subjects Profile if there are no external scenarios)\n\n\nDropouts Profile\n\n\n\nLongitudinal Rates Profile\n\n\n\nGroup Response Profile\n\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nP(TS)\nProportion of trial success (early success + late success)\n\n\nP(TF)\nProportion of trial futility (early futility + late futility)\n\n\nSims\nSimulation number. Only present in weeks and patients files.\n\n\nGroup\nOnly present if pivoted"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#design-report",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#design-report",
    "title": "Simulation",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#per-scenario-graphs",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#per-scenario-graphs",
    "title": "Simulation",
    "section": "Per Scenario Graphs",
    "text": "Per Scenario Graphs\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select that row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks. (Not displayed if the ‘y’ value must lie in the interval 0-1.\nGroup – on some graphs the results show results for the treatment effect in a specific group or across groups, this drop down allows the user to select which.\nSimulation – on some graphs the data shown is from a specific simulation, this control allows the user to select which one.\nInterim – on some graphs the data shown is from a specific interim in a specific simulation, this control allows the user to select which one.\n\n\nOutcome and Subject Allocation\n\nContinuousDichotomousTime-to-Event\n\n\n\nRelative Response and Allocation\n\n\n\n\n\n\nFigure 6: Relative Response and Allocation.\n\n\n\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations in each group plotted as a green bar.\nThe true difference in response between the study treatment and control in each group as a black cross.\nThe estimated mean difference in response (“treatment effect”) and the 2.5-97.5% interquartile range of the observed estimates across the simulations in each group as a red circle with vertical red error bars.\nThe estimated mean overall difference from the individual control responses and 2.5-97.5% interquartile range of the observed estimates across the simulations across the groups as a red circle with vertical red error bars.\nThe true population weighted across groups difference in response between the study treatment and control, calculated as the average of the true difference in response in each group weighted by the true population fractions of each group as defined in the actual profile, as a grey triangle.\nThe true design enriched across groups difference between the study treatment and control, calculated as the average of the true difference in response in each group weighted by the actual numbers of subjects recruited into each group, as a black star.\n\n\n\nResponse and Allocation\n\n\n\n\n\n\nFigure 7: Response and Allocation.\n\n\n\nThis is similar to the previous graph, except it shows the allocation to, and response on, the study treatment and control arms separately and not the treatment difference.\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations to control as a blue bar and to the study treatment arm as a green bar.\nThe true mean response to the study treatment in each group as a black cross.\nThe true mean response to the control in each group as a black diamond.\nThe estimated mean response on the study treatment arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as a red circle with vertical red error bars.\nThe estimated mean response on the control arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as a orange circle with vertical orange error bars.\n\n\n\n\n\nOdds Ratio and Allocation\n\n\n\n\n\n\nFigure 8: Odds Ratio and Allocation.\n\n\n\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations in each group plotted as a green bar.\nThe true response odds ratio between the study treatment and control in each group as a black cross.\nThe estimated response odds ratio (“treatment effect”) and the 2.5-97.5% interquartile range of the observed estimates across the simulations in each group as a red circle with vertical red error bars.\nThe estimated mean overall response odds ratio and 2.5-97.5% interquartile range of the observed estimates across the simulations across the groups as a red circle with vertical red error bars.\nThe true population weighted across groups response odds ratio between the study treatment and control, calculated as the average of the true odds ratio in each group weighted by the true population fractions of each group as defined in the actual profile, as a grey triangle.\nThe true design enriched across groups odds ratio between the study treatment and control, calculated as the average of the true odds ratio in each group weighted by the actual numbers of subjects recruited into each group, as a black star.\n\n\n\nResponse and Allocation\n\n\n\n\n\n\nFigure 9: Response and Allocation.\n\n\n\nThis is similar to the previous graph, except it shows the response rate on the study treatment and control arms separately and not the odds ratio.\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations to control as a blue bar and to the study treatment arm as a green bar.\nThe true mean response rate for the study treatment in each group as a black cross.\nThe true mean response rate for the control in each group as a blue triangle.\nThe estimated mean response rate on the study treatment arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as a red circle with vertical red error bars.\nThe estimated mean response rate on the control arms and the 2.5-97.5% interquartile range of the observed estimates across the simulations as an orange circle with vertical orange error bars.\n\n\n\n\n\nHazard Ratio and Allocation\n\n\n\n\n\n\nFigure 10: Hazard Ratio and Allocation.\n\n\n\nThis graph displays a histogram of the mean number of subjects recruited into each group. These plots show:\n\nThe mean allocation over all simulations in each group plotted as a green bar.\nThe true hazard ratio between the study treatment and control in each group as a black cross.\nThe estimated hazard ratio (“treatment effect”) and the 2.5-97.5% interquartile range of the observed estimates across the simulations in each group as a red circle with vertical red error bars.\nThe estimated mean overall hazard ratio and 2.5-97.5% interquartile range of the observed estimates across the simulations across the groups as a red circle with vertical red error bars.\nThe true population weighted across groups hazard odds ratio between the study treatment and control, calculated as the average of the true hazard ratio in each group weighted by the true population fractions of each group as defined in the actual profile, as a grey triangle.\nThe true design enriched across groups hazard ratio between the study treatment and control, calculated as the average of the true hazard ratio in each group weighted by the actual numbers of subjects recruited into each group, as a black star.\n\n\n\nHazard Rates\n\n\n\n\n\n\nFigure 11: Hazard Ratio graph.\n\n\n\nThe Hazard Rates graphs shows the number of events in each arm, the raw hazard ratio and fitted hazard ratios.\n\nThe blue show the number of events in the control arm.\nThe gray bar shows the number of events in the treatment arm.\nThe raw hazard ratio in each arm is shown by a gray circle.\nThe fitted hazard ratio in the control arm is shown by an orange diamond with orange bars indicating the 2.5-97.5% interquartile range.\nThe fitted hazard ratio in the treatment arm is shown by a red diamond, with red bars indicating the 2.5-97.5% interquartile range.\n\n\n\n\n\n\n\nProb. Group Compared to CSD/CSHRD Futility\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 12: Probability Response Compared to CSD for Futility plot.\n\n\n\nThis graph shows for each group and the across groups analysis, the probability of ‘beating’ the CSD for futility (the definition of ‘beating’ will depend on whether a higher endpoint score is a better or worse outcome for the subject and whether the trial is for superiority or non-inferiority).\nNote that though This is the comparison against the Futility CSD it is the probability of being better than it, higher probabilities mean less likelihood of stopping early for futility or declaring futility in the final evaluation.\n\nThe mean probability is plotted as solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\n\n\n\nFigure 13: Probability Response Compared to CSD for Futility plot.\n\n\n\nThis graph shows for each group and the across groups analysis, the probability of ‘beating’ the CSD for futility (the definition of ‘beating’ will depend on whether a higher endpoint score is a better or worse outcome for the subject and whether the trial is for superiority or non-inferiority).\nNote that though this is the comparison against the Futility CSD it is the probability of being better than it, higher probabilities mean less likelihood of stopping early for futility or declaring futility in the final evaluation.\n\nThe mean probability is plotted as a solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\n\n\n\nFigure 14: Probability Response Compared to CSHRD for Futility plot.\n\n\n\nThis graph shows for each group and the across groups analysis, the probability of ‘beating’ the CSHRD for futility (the definition of ‘beating’ will depend on whether a higher endpoint score is a better or worse outcome for the subject and whether the trial is for superiority or non-inferiority).\nNote that though this is the comparison against the Futility CSHRD it is the probability of being better than it, higher probabilities mean less likelihood of stopping early for futility or declaring futility in the final evaluation.\n\nThe mean probability is plotted as a solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\nProb. Group Compared to CSD/CSHRD Success and Group Phase III Success\nThis is the same as the “Prob. Group Compared to CSD/CSHRD Futility” plot, except that the probabilities are either\n\nRelative to control and the CSHRD for success.\nThe probability of Phase III success\n\n\n\nTrial Outcomes by Group\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 15: Relative Response and Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Outcomes by Group plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Outcomes by Group plot.\n\n\n\n\n\n\nThis plot shows as a stacked bar chart the proportion of different outcomes by group, across group and whole study.\nThe outcome types are:\n\nEarly Success (dark green): the group stopped early for success and had not regressed to futile (but it could have regressed to inconclusive) at the final analysis (if there was one).\nLate Success (light green): the group recruitment stopped because the group or study recruitment cap was reached; in the final evaluation of the group data the final evaluation success criteria were met.\nLate Futility (light red): the group recruitment stopped because the group or study recruitment cap was reached; in the final evaluation of the group data the final evaluation futility criteria were met.\nEarly Futility (dark red): the group stopped early for futility and had not regressed to success (but it could have regressed to inconclusive) at the final analysis (if there was one).\nSuccess to Futility Flip-Flop (pink): the group stopped early for success but had regressed to futility at the final analysis.\nFutility to Success Flip-Flop (purple): the group stopped early for futility but had regressed to success at the final analysis.\nInconclusive - Study Cap (dark brown): the group recruitment stopped because the study recruitment cap was reached; in the final evaluation of the group data neither the final evaluation success not the final evaluation futility criteria were met.\nInconclusive – Group Cap (light brown): the group recruitment stopped because the group recruitment cap was reached; in the final evaluation of the group data neither the final evaluation success not the final evaluation futility criteria were met.\n\n\n\nOutcome by Scatterplot\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 18: Group Outcome Scatterplot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Group Outcome Scatterplot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 20: Group Outcome Scatterplot.\n\n\n\n\n\n\nThis is a scatter plot graph that plots the result of a particular group or the ‘across groups’ plotting the estimate of response against the number of subjects recruited into the group or whole trial.\nThe symbol used to plot each simulation indicates the reason for stopping / outcome.\n\nLight blue circle: the group stopped early for success\nDark blue circle: the group did not stop early and was a success in the final analysis\nBrown square: the group did not stop early and the was futile in the final analysis\nRed square: the group stopped early for futility\nLight pink diamond: the group stopped early for success but was futile in the final analysis.\nBrown diamond: the group stopped early for futility but was successful in the final analysis\nYellow cross: the group outcome was inconclusive; the study reached the study cap.\nBlue cross: the group outcome was inconclusive; the group reached the group cap.\nPink cross: the group outcome was inconclusive; the study stopped early.\n\nThere is a control that allows the user to select whether the points are plotted for a particular group or for the whole study – using the across groups treatment estimate.\n\n\nDistribution of Early Stopping (Futility)\n\nContinuousDichotomousTime-to-Event\n\n\n\n\n\n\n\n\nFigure 21: Distribution of Early Stopping plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 22: Distribution of Early Stopping plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Distribution of Early Stopping plot.\n\n\n\n\n\n\nThis plot shows the proportion of times each group has stopped early for futility as a brown bar, plus box and whisker plots showing the distribution, in time in weeks, of when those early stops occurred.\n\n\nDistribution of Early Stopping (Success)\nThis plot is the same as for the “Distribution of Early Stopping (Futility)” plot, except it shows the proportion of times each group has stopped early for success and the distribution in stopping times of those stops.\n\n\nCumulative Operating Characteristics Plot\n\n\n\n\n\n\n\n\nFigure 24\n\n\n\n\n\n\n\n\n\n\nFigure 25\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\nPer Sim and Interim Relative Response and Allocation\nThese graphs exclusive to continuous and dichotomous endpoint designs show similar information to the Relative Response and Allocation graph above, but show the results for a single simulation, or single interim of a single simulation. The individual interim results can only be shown for simulations for which ‘weeks’ files were output.\n\n\nPer Sim and Interim Relative Response and Allocation\nThese graphs show similar information to the Outcome and Allocation graph above, but show the results for a single simulation, or single interim of a single simulation. The individual interim results can only be shown for simulations for which ‘weeks’ files were output.\n\n\nExplore Final Futility/Success Criteria\n\n\n\n\n\n\n\n\nFigure 26\n\n\n\n\n\n\n\n\n\n\nFigure 27\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select criteria to use: posterior probability of beating the CSD/CSHRD or probability of phase III success, and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis).\nFor the given target the proportion of trials that would meet each of the criteria over the range of threshold values is plotted for each group and across group treatment effects.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\nExplore Early Success/Futility Eval Criteria\n\n\n\n\n\n\n\n\nFigure 28\n\n\n\n\n\n\n\n\n\n\nFigure 29\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (as in the examples above where the shape of the “existing stopping rules” line indicates that no early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which stopping criteria is evaluated and from which interim stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\nMCMC Trace plots\n\n\n\n\n\n\nFigure 30: MCMC Trace Plot.\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC. (See the description of the MCMC file contents below 16.6).\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/index.html#across-scenario-graphs",
    "href": "documentation/v72/userguides/enrichment/simulation/index.html#across-scenario-graphs",
    "title": "Simulation",
    "section": "Across Scenario Graphs",
    "text": "Across Scenario Graphs\nTo view multiple graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot. You can select the graph type, filter the design variants and filter which scenarios displayed:\n\n\n\n\n\n\nFigure 31: Window that appears when aggregating simulation results.\n\n\n\n\nSelected Groups\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each group was successful in a trial that was successful.\n\n“Successful” –the arm was correctly successful: it was successful, marked as “Should succeed” on the VSR tab and the trial was successful.\n“Should not succeed” – the arm was incorrectly successful: it was successful but not marked as “Should succeed” on the VSR tab and the trial was successful.\nUnsuccessful – the trial was successful but the group was not.\n\n\n\n\n\n\n\nFigure 32\n\n\n\n\n\nQOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each group. There is a dropdown control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\n\n\n\n\n\nFigure 33\n\n\n\n\n\nPpn Success\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\n\n\n\n\n\nFigure 34\n\n\n\n\n\nResponse\nThis graph shows a group response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\n\n\n\n\n\nFigure 35\n\n\n\n\n\nAllocation\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm in each group over the simulations.\n\n\n\n\n\n\nFigure 36\n\n\n\n\n\nTotal Subjects\nThis graph shows the mean total sample size for each scenario at different maximum sample sizes (the different variants).\n\n\n\n\n\n\nFigure 37"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, they can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response (change from baseline) for the control arm in each group.\n\n\nMean Ctrl Response: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of response (change from baseline) for the study treatment in each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the across groups effect size (the estimate across the groups of the difference between response on the study treatment and the historic control rate or the mean response on the control arm).\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘sigma’ that is the common standard deviation of the distributions of the final endpoints for each treatment arm.\n\n\nSD Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for sigma.\n\n\nMean Overall Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘the across groups sigma’ that is the common standard deviation of the distributions of the final endpoints for the pooled control subjects and pooled study treatment subjects.\n\n\nSD Overall Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for the pooled sigma.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response for the scenario\n\n\nTrue SD Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. This is the true standard deviation of the control response for the scenario.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response in each group for the scenario.\n\n\nTrue SD Trt Resp: &lt;Group&gt;\nOne per group\nThe standard deviation of the treatment response in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met\n\n\nPpn Futility Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met at the final evaluation\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Baseline Beta\n1\nThis is the average (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nSD Baseline Beta\n1\nThis is the standard deviation (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nMean Baseline: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nSD Baseline &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nTrue Mean Baseline &lt;Group&gt;\nOne per group\nTrue mean baseline for the scenario\n\n\nTrue SD Baseline &lt;Group&gt;\nOne per group\nTrue standard deviation of baseline for the scenario\n\n\n\n\n\n\nThese parameters are unused in ED unless the Time Course Hierarchical or ITP longitudinal analysis models are used.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt;Visit&gt;\nGroup * Arms * Visits\nThis is the mean (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit.\n\n\nSE Mean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt; Visit&gt;\nGroup * Arms * Visits\nThis is the Standard Error (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit.\n\n\n\n\n\n\nThis is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results.\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nSD Sigma\n1\nThis is the SD (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nMean Beta\n1\nThis is the mean (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nSD Beta\n1\nThis is the SD (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nPPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the within group difference between treatment and control was significant.\n\n\n\n\n\n\nThe columns in this table are the same as for the “Frequentist” results, but are the results of an ANCOVA test."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#highlights",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#highlights",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "These are the columns displayed on the simulations tab after simulations are completed, they can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Success Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups success criteria have been after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where the study was a success after final evaluation.\n\n\nPpn Futility Criteria Met (Final): &lt;group&gt;\nOne per Group\nThe proportion of simulations where each groups futility criteria have been met after final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations where the study was futile after final evaluation.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Succ to Futility Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn Futility to Succ Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inconclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit\n\n\nPpn Correct Groups\n1\nThe proportion of simulations that met the success criteria and selected groups marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups flagged as “Should succeed” is used.\n\n\nPpn Incorrect Groups\n1\nThe proportion of simulations that met the success criteria and selected the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab. For each simulation the fraction of the total number of groups not flagged as “Should succeed” is used\n\n\nVersion\n1\nThis the version number of the FACTS GUI that was used to create the parameters that were passed to the design engine that ran the simulations."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#allocation",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#allocation",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\nNum Subj 80%\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Group&gt;, &lt;Arm&gt;\nOne for each arm in each group\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nMean Accrual Duration\n1\nThis is the mean (over the simulations) of the duration of the accrual period, from the start of the trial to last patient first visit"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#response",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#response",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Ctrl Response: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the mean estimate of response (change from baseline) for the control arm in each group.\n\n\nMean Ctrl Response: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the mean (over the simulations) of the average of the response on all the control arms across all the groups. (Note: this is not part of the response model, but estimated separately)\n\n\nMean Trt Response: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of response (change from baseline) for the study treatment in each group.\n\n\nMean Trt Response: Across groups\n1\nThis is the mean (over the simulations) of the across groups effect size (the estimate across the groups of the difference between response on the study treatment and the historic control rate or the mean response on the control arm).\n\n\nSD Control Resp: &lt;Group&gt;\nOne per group (if control treatment arm included)\nThese columns are -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the mean estimate of the response for the control arm in each group.\n\n\nSD Control Resp: Across groups\n1 (if control treatment arm included)\nThis column is -9999 if there are no control arms included in the design. This is the standard deviation (over the simulations) of the average of the mean estimates of response for all the control arms across the groups.\n\n\nSD Trt Resp: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the response for the study treatment in each group across the simulations. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nSD Trt Resp: Across groups\n1\nThis is the standard deviation (over the simulations) of the estimate of the across groups effect size.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘sigma’ that is the common standard deviation of the distributions of the final endpoints for each treatment arm.\n\n\nSD Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for sigma.\n\n\nMean Overall Sigma\n1\nThis is the mean (over the simulations) of the mean estimate for ‘the across groups sigma’ that is the common standard deviation of the distributions of the final endpoints for the pooled control subjects and pooled study treatment subjects.\n\n\nSD Overall Sigma\n1\nThis is the standard deviation (across the simulations) of the mean estimates for the pooled sigma.\n\n\nTrue Mean Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. True mean control response for the scenario\n\n\nTrue SD Ctrl Resp: &lt;Group&gt;\nOne per group\nThis column is -9999 if there are no control arms included in the design. This is the true standard deviation of the control response for the scenario.\n\n\nTrue Mean Trt Resp: &lt;Group&gt;\nOne per group\nTrue mean treatment response in each group for the scenario.\n\n\nTrue SD Trt Resp: &lt;Group&gt;\nOne per group\nThe standard deviation of the treatment response in each group for the scenario.\n\n\nMean Mu_Theta\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Mu_Theta\n1\nThis is the standard deviation (over the simulations) of the hierarchical mean of theta. -9999 if groups are modeled separately.\n\n\nMean Tau_theta\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nSD Tau_theta\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group. -9999 if groups are modeled separately.\n\n\nMean Mu_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Mu_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nMean Tau_Gamma\n1\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control.\n\n\nSD Tau_Gamma\n1\nThis is the standard deviation (across the simulations) of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group. -9999 if groups are modeled separately or there is no control."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#observed",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#observed",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs: ‘Execution &gt; Accrual’, ‘Execution &gt; Dropout Rate’, ‘Virtual Subject Response &gt; Composite’. This is the same name as used for the results directory.\n\n\nMean Complete &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the control arm which have had their endpoint observed in this scenario.\n\n\nMean Complete &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of the number of subjects recruited per group for the treatment arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Control\nOne per group\nThis is the mean (over the simulations) of information observed per group for the control arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Complete Info &lt;Group&gt; Treatment\nOne per group\nThis is the mean (over the simulations) of information observed per group for the treatment arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#probabilities",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#probabilities",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nProb. Phase III Success: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of success in phase 3 for the study treatment in each group.\n\n\nProb. Phase III Success: Across groups\n1\nThis is the average (over the simulations) of the probability of success in phase 3 for the across groups effect.\n\n\nProb. CSD (Success): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for success.\n\n\nProb. CSD (Success): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for success.\n\n\nProb. CSD (Futility): &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the probability of the response being better than the response on control by the CSD for that group for futility.\n\n\nProb. CSD (Futility): Across groups\n1\nThis is the average (over the simulations) of the probability of the across group effect being better than the across group CSD for futility."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#stopping-rules",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#stopping-rules",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria before the final analysis.\n\n\nPpn CSD Success Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria before the final analysis.\n\n\nPpn CSD Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria before the final analysis.\n\n\nPpn CSD Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria before the final analysis.\n\n\nPpn P3 Success Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Success Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn P3 Futility Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn P3 Futility Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Success: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Success: Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria before the final analysis.\n\n\nPpn Combined Futility: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Combined Futility: Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria before the final analysis.\n\n\nPpn Success Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations where the across groups success criteria has been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met\n\n\nPpn Futility Criteria: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met before the final evaluation, this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Futile\n1\nThis is the proportion of simulations in which the study futility criteria were met"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#evaluation-rules",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#evaluation-rules",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn CSD Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD futility criteria at the final analysis.\n\n\nPpn CSD Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn P3 Futility (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the CSD success criteria at the final analysis.\n\n\nPpn CSD Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across groups analysis met the CSD success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn P3 Success (Final) Met: Across Groups\n1\nThis is the proportion of simulations where the across group analysis met the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Futility (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Futility (Final): Across groups\n1\nThis is the proportion of simulations where the across group analysis met both the CSD futility criteria and the probability of success in Phase 3 futility criteria at the final analysis.\n\n\nPpn Combined Success (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Combined Success (Final): Across groups\n1\nThis is the proportion of simulations where the across groups analysis met both the CSD success criteria and the probability of success in Phase 3 success criteria at the final analysis.\n\n\nPpn Success Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where each groups success criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria and any across groups criteria were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThis is the proportion of simulations where the across groups success criteria has been met at the final evaluation, this means that the Phase 3 and/or CSD criteria were all satisfied.\n\n\nPpn Study Success\n1\nThis is the proportion of simulations in which the study success criteria were met at the final evaluation\n\n\nPpn Futility Criteria Met (Final): &lt;Group&gt;\nOne per group\nThis is the proportion of simulations where the each groups futility criteria have been met at the final evaluation, this means that the Phase 3 and/or CSD criteria, and any across groups criteria were all satisfied.\n\n\nPpn Study Futile (Final)\n1\nThis is the proportion of simulations in which the study futility criteria were met at the final evaluation"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#hierarchical-prior-parameters",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#hierarchical-prior-parameters",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean BAC Mu: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD BAC Mu: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the mean of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nMean Bac Tau: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used.\n\n\nSD Bac Tau: &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the mean estimate of the standard deviation of the hierarchical distribution of the response on the control arm in the group and the response on the control arms in the specified historical studies. -9999 if BAC not used."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#baseline",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#baseline",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Baseline Beta\n1\nThis is the average (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nSD Baseline Beta\n1\nThis is the standard deviation (over the simulations) of the β that relates baseline to subject response. Values are -9999 if baseline adjustment not used.\n\n\nMean Baseline: &lt;Group&gt;\nOne per group\nThis is the average (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nSD Baseline &lt;Group&gt;\nOne per group\nThis is the standard deviation (over the simulations) of the baseline. Values are -9999 if baseline is not used\n\n\nTrue Mean Baseline &lt;Group&gt;\nOne per group\nTrue mean baseline for the scenario\n\n\nTrue SD Baseline &lt;Group&gt;\nOne per group\nTrue standard deviation of baseline for the scenario"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#model-parameters",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#model-parameters",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "These parameters are unused in ED unless the Time Course Hierarchical or ITP longitudinal analysis models are used.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nMean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt;Visit&gt;\nGroup * Arms * Visits\nThis is the mean (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit.\n\n\nSE Mean Longmod Resp: &lt;Group&gt; &lt;Arm&gt; &lt; Visit&gt;\nGroup * Arms * Visits\nThis is the Standard Error (over the simulations) of the estimate of the mean response for that particular arm, in that group at that visit."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#simulation-results",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#simulation-results",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "This is a display of the contents of the simulations.csv files, with windows available via the right click menu that show various relevant column groupings similar to the summary results."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#frequentist-results",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#frequentist-results",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "Column Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario. It is constructed from the names of all the profiles that have been combined to create the scenario.\n\n\nPpn Signif (Grp)\n1\nThis is the proportion of simulations where the difference in response between groups was significant\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nPpn Signif (Trt)\n1\nThis is the proportion of the simulations where the overall difference between treatment and control was significant\n\n\nPpn Signif (Grp x Trt)\n1\nThis is the proportion of the simulations where the treatment x group interaction term was significant.\n\n\nMean Trt Effect: &lt;Group&gt;\nOne per group\nThis is the mean (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nSD Trt Effect: &lt;Group&gt;\nOne per group\nThis is the SD (over the simulations) of the frequentist estimate of the treatment effect within each group.\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nSD Sigma\n1\nThis is the SD (over the simulations) of the frequentist estimate of the SD of the response across all subjects.\n\n\nMean Beta\n1\nThis is the mean (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nSD Beta\n1\nThis is the SD (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nPPn Signif: &lt;Group&gt;\nOne per group\nThis is the proportion of the simulations where the within group difference between treatment and control was significant."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#frequentist-ancova-results",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#frequentist-ancova-results",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "",
    "text": "The columns in this table are the same as for the “Frequentist” results, but are the results of an ANCOVA test."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-summary.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-summary.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe columns in summary.csv are common across all the FACST Dose Finding design engines, hence there are columns in the file when using the N-CRM design engine with no contents as they are for results no applicable to this engine.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Project\n1\nThe name of the “.facts” file in the FACTS GUI that was used to generate the simulations.\n\n\nScenario\n1\nThe name of the scenario – This is the various profile names that make up the scenario, concatenated together.\n\n\nTimestamp\n1\nThe date and time when the simulations started.\n\n\nVersion\n1\nThe version number of the FACTS GUI that ran the simulations.\n\n\nNsim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\n80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nMean Alloc &lt;Group&gt; \n2*G\nThe mean number of subjects recruited into each arm in each group. If a control arm is not included the column is still present, with value ‘0’.\n\n\nMean Trt Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response on the study treatment arm in each group.\n\n\nMean Trt Effect (Overall)\n1\nThe mean of the mean estimate of the across groups treatment difference.\n\n\nSE Trt Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response on the study treatment arm in each group.\n\n\nSE Trt Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups treatment difference.\n\n\nMean Control Resp &lt;Group&gt;\nG\nThe mean of the mean estimates of response on the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\nThe mean of the average of the control response across the groups\n\n\nSE Control Resp &lt;Group&gt;\nG\nThe standard error, over the simulations, of the mean estimate of response rate on the control arm in each group.\n\n\nSE Avg. Control Effect (Overall)\n1\nThe standard error, over the simulations, of the mean estimate of the across groups control response rate.\n\n\nMean Sigma\n1\nThe mean of the estimate of the standard deviation in the endpoint from the estimates of the response of the individual arms in each group.\n\n\nSE Sigma\n1\nThe standard error of the estimate of the standard deviation in the endpoint from the estimates of the response of the individual arms in each group.\n\n\nMean Overall Sigma\n1\nThe mean of the estimate of the standard deviation in the endpoint from the estimates of the treatment difference across the groups.\n\n\nSE Overall Sigma\n1\nThe standard error of the estimate of the standard deviation in the endpoint from the estimates of the treatment difference across the groups.\n\n\nTrue Mean Trt Resp &lt;Group&gt;\nG\nTrue mean treatment response for the scenario\n\n\nTrue Mean Control Resp &lt;Group&gt;\nG\nTrue mean control response for the scenario\n\n\nMean Baseline Beta\n1\nThe mean of the mean estimates of the baseline adjustment\n\n\nSE Baseline Beta\n1\nThe standard error of the mean estimates of the baseline adjustment\n\n\nMean Baseline &lt;Group&gt;\nG\nThe mean of the mean estimate of the baseline\n\n\nSE Baseline &lt;Group&gt;\nG\nThe standard error of the mean estimates of the baseline\n\n\nSD Baseline &lt;Group&gt;\nG\nThe mean of the SDs of the estimates of the baseline\n\n\nSE SD Baseline &lt;Group&gt;\nG\nThe standard error of the estimates of the SD of the baseline\n\n\nTrue Mean Baseline &lt;Group&gt;\nG\nThe true mean baseline from the scenario\n\n\nTrue SD Baseline &lt;Group&gt;\nG\nThe true standard deviation of the baseline for the scenario\n\n\nMean Mu Theta\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean study treatment difference from control in each group.\n\n\nSE Mu Theta\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean study treatment difference from control in each group.\n\n\nMean Tau Theta\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the mean study treatment difference from control in each group.\n\n\nSE Tau Theta\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of the hierarchical distribution of the study treatment difference from control in each group.\n\n\nMean Mu Gamma\n1\nThe mean, over the simulations, of the mean estimate of the mean of hierarchical distribution of the mean response on the control arm in each group.\n\n\nSE Mu Gamma\n1\nThe standard error, over the simulations, of the mean estimate of the mean of the hierarchical distribution of the mean response on the control arm in each group.\n\n\nMean Tau Gamma\n1\nThe mean, over the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group.\n\n\nSE Tau Gamma\n1\nThe standard error, across the simulations, of the mean estimate of the standard deviation of hierarchical distribution of the mean response on the control arm in each group.\n\n\nMean Longmod Resp &lt;Group&gt;  \nG * A * V\nThis is the mean, over the simulations, of the response in a particular group, on a particular arm at a particular visit. These columns are only present if a TCH or ITP longitudinal model is being fitted (other models do not produce an estimate of the mean response at a visit).\n\n\nSE Mean Longmod Resp &lt;Group&gt;  \nG * A *V\nThis is the Standard Error, over the simulations, of the mean estimate of response at a visit. These columns are only present if a TCH or ITP longitudinal model is being fitted (other models do not produce an estimate of the mean response at a visit).\n\n\nMean Pr Ph3 Success &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of success in phase 3 of the study treatment in each group.\n\n\nMean Pr Ph3 Success 99\n1\nThe mean, across the simulations, of the mean probability of success in phase 3 of the across groups treatment difference.\n\n\nMean Pr CSD (Success) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for success in each group.\n\n\nMean Pr CSD (Success) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for success.\n\n\nMean Pr CSD (Futility) &lt;Group&gt;\nG\nThe mean, over the simulations, of the mean probability of the study treatment arm being better than the control by the CSD for futility in each group.\n\n\nMean Pr CSD (Futility) 99\n1\nThe mean, over the simulations, of the mean probability of the across groups treatment difference being better than the CSD for futility.\n\n\nPpn CSD Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its futility criterion in each group.\n\n\nPpn Ph3 Futility 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its CSD success criterion.\n\n\nPpn Ph3 Success &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability of success in phase 3 of the response on the study treatment arm met its success criterion in each group.\n\n\nPpn Ph3 Success 99\n1\nThe proportion of simulations in which the across group treatment difference met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility &lt;Group&gt;\nG\nThe proportion of simulations where each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success &lt;Group&gt;\nG\nThe proportion of simulations where each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success 99\n1\nThe proportion of simulations where the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations where each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success\n1\nThe proportion of simulations where the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met &lt;Group&gt;\nG\nThe proportion of simulations where each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile\n1\nThe proportion of simulations where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn CSD Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD futility criterion in each group.\n\n\nPpn CSD Futility (Final) 99\n1\nThe proportion of simulations in which after all data has been collected the across group treatment difference met its CSD futility criterion.\n\n\nPpn Ph3 Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its futility criterion in each group.\n\n\nPpn Ph3 Futility (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 futility criterion.\n\n\nPpn CSD Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the estimate after all data has been collected of the response on the study treatment arm met its CSD success criterion in each group.\n\n\nPpn CSD Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its CSD success criterion.\n\n\nPpn Ph3 Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations in which the predicted probability after all data has been collected of success in phase 3 of the response on the study treatment arm, met its success criterion in each group.\n\n\nPpn Ph3 Success (Final) 99\n1\nThe proportion of simulations in which the across group treatment difference after all data has been collected met its probability of success in phase 3 success criterion.\n\n\nPpn Combined Futility (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Futility (Final) 99\n1\nThe proportion of simulations where the across groups treatment effect after all data has been collected met both the CSD futility criteria and the probability of success in Phase 3 futility criteria.\n\n\nPpn Combined Success (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each group met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Combined Success (Final) 99\n1\nThe proportion of simulations where after all data has been collected the across groups treatment effect met both the CSD success criteria and the probability of success in Phase 3 success criteria.\n\n\nPpn Success Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups success criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied.\n\n\nPpn Study Success (Final)\n1\nThe proportion of simulations where after all data has been collected the study was a success (whether stopping early or reaching full accrual).\n\n\nPpn Futility Criteria Met (Final) &lt;Group&gt;\nG\nThe proportion of simulations where after all data has been collected each groups futility criteria have been met either for early stopping or final evaluation – for early stopping this means that the Phase 3 and/or CSD criteria, the across groups criteria and the minimum number of subjects to stop were all satisfied\n\n\nPpn Study Futile (Final)\n1\nThe proportion of simulations after all data has been collected where the study was futile (whether stopping early or reaching full accrual).\n\n\nPpn Outcome 1\n1\nThe proportion of simulations that stopped early for success.\n\n\nPpn Outcome 2\n1\nThe proportion of simulations that reached full accrual and declared success on final evaluation.\n\n\nPpn Outcome 3\n1\nThe proportion of simulations that reached full accrual and declared futility on final evaluation.\n\n\nPpn Outcome 4\n1\nThe proportion of simulations that stopped early for futility\n\n\nPpn Outcome 5\n1\nThe proportion of simulations that stopped early for success but were deemed futile on final evaluation.\n\n\nPpn Outcome 6\n1\nThe proportion of simulations that stopped early for futility but were deemed successful on final evaluation.\n\n\nPpn Outcome 7\n1\nThe proportion of simulations that reached full accrual and were inconclusive.\n\n\nMean Study Accrual Stop Week\n1\nThe mean study duration of accrual – from start of accrual to last patient first visit.\n\n\nMean BAC Mu &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Mu &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nMean BAC Tau &lt;Group&gt;\nG\nThe mean, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSE BAC Tau &lt;Group&gt;\nG\nThe standard error, over the simulations, of the posterior standard deviation of the Bayesian Augmented Control hierarchical distribution for each group."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-summary_freq.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-summary_freq.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn Significant (Grp)\n1\nThe proportion of simulations with a significant group effect.\n\n\nMean Overall Trt Effect\n1\nThis is the mean (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nSE Mean Overall Trt Effect\n1\nThis is the Standard Error (over the simulations) of the frequentist estimate of the overall treatment effect\n\n\nPpn Significant (Trt)\n1\nThe proportion of simulations with an overall significant treatment effect.\n\n\nsignificant (GrpxTrt)\n1\nThe proportion of simulations where the group interaction term was significant.\n\n\nMean Trt Effect &lt;group&gt;\nG\nThe mean, over the simulations, of the estimate of the mean treatment effect in each group.\n\n\nSE Trt Effect &lt;group&gt;\nG\nThe standard error, over the simulations, of the estimate of the mean treatment effect in each group\n\n\nMean Sigma\n1\nThe mean, over the simulations, of the estimate of the standard deviation in the final response.\n\n\nSE Sigma\n1\nThe standard error, over the simulations, of the estimate of the standard deviation in the final response.\n\n\nMean Beta\n1\nThis is the mean (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nSE Beta\n1\nThis is the SD (over the simulations) of the estimate of the regression coefficient for the baseline.\n\n\nPpn Significant &lt;group&gt;\nG\nThe proportion of simulations in the which the treatment effect was significant in each group."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n#Sims\n1\n✔\n\nSimulation number\n\n\nWeeks (Duration)\n1\n✔\n\nThe week of final analysis – the total duration of the simulation.\n\n\n#Week\n1\n\n✔\nWeek\n\n\nNo. Subj\n\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nAlloc &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe number of subjects allocated to each arm in each group.\n\n\nMean Trt Resp &lt;group&gt;\nG\n✔\n✔\nThe estimated mean response of the study treatment in the group.\n\n\nTrt Effect (Overall)\n1\n✔\n✔\nThe estimated mean treatment difference across the groups.\n\n\nSD Mean Trt Resp &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the study treatment in the group.\n\n\nSD Trt Effect (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the mean treatment difference across the groups.\n\n\nMu Theta\n1\n✔\n✔\nThe mean of the hierarchical distribution of the treatment differences over all the groups.\n\n\nTau Theta\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the treatment differences over all the groups.\n\n\nMean Control Resp &lt;group&gt;\nG\n✔\n✔\nThe estimated mean response of the control arm in each group.\n\n\nAvg. Control Resp (Overall)\n1\n✔\n✔\nThe average overall mean response of the control arm over all the groups. (Note: this is not part of the response model, but estimated separately).\n\n\nSD Mean Control Resp &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean response of the control arm in each group.\n\n\nSD Avg. Control Resp (Overall)\n1\n✔\n✔\nThe standard deviation of the estimate of the average response over all the control arms.\n\n\nMu Gamma\n1\n✔\n✔\nThe mean of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nTau Gamma\n1\n✔\n✔\nThe standard deviation of the hierarchical distribution of the responses on the control arms across all the groups.\n\n\nSigma\n1\n✔\n✔\nThe estimate of the standard deviation of the final responses given the per-group response model.\n\n\nSD Sigma\n1\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the final responses given the per-group response model.\n\n\nSigma Overall\n1\n✔\n✔\nThe estimate of the standard deviation of the final response given the across-groups response model.\n\n\nSD Sigma Overall\n1\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the final response given the across group response model.\n\n\nTrue Mean Trt Resp &lt;group&gt;\nG\n✔\n✔\nTrue mean treatment response for the scenario\n\n\nTrue SD Trt Resp &lt;group&gt;\nG\n✔\n✔\nTrue standard deviation of treatment response for the scenario\n\n\nTrue Mean Control Resp &lt;group&gt;\nG\n✔\n✔\nTrue mean control response for the scenario\n\n\nTrue SD Control Resp &lt;group&gt;\nG\n✔\n✔\nTrue standard deviation of control response for the scenario\n\n\nBaseline Beta\n1\n✔\n✔\nThe estimated mean of the baseline adjustment\n\n\nSD Baseline Beta\n1\n✔\n✔\nThe estimated standard deviation of the baseline adjustment\n\n\nMean Baseline &lt;group&gt;\nG\n✔\n✔\nThe estimated mean of the baseline\n\n\nSE Baseline &lt;group&gt;\nG\n✔\n✔\nThe estimated standard error of the baseline\n\n\nSD Baseline &lt;group&gt;\nG\n✔\n✔\nThe estimated mean of the standard deviation of baseline\n\n\nTrue Mean Baseline &lt;group&gt;\nG\n✔\n✔\nTrue mean baseline from the scenario\n\n\nTrue SD Baseline &lt;group&gt;\nG\n✔\n✔\nTrue standard deviation of baseline from the scenario\n\n\nNum Completed &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe number of subjects completed (final endpoint available) in each arm in each group.\n\n\nComplete Info &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe amount of complete information in each arm in each group – however that has been defined in the definition of interim timing – subjects enrolled, subjects complete at a certain visit or subjects who had the opportunity to complete at a certain visit.\n\n\nMean Raw Response &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe mean raw response\n\n\nSD Mean Raw Response &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nThe standard deviation of the raw response\n\n\nNum Dropouts &lt;group&gt; &lt;arm&gt;\n2 * G\n✔\n✔\nNumber of dropouts seen\n\n\nPr Ph3 Success &lt;group&gt;\nG\n✔\n✔\nThe probability of success in phase 3 of the study treatment for each group.\n\n\nPr Ph3 Success 99\n1\n✔\n✔\nThe probability of success in phase 3 of the across groups treatment difference.\n\n\nPr CSD (Success) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the success NIM) of the study treatment for each group.\n\n\nPr CSD (Success) 99\n1\n✔\n✔\nThe probability of being better than control by the success CSD (or as good as control by the NIM) of the across groups treatment difference.\n\n\nPr CSD (Futility) &lt;group&gt;\nG\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the study treatment for each group.\n\n\nPr CSD (Futility) 99\n1\n✔\n✔\nThe probability of being better than control by the futility CSD (or as good as control by the futility NIM) of the across groups treatment difference.\n\n\nCSD Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for early stopping for futility for the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for early stopping for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success 99\n1\n✔\n✔\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for early stopping for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Futility 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for futility in the across groups analysis, 0 = they are not all met.\n\n\nCombined Success &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success for each group, 0 = they are not all met. Whether the across group stopping condition is also met if required is not included in this flag.\n\n\nCombined Success 99\n1\n✔\n✔\nA flag: 1 = the required CSD, success in phase 3 and minimum sample size criteria are all met for early stopping for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required success criteria were met. 0 = Not met.\n\n\nSuccess Criteria Met 99\n1\n✔\n✔\nA flag: 1 = the required success criteria were met. 0 = Not met.\n\n\nFutility Criteria Met &lt;group&gt;\nG\n✔\n✔\nA flag: 1 = the required futility criteria were met. 0 = Not met.\n\n\nFutility Criteria Met 99\n1\n✔\n✔\nA flag: 1 = the required futility criteria were met. 0 = Not met.\n\n\nCSD Futility (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nCSD Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the futility CSD was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility for each group, 0 = the posterior probability was above the threshold.\n\n\nPh3 Futility (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was below the threshold for the final evaluation for futility in the across groups analysis, 0 = the posterior probability was above the threshold.\n\n\nCSD Success (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nCSD Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of being better than control by the success CSD was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success for each group, 0 = the posterior probability was below the threshold.\n\n\nPh3 Success (Final) 99\n1\n✔\n\nA flag: 1 = the posterior probability of successful in a subsequent phase 3 trial was above the threshold for the final evaluation for success in the across groups analysis, 0 = the posterior probability was below the threshold.\n\n\nCombined Futility (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are met for the final evaluation for futility for each group, 0 = they are not all met. Whether the across group final futility condition is also met if required is not included in this flag.\n\n\nCombined Futility (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for futility in the across group analysis, 0 = they are not all met.\n\n\nCombined Success (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success for each group, 0 = they are not all met. Whether the across group final success condition is also met if required is not included in this flag.\n\n\nCombined Success (Final) 99\n1\n✔\n\nA flag: 1 = the required CSD and success in phase 3 criteria are all met for the final evaluation for success in the across groups analysis, 0 = they are not all met.\n\n\nSuccess Criteria Met (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nSuccess Criteria Met (Final) 99\n1\n✔\n\nA flag: 1 = the group’s success criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met (Final) &lt;group&gt;\nG\n✔\n\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nFutility Criteria Met (Final) 99\n1\n✔\n\nA flag: 1 = the group’s futility criteria were met, 0 = otherwise.\n\n\nFutile Study\n1\n✔\n✔\nA flag: 1 = the study was futile overall, 0 = otherwise.\n\n\nSuccessful Study\n1\n✔\n✔\nA flag: 1 = the study was successful overall, 0 = otherwise.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome: = Early success = Late success = Late futility = Early futility = Success to futility flip-flop = Futility to success flip-flop = Inconclusive\n\n\nGroup Outcome &lt;group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: = Early success = Late success = Late futility = Early futility = Success to futility flip-flop = Futility to success flip-flop = Inconclusive\n\n\nGroup Stop Type &lt;group&gt;\nG\n✔\n✔\nA flag categorizing the group outcome: = Success = Group Cap = Futility = Other (group stopped because study stopped early) = Study cap\n\n\nGroup Stop Week &lt;group&gt;\nG\n✔\n✔\nThe week the group stop decision was taken. There may be further follow-up time before the group analysis was completed.\n\n\nAccrual Stop Week\n1\n✔\n✔\nThe week the study stop decision was taken. There may be further follow-up time before the study analysis was completed.\n\n\nBAC Mu &lt;group&gt;\nG\n✔\n✔\nThe mean estimated value of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Mu &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the mean of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nBAC Tau &lt;group&gt;\nG\n✔\n✔\nThe mean estimated value of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nSD BAC Tau &lt;group&gt;\nG\n✔\n✔\nThe standard deviation of the estimate of the standard deviation of the Bayesian Augmented Control hierarchical distribution for each group.\n\n\nLinear Regression Alpha &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the linear regression longitudinal model the mean estimate of the constant offset in the change in response from this visit to the final visit\n\n\nLinear Regression Beta &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the linear regression longitudinal model the mean estimate of the coefficient of change in response from this visit to the final visit\n\n\nLinear Regression Lambda &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the linear regression longitudinal model the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\n\n\nTCH Alpha &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the time course hierarchical longitudinal model the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit.\n\n\nTCH Lambda &lt;model&gt; &lt;visit&gt;\nLM*V\n✔\n✔\nOnly if using the time course hierarchical longitudinal model the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.\n\n\nTCH Tau &lt;model&gt;\nLM\n✔\n✔\nOnly if using the time course hierarchical longitudinal model the mean estimate of the SD of the per subject random effect\n\n\nITP K &lt;model&gt;\nLM\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the ITP shape parameter\n\n\nITP Lambda &lt;model&gt;\nLM\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the SD of the residual error.\n\n\nITP Tau &lt;model&gt;\nLM\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the SD of the per subject random effect\n\n\nITP Omega &lt;group&gt; &lt;arm&gt;\nG*2\n✔\n✔\nOnly if using the ITP longitudinal model the mean estimate of the mean treatment arm effect.\n\n\nLongmod Resp &lt;Group&gt; &lt;Arm&gt; &lt;Visit&gt;\nG*A*V\n✔\n✔\nOnly if using the time course hierarchical or ITP longitudinal model the mean estimate of the response at each visit, reported per group per arm per visit"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-simulations_freq.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-simulations_freq.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nOverall Group p-value\n1\nThe ANCOVA overall group p-value\n\n\nOverall Trt p-value\n1\nThe ANCOVA overall treatment p-value\n\n\nGroup x Trt p-value\n1\nThe ANCOVA treatment-group interaction p-value\n\n\nEst. Trt Effect &lt;group&gt;\nG\nThe mean estimate of the treatment effect for each group.\n\n\nLower CI Trt Effect &lt;group&gt;\nG\nThe lower bound of the 95% confidence interval for the estimate of the treatment effect for each group.\n\n\nUpper CI Trt Effect &lt;group&gt;\nG\nThe upper bound of the 95% confidence interval for the estimate of the treatment effect for each group.\n\n\np-value Trt Effect &lt;group&gt;\nG\nThe p-value for the within group treatment effect for each group.\n\n\nSigma\n1\nThe estimate of the standard deviation in the subjects’ final response.\n\n\nBeta\n1\nThe mean estimate of the baseline adjustment\n\n\nBeta SD\n1\nThe standard deviation of the estimate of the baseline adjustment"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-patientsnnnnn.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nRegion index\n\n\nDateInWeeks\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nGroup\n1\nThe index number of the group the subject belongs to.\n\n\nArm\n1\nA flag indicating the arm the subject was randomized to: 0 = Control, 1 = Study Treatment.\n\n\nLastVisit#\n1\nThe index of the last visit for which subject data is available\n\n\nDropout\n1\nA flag indicating if the subject has dropped out, 0 = not dropped out, 1= dropped out.\n\n\nBaseline\n1\nThe baseline. Column is only present if baseline is included.\n\n\nVisit &lt;visit&gt;\nV\nThe subject’s endpoint score for that visit: -9999 = not available."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v72/userguides/enrichment/simulation/continuous.html#contents-of-mcmcnnnnn.csv",
    "title": "Simulation Tab - Continuous Endpoint",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nTrtResp &lt;group&gt;\nD\nThe estimate of the mean treatment difference in each group\n\n\nOverall Theta\n1\nThe overall estimate of the treatment difference\n\n\nCtlResp &lt;group&gt;\n\nThe estimate of the mean response on the control arm in each group\n\n\nOverall Gamma\n\nThe overall mean response on the control arms\n\n\nSigma\n1\nThe estimate of the SD of the endpoint\n\n\nLongmod &lt;model&gt; &lt;param&gt; &lt;visit&gt; these vary significantly from design to design depending on the model used and the number of model instances. The parameter names correspond to the symbols used for the parameters on the Design &gt; Longitudinal tab\nL * P * V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/analysis.html",
    "href": "documentation/v72/userguides/enrichment/analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "The analysis tab allows the user to supply a specific data set for analysis by the design specified in the Design tab of the “.facts” file.\nClicking on the “Use Design to Analyze Data” button, will create an empty “subject.csv” file in the main simulation results directory and an ‘Analysis’ sub-directory there for running the analysis and saving the outputs.\nAlternatively, clicking on the “Import Data to Analyze” launches a file browser, allowing the user to select a ‘.csv’ file to load as the data to analyze. This is a shortcut for first clicking on the “Use Design to Analyze Data” button, and then clicking on the “Select File to Create New Analysis” button on the subject data tab.\nAfter enabling data analysis, the analysis screen is shown with no data loaded. By clicking on the “Subject Data” tab the user is now able to enter data values directly, or to load a ‘.csv’ file already containing data:"
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/analysis.html#the-subject.csv-file-format",
    "href": "documentation/v72/userguides/enrichment/analysis.html#the-subject.csv-file-format",
    "title": "Analysis",
    "section": "The subject.csv file format",
    "text": "The subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nThe format of the file is the same as the ‘patientsNNNN.csv’ output file (continuous, dichotomous), or time-to-event) and the column values described above."
  },
  {
    "objectID": "documentation/v72/userguides/enrichment/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "href": "documentation/v72/userguides/enrichment/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "title": "Analysis",
    "section": "Converting arrival date value from days to weeks",
    "text": "Converting arrival date value from days to weeks\nFrom FACTS 7.0 the value in the Date field is interpreted as being in weeks (rather than days as in previous versions). If you have existing data, a simple conversion tool is provided “Convert Date from Days to Weeks” that simply divides all of thete values by 7. Having run the conversion, you then need to save the modified data. The values of the Date field will only make a difference to TTE analyses.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: The data as provided to the analysis tab with dates in days.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: The same dates, but converted to weeks."
  },
  {
    "objectID": "documentation/v72/userguides/dr.html",
    "href": "documentation/v72/userguides/dr.html",
    "title": "Design Report",
    "section": "",
    "text": "This document describes usage and how to update the installation of the Design Report Generator, an automated report generation tool, included with FACTS. It is intended for anyone concerned with using the automated report generation facility. The Report Generator is available only for the desktop version of FACTS."
  },
  {
    "objectID": "documentation/v72/userguides/dr.html#purpose-and-scope-of-this-document",
    "href": "documentation/v72/userguides/dr.html#purpose-and-scope-of-this-document",
    "title": "Design Report",
    "section": "",
    "text": "This document describes usage and how to update the installation of the Design Report Generator, an automated report generation tool, included with FACTS. It is intended for anyone concerned with using the automated report generation facility. The Report Generator is available only for the desktop version of FACTS."
  },
  {
    "objectID": "documentation/v72/userguides/dr.html#software-prerequisites",
    "href": "documentation/v72/userguides/dr.html#software-prerequisites",
    "title": "Design Report",
    "section": "Software prerequisites",
    "text": "Software prerequisites\nIn order to run the report generator directly from FACTS, you will need the following installed on your Windows machine on which you are using FACTS:\n\nA reasonably recent version of R (v3.5.3+) along with the R libraries ‘rmarkdown’, ‘xtable’ and ‘stringi’ installed.\nRStudio (the Design Report Generator uses two packages that come with RStudio – “mathjax” and “pandoc”, these can now be obtained separately but given the ubiquity of RStudio FACTS simply requires you to have that installed).\nMicrosoft Word or Open Office\nFACTS v7.1 or later\n\n\n\n\n\n\n\nNote\n\n\n\nRecently we’ve experienced some problems with “pandoc”, downloading and installing from the pandoc website (https://pandoc.org), then re-starting Windows has fixed them."
  },
  {
    "objectID": "documentation/v72/userguides/dr.html#sec-setup",
    "href": "documentation/v72/userguides/dr.html#sec-setup",
    "title": "Design Report",
    "section": "Setup",
    "text": "Setup\nYou will need to inform FACTS of the location of R and RStudio on your computer.\nTo do this start FACTS and go to menu option: “Settings &gt; Options” and then to the “R Configuration” tab.\n\n\n\n\n\n\nFigure 1\n\n\n\nSelect the R and RStudio versions you would like to use.\nIf the version you wish to use isn’t shown, or the path is incorrect, use the “Edit” button to open a file browser to navigate to the version of R that you wish to use and select the appropriate version of “R.exe”. Similarly for RStudio.exe.\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "documentation/v72/userguides/dr.html#steps-for-generating-a-design-report",
    "href": "documentation/v72/userguides/dr.html#steps-for-generating-a-design-report",
    "title": "Design Report",
    "section": "Steps for generating a design report",
    "text": "Steps for generating a design report\nTo generate a design report from FACTS, you will need to do the following sequence of steps:\n\nSimulate your design in FACTS.\n\n\n\n\n\n\n\nNote\n\n\n\nIn case you have previously generated a report for your design using the report generator, you will need to make sure that the previous version of the report is not open in Word when you generate a new report.\n\n\n\nAfter setting valid R and RStudio versions/paths in Settings &gt; Options, go to the “Simulation” tab and click on the “Design Report” button. A command prompt Window will open detailing the report generation process. The very first run of the design report generation process will take a bit longer than subsequent runs as it will have to install all relevant R packages in a FACTS specific location.\nOnce the command prompt closes, the Word document will be automatically opened. It will first give you a warning regarding the document containing fields that may refer to other files, click “Yes” and then a subsequent warning regrading update the Table of Contents will appear. Click “Update entire table” and your report should appear.\n\n\n\n\n\n\n\nFigure 3: Example FACTS design report for a Core continuous endpoint design"
  },
  {
    "objectID": "documentation/v72/userguides/dr.html#usage-notes",
    "href": "documentation/v72/userguides/dr.html#usage-notes",
    "title": "Design Report",
    "section": "Usage Notes",
    "text": "Usage Notes\n\nLocation of the generated Word file\nThe generated report document is stored in the “results” folder of your FACTS simulation (e.g., if your FACTS project is saved as CoreDesign.facts, the folder named CoreDesign_results is the corresponding ‘results’ folder).\n\n\nSuggested steps after generating the report\nA typical workflow after initially generating the report is as follows:\n\nReview the generated report for correctness, and fix minor typographical and formatting errors if any.\nIf you would like to use a predefined Word template, you could apply the template to your report.\nModify the default table style and add table cross-references as needed.\nAdd additional text and figures as needed. For example, you can copy graphs displayed in the FACTS GUI by clicking on the “Export options” button next to the r graph, selecting “Copy image to clipboard” and pasting it in the generated report.\n\n\n\nLocation of the Report Generator source files\nThe Design Report Generator uses R and Rmarkdown to create the report document. The script files used by the Report Generator are stored in the “ReportGenerator” folder under your FACTS installation folder. (It might be named something like C:\\Program Files (x86)\\BerryConsultants\\FACTS 7.1.0\\ReportGenerator). The files within this folder with file extensions of “.R”, and “.Rmd” are the ones required by the Report Generator. If an updated version of the Report Generator is made available either as a bug-fix or intermediate release, it will consist of updated “.R” and “.Rmd” files. Simply replacing the corresponding files in the original installation folder will deploy the updated version."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html",
    "href": "documentation/v72/userguides/2dcrm.html",
    "title": "2D-CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) 2D-CRM design engine. It is intended for all end users of the system.\n\n\n\nThis document covers the FACTS Dose Escalation 2D-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6 or later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nPlease cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#purpose-of-this-document",
    "href": "documentation/v72/userguides/2dcrm.html#purpose-of-this-document",
    "title": "2D-CRM",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) 2D-CRM design engine. It is intended for all end users of the system."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#scope-of-this-document",
    "href": "documentation/v72/userguides/2dcrm.html#scope-of-this-document",
    "title": "2D-CRM",
    "section": "",
    "text": "This document covers the FACTS Dose Escalation 2D-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6 or later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#citing-facts",
    "href": "documentation/v72/userguides/2dcrm.html#citing-facts",
    "title": "2D-CRM",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#facts-7.0",
    "href": "documentation/v72/userguides/2dcrm.html#facts-7.0",
    "title": "2D-CRM",
    "section": "FACTS 7.0",
    "text": "FACTS 7.0\nThere have been no changes to 2D-CRM in FACTS 7.0."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#facts-6.5",
    "href": "documentation/v72/userguides/2dcrm.html#facts-6.5",
    "title": "2D-CRM",
    "section": "FACTS 6.5",
    "text": "FACTS 6.5\nThere have been no changes to 2D-CRM in FACTS 6.5."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#facts-6.4.1-changes-to-2d-crm",
    "href": "documentation/v72/userguides/2dcrm.html#facts-6.4.1-changes-to-2d-crm",
    "title": "2D-CRM",
    "section": "FACTS 6.4.1 Changes to 2D-CRM",
    "text": "FACTS 6.4.1 Changes to 2D-CRM\nCompared to 6.4 there have been slight changes to address circumstances where the model could fail to converge, and parameter estimates could reach very large values. This was due to wanting to allow 0 doses in 2D-CRM, because in the 2D setting it is sometimes the case that there have been monotherapy escalation studies and there is a desire to incorporate this data in the 2D trial. To do this a 0 dose is included for the other drug so there are monotherapy combinations in the design (possibly excluded from being actually given during the 2D trial). This means the posterior parameter estimates from the monotherapy can be used for that drug’s prior, or the data from the monotherapy trial can be included as “Prior Toxicities”.\nUp to this release (6.4.1) there were two problems doing this:\n\nInherited from the N-CRM code, FACTS was still storing the logs of the dose strengths, so doses could not be 0, and the advice was to use small values instead such as 0.001.\nThe presence of a combination with very small values for the transformed dose strength (and now 0 values) causes problems for the model’s stability, in particular when the dose strengths are 0 it becomes impossible for the model to have anything other than 0 DLT rate at the 0,0 combination.\n\nIn FACTS 6.4.1 these problems have been addressed by the following changes:\n\nFACTS now allows 0 strength doses.\nIt there is any combination where both dose strengths after transformation are less than 0.001 then this combination must be excluded from the trial and can have no prior toxicities specified for it (however prior observations of no toxicities are allowed).\nIf both drugs include 0 strength doses, the lower asymptote for the Response Model (the range is usually [0, 1]) must be raised slightly, e.g. to 0.0001.\n\nRestrictions 2 and 3 are checked when there is an attempt to run simulations, and if violated a warning message is displayed and the simulations not run."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#facts-6.4-changes-to-2d-crm",
    "href": "documentation/v72/userguides/2dcrm.html#facts-6.4-changes-to-2d-crm",
    "title": "2D-CRM",
    "section": "FACTS 6.4 Changes to 2D-CRM",
    "text": "FACTS 6.4 Changes to 2D-CRM\nThere were no changes to the 2D-CRM simulator for FACTS 6.4."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#facts-6.3-changes-to-2d-crm",
    "href": "documentation/v72/userguides/2dcrm.html#facts-6.3-changes-to-2d-crm",
    "title": "2D-CRM",
    "section": "FACTS 6.3 Changes to 2D-CRM",
    "text": "FACTS 6.3 Changes to 2D-CRM\nThere were no changes to the 2D-CRM simulator for FACTS 6.3."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#the-use-of-the-2d-crm-simulator-to-design-a-trial",
    "href": "documentation/v72/userguides/2dcrm.html#the-use-of-the-2d-crm-simulator-to-design-a-trial",
    "title": "2D-CRM",
    "section": "The use of the 2D-CRM simulator to design a trial",
    "text": "The use of the 2D-CRM simulator to design a trial\nThe typical pattern of use of the simulator is as follows:\n\nEnter the constraints and objectives of the study: maximum sample size and the bounds of the different toxicity bands.\nSelect the choice of and overdose control thresholds.\nSpecify a small number of expected toxicity responses as VSR scenarios.\nSelect the desired run-in and dose escalation rules.\nSpecify an initial set of priors for the model.\nRun a small number of simulations (25 is usually adequate at this stage) of each scenario.\nCheck the performance of the design:\n\nDo the majority of the final combinations being selected have an acceptable level of toxicity?\nIs the overall level of toxicity observed in the trial acceptable?\nIn the scenarios where the performance is worst, observe the behavior in individual simulations.\n\nTry a number of modifications to the design and simulate each modification individually. Increasing the number of simulations (usually 100 is still adequate at this stage).\nConsider if more VSRs need to be included to check a greater range of possible actual toxicity rate patterns.\nCombine modifications with promising results and re-simulate, (usually 500 is an adequate number at this stage).\n\nThe design changes that are typically considered are:\n\nExclude low dose combinations that are certain to be too weak.\nExclude high dose combinations that are certain to be too toxic.\nModify the run-in to be more aggressive if it is taking too many subjects and cycles to reach the escalation phase.\n\nThe ”Huang” diagonals method is the least aggressive, both the “Ivanova” and “Tri-axial” can skip some combinations, finally consider a user specified run-in sequence.\nThe “Huang” and the “Tri-axial” can make use of the “multiple simultaneous cohorts” option in the run-in phase to test multiple combinations simultaneously.\n\nIf the escalation phase is too aggressive – incurring too many toxicities or selecting combinations with high toxicity true toxicity too often - then options are:\n\nMake the run-in expand at the dose below observed toxicities, not at the toxicity\nMake the priors for the model less informative (if the priors are informative and predicting low toxicity at higher combinations), the greater uncertainty in the toxicity at higher doses may then result in the estimate of their toxicity exceeding the overdose control threshold.\nMake the priors for the model predict higher levels of toxicity.\nIncrease the overdose control thresholds.\nShift the toxicity band bounds downwards.\nAdd prior toxicity data.\nIncrease the number of subjects before escalation.\n\nIf the escalation phase is not aggressive enough – selecting combinations with too low a toxicity too often - then options are:\n\nMake the run-in expand at the dose where toxicity observed not a dose below\nMake the priors for the model more informative (if the priors are uninformative) this will allow lack of toxicity on lower doses reduce the predicted toxicity at higher, untested doses, bringing the probability of toxicity at the higher doses below the dose escalation threshold.\nMake the priors for the model predict lower levels of toxicity.\nLower the overdose control thresholds.\nShift the toxicity band bounds upwards.\nAdd prior toxicity data.\nDecrease the number of subjects before escalation."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#general-constraints-and-assumptions",
    "href": "documentation/v72/userguides/2dcrm.html#general-constraints-and-assumptions",
    "title": "2D-CRM",
    "section": "General constraints and assumptions",
    "text": "General constraints and assumptions\nThe design engine is limited to phase 1/2a designs with dichotomous endpoints toxicity/no-toxicity. There is no longitudinal modeling or analysis of covariates included. All results of a cohort are assumed to be available before the next cohort is treated."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#models-and-methods",
    "href": "documentation/v72/userguides/2dcrm.html#models-and-methods",
    "title": "2D-CRM",
    "section": "Models and Methods",
    "text": "Models and Methods\nFor a full description of the models and methods implemented in the 2D-CRM see the Software Specification document [Spec].\nIn the current version there is a single statistical model used to analyse the data – the 5 parameter Bayesian Logistic Regression Model as described in (Neuenschwander, Branson, and Gsponer 2008). The model is used to compute the posterior probability that the toxicity rate at each combination is in one of 4 toxicity bands – “underdosing”, “target toxicity”, “excess toxicity” and “unacceptable toxicity”. The boundaries between the bounds are settable by the user.\nThe posterior probability that the rate is in the target toxicity rate is used to target the dose allocation – the method attempts to allocate to the Maximum Target Toxicity (MTT). The posterior probability that the rate is in the “excess toxicity” and “unacceptable” or just the “unacceptable” bands can be used to enforce an “overdose control”. Combinations with a posterior probability that exceed the specified threshold are excluded from selection or allocation.\nThe dose combinations are explored in two phases:\n\nOptionally the trial may begin with a “small cohort run-in” or the allocation of a single cohort to a specified combination. This phase ends when the sequence of combinations to explore is exhausted or excluded by combinations at which toxicities have been observed. There is no model fitting during this phase.\nThe trial then proceeds looking for the dose combination with the highest probability of having a toxicity rate in the target toxicity range using dose escalation, limited by dose escalation rules.\n\n\nRun-in methods\nThere are 4 run-in methods available:\n\nContour\n\nThis explores the reverse diagonals, assuming that the expected toxicity at the dose increments of the two doses are roughly equal.\n\nRow by row\n\nThis explores the doses of drug 1 at the first dose of drug 2, then increments drug 2, steps back a specified number of doses in drug 1 and then increments through the doses of drug 1 again. This method favours escalation of drug 1 over escalation of drug 2.\n\nTri-axial\n\nThis explores the combinations on each axis and on the diagonal. For small numbers of dose increments it is similar to the Huang run-in. For larger numbers of dose increments it is more parsimonious.\n\nCustom\n\nThis explores a specific sequence of combinations stopping when the first toxicity is observed or the sequence is completed.\n\n\nOr run-in can be skipped.\nFor more details see the Software Specification document [Spec].\n\n\nEscalation methods\nThere are 4 escalation methods available. They all work in the same basic framework:\n\nThe dose combinations where sufficient subjects have been tested and lesser dose combinations are found.\nThe dose combinations that can be escalated to beyond these are found.\nThe dose combinations where the overdose control limits are exceeded are removed.\nThe target dose combinations from the remaining set are selected according to the selected method.\n\nThe 4 methods are:\n\nContour\n\nThis explores every MTT on every row and every column (frequently these overlap, but not necessarily).\n\nWalk\n\nThis explores the MTT that is at or adjacent to the last combination tested (excluding the dose that is an increment in both drugs from the last combination tested).\n\nTri-axial\n\nThis explores the MTT on each axis and on the diagonal. Once these have the required maximum subjects on MTT, any combinations off the axis and diagonal that are the MTT are explored.\n\nAllocate MTT\n\nThis allocates to the combination that is the overall MTT of all the combinations that it is permitted to allocate to."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#outstanding-issues-absent-features",
    "href": "documentation/v72/userguides/2dcrm.html#outstanding-issues-absent-features",
    "title": "2D-CRM",
    "section": "Outstanding Issues / Absent Features",
    "text": "Outstanding Issues / Absent Features\nCurrently the most obvious absent features are:\n\nmodeling of additional endpoints such as efficacy or drug exposure\nthe modeling of ordinal toxicity\nthe simulation of open enrolment where subjects are allocated on subject by subject basis as they become available for enrolment, with a limit on the number of subjects who can being treated but for whom final endpoint has not been observed."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#study-info",
    "href": "documentation/v72/userguides/2dcrm.html#study-info",
    "title": "2D-CRM",
    "section": "Study Info",
    "text": "Study Info\nOn the Study Info tab the user can\n\nSpecify the maximum trial size (in subjects not cohorts, as subjects tested in the run-on phase count to the overall total).\nSpecify the parameters of the target, the user specifies the boundaries between the different toxicity bands, and whether using overdose limits and if so which ones and what the overdosing thresholds are.\n\n\n\n\n\n\n\nFigure 4: Study info tab: 2D-CRM"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#treatment-arms",
    "href": "documentation/v72/userguides/2dcrm.html#treatment-arms",
    "title": "2D-CRM",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nOn the Treatment Arms tab the user specifies the names of the drugs, and the number, relative strength and names of the doses of each drug. The user can either click the ‘Add’ button the required number of times or use Auto generate doses, specifying the number of doses to create and initial dose level. Auto generate deletes all the pre-existing doses before generating the new ones.\nAfter creating the desired number of entries the user can edit the strengths and names of the doses.\nIt is up to the user to ensure that the dose strengths are in monotonically increasing order.\nIt is now (FACTS 6.4.1 and later) possible to enter dose strengths of 0, to create dose combinations that are monotherapies. This is particularly useful for enabling prior information from monotherapy trials to be included in this design.\n\n\n\n\n\n\nFigure 5: Treatment Arms tab"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#exclude",
    "href": "documentation/v72/userguides/2dcrm.html#exclude",
    "title": "2D-CRM",
    "section": "Exclude",
    "text": "Exclude\nOn the Study &gt; Exclude tab the user can specify dose combinations that cannot be used in the trial. By clicking on any square on the grid the corresponding dose combination is cycled through the following states: ‘Toxic’, ‘Not Available’, ‘Ineffective’ and ‘Available’. The initial state is ‘Available’ for all combinations.\nIf a combination is specified as ‘Toxic’ then all the combinations where both drugs are at the same or higher strength are also excluded.\nIf a combination is specified as ‘Ineffective’ then all combinations where both drugs are at the same or lower strength are also excluded.\nIf a combination is specified as ‘Not Available’ then just that combination is excluded with no consequences for the combinations around it.\nIf there are any combinations where, after transformation of the dose strengths, both drugs have a dose strength of less than 0.001 this must be excluded from the trial (mark them “ineffective” or “not available”). Any toxicities occurring on such a combination would be data that the model would have difficulty fitting.\n\n\n\n\n\n\nFigure 6: The Exclude tab"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#explicitly-defined",
    "href": "documentation/v72/userguides/2dcrm.html#explicitly-defined",
    "title": "2D-CRM",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nOn the VSR &gt; Explicitly Defined tab, the user enters the toxicity rate to use when simulating subject responses, for every dose combination. The cells are colored according to the toxicity band coloring convention used in FACTS (under-dosing: blue, target: green, excess: orange, unacceptable: red) according to the toxicity rate entered.\n\n\n\n\n\n\nFigure 7: VSR Explicitly Defined tab\n\n\n\n\nParametric\nOn the VSR &gt; Parametric tab, the user enters the parameters for a model that generates a toxicity rate to use when simulating subject responses, for every dose combination. The cells are colored using toxic band coloring according to the toxicity rate of that dose combination.\nThe model used is the 5 parameter BLRM model that is used as the Toxicity response model on the Design &gt; Response Model tab, but the user can specify different ‘effective dose strength’ models from those used in the analysis.\n\n\n\n\n\n\nFigure 8: VSR Parametric tab\n\n\n\nFor each profile the user specifies:\n\nFor each drug the ‘effective dose strength’ model.\n\nThe reference dose (where the effective dose strength is 1 and the toxicity rate due to that drug is given by the Alpha parameter alone).\nWhether the effective dose strength is the ratio of dose strength to the reference dose or the exponential of the difference of the to dose strength from the reference dose.\n\nFor each drug the values of the coefficients of the single drug models entered as ln(alpha) and ln(Beta)\nThe value of Eta, the drug-drug interaction term – Eta is the log-odds ratio between the interaction and the non-interaction model at the reference dose combination.\n\nIt is perfectly reasonable to use the parametric models by simply varying the parameters using trial and error until the desired set of toxicity rates is achieved."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#start-up",
    "href": "documentation/v72/userguides/2dcrm.html#start-up",
    "title": "2D-CRM",
    "section": "Start-up",
    "text": "Start-up\nThe Start-up tab allows the user to specify what is commonly referred to as a single patient run-in, in the FACTS 2D-CRM it’s allowed to be a “small cohort“ run-in. (The paper (Ivanova et al. 2003) showed that smaller target toxicity rates benefited from larger cohort sizes in the run-in).\nOn this tab the user can specify\n\nThe scheme to use for the run-in: “None”, “Contour”, “Row by Row”, “Tri-axial” or “Custom”. These are described above and in [Spec].\n\nIf “None” is specified then the table for specifying specific dose combinations is used to specify the dose combination(s) to be assigned to the starting cohort(s). These will be the full sized cohorts used in the escalation phase.\n\nThe cohort size to use in the run-in. A size of ‘1’ yields a ‘single patient run-in’, but other values are possible and may give better results, (see (Ivanova et al. 2003)).\nFor the ‘Row by row’ run-in scheme, the number of drug 1 dose increments to back-off each time drug 2 is incremented.\nThe range of the run-in. If the user does not want the run-in to possibly run all the way up to the maximum dose strengths, even if no toxicities have been observed, it is possible to specify a rectangular area of dose combinations for the run-in to include by specifying the starting combination (the lower left corner of the rectangle) and the stopping dose (the upper right corner combination). Note that as you would expect, regardless of whether this feature is used or not, dose combinations that have been excluded from the study on the Stud &gt; Exclude tab are excluded from the run-in.\nThe dose allocation scheme: (only applies to the Contour and Tri-axial run-in schemes):\n\nallocate a single cohort;\nallocate the cohort across the next possible dose combinations;\nor allocate a cohort to each of the next dose combinations and optionally specify a maximum number of cohorts / dose combinations that can be tested at a single step.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Row by row and Custom run-ins only identify one dose combination to test next, and so use “single cohort” allocation regardless of the setting of this control.\n\n\n\nWhere to allocate after the end of the run-in it it ends due to observing toxicities, either:\n\nexpand the allocation at the combinations where toxicity has been observed,\nor at one dose increment below where toxicity has been observed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the run-in ends because it has reached the end of the combinations to be tested during run-in without seeing a toxicity, then dose escalation starts by expanding at the allocation of the maximum dose combination(s).\n\n\n\nIf use of a Custom run-in has been specified, then the sequence of dose combinations is specified.\n\n\n\n\n\n\n\nFigure 9: Design Start-up tab"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#dose-escalation",
    "href": "documentation/v72/userguides/2dcrm.html#dose-escalation",
    "title": "2D-CRM",
    "section": "Dose Escalation",
    "text": "Dose Escalation\nThe Dose Escalation tab allows the user to specify the dose escalation and early stopping rules to use during the Dose Escalation phase.\nOn this tab the user can specify:\n\nThe cohort size.\nThe minimum number of subjects: that must be have been tested at a combination before the trial can escalate to a higher dose combination (that is not excluded by the overdose control).\nThe maximum dose increment: in either drug that can be applied from a combination that has been tested, and where the minimum of subjects have been tested, to a higher dose combination.\nThe dose escalation scheme: “Contour”, “Walk”, “Tri-Axial” or “Allocate to MTT”. These are described in (2.3.2) and in [Spec].\nThe dose allocation scheme: (only applies to the “Contour” and “Tri-axial” schemes):\n\nallocate a single cohort;\nallocate the cohort across the next possible doses;\nor allocate a cohort to each of the next doses.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe “Walk” and “Allocate to MTT” escalation schemes only identify one next combination, and so use “single cohort” allocation regardless of the setting of this control.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf “Multiple simultaneous cohorts” is specified, then it is possible to set a limit on the maximum number of cohorts / dose combinations tested in the next step.\n\n\n\nMaximum subjects on MTT. This is the maximum number of subjects to allocate to a combination under Dose Escalation. In the “Contour”, “Walk”, “Tri-Axial” escalation schemes this is currently the only stopping rule. If all the targeted combinations have the maximum number of subjects on them, the trial stops.\n\n\n\n\n\n\n\nFigure 10: Design Dose Escalation\n\n\n\nAll the escalation schemes stop if the maximum number of subjects has been allocated or all combinations are too toxic (the posterior probabilities of toxicity exceed the overdose threshold). The schemes other than “Allocate to MTT” can also stop when the maximum number of subjects on MTT is reached on all the target doses.\nIf the “Allocate to MTT” escalation scheme, is being used then additional stopping rules similar to those available in N-CRM can be enabled. If they are enabled, then the following may be specified:\n\nMinimum subjects on MTT: the trial can only stop with a combination selection if the combination to be allocated to have at least these number of subjects on them.\nStop when Posterior Probability of target Toxicity at MTT is at least: the trial can stop with a combination selection when the combination to be allocated to has a posterior probability that its toxicity rate is in the Target Toxicity band is at least the specified amount.\nMinimum cohort’s accrued: the trial can only stop with a combination selection if the specified minimum number of cohorts has been allocated and completed.\nMinimum toxicities: the trial can only stop for all combinations too toxic if the required minimum number of toxicities have been observed. Whilst all combinations are too toxic but this minimum number of toxicities havs not been observed, subjects are allocated to the lowest dose combination."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#response-model",
    "href": "documentation/v72/userguides/2dcrm.html#response-model",
    "title": "2D-CRM",
    "section": "Response Model",
    "text": "Response Model\nThe Response Model tab displays a description of the model and 4 other components:\n\nA panel where the prior distributions for the model parameters can be input. Unless prior toxicity information is also to be included, it is recommended that uninformative priors are not used as these tend to lead to a failure to re-escalate to a dose if just one toxicity has been observed there, due to overdose control.\n\n\n\n\n\n\n\nTip\n\n\n\nValues that represent weakly informative priors are recommended instead. The prior mean and SD for each Alpha should represent the prior expectation (in log-odds) of toxicity at the reference dose for that drug in isolation. Having set Alpha the prior for Beta should be set to allow gradients that are as steep as plausible, but if set too high, or too broad it may cause problems of not being able to escalate to high doses because of overdose control (even with few toxicities at low doses the uncertainty of the model at higher doses where there is currently no data may mean the posterior estimates of toxicity at these doses exceed the overdose control thresholds). This can be checked looking at example simulations or manually by entering test data on the Analysis tab. The prior for the correlation should usually be zero or slightly negative. A positive prior for Rho means that if the value for Alpha is estimated as low (due to low toxicity being observed near the reference dose) then the value of Beta will be estimated as being low – and this may lead to too low an estimate of toxicity at higher doses. A very strongly negative prior for Rho can lead to the same problems as a too high or broad prior for Beta.\n\n\n\nA panel where the transformations of the virtual dose strengths to values that can be used by the model is specified. This transformation is usually the dose ratio of each dose to the median dose of that drug. Options to use a linear transform and a different reference dose are available.\n\n\n\n\n\n\n\nTip\n\n\n\nThe recommended values for the reference dose are either the center of the dose range (the default “Median dose”) or the dose thought most likely to be in the middle of the target toxicity range for that drug alone.\n\n\n\nRe-scaling the response model. The model estimates rates between 0 and 1 and works best when the observed rates are asymptotically 0 or 1. If the observed toxicity is known to have different minimum and maximum rates (0.1 and 0.5 say) then the model fits is much, much better if these are specified as the asymptotes in place of 0 and 1.\n\n\n\n\n\n\n\nTip\n\n\n\nIf a dose combination of 0,0 is included in the design (even when excluded from be used in the trial, as it must be) the response model must be re-scaled with a minimum asymptote above 0, even if only very slightly (such as 0.0001).\n\n\n\nThe last panel displays the values of the model at the dose combinations using the specified parameters.\n\n\n\n\n\n\n\nFigure 11: Design Response Model\n\n\n\nSpecifying the Priors: The priors for each pair of parameters (α,β) for the response model for each drug are specified via a bivariate normal distribution (as in the single drug N-CRM), with a separate mean and SD for α and β, and a correlation term ρ (Rho). The prior for the interaction term η (Eta) is the mean and Sd of a Normal distribution. Using a Normal prior for Eta allows Eta to be -ve, modeling a negative interaction between the drugs and resulting in a non-monotonic surface. Unless a negative interaction is thought possible, it is recommended that the lognormal prior for Eta is used\nSpecifying the dose transform: For both drugs the transformation of the dose strengths to a range of values suited to the model is specified. For each drug the transformations are relative to ‘reference’ dose, by default the median value in the dose range. The transformation can be linear log(d – d*) or the dose ratio ((d/d*))\nSpecifying the asymptotes: The model fits values in the range (0-1), asymptotically approaching either limit. If desired the range can be reduced by re-scaling so that it asymptotically approaches a specified lower limit &gt; 0 and upper limit &lt; 1.\nChecking the prior: The influence of the prior on the model can be checked by viewing the results of sampling of the model parameters from the specified prior. Options are to view a) 100 samples of the 1-dimensional model for drug 1 or drug 2, or b) 1,000 samples of the 2-dimensional model over both drugs showing the 10%-ile, median or 90%-ile toxicities at each dose combination.\n\n\n\n\n\n\nFigure 12: Design Response Model showing individual drug prior\n\n\n\nThe individual drug graph plots the doses at the log of their transformed dose strength (“Log(X^)”), this enables the logits to be plotted linearly.\nIf there is a transformed dose strength of 0, this is plotted at some notional value that represents -infinity, and to see the plot at the other doses the “Plot excl. first dose” option should be checked."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#prior-toxicities",
    "href": "documentation/v72/userguides/2dcrm.html#prior-toxicities",
    "title": "2D-CRM",
    "section": "Prior Toxicities",
    "text": "Prior Toxicities\nThis tab allows prior information to be included with the data collected during the trial.\nThe user can specify, for any dose combinations a prior observed number of toxicities and number of observations. To allow the prior data to be down weighted, the user is allowed to enter fractional amounts.\nThe specified prior data is simply added to the observed data when fitting the model, but this is logically equivalent to fitting the model with the specified prior to the specified prior data and then using the resulting posterior estimate of the model parameters as the prior for fitting the model to the observed data.\nSome idea of the effect of the prior data can be gained by using the “Analysis” tab, and performing an analysis on a dataset without any additional subject data.\nIf there is a combination where the transformed dose strengths of both drugs are below 0.001, there can be no prior observed toxicities for that combination (it would be impossible for the model to fit them).\n\n\n\n\n\n\nFigure 13: Design Prior Toxicities"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#to-run-simulations",
    "href": "documentation/v72/userguides/2dcrm.html#to-run-simulations",
    "title": "2D-CRM",
    "section": "To run simulations",
    "text": "To run simulations\nEven before simulations have been run, FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Each such combination is a ‘simulation scenario’.\nThe user clicks on the check box for each scenario to be simulated, or simply clicks on “Select All”, then clicks on the “Simulate” button.\nDuring simulation, the user is prevented from modifying any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters into the current “.facts” file, and when the simulations are complete all the simulation results are saved in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there are sub-folders that holds the results for each scenario."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#mcmc-settings",
    "href": "documentation/v72/userguides/2dcrm.html#mcmc-settings",
    "title": "2D-CRM",
    "section": "MCMC Settings",
    "text": "MCMC Settings\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nNumber of samples per imputation (not used in 2D-CRM, the model does not include imputation)\nIf the Number of MCMC samples to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output in a file per simulation, allowing the user to check convergence.\nThe MCMC output thinning parameter can be used to reduce the amount of data output to the MCMC file. It does not reduce the amount of MCMC samples used within the model fitting.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation.\n\n\n\n\n\n\nFigure 15: MCMC Settings"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#how-many-simulations-to-run",
    "href": "documentation/v72/userguides/2dcrm.html#how-many-simulations-to-run",
    "title": "2D-CRM",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%).\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#simulation-results",
    "href": "documentation/v72/userguides/2dcrm.html#simulation-results",
    "title": "2D-CRM",
    "section": "Simulation results",
    "text": "Simulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nAll: A window containing all the summary results columns\nHighlights: a separate window with the results shown on the main tab\nAllocation, Observed: summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity: summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc: summary results of the posterior probabilities of the properties of interest\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\n\nView Graph: opens the FACTS built in graph utility displaying the results for the currently selected scenario. See Error! Reference source not found. below for a description of the graphs.\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options:\n\nOpen results folder: Opens a file browser in the results folder of the scenario, allowing swift access to any of the results files.\nSimulation results: Opens a window displaying the individual simulation results for each simulation of the currently selected scenario\nOpen in R: opens a control that will launch R, first loading the selected files in the results folder as data frames.\nShow Graphs: launches the graph viewer to view the results of the currently selected scenario."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#facts-grid-simulation-settings",
    "href": "documentation/v72/userguides/2dcrm.html#facts-grid-simulation-settings",
    "title": "2D-CRM",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#detailed-simulation-results",
    "href": "documentation/v72/userguides/2dcrm.html#detailed-simulation-results",
    "title": "2D-CRM",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 17) shows the results simulation by simulation. Like the summary results – the initial window hold “highlights”, right clicking on the window displays a menu with options for viewing the “Fitted Toxicity”, “Observed Toxicity” or “All” Columns.\n\n\n\n\n\n\nFigure 16: Detailed Simulation Results\n\n\n\n\n\n\n\n\n\nFigure 17: Detailed Simulation Results"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#cohort-simulation-results",
    "href": "documentation/v72/userguides/2dcrm.html#cohort-simulation-results",
    "title": "2D-CRM",
    "section": "Cohort Simulation Results",
    "text": "Cohort Simulation Results\nAfter viewing individual simulation results, the user may examine detailed results for any simulation with per-cohort data in the table by right-clicking on the row and selecting “Cohort Results”. A separate window opens, showing the result for the selected simulation cohort by cohort. Like the summary and simulation results the initial window displays “highlights” columns, right clicking on the window displays a menu with options for viewing the “Fitted Toxicity”, “Observed Toxicity” or “All” Columns."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#aggregation",
    "href": "documentation/v72/userguides/2dcrm.html#aggregation",
    "title": "2D-CRM",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 18: Aggregation\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nIn other design engines there is the option to pivot the results by dose, this is more complicated in 2D-CRM and the option is not available.\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_, so agg_summary.csv will contain the rows from each of the summary.csv files, cohortsNNN.csv files are aggregated into a single agg_cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#highlights",
    "href": "documentation/v72/userguides/2dcrm.html#highlights",
    "title": "2D-CRM",
    "section": "Highlights",
    "text": "Highlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\nThese results are in the ‘summar.csv’ files in each scenario results folder.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nNum Sims\n1\nThe number of simulations that are being summarised\n\n\nMean subjects\n1\nThe mean number (over the simulations) of subjects randomized\n\n\nPpn Toxic\n1\nThe mean (over the simulations) proportion of the randomized subjects that had a toxic outcome\n\n\nSd Ppn Toxic\n1\nThe standard deviation (over the simulations) of the proportion of subjects that had a toxic outcome.\n\n\nMean true toxicity\n1\nThe mean (over the simulations) of the true probabilities of toxicity that subjects have been exposed to.\n\n\nDuration\n1\nThe mean duration of the simulations in weeks\n\n\nSd Duration\n1\nThe standard deviation of the durations of the simulations\n\n\nPpn All Toxic\n1\nThe proportion of the simulations that stopped because all combinations were judged too toxic (exceeded the overdose control limit).\n\n\nPpn Early Succ\n1\nThe proportion of the simulations that stopped because the stopping condition(s) had been satisfied.\n\n\nPpn Reached Cap\n1\nThe proportion of the simulations that stopped because the maximum nmber of subjects had been reached.\n\n\nPpn MTT Under\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “under-dosing” range.\n\n\nPpn MTT Target\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “target” range.\n\n\nPpn MTT Excess\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “excess” range.\n\n\nPpn MTT Unacceptable\n1\nThe proportion of the dose combinations selected as MTT that actually had a toxicity rate in the “unacceptable” range.\n\n\nPpn Correct Under\n1\nThe proportion of the dose combinations that have a true toxicity rate in the “under-dosing” band over all the simulations that are determined to be “under-dosing”. That is the posterior probability at the end of the simulation that the toxicity rate of that dose combination of being in the “under-dosing” band is greater than the probabilities of being in the “target” band or the combined “excess” and “unacceptable” toxicity bands, and the combination is not excluded by the overdose control conditions.\n\n\nPpn Correct Target\n1\nThe proportion of the dose combinations that have a true toxicity rate in the “target” band over all the simulations that are determined to be “target”. That is the posterior probability at the end of the simulation that the toxicity rate of that dose combination of being in the “target” band is greater than the probabilities of being in the “under-dosing” band or the combined “excess” and “unacceptable” toxicity bands, and the combination is not excluded by the overdose control conditions.\n\n\nPpn Excess+Unacc\n1\nThe proportion of the dose combinations that have a true toxicity rate in the “excess” or “unacceptable” bands over all the simulations that are determined to be “excess” or “unacceptable”. That is the posterior probability at the end of the simulation that the toxicity rate of that dose combination of being in the “excess” or “unacceptable” bands is greater than the probabilities of being in the “under-dosing” band or the “target band”, or the combination is excluded by the overdose control conditions."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#fitted-toxicity",
    "href": "documentation/v72/userguides/2dcrm.html#fitted-toxicity",
    "title": "2D-CRM",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Tox Coeffs\n5\nThe mean (over the simulations) of the estimate of the 5 parameters of the BLRM Dose-Toxicity model\n\n\nSd Tox Coeffs\n5\nThe standard deviation (over the simulations) of the estimate of the 5 parameters of the BLRM Dose-Toxicity model\n\n\nPr(Under): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the posterior probability of being under the target toxicity at each dose combination.\n\n\nPr(Target): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the posterior probability of being in the target toxicity band at each dose combination\n\n\nPr(Excess): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the posterior probability of being in the excess toxicity band at each dose combination\n\n\nPr(Unacc): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulation) of the posterior probability of being in the unacceptable toxicity band at each dose combination\n\n\nPr(Sel-MTT): &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe proportion of times that each dose combination was selected as the combination with the MTT (Maximum Target Toxicity). Some methods may select more than one such combination so that these proportions sum to more than one for each scenario.\n\n\nMean Fitted Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the estimate of the toxicity rate at each dose combination.\n\n\nSd Fitted Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe standard deviation (over the simulations) of the toxicity rate at each dose combination."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#observed-toxicity",
    "href": "documentation/v72/userguides/2dcrm.html#observed-toxicity",
    "title": "2D-CRM",
    "section": "Observed Toxicity",
    "text": "Observed Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Subjects: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the number of subjects allocated to each dose combination.\n\n\nSd Subjects: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe standard deviation (over the simulations) of the number of subjects allocated to each dose combination.\n\n\nMean Obs Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe mean (over the simulations) of the number of observed toxicities at each dose combination.\n\n\nSd Obs Tox: &lt;Dose&gt;, &lt;Dose&gt;\nI*J\nThe standard deviation (over the simulations) of the number of observed toxicities at each dose combination."
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#the-per-simulation-results",
    "href": "documentation/v72/userguides/2dcrm.html#the-per-simulation-results",
    "title": "2D-CRM",
    "section": "The Per-Simulation Results",
    "text": "The Per-Simulation Results\nDouble clicking on any row in the simulation results table, opens a window displaying the individual simulation results. This table shows the individual results that are averaged in the summary table.\nThese values are in the ‘simulations.csv’ for each scenario in the scenario results folder.\nLike the summary results, the per-simulation results are organized into ‘Highlights’, ‘Fitted Toxicity’ and ‘Observed Toxicity’ tables, with the corresponding columns.\nRight clicking on any summary results table brings up a menu that allows the user to:\n\nOpen the per-simulations results window with all columns\nOpen the per-simulations results window with the highlights columns\nOpen the per-simulations results window with the fitted toxicity columns\nOpen the per-simulations results window with the observed toxicity columns\nOpen the results folder for the scenario line that the cursor is on\nOpen the cohort results widow for the currently selected simulation.\n\n\n\n\n\n\n\nFigure 20"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#fitted-ppntoxicity",
    "href": "documentation/v72/userguides/2dcrm.html#fitted-ppntoxicity",
    "title": "2D-CRM",
    "section": "Fitted Ppn(Toxicity)",
    "text": "Fitted Ppn(Toxicity)\nThe Fitted Ppn(Toxicity) graph shows the average mean posterior estimate of the toxicity at each combination, with the cells colored in increasing intensity the higher the fitted toxicity. In addition in each cell the average number of observed toxicities and average total observations is reported.\nThe format is:\nFit: &lt;Average Mean estimate of toxicity&gt;\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\n\n\n\n\n\n\nFigure 21"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#ppntoxicity-bands",
    "href": "documentation/v72/userguides/2dcrm.html#ppntoxicity-bands",
    "title": "2D-CRM",
    "section": "Ppn(Toxicity Bands)",
    "text": "Ppn(Toxicity Bands)\nThe Ppn(Toxicity Bands) graph shows the average mean posterior estimate that the toxicity falls in one of the defined toxicity bands, with the cells colored from faint to intense the higher the probability. The color used depends on the band being displayed: blue for under-dosing, green for target, orange for excess and red for unacceptable.\nIn addition in each cell the average number of observed toxicities and average total observations is reported.\nThe format is:\nProb: &lt;Average Posterior Probability&gt;\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\n\n\n\n\n\n\nFigure 22"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#toxicity-summary-graph",
    "href": "documentation/v72/userguides/2dcrm.html#toxicity-summary-graph",
    "title": "2D-CRM",
    "section": "Toxicity Summary Graph",
    "text": "Toxicity Summary Graph\nThe Toxicity Summary Graph displays a small histogram for each dose combination. In each histogram it shows for that dose combination the posterior probability its toxicity rate is in each of the four toxicity bands.\nIf the posterior probabilities are such that the dose combination is excluded by the overdose control rules, then the background of the histogram is in pink to highlight it.\n\n\n\n\n\n\nFigure 23"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#mean-prselected-mtt",
    "href": "documentation/v72/userguides/2dcrm.html#mean-prselected-mtt",
    "title": "2D-CRM",
    "section": "Mean Pr(Selected MTT)",
    "text": "Mean Pr(Selected MTT)\nThe Mean Pr(Selected MTT) graph shows the average number of times each combination was selected as the, or one of the, combinations with Maximum Target Toxicity. The cells are colored more intensely the greater the Ppn of simulations that the dose combination was selected as MTT.\nThe ‘Selected as MTT’ dose combination is the one with the maximum posterior probability of having a toxicity rate in the target band and not excluded by overdose control. The ‘Walk’ and ‘Allocate to MTT’ dose escalation methods select a single MTT at the end of a trial, but the ‘Contour’ and ‘Tri-Axial’ can select multiple MTTs so when using these dose escalation methods these probabilities can sum to more than 1.\nThe format is:\n&lt;PPn of simulations this combination selected as MTT&gt;\n\n\n\n\n\n\nFigure 24"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#mean-subject-allocation",
    "href": "documentation/v72/userguides/2dcrm.html#mean-subject-allocation",
    "title": "2D-CRM",
    "section": "Mean Subject Allocation",
    "text": "Mean Subject Allocation\nThe Mean Subject Allocation graph shows the average number of subjects allocated to each combination over all the simulations, with the cells colored from white to dark blue the greater the number allocated.\nThe format is:\n&lt;Number of subjects allocated to this combination&gt;\n\n\n\n\n\n\nFigure 25"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#mean-observed-prtoxicity",
    "href": "documentation/v72/userguides/2dcrm.html#mean-observed-prtoxicity",
    "title": "2D-CRM",
    "section": "Mean Observed Pr(Toxicity)",
    "text": "Mean Observed Pr(Toxicity)\nThe Mean Observed Pr(Toxicity) graph shows the average number of observations and number of toxicities along with the mean fitted toxicity across all the simulations. The color intensity reflects the fraction of observations that resulted in toxicity, with the color more intense the closer that fraction gets to 1..\nThe format is:\nFit: &lt;Average Mean Estimate of Toxicity&gt;\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\n\n\n\n\n\n\nFigure 26"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#study-sizes-and-outcomes",
    "href": "documentation/v72/userguides/2dcrm.html#study-sizes-and-outcomes",
    "title": "2D-CRM",
    "section": "Study Sizes and Outcomes",
    "text": "Study Sizes and Outcomes\nThe Study Sizes and Outcomes graph plots a histogram, of the sample sizes of the simulations. The bars are colored to show why the simulated trial stopped:\n\nBecause all dose combinations were too toxic (barred by the overdose control rules)\nBecause defined stopping rule had been met.\nBecause the trial maximum sample size had been reached.\n\n\n\n\n\n\n\nFigure 27"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#fitted-prtoxicity-per-simulationper-cohort",
    "href": "documentation/v72/userguides/2dcrm.html#fitted-prtoxicity-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Fitted Pr(Toxicity) per simulation/per cohort",
    "text": "Fitted Pr(Toxicity) per simulation/per cohort\nThe Fitted Ppn(Toxicity) graph shows the mean posterior estimate of the toxicity at each combination for each simulation, with the cells colored from beige to intense red the higher the toxicity. In addition in each cell the number of observed toxicities and total observations is reported.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can be viewed for the result of the analysis after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\nFit: &lt;Mean estimate of toxicity&gt;\nObs: &lt; Toxicity observations&gt; / &lt;Total observations&gt;\n\n\n\n\n\n\nFigure 28"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#prtoxicity-band-per-simulationper-cohort",
    "href": "documentation/v72/userguides/2dcrm.html#prtoxicity-band-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Pr(Toxicity Band) per simulation/per cohort",
    "text": "Pr(Toxicity Band) per simulation/per cohort\nThe Ppn(Toxicity Bands) graph shows the posterior estimate that the toxicity falls in one of the defined toxicity bands for each simulation, with the cells colored from faint to intense the higher the probability. The color used depends on the band being displayed: blue for under-dosing, green for target, orange for excess and red for unacceptable.\nIn addition in each cell the number of observed toxicities and total observations is reported.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can also be viewed for the result of the analysis after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\nProb: &lt;The Posterior Probability&gt;\nObs: &lt;The number toxicity observations&gt; / &lt;The number of total observations&gt;\n\n\n\n\n\n\nFigure 29"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#summary-graph-per-simulationper-cohort",
    "href": "documentation/v72/userguides/2dcrm.html#summary-graph-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Summary Graph per simulation/per cohort",
    "text": "Summary Graph per simulation/per cohort\nThe Summary graph shows the posterior estimate that the toxicity falls into each of the defined toxicity bands as a small histogram for each dose combination\nIn addition in the cells where the dose combination is excluded by the overdose rules are highlighted with a pink background.\n\n\n\n\n\n\nFigure 30"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#subject-allocation-per-simulationper-cohort",
    "href": "documentation/v72/userguides/2dcrm.html#subject-allocation-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Subject Allocation per simulation/per cohort",
    "text": "Subject Allocation per simulation/per cohort\nThe Subject Allocation graph shows the number of subjects allocated to each combination in a particular simulation with the cells colored from white to dark blue the greater the number allocated.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can also be viewed for the subject allocation after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\n&lt;Number of subjects allocated to this combination&gt;\n\n\n\n\n\n\nFigure 31"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#observed-prtoxicity-per-simulationper-cohort",
    "href": "documentation/v72/userguides/2dcrm.html#observed-prtoxicity-per-simulationper-cohort",
    "title": "2D-CRM",
    "section": "Observed Pr(Toxicity) per simulation/per cohort",
    "text": "Observed Pr(Toxicity) per simulation/per cohort\nThe Observed Pr(Toxicity) graph shows the number of observations and number of toxicities for each combination, for each the simulation. The color intensity reflects the fraction of observations that resulted in toxicity, with the color more intense the closer that fraction gets to 1.The size of the colored square is proportional to the number of observations.\nFor simulations for which a “cohortNNNN.csv” file was output, the graph can also be viewed for the subject allocation after each cohort – allowing the trial to be “stepped through” a cohort at a time.\nThe format is:\n\nObs: &lt;Mean toxicity observations&gt; / &lt;Mean total observations&gt;\nsignifies the combination that is (one of) the combination(s) selected for allocation to for the next cohort. At the end of the simulation, these are the combinations that are selected as the MTT.\nCells excluded by the overdose control rules are marked with a “*”.\n\n\n\n\n\n\n\nFigure 32"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#true-prtoxicity",
    "href": "documentation/v72/userguides/2dcrm.html#true-prtoxicity",
    "title": "2D-CRM",
    "section": "True Pr(Toxicity)",
    "text": "True Pr(Toxicity)\nThe True Ppn(Toxicity) graph shows the toxicity rate simulated at each combination for all the simulations, with the cells colored from beige to intense red the higher the toxicity.\nThe format is:\n&lt;Simulated toxicity rate&gt;\n\n\n\n\n\n\nFigure 33"
  },
  {
    "objectID": "documentation/v72/userguides/2dcrm.html#analysis-tab-graphs",
    "href": "documentation/v72/userguides/2dcrm.html#analysis-tab-graphs",
    "title": "2D-CRM",
    "section": "Analysis tab graphs",
    "text": "Analysis tab graphs\nAfter the analysis the graph shown by default is the “Observed Toxicities” graphs (shown above).\nThe other three graphs available are similar to the “per cohort” graphs available for simulations, namely:\n\nFitted Pr(Toxicity)\nPr(Toxicity band)\nBand Summary Graph\n\n\nFitted Pr(Toxicity)\nThis graph shows the mean fitted toxicity rate at each combination, colored with increasing intensity the higher the rate, and a summary of the observed data:\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\nPr(Toxicity Band)\nThis graph shows the posterior probability of being in a particular toxicity band at each combination, colored with increasing intensity the higher the probability, and a summary of the observed data:\n\n\n\n\n\n\nFigure 42\n\n\n\nThere is a control to select for which toxicity band the probabilities are shown.\n\n\nBand Summary Graph\nThis graph shows the posterior probability of being in each toxicity band at each combination as a simple histogram. Common y-axis range of 0-1 is used for all the histograms. The background of the histograms is white if the combination is within the overdose control rules and pink if the combination is excluded by the overdose control rules. At the top of each histogram is a summary of the observed data (toxicities / total observed):\n\n\n\n\n\n\nFigure 43"
  },
  {
    "objectID": "documentation/v72/examples/CRM/example1.html",
    "href": "documentation/v72/examples/CRM/example1.html",
    "title": "Example 1",
    "section": "",
    "text": "First example"
  },
  {
    "objectID": "documentation/v72/examples/Staged/example1.html",
    "href": "documentation/v72/examples/Staged/example1.html",
    "title": "Example 1",
    "section": "",
    "text": "First example"
  },
  {
    "objectID": "introduction/citation.html",
    "href": "introduction/citation.html",
    "title": "How to Cite",
    "section": "",
    "text": "FACTS Software\nPlease cite FACTS when you are writing papers or preparing slides for which you have used FACTS. The correct way to cite FACTS is to include the FACTS software version number in the text and then point to the full reference, like so: We used FACTS 7.1 [1] to simulate the trial. Here are the most common citation formats:\n\nAPAMLAChicagoHarvardVancouverBibTexBibLaTex\n\n\nFACTS Development Team. (2025). FACTS: Fixed and Adaptive Clinical Trial Simulator [Computer software]. Berry Consultants LLC, Austin, TX, USA. https://www.berryconsultants.com/software/facts/\n\n\nFACTS Development Team. FACTS: Fixed and Adaptive Clinical Trial Simulator. Berry Consultants LLC, 2025, https://www.berryconsultants.com/software/facts/. Accessed 08 May. 2025.\n\n\nFACTS Development Team. 2025. FACTS: Fixed and Adaptive Clinical Trial Simulator [Computer software]. Austin, TX: Berry Consultants LLC. https://www.berryconsultants.com/software/facts/ (accessed May 08, 2025).\n\n\nFACTS Development Team, 2025. FACTS: Fixed and Adaptive Clinical Trial Simulator [Computer software]. Austin, TX: Berry Consultants LLC. Available at: &lt;https://www.berryconsultants.com/software/facts/&gt; (Accessed: 08 May 2025).\n\n\nFACTS Development Team. FACTS: Fixed and Adaptive Clinical Trial Simulator [Computer software]. Austin, TX: Berry Consultants LLC; 2025. Available from: https://www.berryconsultants.com/software/facts/ [cited 2025 May 08].\n\n\n@Manual{FACTS,\ntitle = {{FACTS: Fixed and Adaptive Clinical Trial Simulator}},\nauthor = {{FACTS Development Team}},\norganization = {{Berry Consultants LLC}},\ntype = {Computer Software},\naddress = {Austin, TX, USA},\nyear = {2025},\nnote = {https://www.berryconsultants.com/software/facts/}\n}\n\n\n@Manual{FACTS,\ntitle = {{FACTS: Fixed and Adaptive Clinical Trial Simulator}},\nauthor = {{FACTS Development Team}},\norganization = {{Berry Consultants LLC}},\ntype = {Computer Software},\naddress = {Austin, TX, USA},\nyear = {2025},\nurl = {https://www.berryconsultants.com/software/facts/}\n}\n\n\n\n\n\nPages on the FACTS Knowledge Hub\nPlease cite the FACTS Knowledge Hub when you are writing papers or preparing slides for which you have used the FACTS Knowledge Hub. The correct way to cite the FACTS Knowledge Hub is to point to the full reference and giving the date when the webpage was accessed. There are two options with respect to who to name as the author.\nIf the webpage you are referencing does not contain an explicit author, use “Berry Consultants LLC” as author, like so: For more information on Time Course Hierarchical Longitudinal Models, see the FACTS Knowledge Hub [2]. Here are the most common citation formats using the above example (please substitute the title and link with the actual page you want to cite):\n\nAPAMLAChicagoHarvardVancouverBibTexBibLaTex\n\n\nBerry Consultants LLC. (2025). Longitudinal Models for Continuous Endpoints. Retrieved May 08, 2025, from https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html\n\n\nBerry Consultants LLC. Longitudinal Models for Continuous Endpoints. 2025, https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html. Accessed 08 May. 2025.\n\n\nBerry Consultants LLC. 2025. Longitudinal Models for Continuous Endpoints. https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html (accessed May 08, 2025).\n\n\nBerry Consultants LLC, 2025. Longitudinal Models for Continuous Endpoints. [online] Available at: &lt;https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html&gt; [Accessed 08 May 2025].\n\n\nBerry Consultants LLC. Longitudinal Models for Continuous Endpoints [Internet]. 2025 [cited 2025 May 08]. Available from: https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html\n\n\n@misc{FACTSKH,\ntitle = {{Longitudinal Models for Continuous Endpoints}},\nauthor = {{Berry Consultants LLC}},\nyear = {2025},\nhowpublished = {\\url{https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html}},\nnote = {[Online; accessed 2025-05-08]},\n}\n\n\n@online{FACTSKH,\ntitle = {{Longitudinal Models for Continuous Endpoints}},\nauthor = {{Berry Consultants LLC}},\nyear = {2025},\nurl   = {https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html},\nurldate = {2025-05-08},\n}\n\n\n\nIf the webpage you are referencing does contain an explicit author, like a blog post would, use that author as author, like so: For more information on Optimizing Group Sequential Designs using Machine Learning, see Tom Parke’s blog on the FACTS Knowledge Hub [3]. Here are the most common citation formats using the above example (please substitute the title and link with the actual page you want to cite):\n\nAPAMLAChicagoHarvardVancouverBibTexBibLaTex\n\n\nTom Parke. (2025). Optimizing Group Sequential Designs using Machine Learning. Retrieved May 08, 2025, from https://docs.berryconsultants.com/blog/posts/2025-03-04.html\n\n\nTom Parke. Optimizing Group Sequential Designs using Machine Learning. 2025, https://docs.berryconsultants.com/blog/posts/2025-03-04.html. Accessed 08 May. 2025.\n\n\nTom Parke. 2025. Optimizing Group Sequential Designs using Machine Learning. https://docs.berryconsultants.com/blog/posts/2025-03-04.html (accessed May 08, 2025).\n\n\nTom Parke, 2025. Optimizing Group Sequential Designs using Machine Learning. [online] Available at: &lt;https://docs.berryconsultants.com/blog/posts/2025-03-04.html&gt; [Accessed 08 May 2025].\n\n\nTom Parke. Optimizing Group Sequential Designs using Machine Learning [Internet]. 2025 [cited 2025 May 08]. Available from: https://docs.berryconsultants.com/blog/posts/2025-03-04.html\n\n\n@misc{FACTSKH,\ntitle = {{Optimizing Group Sequential Designs using Machine Learning}},\nauthor = {{Tom Parke}},\nyear = {2025},\nhowpublished = {\\url{https://docs.berryconsultants.com/blog/posts/2025-03-04.html}},\nnote = {[Online; accessed 2025-05-08]},\n}\n\n\n@online{FACTSKH,\ntitle = {{Optimizing Group Sequential Designs using Machine Learning}},\nauthor = {{Tom Parke}},\nyear = {2025},\nurl   = {https://docs.berryconsultants.com/blog/posts/2025-03-04.html},\nurldate = {2025-05-08},\n}\n\n\n\n\n\n\n\n\nReferences\n\n[1] FACTS Development Team, FACTS: Fixed and Adaptive Clinical Trial Simulator. Austin, TX, USA: Berry Consultants LLC, 2025. Available: https://www.berryconsultants.com/software/facts/\n\n\n[2] Berry Consultants LLC, “Longitudinal models for continuous endpoints.” Accessed: Apr. 11, 2025. [Online]. Available: https://docs.berryconsultants.com/documentation/v71/userguides/core/longitudinalmodels/continuous.html\n\n\n[3] Tom Parke, “Optimizing Group Sequential Designs using Machine Learning.” Accessed: Apr. 24, 2025. [Online]. Available: https://docs.berryconsultants.com/blog/posts/2025-03-04.html",
    "crumbs": [
      "Introduction",
      "How to Cite"
    ]
  },
  {
    "objectID": "introduction/index.html",
    "href": "introduction/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the Introduction section — your starting point for learning how to use FACTS. Here, you’ll find everything you need to understand the fundamentals of our software - from step-by-step tutorials and guided walkthroughs to relevant webinars and other community resources.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#what-youll-find-here",
    "href": "introduction/index.html#what-youll-find-here",
    "title": "Introduction",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nGetting Started: Begin with the fundamentals. Understand the initial setup steps, such as downloading and installing FACTS.\nWebinars: Explore in-depth demonstrations led by our experts. Gain clarity on complex features, see best practices in action, and deepen your FACTS knowledge.\nTutorials: Learn how FACTS can create value for you at the design stage of a clinical trial or when conducting research. Navigate to the dropdown menu on the left to see tutorials for different types of clinical trials, from early to late stage.\nAcademic Materials: See how other teams and users are leveraging FACTS to achieve their goals. A broad collection of papers using FACTS, slides from presentations and other materials.\n\nIf you have any questions, don’t hesitate to contact us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html",
    "href": "introduction/gettingStarted/factsMenus.html",
    "title": "The Standard FACTS Menus",
    "section": "",
    "text": "FACTS has a File menu with commands similar to those found in most Windows applications, but also some unique to FACTS. The commands can be found in Table 1.\n\n\n\nTable 1: List of commands in the CRM file menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nNew\nAllows the user to select a trial design type and start a new design; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nOpen\nAllows the user to select an existing FACTS file and open it; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nSave\nSaves the current changes in the design to the currently open FACTS file.\n\n\nSave As\nAllows the current version of the design to be saved to a new FATCS file; this file is then the “currently open” FACTS file going forward.\n\n\nExport Project\nCreates a “Zip” archive containing the current FACTS file and all the simulation results, so these can be easily archived, emailed, saved, etc.\n\n\nRecent Projects\nContains entries for the last ten FACTS files opened: selecting one re-opens it.\n\n\nRecent Folders\nContains entries for the folders that contain the last ten FACTS files opened: selecting one opens a file browser window on that folder allowing a ‘.facts’ file in that folder to be selected (or allowing the user to navigate to some different folder and select a ‘.facts’ file there).\n\n\nExamples\nContains example projects for the different design engines: selecting one opens it.\n\n\nClose\nCloses the current FACTS file, returning the user to the FACTS introduction screen.\n\n\nExit\nCloses FACTS altogether.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html#the-file-menu",
    "href": "introduction/gettingStarted/factsMenus.html#the-file-menu",
    "title": "The Standard FACTS Menus",
    "section": "",
    "text": "FACTS has a File menu with commands similar to those found in most Windows applications, but also some unique to FACTS. The commands can be found in Table 1.\n\n\n\nTable 1: List of commands in the CRM file menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nNew\nAllows the user to select a trial design type and start a new design; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nOpen\nAllows the user to select an existing FACTS file and open it; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nSave\nSaves the current changes in the design to the currently open FACTS file.\n\n\nSave As\nAllows the current version of the design to be saved to a new FATCS file; this file is then the “currently open” FACTS file going forward.\n\n\nExport Project\nCreates a “Zip” archive containing the current FACTS file and all the simulation results, so these can be easily archived, emailed, saved, etc.\n\n\nRecent Projects\nContains entries for the last ten FACTS files opened: selecting one re-opens it.\n\n\nRecent Folders\nContains entries for the folders that contain the last ten FACTS files opened: selecting one opens a file browser window on that folder allowing a ‘.facts’ file in that folder to be selected (or allowing the user to navigate to some different folder and select a ‘.facts’ file there).\n\n\nExamples\nContains example projects for the different design engines: selecting one opens it.\n\n\nClose\nCloses the current FACTS file, returning the user to the FACTS introduction screen.\n\n\nExit\nCloses FACTS altogether.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html#facts-settings-menu",
    "href": "introduction/gettingStarted/factsMenus.html#facts-settings-menu",
    "title": "The Standard FACTS Menus",
    "section": "FACTS Settings Menu",
    "text": "FACTS Settings Menu\nThe “Settings” command menu allows the user to do 2 things:\n\nSet various FACTS options to local settings – see below for details.\nReset the options based on the stored configuration file. This file, “config.xml”, will initially be installed during the FACTS installation process and is stored in the Windows “Program Files” folder, in the sub-folder where FACTS get installed.\nChange the stored configuration file. This command allows you to select a new configuration file and have FACTS copy it to the sub-folder within the Windows “Program Files” folder, where FACTS get installed so it becomes the new stored configuration file. This allows IT support to easily disseminate configuration changes.\nEnter a new or changed license key.\n\n\nSet Options\nThe FACTS Options dialog allows the user to:\n\nSet and Test the connection parameters to access a compute grid for running simulations.\nConfigure the version and location of R or R Studio that can be launched from within FACTS\nSelect how gamma distributions are parameterized.\n\n\nGrid Configuration\nA grid compute facility for running simulations will only be available if your local IT services have set one up. If they have done so, they\n\nMay have already set the appropriate parameters In the FACTS configuration file included with the FACTS installation files.\nInform you of the parameters to be set manually via this dialog\nSend a new configuration file that can be installed using the “Load Options” menu command.\n\nIf modifying the grid options manually, select the “Options” menu command and enter the values on the “Grid Configuration” tab of the displayed dialog window.\n\n\n\n\n\n\nFigure 1: Webservice Configuration\n\n\n\nFirst select the type of interface to the grid to be used, this is either:\n\nVia a network shared drive (with a “sweeper script” running on a client machine to transfer jobs to the grid management system and return results from it).\nVia a web service system using a webserver and database to communicate to a grid management system. The IT group supporting the grid should be able to tell you which interface they have implemented, if any. If access to the grid is via a Network Share it is necessary to specify:\nThe location of the network share folder, usually in the form \\&lt;server name&gt;\\&lt;folder name\\&gt;.\nWhether the grid client is running Windows or Linux (so end-of-line characters can be corrected)\nThe listener delay – this is the interval between “looks” when FACTS is waiting for simulation results to be complete\n\nOnce specified it is possible to use the “Test” button to check that the Network Shared folder is accessible and writeable.\nIf access to the grid is via a web service:\n\nThe location of the web service endpoint.\n\nClicking on “Test Configuration” and will cause FACTS will attempt to connect to the FACTS grid controller. The control will show which components of the connection are working.\nSee the FACTS Installation Guide and FACTS Simple Grid Interface Guide for more details of setting up a grid.\n\n\nR Configuration\nIn FACTS on the Simulation tab there are two controls that launch R – “Open in R” and “ Design Report” (in FACTS 6.2.0 the latter only available for FACTS Core designs).\nTo enable these to work the user must specify where the R or RStudio executable is installed and (if there is more than one version of R installed) which version of R to use.\n\n\n\n\n\n\nFigure 2: The R Configuration Dialog\n\n\n\nThe dialog allows the user to Add, Edit, Test and Remove links to versions of R.\n\n\n\n\n\n\nFigure 3: Adding a link to R\n\n\n\nClicking on “Add” opens a normal Windows directory browser window, the user must navigate to the location of an R installation (for example “C:\\Program Files\\R\\R-2.15.2\\bin”, select the file R.exe, and click “Open”. This adds a new entry on the R configuration dialog.\nClicking on “Edit” operates similarly to “Add” above, except the selected location replaces that currently selected entry on the R configuration dialog rather than adding a new one.\nClicking on “Test” checks whether the currently selected entry on the R configuration dialog is available, if it is not an error dialog is displayed:\n\n\n\n\n\n\nFigure 4: Example of R Configuration error\n\n\n\nClicking on “Remove” removes the currently selected location on the R configuration dialog.\nThe version of R to use by default is selected by clicking on the ‘Active’ check box of the version to use.\n\n\nGamma Distribution Parameters\nIn FACTS a number of parameters require inverse gamma distributions to be specified as priors for the parameter value. There are two different parameterization of the inverse gamma provided so that the user can select the form they find the most intuitive.\n\n\n\n\n\n\nFigure 5: The parameterisation of Inverse Gamma Distributions\n\n\n\nThe first form uses parameters that are the mean of the distribution and the equivalent weight in terms of the equivalent number of observations. The second form uses an ‘Alpha’ and ‘Beta’ parameterization that some statisticians are familiar with and will find natural to use.\n\n\n\nEnter a license key\nIf a new license key is required, this command can be used to enter one. There are two ways of entering a new license:\n\n\n\n\n\n\nFigure 6: Enter FACTS License Key\n\n\n\nThe key can be entered directly, along with the associated Organization name, or by selecting a supplied license file.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html#the-help-menu",
    "href": "introduction/gettingStarted/factsMenus.html#the-help-menu",
    "title": "The Standard FACTS Menus",
    "section": "The Help Menu",
    "text": "The Help Menu\nFACTS has a Help menu with commands to assist you with the use of FACTS, providing links to users guides, tutorial and training videos. The commands are:\n\n\n\nTable 2: List of commands in the CRM help menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nUser Guides\nProvides access to documents such as this one, with (mainly) one user guide to each design type within FACTS. Exceptions to this simple structure are:  1. Core Design User Guide: A guide to the options under the ’Design” tab for FACTS Core for all endpoints.  2. Staged Design User Guide: As the staged design allows the design of one FACTS Core stage followed by a second, most of the interface is common to the basic FACTS Core. This guide describes the differences and additional aspects for all endpoints.  3. Dose Escalation User Guide: This covers all the Dose Escalation engines except for N-CRM and 2D-CRM that have their own. It thus covers the 3+3, mTPI, CRM(Toxicity), CRM(Ordinal), CRM(Efficacy) and bCRM engines.\n\n\nTutorials\nProvides access to all the tutorial documents, which describes detailed examples of use of all the engines in FACTS and many of their options. The examples under the File &gt; Examples menu option largely correspond to the different tutorials described here.\n\n\nDesign Specifications\nThese are technical documents that describe the mathematical models implemented in FACTS in detail.\n\n\nExecution Guides\nThe FACTS GUI can be run in command line mode so simulations can be run/re-run from scripts. With the simulation command line flag, and passed a directory rather than a file, FACTS will run simulations for every “.facts” file in that directory – and recurse into any sub-directories and simulate any “.facts” files there too. A full guide to command line mode can be found here. The FACTS simulation engines are also available in “command line executable” form. There are guides here that document their command line parameters and how to use them to analyse a data set – e.g. to perform an interim analysis whilst executing a trial designed with FACTS.\n\n\nFACTS file XML Specs\nThese guides describe the parameters in the “.facts” files, which are text files in XML format. For expert users understanding this format allows them to use scripts to generate versions of an initial “.facts” file with slight variations in the parameters such as stopping thresholds or priors. Modification of “.facts” files outside of FACTS needs to be done with care, errors may render the file unusable by FACTS.\n\n\nVideos\nProvides access to links to the introductory, training and webinar videos that Berry Consultants has recorded and makes available over the internet to FACTS users.\n\n\nView log…\nIf an error has occurred in FACTS, often the FACTS log file can shed light on what is going wrong. The log file is hidden away in some unfashionable and hard to locate Windows folder; this command option provides easy access to it. Allowing you to email facts support with a description of what occurred, attaching a copy of this log file having saved it somewhere convenient such as your desktop.\n\n\nSupport\nLaunch a simple editor for sending an email to our support account: facts@berryconsultants.net\n\n\nAbout\nDisplays a simple “about box” that includes the detailed version number of FACTS.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/startingFacts.html",
    "href": "introduction/gettingStarted/startingFacts.html",
    "title": "Starting FACTS",
    "section": "",
    "text": "Introduction\nFACTS is usually first started using the FACTS icon installed on the Windows desktop or from the Windows Start &gt; Programs menu.\nWhen the application opens, you are presented with the main introduction screen of FACTS (Figure 1). From here you can start designing an your trial, by double clicking on the design in the list of available designs, or by selecting the design option from the File &gt; New menu.\nFACTS will also be associated the parameter files it writes out with a ‘.facts’ file extension. Clicking on any one of these files will automatically start FACTS with that file opened.\nNote: Depending on your license, some design options may not be available.\n\n\n\n\n\n\nFigure 1: FACTS introduction screen\n\n\n\nFrom this screen a new design can be started by selecting the design type in the list and double clicking it, or clicking on the ‘create new’ button at the bottom of the list.\nIn the ‘Recent Work’ panel, the use can select from the list of most recently opened FACTS files, or from the list of directories where those files were located, opening the folder and selecting a FACTS file from there.\nOnce a FACTS design has been created, the appropriate FACTS design module can be launched by double clicking on the “.facts” file in Windows Explorer.\nOnce a specific type of FACTS design has been selected or an existing FACTS file opened, the FACTS GUI displays the tabs and parameters appropriate to that type of trial design.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Starting FACTS"
    ]
  },
  {
    "objectID": "introduction/references.html#selected-slides",
    "href": "introduction/references.html#selected-slides",
    "title": "Academic Materials",
    "section": "Selected Slides",
    "text": "Selected Slides",
    "crumbs": [
      "Introduction",
      "Academic Materials"
    ]
  },
  {
    "objectID": "introduction/references.html#all-papers-citing-facts",
    "href": "introduction/references.html#all-papers-citing-facts",
    "title": "Academic Materials",
    "section": "All Papers citing FACTS",
    "text": "All Papers citing FACTS\nFor a comprehensive overview of all academic papers citing FACTS, click here (you will be redirected to Google Scholar).",
    "crumbs": [
      "Introduction",
      "Academic Materials"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Knowledge Hub",
    "section": "",
    "text": "FACTS (Fixed and Adaptive Clinical Trial Simulator) is the industry’s most powerful tool for adaptive and fixed trial design.\nIt enables biostatisticians to design, simulate, and optimize trials with speed and precision, reducing risk and driving innovative, data-driven decisions.\nOver half of the top 20 largest pharmaceutical companies in the world and more than 30 academic institutions trust FACTS to assist them in the design, simulation, and implementation of trials."
  },
  {
    "objectID": "blog/posts/2025-04-25.html",
    "href": "blog/posts/2025-04-25.html",
    "title": "April’s FACTS Webinar - Longitudinal Models",
    "section": "",
    "text": "April’s FACTS webinar focused on Longitudinal Models in FACTS. It’s a topic that we could spend 8 hours straight on and not get a full review done, but we squeezed a lot into the hour. The slides are attached here.\nThe general structure of the webinar focused around an example trial similar to the lecanemab phase II trial. Scott Berry presented that trial as a case study in a 2024 FACTS webinar, complete with about a 3 minute clip from the movie Amadeus woven in. We left out some of the adaptations used in that trial, and focused on the longitudinal aspect of the ADCOMS endpoint across 3-, 6-, 9-, and 12-month visits.\nWe first assessed the efficiency gains in the estimation of the mean response for the doses in the trial at the 4th out of the 13 interim analyses. At that point there is roughly 194 complete subjects and another 117 with intermediate, but not complete, data. The longitudinal model increased the effective sample size at the analysis (based on the precision of the posterior distribution) by up to 80 subjects. This efficiency gain of course depends on the assumed correlation of the intermediate data with the final endpoint data.\nThen, we moved past the interim estimation precision to operating characteristic benefits. We aimed to equalize the power across the trials with and without longitudinal models and with varying intra-subject correlation. The improvements in the trial, then were expressed in terms of average sample size. When the correlation between intermediate and final endpoints was large, the average sample size of the trial could be reduced by up to 13% in certain scenarios. In our simulated trial the longitudinal model was especially good at speeding up early stopping for success decisions.\nThe FACTS files used to make OC comparisons are:\n\nSimTrialWithNoLM.facts\nSimTrialWithLM_2longmods.facts\n\n\n\n\n\n\n\nFigure 1: A look at the statistical improvements in the estimation fo the response rate for the control dose and the high dose at the 4th interim out of 13. Improvements in precision are also framed as gains in effective sample size due to incorporating intermediate data."
  },
  {
    "objectID": "blog/posts/2025-03-05.html",
    "href": "blog/posts/2025-03-05.html",
    "title": "ADMTP 2025 Conference Presentation",
    "section": "",
    "text": "In addition to this one, I also gave a talk at ADMTP 2025 on Response Adaptive Randomization (RAR). Click here to download the slides. I continue to be surprised how much is written about RAR in the two-arm setting compared to the multi-arm setting, when the benefits in the latter case are so much clearer. There has also been, over the years, a number talks and papers warning of the risks of RAR. So, I wanted to produce a talk that looked at the simulation results of a straightforward application of RAR when we are testing 3 arms against control and selecting the best one.\nFrom the first interim onwards, the randomization between the treatment arms was in proportion to the probability that the arm had the maximum response. The proportion allocated to the control arm was set to match the arm with the highest allocation. Thus, there is some insurance against time trends. Trial success was based on the Bayesian posterior probability that the response on the selected arm was greater than that of the control arm, calibrated to control the type-1 error at 0.05 (this was imagined in a phase 2 setting).\nWe found that, compared to a fixed trial with equal allocation, there was:\n\nan increase in power,\nan increase in the probability of selecting the right arm,\na reduction in bias and MSE in the estimate of the response on the selected arm,\nan increase in the expected allocation to the selected arm,\nand a 5-7% risk of allocating fewer subjects to the selected arm than would have been the case in the fixed trial.\n\nSurprisingly, these benefits were all observed even with a single interim at the midpoint of the trial to adjust the randomization ratios just once. The benefits all increased with more interims, though with diminishing returns.\n\n\n\n\n\n\nFigure 1: Power, correct arm selection, E(n) on selected arm when successful. Power, selection of correct arm, and allocation to selected arm increases with increased number of interims, but there are diminishing returns."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to our blog — your resource for timely product news, upcoming features, expert opinions on everything clinical trials and conference and webinar materials.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nApril’s FACTS Webinar - Longitudinal Models\n\n\n\n\n\n\nPresentation\n\n\nFACTS\n\n\nLongitudinal Models\n\n\nWebinar\n\n\n\nAn example of the use of a longitudinal model in FACTS focusing on the statistical efficiencies gained.\n\n\n\n\n\nApr 25, 2025\n\n\nNick Berry\n\n\n\n\n\n\n\n\n\n\n\n\nResponse-Adaptive Randomization in Clinical Trials\n\n\n\n\n\n\nRAR\n\n\nExpert Opinion\n\n\nAdaptive Bayesian Design\n\n\nAdvice\n\n\n\nCurrent opinion considering recent publications\n\n\n\n\n\nApr 22, 2025\n\n\nKert Viele\n\n\n\n\n\n\n\n\n\n\n\n\nADMTP 2025 Conference Presentation\n\n\n\n\n\n\nPresentation\n\n\nCRM\n\n\nDose Escalation\n\n\nProject Optimus\n\n\n\nDesigning a seamless P1/P2a open enrollment CRM dose escalation study\n\n\n\n\n\nMar 6, 2025\n\n\nElias Laurin Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nADMTP 2025 Conference Presentation\n\n\n\n\n\n\nPresentation\n\n\nRAR\n\n\nMAMS\n\n\nAdaptive Bayesian Design\n\n\n\nThe Properties of Multi-Arm Response Adaptive Designs\n\n\n\n\n\nMar 5, 2025\n\n\nTom Parke\n\n\n\n\n\n\n\n\n\n\n\n\nADMTP 2025 Conference Presentation\n\n\n\n\n\n\nPresentation\n\n\nMachine Learning\n\n\nOptimization\n\n\nGroup Sequential Design\n\n\n\nOptimizing Group Sequential Designs using Machine Learning\n\n\n\n\n\nMar 4, 2025\n\n\nTom Parke\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "concepts/bayes/primer.html",
    "href": "concepts/bayes/primer.html",
    "title": "Primer",
    "section": "",
    "text": "Bayesian statistics is a paradigm in statistical inference that emphasizes the use of probability to quantify uncertainty in parameters of interest. Unlike frequentist methods that consider parameters as fixed values and probabilities as long-run frequencies of repeated experiments, Bayesian methods treat parameters as random variables and update beliefs about these parameters through observed data.\nIn the context of biostatistics and clinical trials, Bayesian approaches can be particularly powerful due to their flexibility in incorporating prior information, handling complex hierarchical structures, and adapting trial designs in real time.\n\n\n\nBayes’ Theorem\n\n\\[\n   p(\\theta \\mid x) \\;=\\; \\frac{p(x \\mid \\theta) \\, p(\\theta)}{p(x)},\n\\]\n\n\\(p(\\theta \\mid x)\\) is the posterior distribution of the parameter(s) \\(\\theta\\) after observing data \\(x\\).\n\\(p(x \\mid \\theta)\\) is the likelihood, describing how probable the observed data \\(x\\) are under a specific value of \\(\\theta\\).\n\\(p(\\theta)\\) is the prior distribution, reflecting one’s beliefs about \\(\\theta\\) before seeing data.\n\\(p(x)\\) is the marginal likelihood or evidence, often treated as a normalizing constant.\n\n\nPrior Distributions\n\nA prior distribution encodes existing knowledge or expert belief about a parameter before observing new data.\nIn clinical trials, priors can come from historical data, expert opinion, or pilot studies.\n\nPosterior Distributions\n\nAfter data collection, the prior is updated into the posterior, balancing what we believed before with the evidence from the data.\nSummaries of the posterior (e.g., mean, median, credible intervals) provide the final inference on \\(\\theta\\).\n\nPredictive Distributions\n\nBayesian methods naturally extend to predictive distributions, which forecast future observations based on current data and knowledge.\nIn clinical trials, predictive distributions can guide adaptive decision-making, such as adding or dropping arms in a multi-arm study or changing randomization probabilities.\n\nModel Checking and Diagnostics\n\nBayesian methods typically involve complex models that may need diagnostic checks (e.g., posterior predictive checks) to evaluate model fit and reasonableness of priors.\n\n\n\n\n\n\nAdaptive Designs\n\nMany modern clinical trials are adaptive, allowing modifications to the trial as data accumulate.\nBayesian adaptive designs can enable real-time updating of randomization probabilities, early stopping for futility or efficacy, and seamless phase transitions.\n\nBorrowing Information\n\nIn rare diseases or small populations, data may be limited.\nHierarchical Bayesian models allow borrowing of information across subgroups (e.g., biomarkers, disease subtypes), leveraging prior trial data or real-world evidence.\n\nFlexibility\n\nBayesian methods handle complex data structures (e.g., survival data with covariates, longitudinal measurements, high-dimensional biomarker data) within a coherent probabilistic framework.\n\nTransparency in Uncertainty\n\nPosterior distributions directly quantify uncertainty about parameters, often providing intuitive credible intervals (e.g., a 95% credible interval has a 95% probability of containing the true parameter).\n\n\n\n\n\nBayesian statistics offers a flexible and coherent framework for inference, particularly suited to biostatistics and clinical trials where:\n\nAdaptive decisions are made mid-trial,\nBorrowing of information is highly valuable (e.g., rare diseases, historical controls),\n\nComplex modeling scenarios frequently arise (e.g., hierarchical structures, multiple endpoints).\n\nBy updating beliefs as new data accumulate, Bayesian methods naturally align with the iterative learning process that defines scientific discovery and the realities of clinical research.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/bayes/primer.html#key-concepts-in-bayesian-statistics",
    "href": "concepts/bayes/primer.html#key-concepts-in-bayesian-statistics",
    "title": "Primer",
    "section": "",
    "text": "Bayes’ Theorem\n\n\\[\n   p(\\theta \\mid x) \\;=\\; \\frac{p(x \\mid \\theta) \\, p(\\theta)}{p(x)},\n\\]\n\n\\(p(\\theta \\mid x)\\) is the posterior distribution of the parameter(s) \\(\\theta\\) after observing data \\(x\\).\n\\(p(x \\mid \\theta)\\) is the likelihood, describing how probable the observed data \\(x\\) are under a specific value of \\(\\theta\\).\n\\(p(\\theta)\\) is the prior distribution, reflecting one’s beliefs about \\(\\theta\\) before seeing data.\n\\(p(x)\\) is the marginal likelihood or evidence, often treated as a normalizing constant.\n\n\nPrior Distributions\n\nA prior distribution encodes existing knowledge or expert belief about a parameter before observing new data.\nIn clinical trials, priors can come from historical data, expert opinion, or pilot studies.\n\nPosterior Distributions\n\nAfter data collection, the prior is updated into the posterior, balancing what we believed before with the evidence from the data.\nSummaries of the posterior (e.g., mean, median, credible intervals) provide the final inference on \\(\\theta\\).\n\nPredictive Distributions\n\nBayesian methods naturally extend to predictive distributions, which forecast future observations based on current data and knowledge.\nIn clinical trials, predictive distributions can guide adaptive decision-making, such as adding or dropping arms in a multi-arm study or changing randomization probabilities.\n\nModel Checking and Diagnostics\n\nBayesian methods typically involve complex models that may need diagnostic checks (e.g., posterior predictive checks) to evaluate model fit and reasonableness of priors.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/bayes/primer.html#why-bayesian-methods-are-useful-in-biostatistics-and-clinical-trials",
    "href": "concepts/bayes/primer.html#why-bayesian-methods-are-useful-in-biostatistics-and-clinical-trials",
    "title": "Primer",
    "section": "",
    "text": "Adaptive Designs\n\nMany modern clinical trials are adaptive, allowing modifications to the trial as data accumulate.\nBayesian adaptive designs can enable real-time updating of randomization probabilities, early stopping for futility or efficacy, and seamless phase transitions.\n\nBorrowing Information\n\nIn rare diseases or small populations, data may be limited.\nHierarchical Bayesian models allow borrowing of information across subgroups (e.g., biomarkers, disease subtypes), leveraging prior trial data or real-world evidence.\n\nFlexibility\n\nBayesian methods handle complex data structures (e.g., survival data with covariates, longitudinal measurements, high-dimensional biomarker data) within a coherent probabilistic framework.\n\nTransparency in Uncertainty\n\nPosterior distributions directly quantify uncertainty about parameters, often providing intuitive credible intervals (e.g., a 95% credible interval has a 95% probability of containing the true parameter).",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/bayes/primer.html#summary",
    "href": "concepts/bayes/primer.html#summary",
    "title": "Primer",
    "section": "",
    "text": "Bayesian statistics offers a flexible and coherent framework for inference, particularly suited to biostatistics and clinical trials where:\n\nAdaptive decisions are made mid-trial,\nBorrowing of information is highly valuable (e.g., rare diseases, historical controls),\n\nComplex modeling scenarios frequently arise (e.g., hierarchical structures, multiple endpoints).\n\nBy updating beliefs as new data accumulate, Bayesian methods naturally align with the iterative learning process that defines scientific discovery and the realities of clinical research.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/index.html",
    "href": "concepts/index.html",
    "title": "Concepts",
    "section": "",
    "text": "Welcome to the Concepts section — your resource for building a strong intellectual foundation on clinical biostatistics and clinical trial simulation. Here, we break down the core principles you’ll need to understand clinical trials and Bayesian statistics and provide pro tips to get the most out of FACTS.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#what-youll-find-here",
    "href": "concepts/index.html#what-youll-find-here",
    "title": "Concepts",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nBasics of Clinical Trials: Learn the essentials, terminology, and best practices that define the clinical trial landscape. From study design and data collection to regulatory considerations, we’ll walk you through the elements that matter most.\nStatistical & Mathematical Fundamentals: Uncover the foundational concepts in statistics and mathematics that empower you to make data-driven decisions. Understand key analyses, Bayesian approaches, and appreciate why certain metrics and models are essential to reliable results.\nTips & Tricks for Using FACTS: Gain insider know-how that helps you work smarter and faster.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#how-to-make-the-most-of-this-section",
    "href": "concepts/index.html#how-to-make-the-most-of-this-section",
    "title": "Concepts",
    "section": "How to Make the Most of This Section",
    "text": "How to Make the Most of This Section\nUse the Concepts section as a springboard to build your expertise, whether you’re new to clinical trials or looking to refine your analytical skills. By solidifying your grasp of fundamentals and strengthening your analytical toolkit, you’ll be better equipped to understand results and ask the right questions. Check back often as we continue expanding these resources to support your growth.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/goldilocks.html",
    "href": "concepts/adaptiveDesigns/goldilocks.html",
    "title": "Goldilocks Designs",
    "section": "",
    "text": "Goldilocks designs achieve everything group sequential designs do, but may provide a number of additional advantages. Goldilocks designs are widely used both in “Learn” and “Confirm” studies, the approach is accepted by health authorities, and operating characteristics can be determined through simulation (FACTS).\nA Goldilocks trial design is an adaptive clinical trial methodology developed to optimize the sample size dynamically during the course of a trial. Its name references the “just right” principle from the Goldilocks fairy tale—neither too large nor too small. Goldilocks designs seek balance between flexibility and efficiency, by repeatedly updating sample-size decisions based on accumulating trial data, through predictive probability or other Bayesian statistical methods.\nGoldilocks designs allow for stopping the trial early by halting accrual at an interim analysis, but always allow subjects the opportunity to collect complete follow-up before the trial’s final analysis is performed. Goldilocks designs combine the completely observed patient data with the uncertainty associated with the missing data at the time of an interim analysis, and use predictive probabilities to decide if stopping for expected success or futility is appropriate.\nThis is particularly relevant if the primary endpoint involves a delayed observation. If an interim analysis declares early success, and accrual stops, but more data from patients already randomized come in, and the statistic drops below the threshold, can we claim that we have demonstrated efficacy? Goldilocks designs explicitly take this possibility into account and mitigate the risk of a “flip flop” (i.e. declaring “success” early at an interim analysis, based on incomplete data, only to see this flipping into “no success”) once all data (including delayed observations) become available.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Goldilocks Designs"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/goldilocks.html#introduction",
    "href": "concepts/adaptiveDesigns/goldilocks.html#introduction",
    "title": "Goldilocks Designs",
    "section": "",
    "text": "Goldilocks designs achieve everything group sequential designs do, but may provide a number of additional advantages. Goldilocks designs are widely used both in “Learn” and “Confirm” studies, the approach is accepted by health authorities, and operating characteristics can be determined through simulation (FACTS).\nA Goldilocks trial design is an adaptive clinical trial methodology developed to optimize the sample size dynamically during the course of a trial. Its name references the “just right” principle from the Goldilocks fairy tale—neither too large nor too small. Goldilocks designs seek balance between flexibility and efficiency, by repeatedly updating sample-size decisions based on accumulating trial data, through predictive probability or other Bayesian statistical methods.\nGoldilocks designs allow for stopping the trial early by halting accrual at an interim analysis, but always allow subjects the opportunity to collect complete follow-up before the trial’s final analysis is performed. Goldilocks designs combine the completely observed patient data with the uncertainty associated with the missing data at the time of an interim analysis, and use predictive probabilities to decide if stopping for expected success or futility is appropriate.\nThis is particularly relevant if the primary endpoint involves a delayed observation. If an interim analysis declares early success, and accrual stops, but more data from patients already randomized come in, and the statistic drops below the threshold, can we claim that we have demonstrated efficacy? Goldilocks designs explicitly take this possibility into account and mitigate the risk of a “flip flop” (i.e. declaring “success” early at an interim analysis, based on incomplete data, only to see this flipping into “no success”) once all data (including delayed observations) become available.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Goldilocks Designs"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/goldilocks.html#goldilocks-clinical-trial-designs",
    "href": "concepts/adaptiveDesigns/goldilocks.html#goldilocks-clinical-trial-designs",
    "title": "Goldilocks Designs",
    "section": "Goldilocks Clinical Trial Designs",
    "text": "Goldilocks Clinical Trial Designs\nThe Goldilocks trial design, as an adaptive trial framework, draws on the strengths of group sequential designs and additionally emphasizes flexibility, efficiency, and optimal use of data. The concept of the “Goldilocks” design comes from the idea of finding an optimal balance—where the trial is neither too large (wasting resources) nor too small (lacking statistical power), but just right for the research question at hand.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Goldilocks Designs"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/goldilocks.html#key-features-of-goldilocks-trial-designs",
    "href": "concepts/adaptiveDesigns/goldilocks.html#key-features-of-goldilocks-trial-designs",
    "title": "Goldilocks Designs",
    "section": "Key Features of Goldilocks Trial Designs",
    "text": "Key Features of Goldilocks Trial Designs\n\nAdaptive and Flexible: Goldilocks designs allow for multiple adaptive changes based on interim results, such as the addition or removal of treatment arms, changes to patient inclusion/exclusion criteria, or modifications to the statistical model. This flexibility allows the trial to be tailored in real-time to the evolving landscape of treatment options, participant response, and other external factors.\nIncorporation of Multiple Interim Analyses: Goldilocks trials can conduct interim analyses at flexible intervals. This enables real-time decision-making that maximizes the use of accumulated data, including incomplete data.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Goldilocks Designs"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/goldilocks.html#key-aspects-of-goldilocks-designs",
    "href": "concepts/adaptiveDesigns/goldilocks.html#key-aspects-of-goldilocks-designs",
    "title": "Goldilocks Designs",
    "section": "Key aspects of Goldilocks Designs",
    "text": "Key aspects of Goldilocks Designs\n\nBayesian and Predictive Probability Approaches\nGoldilocks designs frequently utilize Bayesian methods, commonly including multiple imputation, and predictive probability calculations at interim analyses to assess the likelihood of trial success at any given point, guiding adaptive sample size decisions. Both Goldilocks designs and group sequential designs can have a frequentist final analysis.\n\n\nAnalytical Calculations of Stopping Boundaries\nGroup sequential designs have analytical calculations that provide stopping boundaries guaranteeing exact Type I error control. It is common to simulate the Type I error of a Goldilocks design to calculate stopping boundaries.\n\n\nStopping for futility\nGoldilocks designs allow futility stopping criteria to be framed in terms of the predictive probability of success: What would the probability of success be, should the trial continue to recruit up to the full sample size and all follow-up information was collected? If this probability is smaller than some predefined threshold (say for instance less than 5%) the trial can stop for futility.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Goldilocks Designs"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/goldilocks.html#example-applications-and-technical-details",
    "href": "concepts/adaptiveDesigns/goldilocks.html#example-applications-and-technical-details",
    "title": "Goldilocks Designs",
    "section": "Example Applications and Technical Details",
    "text": "Example Applications and Technical Details\nA foundational paper explicitly outlining the “Goldilocks” approach was authored by Broglio, Connor, and Berry (2014) (1). In their seminal work, they introduce a Bayesian predictive probability framework to guide sample size adaptations, clearly illustrating how interim results influence sample-size decisions: In this paper, Broglio et al. provide detailed simulations and comparisons illustrating how Goldilocks designs can outperform traditional designs, particularly when initial estimates of treatment effects are uncertain. They demonstrate that these designs can substantially reduce the number of patients exposed to ineffective treatments without sacrificing statistical rigor. Tables and figures in the paper present simulations showing how predictive probabilities shift dynamically with accumulating data, and how this influences ongoing sample size decisions. Goldilocks designs represent a significant advance in clinical trial methodology, providing a mechanism for dynamically adjusting sample size to achieve optimal efficiency and ethical balance. They differ from traditional group sequential designs through continuous or periodic Bayesian predictive adaptation, making them well-suited to modern clinical trial challenges such as uncertain effect sizes, limited patient populations, or time-critical situations like pandemics. By integrating real-time data monitoring with flexible decision-making, Goldilocks designs help ensure trials are appropriately powered without unnecessary resource expenditure or patient exposure to inferior treatments.\nIf you have more questions, please contact Dr Nick Berry (North America; Email) or Dr Elias Laurin Meyer (Europe; Email).\nKey Papers:\n\nBroglio KR, Connor JT, Berry SM. Not too big, not too small: a Goldilocks approach to sample size selection. J Biopharm Stat. 2014;24(3):685–705. PubMed\nBerry DA. Emerging innovations in clinical trial design. Clin Pharmacol Ther. 2016;99(1):82–91. PubMed (This paper discusses general Bayesian adaptive methodologies including Goldilocks-style adaptations.)\nConnor JT, Broglio KR, Durkalski V, et al. The Stroke Hyperglycemia Insulin Network Effort (SHINE) trial: An adaptive trial design case study. Clin Trials. 2015;12(4):367–375. PubMed (This practical case study illustrates how a Goldilocks approach was implemented in a real-world clinical trial setting.)\nBroglio K, Meurer WJ, Durkalski V, Pauls Q, Connor J, Berry D, Lewis RJ, Johnston KC, Barsan WG. Comparison of Bayesian vs Frequentist Adaptive Trial Design in the Stroke Hyperglycemia Insulin Network Effort Trial. JAMA Netw Open. 2022 May 2;5(5):e2211616. doi: 10.1001/jamanetworkopen.2022.11616. PMID: 35544137; PMCID: PMC9096598. (The project described in ref. 3 also included a prospective comparison of a Goldilocks and Group Sequential design for the same trial).\nLewis RJ. The pragmatic clinical trial in a learning healthcare system. Clin Trials. 2016;13(5):484–492. PubMed (This discusses adaptive and pragmatic trials, including concepts that underpin Goldilocks-style adjustments.)\nZhan T, Zhang H, Hartford A, Mukhopadhyay S. Modified Goldilocks Design with strict type I error control in confirmatory clinical trials, Journal of Biopharmaceutical Statistics. 2020;30:5, 821-833, DOI: 10.1080/10543406.2020.1744620 (This paper presents a version of Goldilocks design with an analytical approach to controlling type-1 error, rather than relying on simulations).\nLan KKG, DeMets DL. Discrete sequential boundaries for clinical trials. Biometrika. 1983;70(3), 659-663 (This paper laid the foundations for group sequential designs).\nJennison C, Turnbull BW. Group Sequential Methods with Applications to Clinical Trials. 2000, CRC Press\nPike MC. Goldilocks trials: A new approach to adaptive designs. Statistics in Medicine. 2019;38(10), 1801-1810\n\nExamples for published applications of Goldilocks designs (** if this led to market approval):\n\n** Wilber DJ et al. Comparison of antiarrhythmic drug therapy and radiofrequency catheter ablation in patients with paroxysmal atrial fibrillation: a randomized controlled trial. JAMA. 2010;303(4):333-40. doi: 10.1001/jama.2009.2029\n** Philpott JM et al. The ABLATE Trial: Safety and Efficacy of Cox Maze-IV Using a Bipolar Radiofrequency Ablation System. Ann Thorac Surg. 2015;100(5):1541-6; discussion 1547-8. doi: 10.1016/j.athoracsur.2015.07.006.\nKonstam MA et al. Impact of Autonomic Regulation Therapy in Patients with Heart Failure: ANTHEM-HFrEF Pivotal Study Design. Circ Heart Fail. 2019;12(11):e005879. doi: 10.1161/CIRCHEARTFAILURE.119.005879.\nPremchand RK et al. Autonomic regulation therapy via left or right cervical vagus nerve stimulation in patients with chronic heart failure: results of the ANTHEM-HF trial. J Card Fail. 2014;20(11):808-16. doi: 10.1016/j.cardfail.2014.08.009.\nWhite WB et al. A cardiovascular safety study of LibiGel (testosterone gel) in postmenopausal women with elevated cardiovascular risk and hypoactive sexual desire disorder. Am Heart J. 2012;163(1):27-32. doi: 10.1016/j.ahj.2011.09.021.\nLorusso R et al. Sutureless versus Stented Bioprostheses for Aortic Valve Replacement: The Randomized PERSIST-AVR Study Design. Thorac Cardiovasc Surg. 2020;68(2):114-123. doi: 10.1055/s-0038-1675847.\nDuytschaever M et al. Paroxysmal Atrial Fibrillation Ablation Using a Novel Variable-Loop Biphasic Pulsed Field Ablation Catheter Integrated With a 3-Dimensional Mapping System: 1-Year Outcomes of the Multicenter inspIRE Study. Circ Arrhythm Electrophysiol. 2023;16(3):e011780. doi: 10.1161/CIRCEP.122.011780.\n** Nogueira RG et al. Thrombectomy 6 to 24 Hours after Stroke with a Mismatch between Deficit and Infarct. N Engl J Med. 2018;378(1):11-21. doi: 10.1056/NEJMoa1706442. Epub 2017 Nov 11. PMID: 29129157.\n** Swanson CJ et al. A randomized, double-blind, phase 2b proof-of-concept clinical trial in early Alzheimer’s disease with lecanemab, an anti-Aβ protofibril antibody. Alzheimers Res Ther. 2021;13(1):80. doi: 10.1186/s13195-021-00813-8.\n** Castro M et al. Effectiveness and safety of bronchial thermoplasty in the treatment of severe asthma: a multicenter, randomized, double-blind, sham-controlled clinical trial. Am J Respir Crit Care Med. 2010;181(2):116-24. doi: 10.1164/rccm.200903-0354OC.\n** Conway CR et al. A prospective, multi-center randomized, controlled, blinded trial of vagus nerve stimulation for difficult to treat depression: A novel design for a novel treatment. Contemp Clin Trials. 2020;95:106066. doi: 10.1016/j.cct.2020.106066.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Goldilocks Designs"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "",
    "text": "Adaptive designs have, rightfully, become pervasive in clinical trials. Early stopping for efficacy or futility, as in group sequential, goldilocks, and promising zone designs, aim to make decisions before the maximum sample size has been reached. These trials can decrease expected sample size without sacrificing power or type I error. Response adaptive randomization, arm dropping, and other randomization manipulation strategies have been researched thoroughly and implemented widely. Adaptive allocation can focus randomization so that more data is collected on arms of interest. These methods are used broadly and recognized as cutting edge methods that diverge from the classic paradigm of how information is used in a clinical trial.\nA frontier that has similar goals as the previously mentioned adaptations, but has not been the focus of as much research is the use of early endpoint data in adaptive decision making. It’s a seemingly obvious statistical observation that a well-designed clinical trial should use all data available to it to make decisions. Despite that, it is common to ignore early data about subjects in a clinical trial, often to the extent that only subjects with complete information are included in statistical analyses.\nLongitudinal modeling leverages data collected from individual subjects before the final endpoint time to improve estimation of the final endpoint. For the purposes of this article, this inclusion of early subject data in the primary analysis model is done through multiple imputation of the final endpoint given early subject data. The statistical model that links early endpoint data with the final endpoint to allow for the imputation is called the longitudinal model. This imputation through the longitudinal model allows for improvements in the estimation of the final endpoint, and, as a result, improves the efficiency in clinical trial decision making.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html#introduction",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html#introduction",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "",
    "text": "Adaptive designs have, rightfully, become pervasive in clinical trials. Early stopping for efficacy or futility, as in group sequential, goldilocks, and promising zone designs, aim to make decisions before the maximum sample size has been reached. These trials can decrease expected sample size without sacrificing power or type I error. Response adaptive randomization, arm dropping, and other randomization manipulation strategies have been researched thoroughly and implemented widely. Adaptive allocation can focus randomization so that more data is collected on arms of interest. These methods are used broadly and recognized as cutting edge methods that diverge from the classic paradigm of how information is used in a clinical trial.\nA frontier that has similar goals as the previously mentioned adaptations, but has not been the focus of as much research is the use of early endpoint data in adaptive decision making. It’s a seemingly obvious statistical observation that a well-designed clinical trial should use all data available to it to make decisions. Despite that, it is common to ignore early data about subjects in a clinical trial, often to the extent that only subjects with complete information are included in statistical analyses.\nLongitudinal modeling leverages data collected from individual subjects before the final endpoint time to improve estimation of the final endpoint. For the purposes of this article, this inclusion of early subject data in the primary analysis model is done through multiple imputation of the final endpoint given early subject data. The statistical model that links early endpoint data with the final endpoint to allow for the imputation is called the longitudinal model. This imputation through the longitudinal model allows for improvements in the estimation of the final endpoint, and, as a result, improves the efficiency in clinical trial decision making.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html#importance-in-clinical-trials",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html#importance-in-clinical-trials",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "Importance in Clinical Trials",
    "text": "Importance in Clinical Trials\nWhile in early stages of development, frequent assessments of outcomes like biomarker levels, symptom scores, or surrogate endpoints are more common, we argue that for confirmatory trials, too, we should assess whether the efficiency gain of longitudinal models justify more frequent repeat measures, even if it comes at an incremental operational cost.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html#benefits-of-longitudinal-modeling",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html#benefits-of-longitudinal-modeling",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "Benefits of Longitudinal Modeling",
    "text": "Benefits of Longitudinal Modeling\nBy leveraging repeated measurements, longitudinal modeling enables the following:\n\nEnhanced Statistical Estimation Efficiency\nMultiple imputation of missing final endpoint data increases statistical efficiency by capturing currently enrolled subjects’ trajectories and measuring within‑subject variability across multiple time points. Imputing final data for subjects with only early endpoint data allows the estimated treatment effect to incorporate likely trajectories of enrolled subjects, thereby estimating an effect closer to what will be observed once all subjects’ final data are available. Additionally, subjects without a final endpoint who are imputed contribute to the effective sample size—albeit with partial weights—improving precision in the estimation of the final endpoint response.\nReduced Sample Size Requirements\nDue to the increased statistical efficiency described above, trials using longitudinal modeling often require fewer participants than traditional designs to achieve equivalent power. This reduction can lead to faster, more cost‑effective trials and reduced patient exposure to potential risks. Alternatively, if the sample size is held constant, the same efficiency gains can be realized as increased power.\nImproved Predictive Probability Calculations\nUtilizing the longitudinal model’s knowledge of standard subject progression enhances treatment effect estimates and enables better predictions of trial results at later time points. In adaptive designs—such as the “Goldilocks” design—predictive probabilities can leverage these imputation models to forecast final endpoint values for subjects who have enrolled and have partial data but have not yet reached their final endpoint.\nImproved Understanding of Disease Progression\nLongitudinal approaches explicitly model likely final‑endpoint values conditional on observed early data. This allows for direct analysis of disease trajectories within each trial arm, characterizing how diseases evolve naturally and how interventions modify this progression—ultimately supporting more informative clinical conclusions.\nIncorporation of Prior Information (or Not)\nMany indications are well understood before trial onset, and subjects’ likely outcomes throughout follow‑up are known in advance. Longitudinal models can be structured with informative priors based on past data, benefiting decision‑making even before any subject has complete data. Conversely, models can be specified with non‑informative priors and estimated solely from trial data as it accrues.\nFlexibility in Handling Missing Data\nMissing data—a frequent challenge in trials—can be robustly managed with longitudinal imputation methods. Subjects lost to follow‑up (dropouts) can be imputed using the same models as those for interim data, under the assumption that missingness is at random conditional on early endpoint data—an assumption less restrictive than missing completely at random.\nIndividualized Patient Insights\nLongitudinal analyses support individual‑level predictions and personalized medicine approaches. These insights help clinicians understand each patient’s disease trajectory over time, informing tailored therapeutic decisions and enabling personalized healthcare strategies.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html#challenges-and-methodological-considerations",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html#challenges-and-methodological-considerations",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "Challenges and Methodological Considerations",
    "text": "Challenges and Methodological Considerations\nLongitudinal modeling often requires more advanced statistical methods than analyses based solely on final endpoints. In multiple‑imputation frameworks, this complexity translates into increased computational intensity. There are limited off‑the‑shelf resources for designing and implementing trials with longitudinal imputation as the primary analysis; in some cases, custom code must be developed.\nKey challenges center on model assumptions. The chosen parametric form, correlation structure, and variance components must be specified in advance and may not perfectly match collected data. Typically, models are selected after careful consideration of their risks, and model‐checking procedures are implemented routinely in adaptive designs.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html#applications-by-berry-consultants-and-facts",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html#applications-by-berry-consultants-and-facts",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "Applications by Berry Consultants and FACTS",
    "text": "Applications by Berry Consultants and FACTS\nBerry Consultants has championed longitudinal modeling in adaptive trial design. Their FACTS software (Fixed and Adaptive Clinical Trial Simulator) integrates these techniques into a general adaptive‐design platform, enabling easy exploitation of intermediate endpoint data for simulation and analysis.\n\nContinuous Endpoints\n\nTime‑Course Hierarchical Modeling (TCH)\nBayesian hierarchical models estimate the proportion of the final endpoint effect observable at an early time point.\nIntegrated Two‑Parameter (ITP) Models\nCombine patient‑level random effects with longitudinal observations to estimate individual trajectories and population‑level drug effects under a specified parametric shape.\nLinear Regression Imputation\nUses a simple slope‐intercept model to predict final responses from early observations, re‑estimating regression parameters at each interim analysis.\nKernel Density Models\nLink early to final endpoint data via nonparametric estimates of the conditional distribution of the final response.\n\n\n\nDichotomous and Time‑to‑Event Endpoints\n\nBeta‑Binomial or Logistic Regression (Dichotomous)\nFACTS can multiply impute final binary outcomes from early markers using these models.\nSurvival Imputation (Time‑to‑Event)\nFinal event times can be imputed based on pre‑event predictors (continuous, binary, or time‑to‑event), with a range of models available for predictor evolution.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html#conclusion",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html#conclusion",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "Conclusion",
    "text": "Conclusion\nLongitudinal modeling offers substantial efficiency gains in clinical trial design, enabling more informative studies with potentially smaller sample sizes, richer disease‑progression insights, and individualized patient analyses. While these methods demand careful attention to statistical assumptions and computational resources, their benefits strongly advocate for broader adoption in clinical research frameworks.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/longitudinalModels.html#key-references",
    "href": "concepts/adaptiveDesigns/longitudinalModels.html#key-references",
    "title": "Longitudinal Modeling in Clinical Trial Design",
    "section": "Key References",
    "text": "Key References\n\nBerry SM, Carlin BP, Lee JJ, Muller P. Bayesian Adaptive Methods for Clinical Trials. Chapman & Hall/CRC; 2010.\n\nSaville BR, Berry SM. Efficiencies of platform clinical trials: A vision of the future. Clin Trials. 2016;13(3):358–366. doi:10.1177/1740774515626362\n\nBerry DA. Emerging innovations in clinical trial design. Clin Pharmacol Ther. 2016;99(1):82–91. doi:10.1002/cpt.282\n\nQuintana M, Saville BR, Vestrucci M, et al. Design and Statistical Innovations in a Platform Trial for Amyotrophic Lateral Sclerosis. Ann Neurol. 2023;94(3):547–560. doi:10.1002/ana.26714\n\nBerry SM, Petzold EA, Dull P, et al. A response‑adaptive randomization platform trial for efficient evaluation of Ebola virus treatments: A model for pandemic response. Clin Trials. 2016;13(1):22–30. doi:10.1177/1740774515621721\n\nDiggle PJ, Liang KY, Zeger SL. Analysis of Longitudinal Data. Oxford University Press; 2002.\n\nBerry SM, Carlin BP, Lee JJ, Muller P. Bayesian Adaptive Methods for Clinical Trials. CRC Press; 2010.\n\nFACTS Software. Berry Consultants. https://www.berryconsultants.com/software/\n\nIf you have a need for a longitudinal model in your trial design or would like to learn more about its benefits, please contact us.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Longitudinal Modeling in Clinical Trial Design"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html",
    "href": "concepts/facts/LinearRegressionLMPriors.html",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "",
    "text": "Jump to widget\n\n\n\nClick to jump straight to prior specification application.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html#prior-specification-for-the-linear-regression-multiple-imputation-model",
    "href": "concepts/facts/LinearRegressionLMPriors.html#prior-specification-for-the-linear-regression-multiple-imputation-model",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "Prior Specification for the Linear Regression Multiple Imputation Model",
    "text": "Prior Specification for the Linear Regression Multiple Imputation Model\nThe default setting of “Same priors across all model instances and visits,” implies that each parameter \\(\\alpha\\), \\(\\beta\\), and \\(\\lambda\\) have the same prior for all visits \\(t\\). Estimation of the posterior distribution for these parameters is still done independently for each model instance.\n\nSame prior for all visits and model instances\nThe one prior across all model instance are formulated as: \\[\\alpha_t \\sim \\text{N}\\left(\\alpha_\\mu, \\alpha_{\\sigma}^2\\right)\\] \\[\\beta_t \\sim \\text{N}\\left(\\beta_\\mu, \\beta_{\\sigma}^2\\right)\\] \\[\\lambda_t^2 \\sim \\text{IG}\\left(\\frac{\\lambda_n}{2}, \\frac{\\lambda_\\mu^2 \\lambda_n}{2}\\right)\\]\n\n\nSame prior for all model instances, different prior per visit\nSince each visit will likely have a different estimated intercept and slope needed to accurately impute the final endpoint, the above prior formulation may not be desirable if specifying priors that are not extremely diffuse – especially on the \\(\\beta\\) parameters. Instead, selecting “Specify priors per visit across all model instances,” will share the prior specification across all instances of the model, but allows for different priors to be put on the parameters associated with each visit. The user inputted prior parameters are now subscripted with \\(t\\) to denote the visit they correspond to. These priors apply to all model instances:\n\\[\\alpha_t \\sim \\text{N}\\left(\\alpha_{\\mu_t}, \\alpha_{\\sigma_t}^2\\right)\\] \\[\\beta_t \\sim \\text{N}\\left(\\beta_{\\mu_t}, \\beta_{\\sigma_t}^2\\right)\\] \\[\\lambda_t^2 \\sim \\text{IG}\\left(\\frac{\\lambda_{n_t}}{2}, \\frac{\\lambda_{\\mu_t}^2 \\lambda_{n_t}}{2}\\right)\\]\n\n\nDifferent prior for all model instances and visits\nIt is also possible to specify priors “Per model instance and visit,” in which every visit has separate priors, and those differing priors vary across model instances. This is the most flexible prior specification method. The user inputted prior parameters are now subscripted by both t for visit and i for model instance. \\[\\alpha_{ti} \\sim \\text{N}\\left(\\alpha_{\\mu_{ti}}, \\alpha_{\\sigma_{ti}}^2\\right)\\] \\[\\beta_{ti} \\sim \\text{N}\\left(\\beta_{\\mu_{ti}}, \\beta_{\\sigma_{ti}}^2\\right)\\] \\[\\lambda_{ti}^2 \\sim \\text{IG}\\left(\\frac{\\lambda_{n_{ti}}}{2}, \\frac{\\lambda_{\\mu_{ti}}^2 \\lambda_{n_{ti}}}{2}\\right)\\]",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html#sec-priorSpecWidget",
    "href": "concepts/facts/LinearRegressionLMPriors.html#sec-priorSpecWidget",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "Prior Specification Widget",
    "text": "Prior Specification Widget\n\nInterpretation of parameters\n\n\\(\\alpha_{t}\\)\n\nThe expected response on the final endpoint when the early visit \\(t\\) has a response of 0\n\n\\(\\beta_{t}\\)\n\nIf \\(\\alpha=0\\), then \\(\\beta\\) is how many times larger the final endpoint response is than the early endpoint at visit \\(t\\). If \\(\\beta=0\\), then no matter what the early visit response is, the expectation for the final visit is \\(\\alpha\\). If \\(\\beta=1\\), then for any early response the expectation of the final response is the \\(\\text{early response} + \\alpha\\). A \\(\\beta \\lt 1\\) generally implies that the final endpoint is expected to regress towards 0 (when \\(\\alpha=0\\)), and a \\(\\beta \\gt 1\\) implies that the final response is expected to keep growing relative to the early visit response.\n\n\\(\\lambda_{t}\\)\n\nThe standard deviation around the expectation of the final visit response. This dictates how close the imputed final endpoint responses are to the mean response for a subject given \\(\\alpha\\) and \\(\\beta\\). Lower \\(\\lambda_t\\) implies higher correlation between the early visit response and final visit response.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 1000\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(htmltools)\n\nalignCenter &lt;- function(el) {\n  htmltools::tagAppendAttributes(el,\n                                 style=\"margin-left:auto;margin-right:auto;\"\n  )\n}\n\nsketch = htmltools::withTags(table(\n  class = 'display',\n  thead(\n    tr(\n      th(rowspan = 2, ''),\n      th(rowspan = 2, style = \"border-right: solid 1px;\",'Observed Visit Data'),\n      th(colspan = 2, class=\"  dt-center\", style = \"border-right: solid 1px;\",'\\u3B1 priors'),\n      th(colspan = 2, class=\"  dt-center\", style = \"border-right: solid 1px;\",'\\u3B2 priors'),\n      th(colspan = 2, class=\"  dt-center\", style = \"border-right: solid 1px;\",'\\u3BB priors')\n    ),\n    tr(\n      th(\"mean\"),\n      th(style = \"border-right: solid 1px;\", \"SD\"),\n      th(\"mean\"),\n      th(style = \"border-right: solid 1px;\", \"SD\"),\n      th(\"center\"),\n      th(style = \"border-right: solid 1px;\", \"weight\"),\n    )\n  )\n))\n\nui &lt;- fluidPage(\n  tags$head(\n    # Note the wrapping of the string in HTML()\n    tags$style(HTML(\"\n      .my_col_class {\n         align-content: center;\n      }\")\n    )\n  ),\n\n  titlePanel(h1(\"Linear Regression LM Priors\", align = \"center\")),\n  alignCenter(sliderInput(\"numVisits\", \"Number of visits:\",\n                          min = 2, max = 20, value = 5, step = 1)),\n  DTOutput(\"dataInputTable\"),\n  h5(\"Double click on a cell to edit.\", align = \"center\"),\n  br(),\n  titlePanel(h2(\"Plot a subject's prior predictive\", align = \"center\")),\n  fluidRow(\n    #column(5, offset = 1, uiOutput(\"slider\")),\n    column(5, offset = 1, uiOutput(\"slider\")),\n    column(6, fluidRow(\n      column(6, offset = 2, checkboxInput(\"fixAlpha\", \"Fix alpha at its mean?\", value = FALSE, width = \"100%\")),\n      column(6, offset = 2, checkboxInput(\"fixBeta\", \"Fix beta at its mean?\", value = FALSE, width = \"100%\")),\n      column(6, offset = 2, checkboxInput(\"removePredictive\", \"Remove endpoint prior predictive?\", value = FALSE, width = \"100%\"))\n    ))\n  ),\n  fluidRow(\n    column(6, plotOutput(\"visitToFinalPlot\")),\n    column(6, plotOutput(\"priorPredictive\"))\n  )\n\n)\n\ngetLowerMedianUpper = function(earlyVisitVal, alpha = c(0,1), beta = c(0,1), lambda = c(1,1)) {\n  distMeanFinal = c(alpha[1] + beta[1]*earlyVisitVal,\n                    sqrt(alpha[2]^2 + beta[2]^2*earlyVisitVal^2))\n\n  deviates = rnorm(10000)\n  deviates = (deviates - mean(deviates))/(sd(deviates))\n\n  if(any(is.na(lambda))) {\n    samps = (deviates*distMeanFinal[2] + distMeanFinal[1])\n  } else {\n    samps = (deviates*distMeanFinal[2] + distMeanFinal[1]) + rnorm(10000, 0, sd = sqrt(1/rgamma(10000, lambda[2]/2, lambda[1]^2*lambda[2]/2)))\n  }\n\n  distValueFinal = c(mean(samps), sd(samps))\n\n  return(list(\"meanFinal\" = data.frame(lower = distMeanFinal[1] + qnorm(.025)*distMeanFinal[2],\n                                       median = distMeanFinal[1],\n                                       upper = distMeanFinal[1] + qnorm(0.975)*distMeanFinal[2]),\n              \"predictionFinal\" = data.frame(lower = quantile(samps,.025),\n                                             median = median(samps),\n                                             upper = quantile(samps,.975))))\n}\n\n\n\nserver &lt;- function(input, output, session) {\n\n  df = data.frame(VisitResponse = c(2,5,3,7,11),\n                  alphaPriorMean = 0,\n                  alphaPriorSD = 2,\n                  betaPriorMean = 1,\n                  betaPriorSD = 2,\n                  lambdaPriorCenter = 5,\n                  lambdaPriorWeight = 3)\n  df[5,] = c(5, NA, NA, NA, NA, NA, NA)\n  row.names(df) = paste(\"Visit\", 1:5)\n\n  ## Render DF to actually change\n  output$dataInputTable = renderDT(datatable(df,\n                                             options = list(\n                                               pageLength = 20,\n                                               dom = \"t\",\n                                               autoWidth = TRUE,\n                                               columnDefs = list(list(className = 'dt-center', orderable = FALSE, width = '75px', targets = 0:7),\n                                                                 list(width = \"150px\", targets = 0:1))\n                                             ),\n                                             container = sketch,\n                                             rownames = TRUE,\n                                             # escape = FALSE,\n                                             selection = 'none',\n                                             editable = list(target = \"cell\")\n  ) |&gt; formatStyle(c(1,3,5,7), `border-right` = \"solid 1px\") |&gt;\n    formatRound(1, digits = 4) |&gt; formatRound(2:7, digits = 2) |&gt;\n    formatStyle(0,\n                target = \"row\",\n                backgroundColor = styleEqual(paste(\"Visit\",input$lastVisitWithData),\n                                             \"lightblue\",\n                                             'white'))\n  )\n\n  ## Update from Conditional\n  proxy = dataTableProxy('dataInputTable')\n\n  observeEvent(input$dataInputTable_cell_edit, {\n    info = input$dataInputTable_cell_edit\n    i = info$row\n    j = info$col\n    v = info$value\n\n    if(i &lt; nrow(df) | j == 1) {\n      df &lt;&lt;- editData(df, info)\n    } else {\n      df[i,j] &lt;&lt;- NA\n    }\n    replaceData(proxy, df)\n  })\n\n  observe({\n    nv = input$numVisits\n    if(nv &gt; nrow(df)) {\n      tempd = df\n      for(i in 1:(nv-nrow(df))) {\n        tempd = rbind(tempd, setNames(data.frame(c(tempd[nrow(tempd),])), names(tempd)))\n        rownames(tempd)[nrow(tempd)] = paste(\"Visit\", nrow(tempd))\n        tempd[nrow(tempd)-1,-1] = tempd[nrow(tempd)-2,-1]\n      }\n      df &lt;&lt;- tempd\n    } else if(nv &lt; nrow(df)) {\n      df &lt;&lt;- df[1:nv,]\n      df[nv,-1] &lt;&lt;- NA\n    }\n    replaceData(proxy, df)\n  })\n\n  sliderParams &lt;- reactiveValues(max = 5, value = 3)\n  output$slider &lt;- renderUI({\n    sliderInput(\"lastVisitWithData\", \"Last complete visit:\", min = 1, max = sliderParams$max, value = sliderParams$value, step = 1)\n  })\n  observeEvent(input$numVisits, {\n    sliderParams$max = input$numVisits\n    if(!is.null(input$lastVisitWithData)) {\n      sliderParams$value &lt;- min(input$lastVisitWithData, input$numVisits)\n    } else {\n      sliderParams$value = 3\n    }\n  })\n\n  output$priorPredictive = renderPlot({\n    req(input$lastVisitWithData)\n    input$dataInputTable_cell_edit\n\n    lvIndex = input$lastVisitWithData\n    finalVisitIndex = input$numVisits\n    tempDF = df\n    dataToPlot = getLowerMedianUpper(tempDF[lvIndex,1],\n                                     alpha = c(tempDF[lvIndex,2], ifelse(input$fixAlpha, 0, tempDF[lvIndex,3])),\n                                     beta = c(tempDF[lvIndex,4], ifelse(input$fixBeta, 0, tempDF[lvIndex,5])),\n                                     lambda = c(tempDF[lvIndex,6], tempDF[lvIndex,7]))\n\n    tempDF$RowVisitIndex = 1:nrow(tempDF)\n    tempDF$visitKnown = \"included\"\n    tempDF$visitKnown[tempDF$RowVisitIndex &gt; lvIndex] = \"excluded\"\n\n    # tempDF = rbind(setNames(data.frame(c(tempDF[1,])), names(tempDF)), tempDF)\n    # tempDF[1,1] = 0\n    # tempDF$RowVisitIndex[1] = 0\n    # rownames(tempDF)[1] = \"Baseline\"\n\n    p1 = ggplot() +\n      geom_point(dat = tempDF, aes(x = RowVisitIndex, y = VisitResponse, color = visitKnown), size = 3) +\n      scale_color_manual(breaks = c(\"included\", \"excluded\"), values = c(\"black\", \"gray70\"), guide = \"none\") +\n      coord_cartesian(xlim = c(0, finalVisitIndex)) +\n      scale_x_continuous(breaks = 0:finalVisitIndex, labels = c(\"Baseline\", 1:finalVisitIndex)) +\n      xlab(\"Visit\") + ylab(\"Response\") + ggtitle(\"Predicting Final Endpoint of a Subject\") +\n      theme_bw() +\n      theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"left\", legend.position = \"bottom\", legend.direction = \"vertical\")\n\n    if(lvIndex &lt; finalVisitIndex) {\n      if(!input$removePredictive) {\n        p1 = p1 +\n          geom_segment(data = dataToPlot[[2]], aes(x = finalVisitIndex, y = lower, yend = upper), color = \"darkgreen\", linewidth = 2.5) +\n          annotate(geom = \"point\", x = finalVisitIndex, y = dataToPlot[[2]]$median, color = \"darkgreen\", size = 3, shape = 18) +\n          geom_ribbon(data = NULL, aes(x = c(lvIndex, finalVisitIndex),\n                                       ymin = c(tempDF$VisitResponse[lvIndex], dataToPlot[[2]]$lower),\n                                       ymax = c(tempDF$VisitResponse[lvIndex], dataToPlot[[2]]$upper),\n                                       fill = \"preds\"),  color = NA, alpha = .2)\n      }\n      p1 = p1 +\n        geom_segment(data = dataToPlot[[1]], aes(x = finalVisitIndex, y = lower, yend = upper), color = \"darkblue\", linewidth = 1.5) +\n        annotate(geom = \"point\", x = finalVisitIndex, y = dataToPlot[[1]]$median, color = \"darkblue\", size = 3, shape = 18) +\n        annotate(geom = \"segment\", x = lvIndex, xend = finalVisitIndex, y = tempDF$VisitResponse[lvIndex], yend = dataToPlot[[1]]$median, linetype = \"dashed\", color = \"darkblue\")+\n        geom_ribbon(data = NULL, aes(x = c(lvIndex, finalVisitIndex),\n                                     ymin = c(tempDF$VisitResponse[lvIndex], dataToPlot[[1]]$lower),\n                                     ymax = c(tempDF$VisitResponse[lvIndex], dataToPlot[[1]]$upper)),\n                    fill = \"darkblue\",  color = NA, alpha = .4)\n    } else {\n      p1 = p1 + annotate(geom=\"text\", label = \"Final Visit Value Known\",\n                         alpha = .5, size = 10, x = (finalVisitIndex)/2, y = Inf, vjust = 1.3)\n    }\n\n    p1 = p1 + scale_fill_manual(NULL, breaks = c(\"preds\"), limits = c(\"preds\"), values = c(\"darkgreen\"), labels = c(\"Prior predictive distribution for final endpoint of subject.\"))\n    #  guides(fill = guide_legend(override.aes = list(limits = c(\"darkgreen\", \"darkblue\"), labels = c(\"Prior distribution for mean of imputed value.\", \"Prior predictive distribution for final endpoint of subject.\"))))\n\n    p1\n  })\n\n  output$visitToFinalPlot = renderPlot({\n    input$dataInputTable_cell_edit\n    req(input$lastVisitWithData)\n\n    lvIndex = input$lastVisitWithData\n    finalVisitIndex = input$numVisits\n\n    tempDF = df\n\n    min_s = ifelse(min(tempDF$VisitResponse, na.rm = TRUE) &lt; 0, (min(tempDF$VisitResponse, na.rm = TRUE)+1)*1.1, 0)\n    max_s = ifelse(max(tempDF$VisitResponse, na.rm = TRUE) &gt; 0, (max(tempDF$VisitResponse, na.rm = TRUE)+1)*1.1, 0)\n\n    s = seq(min_s-5, max_s+5, length.out = 101)\n\n    meanDist_mean = tempDF$alphaPriorMean[lvIndex] + tempDF$betaPriorMean[lvIndex]*s\n    if(!input$fixAlpha & !input$fixBeta) {\n      meanDist_sd = sqrt(tempDF$alphaPriorSD[lvIndex]^2 + tempDF$betaPriorSD[lvIndex]^2*s^2)\n    } else if(input$fixAlpha & !input$fixBeta) {\n      meanDist_sd = sqrt(tempDF$betaPriorSD[lvIndex]^2*s^2)\n    } else if(!input$fixAlpha & input$fixBeta) {\n      meanDist_sd = rep(sqrt(tempDF$alphaPriorSD[lvIndex]^2), length(s))\n    } else {\n      meanDist_sd = rep(0, length(meanDist_mean))\n    }\n\n    plotDF = data.frame(earlyVis = s,\n                        lower = meanDist_mean + qnorm(0.025)*meanDist_sd,\n                        median= meanDist_mean,\n                        upper = meanDist_mean + qnorm(0.975)*meanDist_sd)\n\n    if(lvIndex &lt; finalVisitIndex) {\n      if(!input$removePredictive) {\n        numSamps = 2500\n\n        deviates = rnorm(numSamps)\n        deviates = (deviates - mean(deviates))/(sd(deviates))\n        normigsamps = rnorm(numSamps, 0, sd = sqrt(1/rgamma(numSamps, tempDF$lambdaPriorWeight[lvIndex]/2, tempDF$lambdaPriorCenter[lvIndex]^2*tempDF$lambdaPriorWeight[lvIndex]/2)))\n\n        distVals = matrix(NA, ncol = 2, nrow = length(s))\n        for(i in 1:length(s)) {\n          distVals[i,] = quantile((deviates*meanDist_sd[i] + meanDist_mean[i] + normigsamps), c(0.025, 0.975))\n        }\n\n        plotDF$lowerPred = distVals[,1]\n        plotDF$upperPred = distVals[,2]\n      }\n\n      p2 = ggplot(data = plotDF) + geom_abline(aes(slope = tempDF$betaPriorMean[lvIndex], intercept = tempDF$alphaPriorMean[lvIndex]), color = \"darkblue\", linewidth = 1.5) +\n        geom_ribbon(aes(x = s, ymin = lower, ymax = upper, fill = \"means\"), color = NA, alpha = 0.4)\n\n      if(!input$removePredictive) {\n        p2 = p2 + geom_ribbon(aes(x = s, ymin = lowerPred, ymax = upperPred), fill = \"darkgreen\", color = NA, alpha = 0.2)\n      }\n      p2 = p2 +\n        coord_cartesian(xlim = c(min_s, max_s)) +\n        xlab(\"Early Visit Known Value\") + ylab(\"Expectation of Final Visit Response\") + ggtitle(\"Expectation of Final Visit Given Early Visit\") +\n        theme_bw() +\n        theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"right\", legend.position = \"bottom\", legend.direction = \"vertical\")\n    } else {\n      p2 = ggplot(data = NULL) + geom_abline(aes(fill = \"means\"), slope = 1, intercept = 0, color = \"darkblue\", linewidth = 1.5) +\n        coord_cartesian(xlim = c(min_s, max_s), ylim = c(min_s, max_s)) +\n        annotate(geom=\"text\", label = \"Final Visit Value Known\",\n                 alpha = .5, size = 10, x = (max_s + min_s)/2, y = Inf, vjust = 1.3) +\n        xlab(\"Early Visit Known Value\") + ylab(\"Expectation of Final Visit Response\") + ggtitle(\"Expectation of Final Visit Given Early Visit\") +\n        theme_bw() +\n        theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"right\", legend.position = \"bottom\", legend.direction = \"vertical\")\n    }\n    p2 = p2 + scale_fill_manual(NULL, breaks = c(\"means\"), limits = c(\"means\"), values = c(\"darkblue\"), labels = c(\"Prior distribution for mean of imputed value.\"))\n    p2\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  }
]