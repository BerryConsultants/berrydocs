[
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html",
    "href": "concepts/facts/DropoutsDeepdive.html",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "For the continuous and dichotomous engines, and the multiple endpoint engine, the default dropout scenario is that no subjects drop out of the study before observing their final endpoint data. If dropouts are expected, the user can specify either the “Dropouts per Dose,” or “Dropouts per Dose per Visit.”\n\n\nIf “Dropouts per Dose” is selected, then each subject has a probability of not having an observable final endpoint value equal to the dropout rate of the dose that subject is randomized to. If each subject has multiple visits and “Dropouts per Dose” is selected, then the conditional probability of dropping out before each visit given that the subject had not dropped out up to the visit before rates are all equal. In other words, if the total dropout rate is π_D, the probability of dropping out between visits i and i+1 given that the subject had not dropped out at visit \\(i\\) is \\[\n1-\\left(1-\\pi_D\\right)^{(\\frac{1}{V})}\\text{ where } V \\text{ is the total number of visits.}\n\\] The following interactive widget takes in the desired dropout rate for a single dose and the number of visits a subject will have during their follow-up, and returns information about the dropout rate that FACTS will simulate by visit for that dose.\nThe columns in the dynamic output table are:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"Dropout Per Dose\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"dropoutRate\", \"Desired Dropout Rate:\", value = 0.1, min = 0.0, max = 1.0, step = 0.05),\n      sliderInput(\"numVisits\", \"Number of visits:\", \n                  min = 1, max = 20, value = 5)\n    ),\n    mainPanel(\n      tableOutput(\"visitsGuide\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  output$visitsGuide &lt;- renderTable({\n    \n    if(is.na(input$dropoutRate)) {stop(safeError(\"Please enter a dropout rate.\"))}\n    else if(input$dropoutRate &lt; 0 | input$dropoutRate &gt; 1) {stop(safeError(\"Dropout rate must be between 0 and 1.\"))}\n    else {\n      df = data.frame(Visits = 1:input$numVisits,\n                                                     ConditionalDropoutRate = 1-(1-input$dropoutRate)^(1/input$numVisits))\n      df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n      df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  \n      df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n      colnames(df) = c(\"Visit number\",\n                \"Conditional Visit Dropout Rate\",\n                \"Marginal Visit Dropout Rate\",\n                \"Cumulative Dropout Prob.\")\n    df}\n  }, digits = 4)\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\nIf “Dropouts per Dose per Visit” is selected, then each subject has a user specified probability of dropping out before a visit v that is specified as the conditional probability of dropping out before visit v given that that they had not dropped out by visit v-1. This leads to a total dropout rate π_D for a participant that is equal to:\n\\[\n\\pi_D = 1-\\Pi_{v=0}^V(1-\\pi_v)\n\\] When specifying Dropouts Per Dose Per Visit in FACTS, the user inputs the conditional visit dropout rate for each visit. This can make it not completely intuitive to get the exact desired cumulative dropout rate. The below application allows the user to enter either the conditional visit dropout rates or the marginal visit dropout rates for a single dose, and provides the implied dropout profile from those inputs. Again, the columns of the table represent:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(htmltools)\n\nalignCenter &lt;- function(el) {\n  htmltools::tagAppendAttributes(el,\n                                 style=\"margin-left:auto;margin-right:auto;\"\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(\n    # Note the wrapping of the string in HTML()\n    tags$style(HTML(\"\n      .my_col_class {\n         align-content: center;\n      }\")\n    )\n  ),\n  \n  fluidRow(\n    titlePanel(h1(\"Dropout Per Dose Per Visit\", align = \"center\")),\n    alignCenter(sliderInput(\"numVisits\", \"Number of visits:\", \n            min = 1, max = 20, value = 5)),\n    DTOutput(\"dataInputTable\")\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  df = data.frame(Visits = 1:5,\n                       ConditionalDropoutRate = 0.05)\n  df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n  df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n  colnames(df) = c(\"Visit number\",\n                   \"Conditional Visit Dropout Rate\",\n                   \"Marginal Visit Dropout Rate\",\n                   \"Cumulative Dropout Prob.\")\n  \n  ## Render DF to actually change\n  output$dataInputTable = renderDT(datatable(df, options = list(\n    pageLength = 20,\n    dom = \"t\",\n    autoWidth = TRUE,\n    columnDefs = list(list(width = '150px', targets = 1:4, orderable = FALSE))), selection = 'none', editable = \"cell\") |&gt; formatRound(2:4, digits = 4))\n  \n  ## Update from Conditional\n  \n  proxy = dataTableProxy('dataInputTable')\n  \n  observeEvent(input$dataInputTable_cell_edit, {\n    info = input$dataInputTable_cell_edit\n    str(info)\n    i = info$row\n    j = info$col\n    v = info$value\n    df &lt;&lt;- editData(df, info)\n    if(j == 2) {\n      ## Update other columns from Conditional\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    } else if(j == 3) {\n      ## Update other columns from Marginal\n      df[,4] &lt;&lt;- cumsum(df[,3])\n      df[,2] &lt;&lt;- c(df[1,3],\n                   df[-1,3]/(1-df[-nrow(df),4]))\n    } else if(j == 4) {\n      ## Update other columns from Cumulative\n      \n      df[info$row, 3] &lt;&lt;- df[info$row, 4] - ifelse(info$row == 1, 0, df[info$row-1, 4])\n      df[info$row, 2] &lt;&lt;- ifelse(info$row == 1, df[info$row,3], df[info$row,3]/(1-df[info$row-1,4]))\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    }\n    replaceData(proxy, df) \n  })\n  \n  observe({\n    input$numVisits\n    nv = input$numVisits\n    if(nv &gt; nrow(df)) {\n      if(nrow(df) == 0) {\n        df &lt;&lt;- data.frame(Visits = 1,\n                   ConditionalDropoutRate = 1-(1-.1)^(1))\n        df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n        df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n        df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n        colnames(df) = c(\"Visit number\",\n                         \"Conditional Visit Dropout Rate\",\n                         \"Marginal Visit Dropout Rate\",\n                         \"Cumulative Dropout Prob.\")\n      } else {\n        for(i in 1:(nv-nrow(df))) {\n          df &lt;&lt;- rbind(df, setNames(data.frame(matrix(c(1+nrow(df), 0.05, 0, 0), nrow = 1)), names(df)))\n        }\n        df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n        df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n      }\n    } else if(nv &lt; nrow(df)) {\n      df &lt;&lt;- df[1:nv,]\n    }\n    replaceData(proxy, df) \n  })\n}\n\nshinyApp(ui = ui, server = server)\nIn order to simulate dropouts from the below profile in FACTS, enter the Conditional Visit Dropout Rate into the FACTS Dropout Rate tab.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose",
    "href": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "If “Dropouts per Dose” is selected, then each subject has a probability of not having an observable final endpoint value equal to the dropout rate of the dose that subject is randomized to. If each subject has multiple visits and “Dropouts per Dose” is selected, then the conditional probability of dropping out before each visit given that the subject had not dropped out up to the visit before rates are all equal. In other words, if the total dropout rate is π_D, the probability of dropping out between visits i and i+1 given that the subject had not dropped out at visit \\(i\\) is \\[\n1-\\left(1-\\pi_D\\right)^{(\\frac{1}{V})}\\text{ where } V \\text{ is the total number of visits.}\n\\] The following interactive widget takes in the desired dropout rate for a single dose and the number of visits a subject will have during their follow-up, and returns information about the dropout rate that FACTS will simulate by visit for that dose.\nThe columns in the dynamic output table are:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"Dropout Per Dose\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"dropoutRate\", \"Desired Dropout Rate:\", value = 0.1, min = 0.0, max = 1.0, step = 0.05),\n      sliderInput(\"numVisits\", \"Number of visits:\", \n                  min = 1, max = 20, value = 5)\n    ),\n    mainPanel(\n      tableOutput(\"visitsGuide\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  output$visitsGuide &lt;- renderTable({\n    \n    if(is.na(input$dropoutRate)) {stop(safeError(\"Please enter a dropout rate.\"))}\n    else if(input$dropoutRate &lt; 0 | input$dropoutRate &gt; 1) {stop(safeError(\"Dropout rate must be between 0 and 1.\"))}\n    else {\n      df = data.frame(Visits = 1:input$numVisits,\n                                                     ConditionalDropoutRate = 1-(1-input$dropoutRate)^(1/input$numVisits))\n      df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n      df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  \n      df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n      colnames(df) = c(\"Visit number\",\n                \"Conditional Visit Dropout Rate\",\n                \"Marginal Visit Dropout Rate\",\n                \"Cumulative Dropout Prob.\")\n    df}\n  }, digits = 4)\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose-per-visit",
    "href": "concepts/facts/DropoutsDeepdive.html#dropouts-per-dose-per-visit",
    "title": "Dropout Rate Explanation",
    "section": "",
    "text": "If “Dropouts per Dose per Visit” is selected, then each subject has a user specified probability of dropping out before a visit v that is specified as the conditional probability of dropping out before visit v given that that they had not dropped out by visit v-1. This leads to a total dropout rate π_D for a participant that is equal to:\n\\[\n\\pi_D = 1-\\Pi_{v=0}^V(1-\\pi_v)\n\\] When specifying Dropouts Per Dose Per Visit in FACTS, the user inputs the conditional visit dropout rate for each visit. This can make it not completely intuitive to get the exact desired cumulative dropout rate. The below application allows the user to enter either the conditional visit dropout rates or the marginal visit dropout rates for a single dose, and provides the implied dropout profile from those inputs. Again, the columns of the table represent:\n\nVisit number\n\nThe index of the visit that a subject would observe.\n\nConditional Visit Dropout Rate\n\nThe probability that a subject drops out before row specified visit given that they have not dropped out after the previous visit.\n\nMarginal Visit Dropout Rate\n\nThe probability that a subject that has not yet been enrolled in the study will dropout before row specified visit and after the previous row’s visit.\n\nCumulative Dropout Prob.\n\nThe probability that a subject will drop out at the row specified visit or before. At the last visit row this value will equal the Desired Dropout Rate.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(htmltools)\n\nalignCenter &lt;- function(el) {\n  htmltools::tagAppendAttributes(el,\n                                 style=\"margin-left:auto;margin-right:auto;\"\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(\n    # Note the wrapping of the string in HTML()\n    tags$style(HTML(\"\n      .my_col_class {\n         align-content: center;\n      }\")\n    )\n  ),\n  \n  fluidRow(\n    titlePanel(h1(\"Dropout Per Dose Per Visit\", align = \"center\")),\n    alignCenter(sliderInput(\"numVisits\", \"Number of visits:\", \n            min = 1, max = 20, value = 5)),\n    DTOutput(\"dataInputTable\")\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  df = data.frame(Visits = 1:5,\n                       ConditionalDropoutRate = 0.05)\n  df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n  df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n  df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n  colnames(df) = c(\"Visit number\",\n                   \"Conditional Visit Dropout Rate\",\n                   \"Marginal Visit Dropout Rate\",\n                   \"Cumulative Dropout Prob.\")\n  \n  ## Render DF to actually change\n  output$dataInputTable = renderDT(datatable(df, options = list(\n    pageLength = 20,\n    dom = \"t\",\n    autoWidth = TRUE,\n    columnDefs = list(list(width = '150px', targets = 1:4, orderable = FALSE))), selection = 'none', editable = \"cell\") |&gt; formatRound(2:4, digits = 4))\n  \n  ## Update from Conditional\n  \n  proxy = dataTableProxy('dataInputTable')\n  \n  observeEvent(input$dataInputTable_cell_edit, {\n    info = input$dataInputTable_cell_edit\n    str(info)\n    i = info$row\n    j = info$col\n    v = info$value\n    df &lt;&lt;- editData(df, info)\n    if(j == 2) {\n      ## Update other columns from Conditional\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    } else if(j == 3) {\n      ## Update other columns from Marginal\n      df[,4] &lt;&lt;- cumsum(df[,3])\n      df[,2] &lt;&lt;- c(df[1,3],\n                   df[-1,3]/(1-df[-nrow(df),4]))\n    } else if(j == 4) {\n      ## Update other columns from Cumulative\n      \n      df[info$row, 3] &lt;&lt;- df[info$row, 4] - ifelse(info$row == 1, 0, df[info$row-1, 4])\n      df[info$row, 2] &lt;&lt;- ifelse(info$row == 1, df[info$row,3], df[info$row,3]/(1-df[info$row-1,4]))\n      df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n      df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n    }\n    replaceData(proxy, df) \n  })\n  \n  observe({\n    input$numVisits\n    nv = input$numVisits\n    if(nv &gt; nrow(df)) {\n      if(nrow(df) == 0) {\n        df &lt;&lt;- data.frame(Visits = 1,\n                   ConditionalDropoutRate = 1-(1-.1)^(1))\n        df$TotalDropoutRate = 1-cumprod(1-df$ConditionalDropoutRate)\n        df$MarginalDropoutRate = c(df$TotalDropoutRate[1], df$TotalDropoutRate[-1] - df$TotalDropoutRate[-nrow(df)])\n        df = df[,c(\"Visits\", \"ConditionalDropoutRate\", \"MarginalDropoutRate\", \"TotalDropoutRate\")]\n        colnames(df) = c(\"Visit number\",\n                         \"Conditional Visit Dropout Rate\",\n                         \"Marginal Visit Dropout Rate\",\n                         \"Cumulative Dropout Prob.\")\n      } else {\n        for(i in 1:(nv-nrow(df))) {\n          df &lt;&lt;- rbind(df, setNames(data.frame(matrix(c(1+nrow(df), 0.05, 0, 0), nrow = 1)), names(df)))\n        }\n        df[,4] &lt;&lt;- 1-cumprod(1-df[,2])\n        df[,3] &lt;&lt;- c(df[1,4], df[-1,4] - df[-nrow(df),4])\n      }\n    } else if(nv &lt; nrow(df)) {\n      df &lt;&lt;- df[1:nv,]\n    }\n    replaceData(proxy, df) \n  })\n}\n\nshinyApp(ui = ui, server = server)\nIn order to simulate dropouts from the below profile in FACTS, enter the Conditional Visit Dropout Rate into the FACTS Dropout Rate tab.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Dropout Rate Explanation"
    ]
  },
  {
    "objectID": "concepts/facts/InverseGammaDistribution.html",
    "href": "concepts/facts/InverseGammaDistribution.html",
    "title": "Inverse Gamma Distribution in FACTS",
    "section": "",
    "text": "The Inverse Gamma distribution is used as a prior for most variances in FACTS. The standard parameterization of the Inverse Gamma distribution using \\(\\alpha\\) and \\(\\beta\\) as the shape and scale parameter is not always intuitive for specifying a prior. In order to assist with prior specification, FACTS reparameterizes the Inverse Gamma distribution to be a function of the expected center of the standard deviation and a prior weight.\nThe application below is intended to help users of FACTS understand what the distribution they are specifying for the prior of a variance actually looks like.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\n\nqinvgamma = function (p, shape, rate = 1, scale = 1/rate, lower.tail = TRUE, \n                      log.p = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  qgamma(1 - p, shape, rate, lower.tail = lower.tail, log.p = log.p)^(-1)\n}\npinvgamma = function (q, shape, rate = 1, scale = 1/rate, lower.tail = TRUE, \n                      log.p = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  pgamma(1/q, shape, rate, lower.tail = !lower.tail, log.p = log.p)\n}\ndinvgamma = function (x, shape, rate = 1, scale = 1/rate, log = FALSE) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  log_f &lt;- dgamma(1/x, shape, rate, log = TRUE) - 2 * log(x)\n  if (log) \n    return(log_f)\n  exp(log_f)\n}\nrinvgamma = function (n, shape, rate = 1, scale = 1/rate) \n{\n  if (missing(rate) && !missing(scale)) \n    rate &lt;- 1/scale\n  1/rgamma(n, shape, rate)\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  tags$head(\n    tags$style(HTML(\"\n      #radioButtonDiv {\n      display: flex;\n      justify-content: center;\n      }\"\n    ))\n  ),\n  withMathJax(),\n  titlePanel(h1(\"Inverse Gamma Distribution in FACTS\", align = \"center\")),\n  h5('$$\\\\sigma^2 \\\\sim \\\\text{IG}\\\\left(\\\\alpha=\\\\frac{\\\\text{weight}}{2}, \\\\beta=\\\\frac{\\\\text{center}^2\\\\;*\\\\;\\\\text{weight}}{2}\\\\right)$$'),\n  sidebarLayout(\n    sidebarPanel(width = 3,\n                 style = \"border: 1px solid #000000\",\n                 titlePanel(h4(\"Center/Weight Parameterization\")),\n                 fluidRow(\n                   column(width = 10, offset = 1, numericInput(inputId = \"center\", label = \"Center of SD:\", value = 5, min = 0, max = Inf, step = \"any\")),\n                   column(width = 10, offset = 1, numericInput(inputId = \"weight\", label = \"Weight:\", value = 2, min = 0.001, max = Inf, step = \"any\")),\n                 ),\n                 titlePanel(h4(\"Alpha/Beta Parameterization\")),\n                 fluidRow(\n                   column(width = 10, offset = 1, numericInput(inputId = \"alpha\", label = \"Alpha:\", value = 1, min = 0.0005, max = Inf, step = \"any\")),\n                   column(width = 10, offset = 1, numericInput(inputId = \"beta\", label = \"Beta:\", value = 25, min = 0, max = Inf, step = \"any\"))\n                 )\n    ),\n    mainPanel(width = 9,\n              wellPanel(style = \"background: white; border: 1px solid #000000\",\n                        fluidRow(\n                          column(12,\n                                 div(\n                                   radioButtons(\"whichParam\", \n                                                \"Which parameter should be summarized?\", \n                                                choiceNames = c(\"Variance \\\\((\\\\sigma^2)\\\\)\", \"Std. Dev. \\\\((\\\\sigma)\\\\)\"), \n                                                choiceValues = c(\"sigma2\", \"sigma\"), \n                                                selected = \"sigma2\", \n                                                inline = TRUE),\n                                   id = \"radioButtonDiv\")\n                          )\n                        ),\n                        uiOutput(\"sectionTitle\"),\n                        fluidRow(\n                          column(width = 4, value_box(\"Mode\", value= uiOutput(\"mode\"), theme = value_box_theme(bg = \"#0b2545\"))),\n                          column(width = 4, value_box(\"Median\", value= uiOutput(\"median\"), theme = value_box_theme(bg = \"#ba5a31\"))),\n                          column(width = 4, value_box(\"Mean\", value= uiOutput(\"mean\"), theme = value_box_theme(bg = \"#06402b\")))\n                        ),\n                        br(),\n                        plotOutput(\"igDistributionPlot\")\n              )\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output, session) {\n  \n  update &lt;- reactiveVal(TRUE)\n  \n  observeEvent(input$center | input$weight, {\n    cat(\"CenterWeightChanged\\n\")\n    ctr = input$center\n    wgt = input$weight\n    \n    if(update() & !is.null(ctr) & !is.null(wgt) & !is.na(ctr) & !is.na(wgt) & ctr &gt; 0 & wgt &gt; 0) {\n      a = wgt/2\n      b = ctr^2*wgt/2\n      \n      updateNumericInput(session, \"alpha\", value = a)\n      updateNumericInput(session, \"beta\", value = b)\n      \n      update(FALSE)\n    } else {\n      update(TRUE)\n    }\n  })\n  observeEvent(input$alpha | input$beta, {\n    cat(\"AlphaBetaChanged\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(update() & !is.null(a) & !is.null(b) & !is.na(a) & !is.na(b) & a &gt; 0 & b &gt; 0) {\n      wgt = 2*a\n      ctr = sqrt(b/a)\n      \n      updateNumericInput(session, \"center\", value = ctr)\n      updateNumericInput(session, \"weight\", value = wgt)\n      \n      update(FALSE)\n    } else {\n      update(TRUE)\n    }\n  })\n  \n  meanHolder = reactiveVal(NA)\n  \n  output$mean = renderText({\n    cat(\"CalcMean\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(a &gt; 1) {\n      if(input$whichParam == \"sigma2\") {\n        meanHolder(b/(a-1))\n        return(round(b/(a-1), 2))\n      } else {\n        tmp = mean(sqrt(rinvgamma(10000, a, b)))\n        meanHolder(tmp)\n        return(round(tmp, 2))\n      }\n    } else {\n      meanHolder(NA)\n      return(\"-\")\n    }\n  })\n  output$median = renderText({\n    cat(\"CalcMedian\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(input$whichParam == \"sigma2\") {\n      return(round(qinvgamma(0.5, shape = a, rate = b), 2))\n    } else {\n      return(round(sqrt(qinvgamma(0.5, shape = a, rate = b)),2))\n    }\n  })\n  \n  output$mode = renderText({\n    cat(\"CalcMode\\n\")\n    a = input$alpha\n    b = input$beta\n    \n    if(input$whichParam == \"sigma2\") {\n      return(round(b/(a+1),2))\n    } else {\n      lmode = 0\n      hmode = max(b/(a+1), 2)\n      smode = seq(lmode, hmode, length.out = 100001)\n      dmode = dinvgamma(smode, a, b)*(1/(2*sqrt(smode)))\n      calcMode = sqrt(smode[which.max(dmode)])\n      \n      return(round(calcMode, 2))\n    }\n  })\n  \n  output$sectionTitle = renderUI({\n    cat(\"ChangeHeader\\n\")\n    if(input$whichParam == \"sigma2\") {\n      return(h4(\"Characteristics of the Variance\"))\n    } else {\n      return(h4(\"Characteristics of the Standard Deviation\"))\n    }\n  })\n  \n  output$igDistributionPlot = renderPlot({\n    cat(\"MakePlot\\n\")\n    a = input$alpha\n    b = input$beta\n    wchParam = input$whichParam\n    isolate({\n      if(!is.null(a) & !is.null(b) & !is.null(input$center) & !is.null(input$weight) &\n         a &gt; 0 & b &gt; 0 & input$center &gt; 0 & input$weight &gt; 0) {\n        if(wchParam == \"sigma2\") {\n          lowerbound = 0\n          upperbound = qinvgamma(.995, a, b)\n          upperbound = min(1e15, upperbound)\n          \n          sq0to1 = seq(1e-10, 1-(1e-10), length.out = 1000)\n          sq = qinvgamma(sq0to1, a, b)\n          #  sq = seq(1e-10,upperbound, length.out = 1001)\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b))\n          \n          modeDensity = dinvgamma(b/(a+1), a, b)\n          \n          if(df$density[sum(is.finite(df$density))] &gt; modeDensity*.01) {\n            maxPlot = sq[length(sq)]\n          } else {\n            maxPlot = sq[min(which(df$density &lt; modeDensity*.01 & sq &gt; sq[which.max(df$density)]))]\n          }\n          maxPlot = max(maxPlot, 2*(b/(a+1)))\n          \n          sq0to1 = c(10^seq(-17, -2, length.out = 51), seq(.0101, pinvgamma(maxPlot*1.1, a, b), length.out = 1001))\n          sq = qinvgamma(sq0to1, a, b)\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b))\n          \n          p1 = ggplot(data = df) + geom_line(aes(x = sq, y = density)) + geom_area(aes(x = sq, y = density), fill = \"gray80\") +\n            theme_bw() +\n            xlab(\"Variance\") + ylab(\"Density\") +\n            scale_y_continuous(expand = c(0, 0)) +\n            ggtitle(\"Probability Distribution of the Variance\") +\n            coord_cartesian(xlim = c(0, maxPlot), ylim = c(0, modeDensity*1.3)) +\n            theme(text = element_text(size = 18))\n          \n          if(a &gt; 1) {\n            meantmp = meanHolder()\n            p1 = p1 + geom_vline(xintercept = meantmp, color = \"#06402b\") + annotate(geom=\"label\", color = \"white\", fill = \"#06402b\", x = meantmp, y = modeDensity*1.15, label = \"mean\", angle=90, size = 4.5)\n          }\n          p1 = p1 + geom_vline(xintercept = b/(a+1), color = \"#0b2545\") + annotate(geom=\"label\", color = \"white\", fill = \"#0b2545\", x = b/(a+1), y = modeDensity*1.15, label = \"mode\", angle=90, size = 4.5)\n          \n          if(is.finite(qinvgamma(.5, a, b))) {\n            p1 = p1 + geom_vline(xintercept = qinvgamma(.5, a, b), color = \"#ba5a31\") + annotate(geom=\"label\", color = \"white\", fill = \"#ba5a31\", x = qinvgamma(.5, a, b), y = modeDensity*1.15, label = \"median\", angle=90, size = 4.5)\n          }\n          \n          p1 = p1 + geom_vline(xintercept = b/a) + annotate(geom=\"label\", x = b/a, y = modeDensity*1.15, label = \"center\", angle=90, size = 4.5)\n          \n          p1\n        }\n        else {\n          #df = data.frame(sq = sq,\n          #density = dinvgamma(sq, a, b)*(1/(2*sqrt(sq))))\n          \n          lowerbound = 0\n          upperbound = qinvgamma(.995, a, b)\n          upperbound = min(1e15, upperbound)\n          \n          sq0to1 = seq(1e-10, 1-(1e-10), length.out = 1000)\n          sq = qinvgamma(sq0to1, a, b)\n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b)*(2*sqrt(sq)))#(1/(2*sqrt(sq))))\n          \n          lmode = 0\n          hmode = max(b/(a), 2)\n          smode = seq(lmode, hmode, length.out = 1001)\n          dmode = dinvgamma(smode, a, b)*(2*sqrt(smode))#(1/(2*sqrt(smode)))\n          wchMax = which.max(dmode)\n          if(wchMax &gt; 1) {\n            smode2 = seq(smode[wchMax-1], smode[wchMax + 1], length.out = 1001)\n            dmode2 = dinvgamma(smode2, a, b)*(2*sqrt(smode2))#(1/(2*sqrt(smode2)))\n            wchMax = which.max(dmode2)\n          } else {\n            smode2 = seq(0, smode[2], length.out = 1000)\n            dmode2 = dinvgamma(smode2, a, b)*(2*sqrt(smode2))#(1/(2*sqrt(smode2)))\n            wchMax = which.max(dmode2)\n          }\n          calcMode = sqrt(smode2[wchMax])\n          \n          modeDensity = dinvgamma(calcMode^2, a, b)*(2*calcMode)#(1/(2*calcMode))\n          \n          if(df$density[sum(is.finite(df$density))] &gt; modeDensity*.01) {\n            maxPlot = sq[length(sq)]\n          } else {\n            maxPlot = sq[min(which(df$density &lt; modeDensity*.01 & sq &gt; sq[which.max(df$density)]))]\n          }\n          maxPlot = max(maxPlot, 2*smode2[wchMax])\n          \n          sq = (seq(1e-10,sqrt(maxPlot*1.1), length.out = 1001))^2\n          \n          df = data.frame(sq = sq,\n                          density = dinvgamma(sq, a, b)*(2*sqrt(sq)))#(1/(2*sqrt(sq))))\n          \n          p1 = ggplot(data = df) + geom_line(aes(x = sqrt(sq), y = density)) + geom_area(aes(x = sqrt(sq), y = density), fill = \"gray80\") +\n            theme_bw() +\n            xlab(\"Standard Deviation\") + ylab(\"Density\") +\n            scale_y_continuous(expand = c(0, 0)) +\n            ggtitle(\"Probability Distribution of the Standard Deviation\") +\n            coord_cartesian(xlim = c(0, sqrt(maxPlot)), ylim = c(0, modeDensity*1.3)) +\n            theme(text = element_text(size = 18))\n          \n          if(a &gt; 1) {\n            meantmp = meanHolder()\n            p1 = p1 + geom_vline(xintercept = meantmp, color = \"#06402b\") + annotate(geom=\"label\", color = \"white\", fill = \"#06402b\", x = meantmp, y = modeDensity*1.15, label = \"mean\", angle=90, size = 4.5)\n          }\n          \n          p1 = p1 + geom_vline(xintercept = calcMode, color = \"#0b2545\") + annotate(geom=\"label\", color = \"white\", fill = \"#0b2545\", x = calcMode, y = modeDensity*1.15, label = \"mode\", angle=90, size = 4.5)\n          if(is.finite(sqrt(qinvgamma(.5, a, b)))) {\n            p1 = p1 + geom_vline(xintercept = sqrt(qinvgamma(.5, a, b)), color = \"#ba5a31\") + annotate(geom=\"label\", color = \"white\", fill = \"#ba5a31\", x = sqrt(qinvgamma(.5, a, b)), y = modeDensity*1.15, label = \"median\", angle=90, size = 4.5)\n          }\n          p1 = p1 + geom_vline(xintercept = sqrt(b/a)) + annotate(geom=\"label\", x = sqrt(b/a), y = modeDensity*1.15, label = \"center\", angle=90, size = 4.5)\n          \n          p1\n        }\n      }\n    })\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Inverse Gamma Distribution in FACTS"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/platformTrials.html",
    "href": "concepts/adaptiveDesigns/platformTrials.html",
    "title": "Platform Trials",
    "section": "",
    "text": "A traditional trial typically takes two years to design, create the infrastructure, and begin enrolling patients. One therapy and a control are enrolled and then the trial is over, and all the infrastructure built for the trial is obsolete and the trial addressed a single question. A platform trial is built, with a master protocol, to allow multiple therapies to be investigated in the single trial, being added continuously, all with shared infrastructure. A platform trial allows multiple arms to share a common control, share the common infrastructure of the trial, and it allows the better treatment of patients in the trial. The platform trial is a win-win for all stakeholders – smaller sample size, reduced patients on placebo, reduced costs, reduced timelines, and better inferences for regulators.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Platform Trials"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/platformTrials.html#overview",
    "href": "concepts/adaptiveDesigns/platformTrials.html#overview",
    "title": "Platform Trials",
    "section": "",
    "text": "A traditional trial typically takes two years to design, create the infrastructure, and begin enrolling patients. One therapy and a control are enrolled and then the trial is over, and all the infrastructure built for the trial is obsolete and the trial addressed a single question. A platform trial is built, with a master protocol, to allow multiple therapies to be investigated in the single trial, being added continuously, all with shared infrastructure. A platform trial allows multiple arms to share a common control, share the common infrastructure of the trial, and it allows the better treatment of patients in the trial. The platform trial is a win-win for all stakeholders – smaller sample size, reduced patients on placebo, reduced costs, reduced timelines, and better inferences for regulators.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Platform Trials"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/platformTrials.html#our-experience",
    "href": "concepts/adaptiveDesigns/platformTrials.html#our-experience",
    "title": "Platform Trials",
    "section": "Our experience",
    "text": "Our experience\nScientists from Berry Consultants are internationally renowned leaders in the creation, design, and implementation of adaptive platform trials. Platform trials designed by the team at Berry Consultants are either ongoing or being initiated in oncology, infectious diseases, pulmonary and critical care, neurology, and several rare diseases. In addition, the statisticians from Berry Consultants have extensive experience working with clinical investigators and scientific domain experts in both academic and for-profit settings.\nBerry Consultants has been involved in the design and implementation of the following Adaptive Platform Trials:\n\nHealey ALS Platform Trial (ALS)\niSpy-2 (Breast Cancer)\nDIAN-TU (Alzheimer’s Disease)\nIMI-EPAD (Alzheimer’s Disease)\nPrecision Promise (Pancreatic Cancer)\nGBM-AGILE (Glioblastoma Multiforme)\nACTIV-4, REMAP-CAP, PRINCIPLE, DNDi ANTICOV (COVID-19 adaptive platform trials)\nARLG ADAPT (Antibiotics for Drug Resistant Infections)\nPREPARE REMAP-CAP (Community Acquired Pneumonia)\nPREPARE-ALIC4E (Influenza)\nEBOLA (Gates Foundation Funded Platform)\n\nIf you have a need for a platform trial or would like to learn more about its benefits, please contact us.",
    "crumbs": [
      "Concepts",
      "Adaptive Trials",
      "Platform Trials"
    ]
  },
  {
    "objectID": "concepts/bayes/index.html",
    "href": "concepts/bayes/index.html",
    "title": "The Bayesian Approach",
    "section": "",
    "text": "The Bayesian approach provides a mathematically rigorous and principled methodology for making decisions under arbitrarily complex scenarios. It provides a powerful framework for determining optimal behavior in the face of uncertainty. The Bayesian approach is ideal for many adaptive designs because it provides a naturally sequential learning framework, and allows the efficient and transparent integration of complex clinical trial and external data and natural prediction of future events (e.g., clinical trial results).\nIn contrast to traditional methods, the Bayesian approach itself is very flexible. It is naturally sequential, and can create updated distributions based on the information from a trial – and can be done continuously, without constraints that traditional methods pose. Essentially doing complex adaptive trials from a traditional approach is impossible – whereas from a Bayesian perspective is quite natural and straightforward.\nThe flexibility of modeling is also a huge advantage in flexible designs. Bayesian methods provide a flexible, coherent, and transparent method for the creation and evaluation of disparate information. The approach has computational advantages, the ability to model early predictors and biomarkers, the ability to incorporate prediction in to trial design, combining multiple endpoints together, and synthesizing possibly related populations in to combined analyses.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach"
    ]
  },
  {
    "objectID": "concepts/bayes/index.html#trial-design",
    "href": "concepts/bayes/index.html#trial-design",
    "title": "The Bayesian Approach",
    "section": "",
    "text": "The Bayesian approach provides a mathematically rigorous and principled methodology for making decisions under arbitrarily complex scenarios. It provides a powerful framework for determining optimal behavior in the face of uncertainty. The Bayesian approach is ideal for many adaptive designs because it provides a naturally sequential learning framework, and allows the efficient and transparent integration of complex clinical trial and external data and natural prediction of future events (e.g., clinical trial results).\nIn contrast to traditional methods, the Bayesian approach itself is very flexible. It is naturally sequential, and can create updated distributions based on the information from a trial – and can be done continuously, without constraints that traditional methods pose. Essentially doing complex adaptive trials from a traditional approach is impossible – whereas from a Bayesian perspective is quite natural and straightforward.\nThe flexibility of modeling is also a huge advantage in flexible designs. Bayesian methods provide a flexible, coherent, and transparent method for the creation and evaluation of disparate information. The approach has computational advantages, the ability to model early predictors and biomarkers, the ability to incorporate prediction in to trial design, combining multiple endpoints together, and synthesizing possibly related populations in to combined analyses.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach"
    ]
  },
  {
    "objectID": "concepts/bayes/index.html#modeling-and-meta-analysis",
    "href": "concepts/bayes/index.html#modeling-and-meta-analysis",
    "title": "The Bayesian Approach",
    "section": "Modeling and Meta-Analysis",
    "text": "Modeling and Meta-Analysis\nIn addition to the advantages of using the Bayesian approach in trial design, a key strength of the Bayesian approach is hierarchical modeling. The approach is critical for evaluating the sufficiency and reliability of evidence for supporting a treatment guideline or a payers reimbursement decision—is that the degree of evidence integration or borrowing across multiple information sources is not defined a priori but, instead, depends on the consistency of evidence across the information sources. This provides a quantitative and rigorous methodology that allows firm conclusions to be drawn when the evidence is concordant and, just as important, avoids such conclusions, capturing the increased uncertainty when the heterogeneity in the information is large.\nBayesian methods allow for a formal synthesis of information from clinical trials that ask the same or related questions and are uniquely suited to comparative effectiveness research. There are three important aspects to the modeling. The first is the ability to model study-to-study heterogeneity, the second is the ability to handle indirect treatment comparisons, also called mixed treatment comparisons, and the third is the ability to model nested treatments and combinations of treatments.\nDifferences between trials include varying study designs, eligibility criteria, patient populations, treatment regimens, and outcome measures. Bayesian hierarchical models view the trials included in the meta-analysis as sampled from a larger population of trials. A Bayesian hierarchical model is a random effects model that explicitly accounts for trial heterogeneity and allows for “borrowing of strength” across the trials. If results of trials are similar, there will be greater borrowing of information and this will increase the precision of the estimates. If trial results differ substantially, then there will be less borrowing and appropriately increased uncertainty regarding the conclusions of the meta-analysis. The amount of borrowing across trials is not specified in advance, but is determined by the heterogeneity of the available data.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach"
    ]
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to our blog — your resource for timely product news, upcoming features, and expert perspectives.",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "blog/index.html#what-youll-find",
    "href": "blog/index.html#what-youll-find",
    "title": "Blog",
    "section": "What You’ll Find:",
    "text": "What You’ll Find:\n\nNews & Announcements: Key releases, product enhancements, and milestone events.\nExpert Commentary: Short analyses and tips from our team and community leaders.\nUse Cases: Inspiring stories and practical examples straight from the field.\n\nSubscribe via RSS to stay informed and ahead of the curve.",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "blog/posts/2024-10-12.html",
    "href": "blog/posts/2024-10-12.html",
    "title": "Post3",
    "section": "",
    "text": "TEXT TEXT\nTEXT"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Knowledge Hub",
    "section": "",
    "text": "Welcome to the Fixed and Adaptive Clinical Trial Simulator (FACTS) Knowledge Hub!\nFACTS is a powerful software tool that is trusted by half of the top 20 pharma companies to help them design efficient and successful clinical trials.\nWe are proud that also numerous academic, government and regulatory institutions trust FACTS."
  },
  {
    "objectID": "versions/index.html",
    "href": "versions/index.html",
    "title": "Versions",
    "section": "",
    "text": "Welcome to our section on FACTS Versions — your go-to source for detailed information on what’s new in each version of FACTS and a roadmap about what’s to come.",
    "crumbs": [
      "Versions"
    ]
  },
  {
    "objectID": "versions/index.html#what-youll-find-here",
    "href": "versions/index.html#what-youll-find-here",
    "title": "Versions",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nNew Features: Understand the core functionalities, enhancements, and integrations introduced with each update and planned for the future.\nImprovements & Optimizations: Learn about refinements in performance, stability, and efficiency that help ensure you’re always getting the best experience.\nFixes & Patches: Stay informed about resolved bugs, security patches, and other maintenance updates that keep our product reliable and secure.\nVersion & Update History: Trace the growth of our product through a chronological record of releases, so you’ll know exactly when key changes were introduced.",
    "crumbs": [
      "Versions"
    ]
  },
  {
    "objectID": "versions/index.html#how-to-use-these-notes",
    "href": "versions/index.html#how-to-use-these-notes",
    "title": "Versions",
    "section": "How to Use These Notes",
    "text": "How to Use These Notes\nExplore the roadmap of upcoming features, the latest release notes or browse previous versions to see how our product has evolved over time by selecting the major version (FACTS 6, 7 …) and then the release of interest from the sidebar. With every iteration, we’re committed to delivering meaningful enhancements that align with your needs. Check back often to stay current on all the ways we’re working to make our solution faster, more powerful, and easier to use.",
    "crumbs": [
      "Versions"
    ]
  },
  {
    "objectID": "versions/v7/facts711.html",
    "href": "versions/v7/facts711.html",
    "title": "FACTS 7.1.1 Release Notes",
    "section": "",
    "text": "Introduction\n\n\nFACTS Core and Staged Improvements\n\nIn Staged designs, conditional power of current stage 2 when no control arm is carried to stage 2 will now be handled correctly.\nIn Core and Staged designs, when recruiting subjects deterministically, specifying a deterministic accrual profile without an associated file and saving the associated design will no longer cause FACTS to close unexpectedly.\nIn Core and Staged designs, the Virtual Subject longitudinal responses will handle the fraction of final SD per visit correctly when visits are deleted.",
    "crumbs": [
      "Versions",
      "FACTS 7 Release Notes",
      "FACTS 7.1.1 Release Notes"
    ]
  },
  {
    "objectID": "versions/v7/facts700.html",
    "href": "versions/v7/facts700.html",
    "title": "FACTS 7.0.0 Release Notes",
    "section": "",
    "text": "Introduction\nFACTS 7.0.0 is now available for download via App Center. This release marks the addition of two new FACTS design types: Platform Trial Design – Continuous and Platform Trial Design – Dichotomous.\nPlease contact us regarding any questions.\n\n\nFACTS Platform Trial Features\nWithin these new Platform Trial design types, FACTS users can now:\n\nSimulate a platform trial, for a continuous/dichotomous endpoint, with various trial level participant and arm constraints. In particular, users can specify a maximum enrollment time, number of participants, successful treatments, participants per arm and concurrent treatments.\nSimulate a platform trial with treatments arriving at different times during the trial.\nSpecify simulated mean arm responses/effects to be a fixed value or sampled from a distribution.\nSimulate participant accrual, responses, and dropout rates as per FACTS Core.\nSpecify a constant proportion of participants allocated to the control arm, or an allocation dependent on the number of treatments currently in the trial, with the option of performing response adaptive randomization.\nAnalyze participant data and estimate mean treatment responses using a Bayesian independent arm model, or frequentist p-values, comparing treatment arms to a common control arm.\nSpecify “Trial Update” information and frequency, at which analyses are performed and allocation ratios may get updated.\nSpecify when to evaluate “Treatment Milestones”, at which decisions are made about treatment outcomes.\nSpecify success/futility criteria that apply to all treatments, or to specific treatments.\nClassify treatments as Good, Mediocre or Unacceptable to get summary statistics such as the proportion of ‘Good’ treatments that are successful/inconclusive/unsuccessful and similarly for the other classifications.\nView granular simulation and summary results of various Platform Trial operating characteristics.\nGenerate a Platform Trial design report outlining the characteristics of the simulated design in a Word document.\n\n\n\nFACTS Core and Staged Improvements\n\nIn FACTS Core and Staged designs (except Time-to-Event designs), users can now simulate single arm trials, with options for both Bayesian and Frequentist p-values to be calculated comparing the data on the experimental arm to an objective reference response/response rate specified on the QOI tab.\nFACTS Core and Staged designs (except Time-to-Event designs) will now correctly handle frequentist calculations when a control arm is not present and comparison is performed against an objective reference response/response rate.\np-value calculations have been updated to better accommodate their use at interims, with dropouts and incomplete subjects now handled differently. No incomplete subjects have a final endpoint imputed, but subjects that are known dropouts and have had the opportunity to complete are imputed/ignored according to the “Handle missingness” option for the p-value.\nLOCF behavior has been made consistent. LOCF will impute a participant’s baseline value as their final outcome if a baseline value is observed and no non-baseline visit data is observed.\nFACTS Staged designs will now correctly handle the mirroring of Stage 1 data in Stage 2 for the Dose Response and Longitudinal models.\n\n\n\nFACTS Enrichment Design Improvements\n\nIn FACTS Enrichment Time-to-Event designs, FACTS will now correctly apply the user specified alpha levels per group when calculating frequentist output summaries.\nIn FACTS Enrichment Time-to-Event designs, FACTS will now correctly calculate frequentist output when the underlying design has only specified one group.\nIn FACTS Enrichment designs, FACTS will now enforce group caps to be strictly greater than zero.\n\n\n\nFACTS Dose Escalation Improvements\n\nOn the Analysis tab, FACTS will now enforce the specification of the cohort number when uploading a subject data file to run an analysis.\nIn 2D-CRM, FACTS will now correctly handle a rare situation in the row-by-row run-in scheme.\nIn 2D-CRM, the engines when run in a Linux environment will have a correctly formatted simulation results output header.\n\n\n\nGeneral Improvements\n\nBREAKING CHANGE: FACTS will now consistently handle the “Date” column in a patients file to be in weeks rather than days, making it consistent with the rest of FACTS. “Patients” files generated from FACTS simulations will report the “Date” column as “DateInWeeks” to avoid any ambiguity.\nBREAKING CHANGE: The “Date” column in Deterministic Accrual external data files will need to be manually updated to specify the date in weeks rather than days.\nBREAKING CHANGE: The “Date” column in subject data file provided when running a FACTS Analysis will need to be updated to specify the date in weeks rather than days. If performing FACTS Analysis via the GUI, the FACTS Analysis tab provides a “Convert Date from Days to Weeks” utility that does the conversion.\nThe precision of results output in FACTS will now consistently be up to 6 decimal places for all design types, except for Time-to-Event designs which will display output up to 8 decimal places.\nFACTS will now correctly handle interactions with the latest version of RStudio to date (2023.03.0). This includes the generation of design reports and the importing of FACTS results output to RStudio via the “Open in R” button on the Simulation tab. Note that FACTS will continue to support older version of RStudio.",
    "crumbs": [
      "Versions",
      "FACTS 7 Release Notes",
      "FACTS 7.0.0 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts616.html",
    "href": "versions/v6/facts616.html",
    "title": "FACTS 6.1.6 Release Notes",
    "section": "",
    "text": "FACTS 6.1.6 is a maintenance release for FACTS 6.1.0. The first product release of FACTS 6.1.0 was FACTS 6.1.3. Subsequent releases have introduced the following changes:\n\nFACTS 6.1.4: This was FACTS 6.1.3 with some additional logging when using the grid interface.\nFACTS 6.1.5: This was FACTS 6.1.4 with the Dose Escalation N-CRM re-compiled to allow a higher number (40) of dose strengths to be defined when using “Explicit Doses” rather than finely spaced doses.\nFACTS 6.1.6: This was FACTS 6.1.5 with 2 problems fixed in FACTS Core with a TTE endpoint & FACTS Staged Design with a TTE endpoint. In either engine the calculation of a “Current Trial Predictive Probability of Success at Current Enrollment” had 2 problems:\n\nThere was an error in the way timings of future events were simulated in the calculation of the predictive probability. The result was approximately correct, and erred on the conservative side, the error is more manifest if the trial has long follow-up times.\nThere was an error if the design also includes a predictor endpoint. This effects a much smaller set of designs, but the effect was much more marked and its impact was difficult to characterize in general. Our current advice is to not use predictive probability QOIs in combination with a “Predictor” endpoint using FACTS prior to FACTS 6.1.6.\n\n\nUpgrading FACTS 6.1.6 should introduce no changes to the simulation or analysis results relative to FACTS 6.1.3 except in designs using a time-to-event endpoint and a “Predictive Probability of Success in the Current Trial at Current Enrollment” Quantity of Interest.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.1.6 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts650.html",
    "href": "versions/v6/facts650.html",
    "title": "FACTS 6.5.0 Release Notes",
    "section": "",
    "text": "1 Introduction\nFACTS 6.5.0 is now available for download via App Center. Please contact us regarding any questions.\nFACTS users can now:\n\nSpecify frequentist margins (“deltas”) in the calculation of p-value and predictive probability QOIs for FACTS Core and Staged designs (except Time-to-Event designs).\nCreate designs with interims triggered based on predictor events for FACTS Core and Staged Time-to-Event designs with a Time-to-Event predictor.\nCreate designs where the final event endpoint analysis can be performed without any imputation based on the predictor endpoint for FACTS Core and Staged Time-to-Event designs with a predictor endpoint.\nObserve significant improvements in the mixing of MCMC chains within the Bayesian Augmented Control and Active Comparator (BAC and BAAC respectively) hierarchical models for FACTS Core and Staged and Enrichment designs.\nGenerate design reports for FACTS Core Multiple Endpoint designs, FACTS Enrichment designs (Continuous, Dichotomous and Time-to-Event) and FACTS N-CRM designs.\n\n\n\n2 FACTS Core and Staged Improvements\n\nIn FACTS Core and Staged designs (except Time-to-Event designs), FACTS users can now globally specify a frequentist super-superiority/non-inferiority margin on the Quantities of Interest tab under the Standard Evaluation Variables area, which will be applied to the calculation of all p-value QOIs and “Current Trial” Predictive Probability QOIs. Note that this globally defined margin does not apply to “Future Trial” Predictive Probability QOIs, which can have their own separate margin defined. In addition, users now have the option on the “Frequentist Analysis” tab to use the frequentist super-superiority/non-inferiority margin in the frequentist analysis.\nIn FACTS Core and Staged Time-to-Event designs with a Time-to-Event predictor, users can now specify designs with interims triggered based on the number of predictor events that have been observed. In addition, and independently of how interims are triggered, users can now specify maximum event caps based on either Final events or Predictor events.\nIn FACTS Core and Staged Time-to-Event designs with a predictor endpoint, users can now specify the final endpoint analysis to not depend on any imputation from the predictor endpoint. This can be achieved by selecting the “No imputation” option within the “Imputation on Predictor” panel on the Design &gt; Predictor Model &gt; Relationship to Endpoint tab.\nIn FACTS Core and Staged designs, the mixing of MCMC chains within Bayesian Augmented Control and Active Comparator (BAC and BAAC respectively) hierarchical models has been improved significantly; in particular, when the prior on tau^2 in the hierarchical model is forced to be small.\nFACTS Core Multiple Endpoint now provides the ability to generate a design report once the design has been simulated. As a result, all FACTS Core design types can now generate design reports.\nFACTS Core and Staged designs now correctly display a trial as having stopped for futility if all arms have been dropped.\nFACTS Core and Staged designs now correctly prevent interims from being performed beyond full enrollment when the “Discontinue interim analysis beyond full enrolment” setting on the Interims tab is selected.\nFACTS Staged Time-to-Event designs now correctly handle interim timings in Stage 2 for the various data inclusion rules as specified on the Data Inclusion tab, and interim information based on “Just Stage 2 data” or “Stage 2 and included Stage 1 data”, as specified on the Stage 2 Interims tab.\nFACTS Staged Time-to-Event designs now correctly handles interim timings based on complete predictor data, when a predictor is included in the design and the “Primary endpoint is censoring for intermediate predictor” setting is selected.\nFACTS Core and Staged Time-to-Event designs now correctly handle predictor based imputation when using a dichotomous predictor endpoint.\nOn the Analysis tab in FACTS Core and Staged Time-to-Event designs, current trial predictive probabilities that estimate an accrual rate no longer require input data to be sorted by accrual time.\nFACTS Core Multiple Endpoint and FACTS Staged Dichotomous designs will now correctly output p-value trend test QOIs as a single output column, rather than one output column per dose, in summary files.\nFACTS Staged Multiple Endpoint designs will now correctly display the endpoint number when outputting p-value trend test QOIs in summary files.\nFACTS (Staged) Multiple Endpoint designs will now correctly display the posterior probability QOI comparison options (“Rates” and “Log-odds”) for dichotomous endpoints. Changing the endpoint from being dichotomous to continuous will delete posterior probability QOIs using the “Log-odds” comparison.\nFACTS Core and Staged Multiple Endpoint analyses will now correctly handle endpoints with visits schedules. Namely, when an endpoint contains only one visit schedule or when an endpoint’s visit schedule involved non-consecutive visits.\nFACTS Core and Staged Multiple Endpoint analyses will now correctly handle endpoints whose visit schedule contains missing data.\n\n\n\n3 FACTS Enrichment Design Improvements\n\nFACTS Enrichment designs (Continuous, Dichotomous and Time-to-Event) now provide the ability to generate design reports once the designs have been simulated. As a result, all FACTS Enrichment design types can now generate design reports.\nThe mixing of MCMC chains within Bayesian Augmented Control (BAC) hierarchical model has been improved significantly; in particular, when the prior on tau^2 in the hierarchical model is forced to be small.\nThe clustering in Enrichment designs has been improved for situations when the prior on tau^2 is chosen to be small (e.g. 0.01 with weight 1).\nThe patients file output from simulations (patients.csv) now correctly populates the dropout state of patients, and can now be used as subject data input on the Analysis tab without requiring modification.\nMCMC Trace plots are now available for all Enrichment design types when viewing simulation results graphs and when performing analyses. To view these graphs, at least one MCMC file needs to be generated. This can be done by going to the Simulations tab &gt; MCMC Settings.\nExternal data file validation has been improved.\n\n\n\n4 FACTS Dose Escalation Improvements\n\nN-CRM now provides the ability to generate design reports once the design has been simulated.\nIn N-CRM designs which include efficacy, the “Maximum cohorts used to determine MTD” setting on the Allocation Rule tab is now observed correctly.\nIn N-CRM designs, when deriving toxicity/efficacy priors from quantiles at the lowest and highest doses, the “middle” probability option now refers to the median dose rather than the reference dose and is now optional. When pre-6.5 FACTS files are loaded in FACTS 6.5, the “middle” probability option will be ignored and replaced by 0.5.\nIn N-CRM designs, the specification of at least two dose levels is now required when deriving toxicity/efficacy priors from specific quantiles. Previously, the specification of at least three dose levels was required.\n\nFACTS Command Line mode and FLFLL (FACTS Linux File Loader Lite) now correctly handle the processing of N-CRM designs whose prior was derived using the “Legacy Prior” option.\nIn N-CRM designs using open enrollment without accrual pausing, FACTS will now output entries in the cohort file for subjects that have been lost following stopping rules being met.\nIn N-CRM designs using open enrollment, FACTS will no longer prematurely apply stopping conditions when there are still subjects whose outcomes have been observed, but not yet processed.\nIn N-CRM designs using open enrollment, FACTS will now report the selected MTD/MED/OSD and associated state to be the dose where the stopping conditions were first met, unless the MTD has subsequently decreased post stopping rules being met.\nIn N-CRM designs using open enrollment, dose escalation rules when using fine-grained dosing will behave correctly.\nIn N-CRM designs using open enrollment and two groups, stopping rules and dose escalations rules will now apply to the correct group.\n\n\n\n5 General Improvements\n\nFACTS now targets .NET Framework 4.8, the latest major version of the .NET Framework.\nA new “Simulation Duration” table can be viewed when right-clicking on a simulation design scenario. The Simulation Duration table gives a granular view of simulation start and end times, as well as its total duration.\nSeveral major improvements to FLFLL (enterprise licensees only): in particular, the ability to process specific scenarios of a design, the ability to process all FACTS files contained within a specified directory, and the reporting of design scenario validation errors. See FLFLL documentation for details.\nIn FACTS Command Line mode and FLFLL, a new flag is available to specify the number of MCMC samples to generate for imputation purposes.\nIn FACTS Command Line mode, the ability to generate a design report has been added. This can be achieved by adding the -report flag and the -rpath flag, where the latter is used to specify the path to the R executable.\nFACTS now provides links to FACTS introductory videos hosted on YouTube via the Help menu.\nSimulation engine errors in FACTS are now displayed in the GUI more informatively.\nThe remaining time left on a FACTS license is now displayed correctly on the FACTS splash screen and Help menu.\nFACTS can now output up to 99,999 patients/weeks/frequentist/MCMC files. Previously, this was capped at 9,999 files.\nAll designs supporting design report generation can have their design report generated without having to perform an additional command execution step in RStudio, by selecting a valid R installation under Settings &gt; Options &gt; R Configuration.\nFACTS will now correctly handle the serialization/deserialization of text inputs involving the following characters: “&gt;”, “&lt;”, “&”, “ ’ ” and “\\”.\nWhen viewing FACTS simulation results through the GUI, estimates of responses, effects and hazard ratios (for Time-to-Event designs) will be display more obviously in the results column headers.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.5.0 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts640.html",
    "href": "versions/v6/facts640.html",
    "title": "FACTS 6.4.0 Release Notes",
    "section": "",
    "text": "FACTS 6.4.0 is now available for official release. Please contact us regarding any questions.\nThe key features of this release are:\n\nThree new dose response models in FACTS (Staged) Core designs.\nAlternative parametrizations to Posterior Probability Quantities of Interest (QOIs) in FACTS (Staged) Core Dichotomous and Time-to-Event designs.\nThe ability to run FACTS from R and to run FACTS in command-line mode on Linux (Enterprise licensees only).\n\nIn detail the new features in FACTS 6.4.0 are:\n\nThree new dose response models have been added across all FACTS (Staged) Core designs. These new options will appear in the model selection dropdown on the Dose Response tab. The new models are as follows:\n\nThe Simple Hierarchical model – a model in which the mean responses for each of the arms in the design are drawn from a normal distribution, whose mean and variance are estimated by FACTS. The control arm can be included in the hierarchical model, or modeled separately, in which case it has its own prior mean and variance. The control arm cannot be included in this model for Time-to-Event designs.\nThe Simple Linear model – a linear model which assumes that the mean responses for each of the arms in the design are linear functions of the associated arm strength. In particular, the arm with the largest mean response is guaranteed to be either the largest dose or the smallest arm in this model. Note that the “2-Parameter Logistic” model in FACTS (Staged) Core Dichotomous designs has been replaced by the “Simple Linear model”. FACTS (Staged) Core Dichotomous designs making use of the 2-Parameter Logistic model will be automatically migrated to the Simple Linear model.\nThe Simple Hierarchical Linear model – a model which uses a linear model as a base dose-response structure but allows deviations from linearity in a manner similar to the Hierarchical Logistic dose response model. Given appropriate priors, if the data and prior distributions are consistent with linearity, the hierarchical variance parameter will be estimated to be small and the model fit will be essentially linear, but if the data is non-linear the variance parameter will be large allowing a significantly non-linear model fit.\n\nIn FACTS (Staged) Core Dichotomous and TTE designs, Posterior Probability QOIs with alternative parametrizations can be set when creating a new QOI. This can be achieved by selecting the appropriate option in the “Compare” dropdown of the QOI dialog. The options are as follows:\n\nFor FACTS (Staged) Core Dichotomous designs, Posterior Probability QOIs comparing the log-odds ratio of the response rate for each arm against that of a given arm can now be created. Previously, only the response rates could be compared.\nFor FACTS (Staged) Core TTE designs, Posterior Probability QOIs comparing the hazard rates of the response for each arm against that of a given arm can now be created. Previously, only the hazard ratios (HR) could be compared.\n\nEnterprise FACTS licensees will now be able to access and run FACTS Core and Enrichment Design (ED) analysis models from R via an R wrapper, the output of which is an MCMC file pertaining to the model. This can be used to simulate trials that require posterior quantities that FACTS does not include (e.g., probability that a dose has a treatment effect in a certain range) or simulate trials that make decisions that FACTS does not include (e.g., sample size re-assessment).\nEnterprise FACTS licensees will also now be able to run FACTS in command-line mode on Linux via a separate executable: FACTS Linux File Loader Lite (FLFLL). Mono 6.8.0+ is a pre-requisite for running FLFLL. Executing a valid FACTS design with FLFLL will generate the same results output as its Windows GUI counterpart; in particular, it will generate the simulations, summary, weeks and patients files. FLFLL can be used to automate the simulation of multiple (potentially related) FACTS designs and, more generally, can be used as a key component of a more complex trial design simulation pipeline.\n\nThe following features were also implemented in FACTS 6.4.0:\n\nThe control arm can now be modelled separately in TTE predictor dose response models within FACTS (Staged) Core TTE designs.\nFACTS Core designs will now report the time of the stopping decision of the trial through a new simulations output column named “EarlySuccess Time”.\nFACTS now computes lower and upper frequentist CI bounds, bias and coverage at the simulation level for all design types and summarized them in the associated summary file.\nA command line option for the number of samples per imputation called “samples-per-imputation” has now been added to FACTS when run in command-line mode. This applied to FACTS (Staged) Core and ED designs.\nThe analysis tab now accepts subject files with missing values for intermediate visits (denoted by -9999).\nThe analysis tab in Multiple Endpoint now accepts data files when the design includes visits where none of the endpoints are observed.\nThe “Interim vs Final” Scatter plot graph in the “Across Scenarios” now handles interactive selection of QOI and setting of thresholds, including the use of p-value QOIs.\nThe FACTS installer will now include an option to share basic, anonymous usage and crash data with the FACTS team. This option can also be enabled/disabled by going to Setting &gt; Options &gt; Analytics. Any change in this area will take effect the next time FACTS is loaded. By default, FACTS will NOT collect any usage/crash data. However, we strongly encourage licensees to enable this option to help the FACTS team proactively improve the software in the areas that matter the most. We take our licensee’s data privacy and security very seriously, so do not hesitate to get in touch if you have any questions about this feature.\nFACTS will now, by default, automatically calculate the simulation parallelization packet size based on the number of requested simulations. A manual parallelization packet size can be set instead by setting the “Parallelization packet size” checkbox on the Simulation tab. In FACTS command-line mode, the packet size is automatically set unless the user explicitly specifies the “-p” flag.\nInformation about the FACTS license, namely its expiry date, is now available in Help &gt; About.\n[Enterprise licensees only] FACTS will automatically retry any actions involving communication with the FACTS HPC server if initial communication fails (e.g., due to an intermittent connectivity). The following FACTS infrastructure changes were performed as a part of our roadmap to modernize FACTS to make use of the latest available tech stack. Please communicate the following information to your IT team as needed:\nFACTS 6.4.0 will now target .NET Framework 4.5.2. Previous versions of FACTS target .NET Framework 4. Future versions of FACTS will target the latest available .NET Framework version.\nFACTS 6.4.0 is now a 64-bit application and will be installed on the target machine’s Program Files area. Previous versions of FACTS were 32-bit applications installed on the target machine’s Program Files (x86) area. This release addresses some situations in FACTS 6.3.0 and older versions that could cause different simulation results. If any of your designs replicate these circumstances, you are recommended to upgrade to FACTS 6.4.0:\nThe “Pause accrual and wait for completers if stopping rules are met” option on the Stopping Criteria tab of FACTS Dose Escalation N-CRM designs making use of open enrollment did not have the correct behavior when the option was unchecked. This is fixed in FACTS 6.4.0.\nThe standard deviation (SD) of the number of subjects having observed a Cat 2 Toxicity in FACTS Dose Escalation N-CRM designs was calculated incorrectly. This is fixed in FACTS 6.4.0.\nFACTS Dose Escalation N-CRM designs simulations results differed between Windows and Linux (including Windows VMs running on top of Linux) when the Toxicity response Rho parameter was non-zero. The Linux results are now consistent with the Windows results in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims triggered by complete predictor data were not respecting any specified minimum information required on the number of predictor completers before an interim can be triggered. This is fixed in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims triggered by complete predictor data an addition interim at “full predictor data” was being simulated even if not asked for. This is fixed in FACTS 6.4.0.\nFACTS (Staged) Core TTE designs with interims in stage 2 by time, after full accrual a circumstance can arise when follow up stops prematurely and an “inconclusive” result declared. This is fixed in FACTS 6.4.0.\nFACTS Staged TTE with a predictor endpoint and stage 1 data included in stage 2, any stage 1 subjects who had not had their predictor observed by the end of stage 1 had their predictor outcome censored rather than observed in stage 2. This is fixed in FACTS 6.4.0. Finally, there are two unique situations and areas identified in FACTS 6.4.0 (and prior versions) that will be continued developed and improved in future releases:\nIn FACTS Staged Design TTE, where the data inclusion is: “included where we have neither observed an event or the predictor and they are on an arm that is kept in stage 2” and stage 2 interim timings are based on “complete predictor data” and “stage 2 and included stage 1 data”, then FACTS is failing to include the included stage 1 subjects in calculating the timings of the interims in stage 2.\nIn FACTS Stage Design TTE where events are censoring for predictor outcomes, this censoring is not taken into account in the timing of interims by “Predictor Complete”. Resulting in the interims being too early.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.4.0 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts624.html",
    "href": "versions/v6/facts624.html",
    "title": "FACTS 6.2.4 Release Notes",
    "section": "",
    "text": "Berry Consultants would like to announce a new maintenance release, FACTS 6.2.4. FACTS 6.2.4 contains the following fixes to the FACTS 6.2.3 version:\nUpdating to 6.2.4 is recommended for those of you wishing to use predictive probabilities in FACTS Core TTE, in combination with a TTE predictor endpoint:\n\nIn FACTS Core with a Time-to-Event end point and a Time-to-Event predictor, the imputations of final event times for subjects with a predictor event but no final event during the estimation of “predictive probability of success at full enrolment” could produce an error in the prior version. There are two rare situations in FACTS 6.2.3 that uncover a bug in the dose escalation simulator and causes it to produce an error:\nIn FACTS Dose Escalation, in the N-CRM with only 3 doses the simulator could produce an error during some dose escalation decisions.\nIn FACTS Dose Escalation CRM (Toxicity) could produce an error when simulating 2 samples. The remaining, minor fixes in FACTS 6.2.4 are:\nIn FACTS N-CRM, the GUI was improved to handle the “Variant” options making is easier to change them once they were set.\nA fix to FACTS Dose Escalation 3+3 (!) – improved to handle the circumstance when the starting dose is not the lowest dose, and the dose assignment de-escalates to below the starting dose and validates the next lower dose.\n\nPlus we improved the labeling of a class of prior parameters:\n\nIn the FACTS GUI labels of parameters for prior with an Inverse-Gamma distribution the wording has been changed from “mean value” (which is technically incorrect) to “central value”.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.2.4 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts620.html",
    "href": "versions/v6/facts620.html",
    "title": "FACTS 6.2.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.2.0\nBerry Consultants is delighted to announce that FACTS 6.2.0 is ready for release!\nBuilding on FACTS 6.1.0, FACTS 6.2.0 adds new features to “FACTS N-CRM”, the ability to generate a “Design Report” from FACTS Core designs and extending the ability to compute predictive probabilities to FACTS Core TTE and FACTS Staged TTE.\n\nFACTS N-CRM extensions. FACTS has had versions of the CRM with an efficacy endpoint, ordinal toxicity endpoint and 2 groups since its inception. But these were in separate engines and used the old CRM model for analysis. We have now added all these features as options to the N-CRM so they can be used with the 2 parameter Bayesian Logistic Regression model, targeting toxicity bands and the option to use overdose control. These features cannot only now but used with this better methodology, but can be used in combination with each other, and in combination with the other advanced features that were already included in the FACTS DE N-CRM simulator such as, run ins, stopping rules, escalation rules, fine grain dosing and open enrollment.\n\n\n\n\nNew N-CRM Graph\n\n\n\nFACTS Design Report. In FACTS Core there is now the ability to generate a “Design Report” as a MS Word file that describes the design and simulation results. The file is not intended as the final article but as something where the bulk of the straightforward text (and equations) have been provided and should just require polishing, particularly with the details of the indication and trial setting that FACTS is inevitably unaware of.\n\n\n\n\nNew Design Report\n\n\n\nFACTS 6.2 completes the implementation of predictive probabilities. Predictive probabilities in the current trial with a TTE endpoint are considerably more complex than predictive probabilities in the other endpoints. For the other endpoints the expected about of information after full enrollment and full follow-up is known, for time-to-event it can depend on multiple things such as accrual rate and the expected number of events so a degree of simulation within the simulation is required.\n\nFACTS 6.2.0 is fully backwards compatible with FACTS 6.1.0, 6.0.0 and 5 – it can load and run all your FACTS 6.0.0 and FACTS 5 designs – and then use new FACTS 6.2.0 features with those designs. You can have FACTS 6.2.0 and FACTS 6.1.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Dose Escalation, the N-CRM (also known as Bayesian Logistic Regression) now has options for:\n\nAn ordinal toxicity endpoint\nTo simulate a trial across 2 groups (e.g. Adults and Pediatrics)\nAn additional binary Efficacy endpoint\nThese options can be combined with each other and all the other N-CRM options.\n\nFACTS Core TTE\n\nThe ability to compute the predictive probability of success at the full enrollment of the current trial.\n\nFACTS Staged Design TTE\n\nThe ability to compute the predictive probability of success\n\nin Stage 1 at full enrollment\nof Stage 2 (whilst in Stage 1)\nin Stage 2 at full enrollment (whilst in Stage 2).",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.2.0 Release Notes"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html",
    "href": "introduction/gettingStarted/installation.html",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document describes how to install and verify the installation of the FACTS (Fixed and Adaptive Clinical Trial Simulator) software. It is intended for anyone concerned with the installation of the system.\n\n\n\nThis document covers the installation and installation verification FACTS. It is only intended to demonstrate the usage of the software to the extent that it verifies the software will execute properly in the installed environment.\nFor parameter meanings or detail into the internal workings of the design engines or algorithm check the appropriate System Requirements Document. For appropriate guidance on usage check the User Guides.\n\n\n\nThis document is a guide to the FACTS installation, including both the desktop application and the grid components.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#purpose-of-this-document",
    "href": "introduction/gettingStarted/installation.html#purpose-of-this-document",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document describes how to install and verify the installation of the FACTS (Fixed and Adaptive Clinical Trial Simulator) software. It is intended for anyone concerned with the installation of the system.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#scope-of-this-document",
    "href": "introduction/gettingStarted/installation.html#scope-of-this-document",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document covers the installation and installation verification FACTS. It is only intended to demonstrate the usage of the software to the extent that it verifies the software will execute properly in the installed environment.\nFor parameter meanings or detail into the internal workings of the design engines or algorithm check the appropriate System Requirements Document. For appropriate guidance on usage check the User Guides.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#context-of-this-issue",
    "href": "introduction/gettingStarted/installation.html#context-of-this-issue",
    "title": "FACTS Installation Guide",
    "section": "",
    "text": "This document is a guide to the FACTS installation, including both the desktop application and the grid components.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#desktop-requirements",
    "href": "introduction/gettingStarted/installation.html#desktop-requirements",
    "title": "FACTS Installation Guide",
    "section": "Desktop Requirements",
    "text": "Desktop Requirements\nFACTS can be run on a standard system laptop or desktop running Windows 10 or 11 with the Windows .NET framework v4 or higher installed and at least 1 GB per core or more memory.\nIn addition:\n\nFACTS is expected to run on a display with a resolution of at least 1024x768 pixels and preferably greater.\n\nUser choice of non-default Windows styles/themes may result in unexpected and impractical foreground and background color combinations.\nFACTS 7.0 targets .NET Framework 4.8. Future versions of FACTS will target the latest available .NET Framework version.\nFACTS is now a 64-bit application and will be installed on the target machine’s Program Files area. Previous to FACTS 6.4 versions of FACTS were 32-bit applications installed on the target machine’s Program Files (x86) area.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#computation-requirements",
    "href": "introduction/gettingStarted/installation.html#computation-requirements",
    "title": "FACTS Installation Guide",
    "section": "Computation Requirements",
    "text": "Computation Requirements\nFACTS relies on running simulations and these simulations can be very computationally intensive. When running simulations, each simulation can be run separately (they do not depend on the results of other simulations) though to do so can be somewhat inefficient – repeatedly starting new processes and generating separate output files for every simulation that will need to be gathered together in a single “simulations.csv” file and then summarized. Thus FACTS allows the user to specify a “packet size” and the total number of simulations for each scenario to be simulated is divided by this packet size to create a set of independent jobs.\nIf the simulations are run on the users laptop or PC, FACTS will spawn a simulation thread for every core on the local machine up to the maximum number of simulation ‘packets’ that have been requested. The simulations are run at reduced priority so it is possible to continue to use the machine e.g. for email or Word whilst they run. Thus usually 2 or 4 sets of simulations are run in parallel depending on the processor in the laptop or PC.\nThere are a number of options for speeding up the running of FACTS simulations:\n\nThe simplest technically (and the approach we used to take at Berry Consultants) is to have a large multi-core server (say 32 core) remotely accessible to FACTS users and FACTs installed on it. To use, the user copies the “.facts” files to be simulated to a network shared directory which can be accessed from the server. Then after remotely logging into to the server, the user copies these files to a drive on the server, runs the simulations, zips up the results (within the FACTS GUI there is the FACTS File &gt; Export Project menu command to do this) and copies them back to the network shared drive and thence to their local machine.\nUse the FACTS network share folder “grid” interface, implemented using file transfers to and from a shared network drive. On a machine that can act as a client to a grid of compute nodes managed by one of the standard grid management packages (they used to be called “SunGrid” and “Condor” but have metamorphosed over the years) a “sweeper script” runs that transfers jobs to the grid. The jobs automatically transfer their results back to this shared drive. FACTS copies the job to a unique subfolder on the shared network location and then watches for a change in the lock file name - “submitted”, “running”, “complete” that are managed by the sweeper script. Once the simulations are complete FACTS copies the results back to the local machine. The fact that the simulations have been submitted to the grid are stored in the “.facts” file. Whenever that “.facts” file is open in FACTS, FACTS will poll the remote network drive to check if the simulations are complete.\nA more sophisticated FACTS grid interface that uses a web services to communicate between the FACTS client and a Linux server running a web-server (Apache Tomcat) and database (MySQL). The web service is used to submit jobs and they are stored in the database. A database process then submits them to the grid, once again managed by one of the standard grid management packages. The simulation results are then stored in the database for FACTS to download once complete. This provides a more robust and manageable interface, but it more work to set up. We can provide documentation and scripts and we can assist in setting this up. This is the form of grid that we now use in-house at Berry Consultants.\nTechnically as 3. (but for a fee) Berry Consultants can set and manage the grid for you in the cloud. Please contact us to discuss your requirements and for pricing. Therefore FACTS is able to offload the simulations from the desktop to be run by an external system. The interface describing the interactions with the external system is described in the FACTS Grid Interface document. With a FACTS Enterprise License, the command line executable files to run simulations externally under either Windows or Linux environments are available upon request.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#installation-instructions",
    "href": "introduction/gettingStarted/installation.html#installation-instructions",
    "title": "FACTS Installation Guide",
    "section": "Installation Instructions",
    "text": "Installation Instructions\nThe FACTS Desktop installation package consists of:\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\nsetup.exe\na Windows installation program\n\n\nSetup.msi\nthe FACTS Microsoft Installer file\n\n\nExamples.zip\na Zip file containing example FACTS projects\n\n\nDocuments.zip\na Zip file containing the FACTS documentation.\n\n\nConfig.xml\nan XML file containing the local configuration settings.\n\n\n\nThese files are usually made available for download from Berry Consultants Microsoft App Center site. Download instructions are in a separate document.\nVersions of these files with the standard file extensions (.msi and .zip) modified are available it may have been these versions that were downloaded to circumvent firewall restrictions and these files will need to be renamed prior to use.\nInstallation will take only a few moments.\n\nEnsure that all the files have the correct file extension and are located on a local drive on the machine on which FACTS is to be installed. Windows can treat installs from networked drives as less trustworthy than installs from local drives and this can result in an incomplete installation.\nRight click the setup.exe Windows installation program and select “Run as Administrator”.\nFollow the instructions on the screen to complete the installation.\nDuring FACTS installation you will have to option to enable FACTS to report Analytics back to the App Center. This allows to see how much FACTS is used and which features in FACTS are being used. It does NOT include any user or license information, we can’t see WHO is doing what, only WHAT is being done. Obviously we’d be grateful if you’d enable them.\nAnalytics are off by default they will only be enabled of you enable them.\nOnce installed analytics can be turned on or off from the FACTS “Settings” menu command.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#config.xml",
    "href": "introduction/gettingStarted/installation.html#config.xml",
    "title": "FACTS Installation Guide",
    "section": "Config.xml",
    "text": "Config.xml\nIncluded with the FACTS installation files is a configuration file that can be edited to local settings before the install files are distributed to users. It is also possible to provide an updated copy of the configuration file to users and ask them to update their default configuration, it is also possible for users to locally modify their configuration and revert to the installed configuration details.\nPrior to installation, a configuration file, ‘config.xml’, is available as one of the installation files. This file can be edited to set up a number of default settings for FACTS.\nThe settings are listed between the tags: &lt;configuration&gt; &lt;userSettings&gt; and &lt;/userSettings&gt; &lt;/configurations&gt;, for example:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;\n&lt;configuration&gt;\n  &lt;userSettings&gt;\n    &lt;GridLocation&gt;C:\\\\work\\\\grid&lt;/GridLocation&gt;\n    &lt;GridOpSys&gt;1&lt;/GridOpSys&gt;\n    &lt;GridListenerDelay&gt;10000&lt;/GridListenerDelay&gt;\n    &lt;LocalRVersions&gt;\n      &lt;value&gt;version=\"3.3.2\" path=\"C:\\\\Program Files\\\\R\\\\R-3.3.2\\\\bin\\\\R.exe\" active=\"1\"&lt;/value&gt;\n      &lt;value&gt;version=\"RStudio\" path=\"C:\\\\Program Files\\\\RStudio\\\\bin\\\\RStudio.exe\"&lt;/value&gt;\n    &lt;/LocalRVersions&gt;\n    &lt;FactsSimulationServicePortURL&gt;http://nowhere.com:8080/axis2/services/FactsSimulationServicePort&lt;/FactsSimulationServicePortURL&gt;\n    &lt;GridSimMethod&gt;0&lt;/GridSimMethod&gt;\n  &lt;/userSettings&gt;\n&lt;/configuration&gt;\nSpecifically, the following values may be adjusted, as desired:\n\nLocalRVersions – a list of available R (or RStudio) versions, each one bracketed by the tags  and  and composed of two parameters “version” which can contain any string to be used to identify that version of R and “path” which should contain location of “.exe” that should be run when the user requests R to be run or a Design Report to be generated.\nGridSimMethod – 0 or 1, Determines how FACTS tries to connect to the grid, 0 means the network file share & sweeper script method (option 2 above) is to be used, 1 means that the Web Service method (option 3 or 4 above) is to be used\nIf the network file share method is to be used to connect to the grid then:\n\nGridLocation – the network location of the network file share.\nGridOpSys – 0 or 1, the type of the operating system that is running on the nodes of the grid: 0 – Linux, 1 – Windows (the simulation engine executables have different names in the two environments).\nGridListenerDelay – the delay (in milliseconds) between each poll of the network file share for changes in the state of the simulation results.\n\nIf the Web Service grid access method is to be used to connect to the grid then:\n\nFactsSimulationServicePortURL – specifies the URL to the FACTS web-service endpoint.\n\n\nNote, this configuration file is only used on the initial load of FACTS – subsequently, a local user configuration file is created in a location under the AppData folder – e.g.:\nC:\\Users\\&lt;user_id&gt;\\AppData\\Local\\Berry_Consultants_Inc\\FACTS_File_Loader_Url_&lt;Windows unique file id &gt;\\6.1.6.17435\\user.config\nAny changes made to the configuration from the UI (under the ‘Settings’ menu) are saved to this local file. – and the original config file is only used if the options are reset.\nNB, these local configuration are FACTS version and build specific (note the version and build number in the directory) – which means that if a new install is run, local configuration modifications will be lost.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#notes-on-access-permissions",
    "href": "introduction/gettingStarted/installation.html#notes-on-access-permissions",
    "title": "FACTS Installation Guide",
    "section": "Notes on access permissions",
    "text": "Notes on access permissions\nFACTS uses the locations C:\\Program Data\\BerryConsultants and &lt;user&gt;\\AppData\\Local\\BerryConsultants, we have seen some IT departments set the default access permissions to deny access to these locations contrary to Microsoft’s intention and the access will need to be granted for FACTS to run. When FACTS runs simulations it spawns one or more simulations processes, and we have encountered environments where these processes do not get permission to write to network drives. If these permissions cannot be changed, it will be necessary for users to save their “.facts” file run in a directory on the local drive before running simulations, so the results can be written there and then copied/moved to the network drive once complete.\n\nLicense Installation\nWhen FACTS is first run, it may require the license to be entered. The user can choose the file when prompted, or cut and paste the information into the dialog box. Alternatively, the file can be dropped in the application folder and it will be picked up when needed. Note, depending on access permissions, it may be necessary to initially load FACTS with admin rights in order to load the license key from file.\nSubsequent runs, and subsequent installations of mod level updates, will not require the license to be re-entered.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/installation.html#installation-verification",
    "href": "introduction/gettingStarted/installation.html#installation-verification",
    "title": "FACTS Installation Guide",
    "section": "Installation Verification",
    "text": "Installation Verification\n\n\n\n\n\n\n\n\n\n\nAction\nExpected result\nActual result\nPass\nFail\n\n\nVerify that the FACTS shortcut appears in the Start Menu and on the user’s desktop\nApplication shortcuts located\n\n\n\n\n\nClick the application shortcut to launch FACTS (Enter license key as necessary)\nApplication opens.\n\n\n\n\n\nClick the Help &gt; About menu item\nAbout box appears.\n\n\n\n\n\nVerify the version number is the same as the version specified by Berry Consultants.\nVersion number is correct\n\n\n\n\n\nIf example projects were installed, select File &gt; Examples &gt; [example_file_name] menu item\nExample project opens.\n\n\n\n\n\nIf example projects were not installed select open and navigate to the location where an example file was saved\nExample project opens.\n\n\n\n\n\nClick the File &gt; Save As menu item.\nChoose a writeable location on the local drive and save a copy of the design.\nNew copy of project created.\n\n\n\n\n\nClick the Simulation tab, with “Locally” selected,\nClick “Select All”\nClick “Simulate”\nSimulations start.  After a short while, results are displayed in the GUI.\n\n\n\n\n\nClick the “View Graph” button.\nGraph window opens.  Displayed graph updates when graph title selected in list.\n\n\n\n\n\n\n\n\n\n\n\n\nThe “Design Report” feature in FACTS Core requires that R, R Studio (for mathjax & pandoc), and the R libraries “markdown”, “xtable” and “stringi” are installed. You will also need Microsoft Word installed to be able to open the generated “.docx” file.\n\n\n\n\n\n\nIn FACTS open the configuration settings and configure the location of R or RStudio on the computer. (See the Design Report User Guide for more details if required).\n\n\n\n\n\n\nIf the example file used above was not a “FACTS Core” example, repeat the steps above with a FACTS Core example, up to and including the “Run Simulations” step.\n\n\n\n\n\n\nPress the “Design Report” button. If you have “R” selected as your FACTS default “R”, this will run in batch mode.\nIf you have RStudio selected as our default “R”, RStudio will now start. In the terminal window you are shown the text of a function call. Copy and paste this text into the R command line and execute the function.\nOnce the function is complete you should have a Word file within the “_results” directory that corresponds to the current “.facts” file.\n\n\n\n\n\n\nOpen the World file and review the contents, it should describe the example you have selected and simulated.\n\n\n\n\n\n\nClick the File &gt; Exit menu item\nApplication closed.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "FACTS Installation Guide"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html",
    "href": "introduction/gettingStarted/downloadInstructions.html",
    "title": "Download Instructions",
    "section": "",
    "text": "This document provides download instructions for all new releases of Berry software; including, the FACTS desktop app installer, FACTS engine executable, ADDPLAN Classic, ADDPLAN Neo and QUOTES. This assumes that you currently have a software license agreement with Berry Consultants, or that you have already gotten in touch with Berry Consultants for a software evaluation. If this is not the case, please get in touch with us by going to https://docs.berryconsultants.com/contact/. Details about all the software we provide can be found at https://berryconsultants.com/software/.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html#purpose-and-scope-of-this-document",
    "href": "introduction/gettingStarted/downloadInstructions.html#purpose-and-scope-of-this-document",
    "title": "Download Instructions",
    "section": "",
    "text": "This document provides download instructions for all new releases of Berry software; including, the FACTS desktop app installer, FACTS engine executable, ADDPLAN Classic, ADDPLAN Neo and QUOTES. This assumes that you currently have a software license agreement with Berry Consultants, or that you have already gotten in touch with Berry Consultants for a software evaluation. If this is not the case, please get in touch with us by going to https://docs.berryconsultants.com/contact/. Details about all the software we provide can be found at https://berryconsultants.com/software/.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html#registration-email",
    "href": "introduction/gettingStarted/downloadInstructions.html#registration-email",
    "title": "Download Instructions",
    "section": "Registration email",
    "text": "Registration email\nAll Berry software releases are hosted on FACTS Cloud, our cloud-hosted web application. You will first be receiving an email from noreply@berryconsultants.net to register with FACTS Cloud as shown below.\n\n\n\n\n\n\nFigure 1\n\n\n\nThe link in the email will remain valid for up to a day, and can only be used once. Note that this is a one time operation: once registered with FACTS Cloud, you will never need to re-register again. Please do not reply to this registration email, as replies are not tracked.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/downloadInstructions.html#set-password",
    "href": "introduction/gettingStarted/downloadInstructions.html#set-password",
    "title": "Download Instructions",
    "section": "Set password",
    "text": "Set password\nWhen clicking on the registration email link, you will be navigated to the FACTS Cloud password setting screen as shown below.\n\n\n\n\n\n\nFigure 2\n\n\n\nWithin the email address field, as an additional security measure, you will need to re-enter exactly the same email address that the FACTS Cloud registration email was sent to. The password will need to be at least 8 characters long, contain lower and upper case characters, contain digits and non-alphanumeric characters.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Download Instructions"
    ]
  },
  {
    "objectID": "introduction/webinars.html",
    "href": "introduction/webinars.html",
    "title": "Videos and Webinars",
    "section": "",
    "text": "Introduction to FACTS\nDose Escalation N-CRM Example\nCore Dichotomous Simple Two-Arms Example\nEnrichment Designs: Basket Trial Example\nFACTS Platform Trial Intro",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#introductions",
    "href": "introduction/webinars.html#introductions",
    "title": "Videos and Webinars",
    "section": "",
    "text": "Introduction to FACTS\nDose Escalation N-CRM Example\nCore Dichotomous Simple Two-Arms Example\nEnrichment Designs: Basket Trial Example\nFACTS Platform Trial Intro",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#training-sessions",
    "href": "introduction/webinars.html#training-sessions",
    "title": "Videos and Webinars",
    "section": "Training Sessions",
    "text": "Training Sessions\n\nFACTS Core Introduction\nFACTS Core Dose Response Models\nFACTS Core Predictive Probabilities\nFACTS Core Arm Dropping and RAR\nFACTS Core Multiple Endpoint\nFACTS Core and Enrichment Designs Longitudinal Modeling\nFACTS Core Time-to-Event\nFACTS Core Time-to-Event Predictors\nFACTS Enrichment Designs Introduction\nFACTS Enrichment Designs Hierarchical Modeling\nFACTS Enrichment Designs Borrowing information on control\nFACTS Core and Enrichment Designs Hierarchical Prior for Control\nFACTS Dose Escalation Introduction\nFACTS Dose Escalation N-CRM Setting the Prior\nFACTS Dose Escalation N-CRM Open enrollment, pseudo subjects, expansion cohort, fine spaced doses\nFACTS Dose Escalation N-CRM Ordinal toxicity, 2 samples, efficacy\nFACTS Dose Escalation 2D-CRM\nFACTS Staged Design Introduction\nFACTS Staged Design First Example\nFACTS Staged Design Advanced Topics\nFACTS Dose Escalation - Hands On Training\nFACTS Staged Design - Hands on Training",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#introduction-to-adaptive-design-using-facts-webinar-series",
    "href": "introduction/webinars.html#introduction-to-adaptive-design-using-facts-webinar-series",
    "title": "Videos and Webinars",
    "section": "Introduction to Adaptive Design Using FACTS Webinar Series",
    "text": "Introduction to Adaptive Design Using FACTS Webinar Series\n\nPhase 1 Dose Escalation\nArm Dropping and RAR\nBasket Trials\nGroup Sequential\nMAMS and Platform Trials\nSeamless Phase 2-3",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#introductory-topics",
    "href": "introduction/webinars.html#introductory-topics",
    "title": "Videos and Webinars",
    "section": "Introductory Topics",
    "text": "Introductory Topics\n\nThe Uses of Trial Simulation in Consulting\nFACTS Core Design Exploration - part 1\nFACTS Core Design Exploration - part 2\nStopping for Futility\nN-CRM Case Study\nIntroduction to Staged Designs\n\n\nDiscussion of FDA Guidance\n\nDiscussion of the new FDA Guidance on Adaptive Designs",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#response-adaptive-randomization",
    "href": "introduction/webinars.html#response-adaptive-randomization",
    "title": "Videos and Webinars",
    "section": "Response Adaptive Randomization",
    "text": "Response Adaptive Randomization\n\nThe pros and cons of Response Adaptive Randomization (RAR)\nRAR and arm dropping in FACTS\nAdaptive Randomization Works\nResponse Adaptive Randomization (RAR) in multi-arm trials seeking to find the best arm",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#case-studies",
    "href": "introduction/webinars.html#case-studies",
    "title": "Videos and Webinars",
    "section": "Case Studies",
    "text": "Case Studies\n\nThe Evolution of a trial from FACTS Core to Staged Design and back\nUsing a biomarker endpoint to trigger phase 3\nDesigning an Adaptive Multiple Dose Trial with FACTS\nPapers on Bayesian Group Sequential Designs\nThe ENRICH trial an Enrichment Design case study\nThe ENRICH Trial re-visited\nA seamless phase 2/3 trial in type-2 diabetes\nA two arm study with adherence model driven effect size and irregular interims\nA Phase 1 dose escalation study using open enrolment\nA successful dose finding study in Alzheimer’s\nA two arm non-inferiority MACE safety study",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#advance-topics",
    "href": "introduction/webinars.html#advance-topics",
    "title": "Videos and Webinars",
    "section": "Advance Topics",
    "text": "Advance Topics\n\nCalibrating hierarchical model priors\nPost Processing FACTS output with R\nUsing FACTS to analyze Ordinal Data\nFACTS predictive Power and Predicting the Trial Outcomes\nDose escalation designs with open enrollment\nFACTS Core TTE Predictors\nEarly decision making PFS and OS - part 1\nEarly decision making PFS and OS - part 2",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "introduction/webinars.html#optimization",
    "href": "introduction/webinars.html#optimization",
    "title": "Videos and Webinars",
    "section": "Optimization",
    "text": "Optimization\n\nPhase 2 sources of efficiency\nPicking the Winner - how to compare trial designs\nAn early look at QUOTES and Estimating eNPV\nApplying Bayesian Optimization to FACTS Trial designs and choosing stopping boundaries\nTrial Design Optimization through the lens of expected Net Present Value",
    "crumbs": [
      "Introduction",
      "Videos and Webinars"
    ]
  },
  {
    "objectID": "contact/index.html",
    "href": "contact/index.html",
    "title": "Contact",
    "section": "",
    "text": "We pride ourselves in delivering fast support and will go above and beyond for you.\n\n\nGeneral Inquiries and Help using FACTS\nPlease contact us directly via e-mail at facts@berryconsultants.com.\n\n\nGet FACTS\nTo directly apply for a free 3-months FACTS Evaluation license, please use the following online form.\nTo directly enquire about a free demo or a regular license, please use the following online form.\nIf you are unsure, feel free to contact us directly via email at facts@berryconsultants.com.\n\n\nGeneral Inquiries about Berry Consultants\nPlease use the following online form.",
    "crumbs": [
      "Contact"
    ]
  },
  {
    "objectID": "documentation/v71/references.html",
    "href": "documentation/v71/references.html",
    "title": "References",
    "section": "",
    "text": "Jennison, C., & Turnbull, B.W. (1999). Group Sequential Methods with Applications to Clinical Trials (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780367805326\nNCSS, LLC. Assurance for Non-Inferiority Tests for the Difference Between Two Proportions. In PASS Sample Size Software Documentation (pp. 289-1-289–31). Retrieved from https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Assurance_for_Non-Inferiority_Tests_for_the_Difference_Between_Two_Proportions.pdf.\nSAS Institute Inc. 2016. Base SAS® 9.4 Procedures Guide: Statistical Procedures, Sixth Edition. Cary, NC: SAS Institute Inc.\nPrior distributions for variance parameters in hierarchical models”, Andrew Gelman, Bayesian Analysis 2006, 1, Number 3 pp 515-533\n Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.\nAgresti, A. 2002.  Categorical Data Analysis. Second Edition. Wiley.\nMee, R. W. 1984. Confidence bounds for the difference between two probabilities. Biometrics. 40, 1175-1176.\nNurminen, M. 1986. Confidence intervals for the ratio and difference of two binomial proportions. Biometrics. 42, 675-676.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "References"
    ]
  },
  {
    "objectID": "documentation/v71/examples/Staged/example2.html",
    "href": "documentation/v71/examples/Staged/example2.html",
    "title": "Example 2",
    "section": "",
    "text": "Second example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "Staged",
      "Example 2"
    ]
  },
  {
    "objectID": "documentation/v71/examples/CRM/example2.html",
    "href": "documentation/v71/examples/CRM/example2.html",
    "title": "Example 2",
    "section": "",
    "text": "Second example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "CRM",
      "Example 2"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html",
    "href": "documentation/v71/userguides/FACTSfromR.html",
    "title": "Calling FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "href": "documentation/v71/userguides/FACTSfromR.html#purpose-and-scope-of-this-document",
    "title": "Calling FACTS from R",
    "section": "",
    "text": "This document describes usage and how to utilize “R” to run FACTS. It is intended for anyone requiring posterior probabilities and decisions that FACTS does not include. It can be used to:\n\nSimulate trials that require posterior quantities that FACTS does not include, e.g.:\n\nThe probability that a dose has a treatment effect in a certain range\n\nSimulate trials that make decisions that FACTS does not include, e.g.:\n\nSample size re-assessment\nRetain the dose meeting goal X and the next lowest dose\n\n\nWarning: this process is not very forgiving of errors, nor very informative when they occur. It is recommended you start with the supplied example and then modify that in steps to the case you actually wish to analyze or simulate.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#software-prerequisites",
    "href": "documentation/v71/userguides/FACTSfromR.html#software-prerequisites",
    "title": "Calling FACTS from R",
    "section": "Software prerequisites",
    "text": "Software prerequisites\nTo call FACTS from R, you will need the following installed on your Windows machine on which you are using FACTS:\n\nA reasonably recent version of R (v3.5.3+).\nFACTS v6.4 or later, the command line executable versions of the FACTS simulation engines – these are currently available to Enterprise licensees.\nThe supplied R file: factR.R",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#factr.r",
    "href": "documentation/v71/userguides/FACTSfromR.html#factr.r",
    "title": "Calling FACTS from R",
    "section": "factR.R",
    "text": "factR.R\nProvides an ‘R’ wrapper for accessing FACTS analysis models for:\n\nCore and Enrichment Design, allowing you to use the following FACTS analysis features:\n\nDose Response models\nLongitudinal models\nHierarchical Prior on Control (borrowing from historical data)\nTTE predictor endpoint\nBAC\n\nInputs\n\nFACTS param file with trial info and model specifications\nData file\n\nOutput\n\nMCMC file",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "href": "documentation/v71/userguides/FACTSfromR.html#steps-for-calling-facts-from-r",
    "title": "Calling FACTS from R",
    "section": "Steps for calling FACTS from R",
    "text": "Steps for calling FACTS from R\nTo call FACTS from R, you will need to do the following sequence of steps:\n\nCreate a (non-adaptive) FACTS project for your Engine type with the general study info: Number of Arms, number and timing of Visits (if using), Dose response (& longitudinal if using) model specification and MCMC setup.\nConfigure VSR and Execution profiles to allow a simple simulation run.\nRun 1 simulation to produce:\n\nA ‘param’ file which will be passed as an input to the R function.\nA ‘patients’ file. This may be useful to illustrate data file format for the input data. See FACTS Execution Guides for details.\nAn ‘mcmc’ file. This will show you what to expect in the output MCMC file.\n\nIf using FACTS to analyze a data set, then\n\nput the data set into the required format\nwrite an R script to call FACTS with the data set\nprocess the MCMC output\n\nIf using FACTS within a simulation framework, then:\n\nWrite an R script that generates the data you wish to simulate and pass to FACTS to analyze\nWrite a loop that\n\ngenerates the data for a simulation\ncalls FACTS with generated data\nprocess the MCMC output\naccumulate the statistics for the overall operating characteristics to be computed\n\nOutput the resulting OCs",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#runfacts-usage-notes",
    "href": "documentation/v71/userguides/FACTSfromR.html#runfacts-usage-notes",
    "title": "Calling FACTS from R",
    "section": "runFACTS() Usage Notes",
    "text": "runFACTS() Usage Notes\n\nRun FACTS MCMC Model from R\nrunFACTS(\n engine, \n data.file = “patients.dat”, \n param.file = “nuk1_e.param, \n mcmc.file.num = 0, \n rng.seed = 1, \n exec.path = getwd()\n)\nReturn Value: runFACTS returns a TRUE/FALSE to indicate a successful/failed execution. In case of errors, R error messages may be printed and in case of a FACTS execution error, a file called ‘error.txt’ will be output, containing the error description.\nArguments:\n\nengine: Name of the FACTS engine to use. Can be one of the following:\n\nFor Core Engines: “contin”, “dichot”, “ME”, “TTE”\nFor Enrichment Design Engines: “ed_contin”, “ed_dichot”, “ed_tte”\n\ndata.file: Name of the input data file. Default is “patients.dat”. This file format should exactly match the file format of the ‘patients’ file corresponding to the ones produced by FACTS for the design you setup in FACTS to specify the analysis model. (See the FACTS Execution Guide under the FACTS Help menu for details.)\nparam.file: Name of the FACTS ‘.param’ file that specifies the model setup. Default is ‘nuk1_e.param’.\nmcmc.file.num: The MCMC output is written to a file named ’mcmcNNNNN.csv. This argument set the NNNNN. Therefore, mcmc.file.num = 1 will create an MCMC output file called mcmc00001.csv. Default value is 0.\nrng.seed: Integer-valued random number generator seed. Will use the value from the ‘.param’ file if unspecified.\nexec.path The path to the directory where the FACTS executable program is available. Default is the current working directory.\n\n\n\nSet Up Files and Folders\nIt is important to pass files and parameters correctly, as there is not much in the way of helpful error messaging. Setting up the required folder and files is not hard but should be done carefully. The following example shows how.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "href": "documentation/v71/userguides/FACTSfromR.html#the-core.dichot.example.facts",
    "title": "Calling FACTS from R",
    "section": "The “core.dichot.example.facts”",
    "text": "The “core.dichot.example.facts”\nIn this example we wish to use FACTS to fit a simple NDLM dose response model across 6 arms (control & 5 doses) with a dichotomous endpoint.\nWe have entered the following parameters:\n\nStudy:\n\nStudy Info:\n\nNon-adaptive\nRecruit subjects continuously\nMax subjects: 300\nResponse is a positive outcome\nTime to final endpoint: 4 weeks\n\nTreatment Arms:\n\nControl and 5 doses with strengths 1, 2, … 5\n\n\nVirtual Subject Response\n\nExplicitly defined\n\nDose Response\n\nresponses: 0.1, 0.1, 0.125, 0.15, 0.2, 0.25\n\n\n\nExecution\n\nAccrual\n\n1 region with mean accrual of 5 subjects per week\n\nDropout\n\nNo dropouts\n\n\nQuantities of Interest\n\nPosterior probability: Pr(P_d &gt; P_Control)\nProbability of being target: Pr(Max)\nDecision Quantity: Pr(P_d &gt; P_Contorl); d=Greatest Pr(Max)\n\nDesign\n\nDose Response\n\nSimple NDLM\n\nInitial Dose ~N(0,22)\nTau IG(1,1) “central value”, “weight”\n\n\nRequentist analysis: none\nAllocation: 1:1:1:1:1:1\nSuccess/Futility Criteria\n\nSuccess: Pr(P_d &gt; P_Control); d= Greatest Pr(Max) &gt; 0.9\n\n\n\nNot all these parameters will effect our analysis, but we have to enter sufficient parameters to be able to run a simulation and get a bin1_e.param file. This can be found in the scenario simulation results folder. We only need to run 1 simulation on order to have one written out. This file is copied to our “Example” directory. If we want to change something in the analysis – the model or the prior for example, we can modify this facts file, re-run one simulation, and copy the new bin1_e.param file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/FACTSfromR.html#dichot-demo.r",
    "href": "documentation/v71/userguides/FACTSfromR.html#dichot-demo.r",
    "title": "Calling FACTS from R",
    "section": "dichot-demo.R",
    "text": "dichot-demo.R\nWe start by setting the current working directory to the “Example” folder, and setting up some file locations and sourcing the factR.R file.\n## Set up Folders and Paths\n\n# This is the directory where the parameter file and patient data must be located\n# It will be where the MCMCM files are written\nsetwd(\"Z:/FACTS test/FACTS 6 Training/FACTS R interface/Example\")\n\n# This must be the location of the factR.R file\nFactR.src = \"../factR.R\"\n\n# This must be the location of the executable files\nExec.dir = \"../WindowsExecutables\"\n\n# Load runFACTS\nsource(FactR.src)\nWe can then copy an example patients file from the simulation results and check that we can run facts.\n# Test to check its working\n# Copy an example patients file from the simulations results to this folder before running.\nsystem.time(\n  runFACTS(\n    engine='dichot', \n    data.file = 'patients00001.csv', \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 1, \n    rng.seed = 1, \n    exec.path = Exec.dir\n  )\n)\n\ngenBinaryData()\nWe now define a simple function to generate the data for a single run of FACTS.\n# generates a data frame that can be used to drive a FACTS analysis\n# dichotomous endpoint\n# no visits\n# n.per.arm: int, the number of subjects to be simulated for each arm\n# rates: int[], the response rate to be simulated for each arm\n# the length if rates defines the number of arms\n# returns a dataframe with n.per.arm * length(rates) simulated subjects\n\ngenBinaryData &lt;- function(nPerArm, rates) {\n  \n  patientID &lt;- 1:(nPerArm * length(rates)) # Generate a list of patients\n  region &lt;- rep(1, nPerArm * length(rates)) # all patients come from region 1\n  date &lt;- 1:(nPerArm * length(rates)) # Generate a list of enrolment dates - here simply one per day\n  doseAlloc &lt;- rep(1:length(rates), each = nPerArm) # Allocate patients equally to each dose\n  lastVisit &lt;- rep(1, nPerArm * length(rates)) # all patients have last visit data\n  dropout &lt;- rep(0, nPerArm * length(rates)) # no patients have dropped out\n  baseline &lt;- rep(-9999, nPerArm * length(rates)) # not simulating baseline\n  visit1 &lt;- rep(0, nPerArm * length(rates)) # create the outcome vector\n\n  for (d in 1:length(rates)){ # get responses for each dose\n  ix &lt;- which(doseAlloc == d) # get indices of patients on dose d\n  # assign them a final response based on the rate to simulate for dose d\n  if (length(ix) &gt; 0) {\n  visit1[ix] &lt;- \n    sample(\n     c(0,1), \n     size = length(ix), \n      replace = TRUE, \n      prob = c(1-rates[d], rates[d])\n    )\n  }\n  \n}\n\ndat &lt;- data.frame(\n  SubjectID = patientID, \n  Region = region, \n  Date = date, \n  Dose = doseAlloc, \n  LastVisit = lastVisit, \n  Dropout = dropout, \n  Baseline = baseline, \n  Visit1 = visit1, \n  row.names = NULL\n)\n\nreturn(dat)\n}\n\n\nrunSims\n########### Toy Example Trial Sim ##########\n### Constants\nDATAFILE = \"patients.csv\"\nMCMCFILE = \"mcmc00000.csv\"\n# function to simulate an example data set with dichotomous endpoint\n# nSims - the number of sims to run\n# nBurnin - the number of MCMC smaples to discard\n# (the number of MCMC samples is specified in the parameter file)\n# details - a boolean. If TRUE the function returns a data frame with\n# the results of each individual simulation,\n# otherwise just the win proportion and probabilities of being control\n\nrunSims &lt;- function(\n    nSims = 10, \n    nBurnin = 1000, \n    rates = c(0.1, 0.1, 0.125, 0.15, 0.2, 0.25),\n    details = FALSE\n) {\n\nwinPpn = 0\npr.gt.ctl.sum &lt;- rep(0, length(rates) - 1)\nif (details) {\nperSim &lt;- data.frame(Sim = 1)\n}\n\nfor(sim in 1:nSims) {\ndat = genBinaryData(nPerArm = 50, rates = rates)\nwrite.csv(dat,DATAFILE, row.names = FALSE)\nif (details) {\nperSim[sim, \"Sim\"] &lt;- sim\n# record true rates and observed rates\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"sim.rate.\", d, sep=\"\")] &lt;- rates[d]\n}\n\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"obs.rate.\", d, sep=\"\")] &lt;- mean(dat[dat[,\"Dose\"]==d, \"Visit1\"])\n}\n}\n\ncat(\"run FACTS: \", sim, \"\\n\")\n\nret &lt;- \n  runFACTS(\n    engine = 'dichot', \n    data.file = DATAFILE, \n    param.file = 'bin1_e.param', \n    mcmc.file.num = 0, \n    rng.seed = sim, \n    exec.path = Exec.dir\n  )\n\ndat = read.csv(MCMCFILE, skip = 1)\n\n# discard burnin rows and just estimates of rate - the \"Pi\" columns\ndat = dat[(nBurnin + 1):nrow(dat), grep(\"Pi\", names(dat))]\n\nif (details) {\n# record est rate\nfor (d in 1:length(rates)) {\nperSim[sim, paste(\"est.rate.\", d, sep=\"\")] &lt;- mean(dat[,paste(\"Pi.\", d, sep=\"\")])\n}\n}\n\n# success if the first dose is not in the top 2 .. i.e. the resposnse on any 2 doses is &gt; control\n\nsuccess &lt;- apply(dat, 1, FUN = function(x) {ifelse(length(x) - which(order(x)==1) &gt;= 2, 1, 0)})\n\nif (details) {\nperSim[sim, \"Pr.Success\"] &lt;- mean(success)\nperSim[sim, \"Success.flag\"] &lt;- ifelse(mean(success) &gt; 0.9, 1,0)\n}\n\nwinPpn = winPpn + ifelse(mean(success) &gt; 0.9, 1,0)\n\n# example: calc pr(theta_d &gt; theta_ctl)\ngt.ctl.flag &lt;- apply(dat, 1, FUN = function(x){x[2:length(x)] &gt; x[1]})\npr.gt.ctl &lt;- apply(gt.ctl.flag,1,sum)\npr.gt.ctl &lt;- pr.gt.ctl / length(gt.ctl.flag[1,])\npr.gt.ctl.sum &lt;- pr.gt.ctl.sum + pr.gt.ctl\n\nif (details) {\nfor (d in 1:length(pr.gt.ctl)) {\nperSim[sim, paste(\"Pr.pi.\", d+1, \"&gt;pi_ctl\", sep=\"\")] &lt;- pr.gt.ctl[d]\n    }\n  }\n}\n\ncat(\"win proportion: \", winPpn/nSims, \"\\n\")\n\nif (details) {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims, perSim))\n} else {\nreturn (list(winPpn/nSims, pr.gt.ctl.sum/nSims))\n}\n}\n\n\nRunning the Example\nHaving sourced the above functions and variables holding paths and fllenames we can run the simulations with the default scenario:\nrunSims(details=FALSE)\n\n&gt; run FACTS: 1\n&gt; run FACTS: 2\n&gt; run FACTS: 3\n&gt; run FACTS: 4\n&gt; run FACTS: 5\n&gt; run FACTS: 6\n&gt; run FACTS: 7\n&gt; run FACTS: 8\n&gt; run FACTS: 9\n&gt; run FACTS: 10\n&gt; win proportion: 0.3\n\n&gt; [[1]]\n&gt; [1] 0.3\n\n&gt; [[2]]\n&gt; Pi.2 Pi.3 Pi.4 Pi.5 Pi.6\n&gt; 0.30572 0.45824 0.51936 0.77944 0.92916\nIf “details” is set to TRUE then the list of results has a dataframe at the end that contains a row per simulation and details of that simulations results.\nHopefully this is sufficient to get you started.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "Calling FACTS from R"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html",
    "href": "documentation/v71/userguides/flfll.html",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems.\n\n\n\nFLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#purpose-and-scope-of-this-document",
    "href": "documentation/v71/userguides/flfll.html#purpose-and-scope-of-this-document",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "",
    "text": "This document provides installation and execution guidance for FACTS Linux File Loader Light (FLFLL), a console (non-GUI) application for running simulations of FACTS designs on both Linux and Windows systems.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#overview",
    "href": "documentation/v71/userguides/flfll.html#overview",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "",
    "text": "FLFLL is intended for anyone requiring FACTS to be run in a command-line environment. A common use-case of FLFLL involves running FACTS within the context of a larger automated pipeline, whereby multiple closely related FACTS designs (e.g., slightly different success/futility criteria thresholds) are simulated using FLFLL and compared using in-house libraries.\nFLFLL requires existing FACTS project files which should be created using the FACTS desktop application GUI. The FACTS project files contain a complex set of interrelated parameters and we do not recommend editing them directly as this will likely make the application unstable. However, Eli Lilly has released an open-source R library, “rFacts” (https://github.com/EliLillyCo/rfacts) which can batch create multiple FACTS files and run these files via FLFLL.\nFLFLL can run under both Linux and Windows, in 2 modes:\n\ngenerate parameter file(s) only\n\nGenerate the param files used as input for the FACTS engine executables (e.g., “nuk1e.param” for Core Continuous/Dichotmous designs, “init.bcrm” for Dose Escalation N-CRM designs etc.).\n\ngenerate full simulations, which we refer to as an “end-to-end” run:\n\nRun full simulations and generate output exactly as generated from FACTS.\n\n\nTo run FLFLL on Linux, the FACTS engines must be present and Mono, a third-party Linux .NET interpreter, must be installed.\nFLFLL can take input from a single FACTS project file, or a directory of one or more FACTS project files, or a directory hierarchy containing other directories, with FACTS project files contained at any level of the directory structure. A directory structure of results will be generated matching the input directory structure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#linux",
    "href": "documentation/v71/userguides/flfll.html#linux",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "Linux",
    "text": "Linux\n\nInstall Mono version 6.12 or later from https://www.mono-project.com/docs/about-mono/releases onto the target machine/server running FLFLL and ensure that all users of FLFLL can run Mono. A simple test would be to ask FLFLL users to run “mono –version”.\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” (via Mono) present in the application folder.\nRetrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server.\nWithin the FLFLL application folder, go to the “bin” folder (which contains the Linux engines used by FLFLL) and elevate the Linux engine permissions to being executable by running “chmod +x [name of linux engine executable here]” for each of the Linux engines. If you do not have permission to do so, please ask your IT administrator to run this command.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#windows",
    "href": "documentation/v71/userguides/flfll.html#windows",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "Windows",
    "text": "Windows\n\nDownload the enterprise release (e.g., facts-6.4-enterprise-release_20210413.1.zip) from App Center (https://appcenter.ms) and unzip it onto the target machine running FLFLL. We recommend using 7Zip (https://www.7-zip.org/) to perform the unzipping rather than the Windows in-built unzipping tool: the latter can result in the corruption of the FLFLL application as a security precaution.\nRetrieve the folder containing the FLFLL application (e.g., facts-6.4-flfll_20210413.1), rename it to a more user-friendly name such as “FLFLL” and place it in an area where all users of FLFLL will have the necessary permissions to execute “FLFLL.exe” present in the application folder.\nIf the machine/server running FLFLL already has a licensed version of FACTS installed on it, the following step can be skipped. Otherwise, retrieve the FACTS license file corresponding to the version of FLFLL being used, ensure it is named “license.txt” and place it in the FLFLL application folder. The license file should contain the following attributes: OrgName, Key and StartDate (date format: yyyy/mm/dd). The license file is only required for the first successful run of FLFLL on the target machine: for security reasons, we would strongly recommend removing the license file from the server entirely once the first successful run of FLFLL has been performed on the target machine/server.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#linux-1",
    "href": "documentation/v71/userguides/flfll.html#linux-1",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "Linux",
    "text": "Linux\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\nmono \"FLFLL.exe\" -file \"home/mono/FLFLL/Input/myfile.facts\" -nSim 1\n-seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n\"home/mono/FLFLL/Output\" -logPath “/home/mono/Log”\nRun FLFLL to generate parameter files only for multiple FACTS project files in a single directory:\nmono \"FLFLL.exe\" -file \"home/mono/FLFLL/Input \" -nSim 1 -seed 3500\n-nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n\"home/mono/FLFLL/Output\" -logPath “/home/mono/Log”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS files older than FACTS 6.2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#windows-1",
    "href": "documentation/v71/userguides/flfll.html#windows-1",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "Windows",
    "text": "Windows\n\nRun FLFLL to generate parameter files only for a single FACTS project file:\nFLFLL.exe -file “C:\\MyDocuments\\FLFLL\\Input\\myfile.facts” -nSim 1\n-seed 3500 -nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n“C:\\MyDocuments\\FLFLL\\Output\" -logPath “C:\\MyDocuments\\Log”\nRun FLFLL to generate parameter files only for multiple FACTS project files:\nFLFLL.exe -file “C:\\MyDocuments\\FLFLL\\Input” -nSim 1 -seed 3500\n-nFreqWeeksFiles 1 -nSubjectFiles 1 -outputPath\n“C:\\MyDocuments\\FLFLL\\Output\" -logPath “C:\\MyDocuments\\Log”\nThe directory “Input” can contain any number of FACTS project files or a directory structure of multiple directories, each containing any number of FACTS project files.\nRun FLFLL to run simulations.\n\nAdd the command line argument “-endToEndRun” to the above examples and add “-skipMissingParamsCheck” if simulating using FACTS file older than FACTS 6.2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#overview-1",
    "href": "documentation/v71/userguides/flfll.html#overview-1",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "Overview",
    "text": "Overview\nUsage: FLFLL.exe [options] where [options] are:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n-[h|help]\nDisplay the help menu. Default (False).\n\n\n-[f|file]\nSpecifies the file or top-level directory to open.\n\n\n-[n|nSim]\nNumber of simulations to run. Default (5).\n\n\n-[p|packet]\nPacket size for parallelization. Default (1000).\n\n\n-[g|grid]\nFlag indicating sims to run on grid. Default (False).\n\n\n-[a|agg]\nAggregation mode. Default (None).\n\n\n-[aggPrefix]\nPrefix for aggregated files. Default (agg).\n\n\n-[nBurn]\nNumber of MCMC burn-in iterations. Default (1000).\n\n\n-[nMCMC]\nNumber of MCMC sample iterations. Default (2500).\n\n\n-[nWeeksFiles]\nNumber of weeks files to generate. Default (100).\n\n\n-[nSubjectFiles]\nNumber of subjects files to generate. Default (1).\n\n\n-[nMCMCFiles]\nNumber of MCMC output files to generate. Default (0).\n\n\n-[nMCMCThin]\nMCMC thinning parameter. Default (0).\n\n\n-[nMCMCImpute]\nMCMC length per imputation parameter. Default (1).\n\n\n-[seed]\nSet the random number seed. Default (3500).\n\n\n-[logPath]\nIf provided, specifies a directory where a log file is generated.\n\n\n-[outputPath]\nSpecifies the directory where output will be generated. Default (“out”).\n\n\n-[endToEndRun]\nFlag indicating if simulations should be run.\n\n\n-[skipMissingParamsCheck]\nFlag indicating to skip checking for missing parameters.\n\n\n-[scenarios]\nFlag indicating which scenarios should be processed.\n\n\n-[useDifferentSeedPerScenario]\nFlag indicating whether to use a different seed for each simulated scenario. Default (False).\n\n\n-[useDifferentSeedPerDesign]\nFlag indicating whether to use a different seed for each simulated design. Default (False).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/flfll.html#arguments",
    "href": "documentation/v71/userguides/flfll.html#arguments",
    "title": "FACTS Linux File Loader Light - FLFLL",
    "section": "Arguments",
    "text": "Arguments\n\n–[h | help] (Help)\nThe –h command line option displays the command line help options in the terminal. No simulations will be performed when the –h option is specified.\n\n\n–[v | version] (Version)\nThe –v command line option displays the FLFLL version in the terminal. No simulations will be performed when the –v option is specified.\n\n\n–[f | file] FILE (Run a specific .facts file)\nThe –f command line option is used to specify the “.facts” file to process. The –f option must be followed by a valid path to an existing “.facts” file or directory containing one or more “.facts” files. Hint: Remember to use quotes around the path if it includes spaces.\nIf the supplied file name is a directory, then FACTS will process each “.facts” file in the directory in turn. As this only starts FACTS once, this can be quite a bit quicker than using a batch file or script that loops and starts FACTS separately for each “.facts” file.\nFurthermore, if the supplied file name is a directory then FACTS also automatically recurses through every sub-directory processing every “.facts” file it finds.\n\n\n–[n | nSim] N (Run N simulations for each scenario)\nThe –n command line option is used to specify the number of simulations to run. The –n option must be followed by an integer value greater than 0. For each scenario in the FACTS project file, the application will run N simulations. Defaults to 5 if unspecified.\n\n\n–[p | packet] N (Set the packet size for simulations)\nThe –p command line option is used to specify the packet size for parallelization of simulations. The –p option must be followed by an integer value greater than 0. When using the –g option to run on a grid, or when running on a multicore machine it is often beneficial to parallelize simulations using the packetization process (see grid documentation for more information on packetization). The packet size must be greater than zero, but as in the GUI, there is no restriction that it be less than the number of simulations. If it is greater than the number of simulations, the simulations will not be packetized. Defaults to 1000 if unspecified.\n\n\n–[g | grid] (Run on grid)\nThe –g command line option instructs the application to send simulations to the grid (assumes that the grid is correctly configured). When running on the grid, the action is still performed synchronously (i.e. FACTS will wait while the simulations run and collect the results before exiting). This option is useful to parallelize long running simulations more than they can be parallelized locally. Defaults to run locally if unspecified.\n\n\n–[a | agg] Mode (Aggregation Mode)\nThe –a command line option specifies the aggregation action to take for completed simulation results. The available modes for this option are:\n\nNone – no aggregation will be performed\nNoPivot – Only standard aggregation will be performed\nPivot – Both standard and pivoted aggregation will be performed.\nDefault, if unspecified, is None.\n\n\n\n–aggPrefix prefix (Prefix for aggregation files)\nThe –aggPrefix command line option specifies the prefix to use when naming aggregated files and must be followed by a valid file prefix. This option is only used when –a is set to NoPivot or Pivot. The aggregation files produced by aggregating across all scenarios will be named using the prefix&lt;_pivot&gt;_(filename).csv pattern, where &lt;_pivot&gt; is included for pivoted files only, and (filename) is replaced by the name of the file being aggregated. Defaults to “agg” if unspecified.\n\n\n–nBurn N (Number of MCMC burn-in iteractions)\nThe –nBurn command line option specifies the number of burn-in MCMC iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 1000 if unspecified.\n\n\n–nMCMC N (Number of MCMC sample iterations)\nThe –nMCMC command line option specifies the number of MCMC sampling iterations to use in the simulation and must be followed by a valid integer value greater than 0. Defaults to 2500 if unspecified.\n\n\n–nWeeksFiles N (Number of weeks files to output)\nThe –nWeeksFiles command line option specifies the number of weeks files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 100 if unspecified.\n\n\n–nSubjectFiles N (Number of subjects files to output)\nThe –nSubjectFiles command line option specifies the number of subject files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 1 if unspecified.\n\n\n–nMCMCFiles N (Number of MCMC files to output)\nThe –nMCMCFiles command line option specifies the number of MCMC sample files to output from the simulation and must be followed by a valid integer value at least 0. Defaults to 0 if unspecified.\nNote: This option potentially produces a very large amount of output data and may fail if sufficient disk space is not available.\n\n\n–nMCMCThin N (MCMC output thinning value)\nThe –nMCMCThin command line option specifies the MCMC thinning value to apply to the MCMC output and must be followed by a valid integer value at least &lt;x&gt;. The thinning parameter applies only to the MCMC output, all MCMC samples are used for analysis. Defaults to &lt;x&gt; if unspecified.\n\n\n–nMCMCImpute N (MCMC length per imputation value)\nThe –nMCMCImpute command line option specifies the number of MCMC sampling iterations to use in the simulation for each imputation. Defaults to 1 if unspecified.\n\n\n–seed (Random number seed)\nThe –seed command line option sets the random number generator seed value. The default value (3500) is the same as that used in the GUI. It can be set to any positive integer.\n\n\n-logPath Path (Path where the optional log file is placed)\nThe –logPath command line option (if given) is used to specify the directory where a log file is generated. If not specified, a log file will not be generated. The –logPath option must be followed by a valid path. If the path does not exist, it will be created.\nNote: Remember to use quotes around the path if it includes spaces.\n\n\n-outputPath Path (Path where output is generated)\nThe –outputPath command line option (if given) is used to specify the directory where parameter files and optional simulation files are placed. If not specified, the output path will be the directory in which the .facts file is present. The –outputPath option must be followed by a valid path. If the path does not exist, it will be created.\n\n\n-endToEndRun (Flag to indicate if simulations should be run also)\nThe –endToEndRun command line instructs FLFLL to run full simulations. If unspecified, only parameter files will be generated.\n\n\n-skipMissingParamsCheck (Flag to indicate to skip checking of missing parameters)\nThe –skipMissingParamsCheck command line instructs FLFLL to skip the process of checking for missing parameters in legacy FACTS project files (.facts). If not specified, when running FACTS projects files prior to version 6.2, errors will prevent FLFLL from completing.\n\n\n-scenarios (Flag indicating which scenarios should be processed)\nThe –scenarios command line instructs FLFLL to run the specified scenarios by name. The names of the scenarios to run should be provided as a comma separated list. If not specified, all scenarios will be processed.\n\n\n-useDifferentSeedPerScenario (Flag indicating whether to use a different seed for each simulated scenario)\nThe –useDifferentSeedPerScenario command line option provides FLFLL with an option to set a different random number seed for each of the scenarios that are being simulated. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated scenario and then adds 1111 to the random number seed of any additional scenarios being simulated. For example, if three scenarios are being simulated and the -seed flag is set to 3500, the first scenario will have a random number seed of 3500, the second scenario a random number of 4611 and the third scenario a random number seed of 5722. This deterministic way of setting different random number seeds allows for reproducible simulation results.\n\n\n-useDifferentSeedPerDesign (Flag indicating whether to use a different seed for each simulated design)\nThe –useDifferentSeedPerDesign command line option provides FLFLL with an option to set a different base random number seed for each of the designs that are being simulated. This option is only used when a directory containing multiple FACTS designs is passed as an argument to the FLFLL command line, rather than a single design. FLFLL simply takes the base random number seed as set in the -seed flag, uses this for the first simulated design and then adds 1234 to the random number seed of any additional designs being simulated. For example, if three designs are being simulated and the -seed flag is set to 3500, the first design will have a base random number seed of 3500, the second scenario a random number of 4734 and the third design a base random number seed of 5968. This deterministic way of setting different random number seeds allows for reproducible simulation results.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Linux File Loader Light - FLFLL"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/index.html",
    "href": "documentation/v71/userguides/core/index.html",
    "title": "FACTS Core Designs",
    "section": "",
    "text": "This document covers the design options that are common across the four FACTS Core Design Engines: Continuous, Dichotomous, Time-to-Event, and Multiple Endpoint. Some design elements are shared across design engines, in which case there is only a single description of them. Others differ based on the endpoint used, in which case separate pages have been created to describe each.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS V7 & V6 installed on Windows 10 or 11. If FACTS is installed on different versions of Windows, or with different Windows ‘themes’ there will be some differences in appearance introduced by Windows. The contents of each tab, however, will generally be consistent with the screenshots in this document.\n\n\nThis is the version of the user guide for inclusion with the FACTS 7.1 release.\n\n\n\nPlease cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/index.html#facts-version",
    "href": "documentation/v71/userguides/core/index.html#facts-version",
    "title": "FACTS Core Designs",
    "section": "",
    "text": "This is the version of the user guide for inclusion with the FACTS 7.1 release.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/index.html#citing-facts",
    "href": "documentation/v71/userguides/core/index.html#citing-facts",
    "title": "FACTS Core Designs",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described in Section 16, below.\nTo view the graphs, select a row of scenario results by clicking on it and then click on the ‘View Graph’ button.\nBelow we have often shown full screen shots of the graphs in the graph manager, but the graph display supports copying just the graph to the clipboard to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘png’ format to a file.\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select that row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nTo view multiple graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot. You can select the graph type, filter the design variants and filter which scenarios displayed:\n\n\n\n\n\n\nFigure 1: Pop up to select scenarios and variants to display.\n\n\n\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks (Not displayed if the ‘y’ value must lie in the interval 0-1.)\nSpace doses evenly – if the x-value is ‘dose’, then data for each dose can either be spaced equally or proportionate to the doses’ effective dose strength.\nIf using a 2D treatment arm model then it is possible to display graphs where the different doses (or “arms”) form the x-axis, then there is an option to show the row factors as separate series – otherwise the different combinations are displayed in effective strength order\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Arms shown in effective strength order\n\n\n\n\n\n\n\n\n\n\n\n(b) Arms shown with row factors as a separate series\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Arms shown in effective strength order\n\n\n\n\n\n\n\n\n\n\n\n(b) Arms shown with row factors as a separate series\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 6: Response and Subject Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Response and Subject Allocation.\n\n\n\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black line shows the true mean response being simulated.\nThe green dashed line shows the mean of the estimated response across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated mean response across the simulations (if less than 20 simulations have been run it simply shows the full spread).\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 8: Response and QOI Selection.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Response and QOI Selection.\n\n\n\n\n\n\nThese plots show the true simulated mean response and the mean and 95% spread of the mean fitted mean responses along with bars showing the proportion of simulations the different treatment arms are the selected target arm for each of the “Probability of being target” QOIs, such as Pr(Max), Pr(EDq relative to ..: Quantile=q) and Pr(MED relative to…: Delta=\\(\\delta\\)).\n\n\n\nThis set of graphs show the distribution over the simulations of the posterior probability estimates of quantities of interest for each dose. All the Posterior Probability, Predictive Probability, P-value and Probability of being Target QOIs that have been defined are available for plotting, via a drop down list.\nNote that in any one analysis the Probability of being Target will sum to 1 across the doses (and can sum to less than one in the case of the MED target if none of the doses look likely to be better than the CSD), whereas all the other QOIs are assessed individually for each dose.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(\\theta_d\\) - \\(\\theta_{Control}\\) &gt; 0.1) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.1) Across Simulations.\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(P_d\\) - \\(P_{Control}\\) &gt; 0.15) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.2) Across Simulations.\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 12: Posterior Mean Response at P(MED relative to Control: Delta}=0.1) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior Mean Response at P(MED relative to Control: Delta}=0.2) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\nThis graph shows a scatter plot of trial outcomes with the selected Target QOI as the y-axis and total number of subjects recruited as the x-axis. The trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as it is possible?\nWhen and with what mean response trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\n\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial. The user can select whether time or subject enrollment is used as the x-axis.\n\n\n\nIf the design has used arm dropping, the following graphs are available.\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 18: Response and Proportion Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Response and Proportion Arm Dropped\n\n\n\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 20: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 22: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 24: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\nThese graphs are available if frequentist analysis is enabled.\n\n\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 26: Distribution of P(LOCF. Bonferroni Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Distribution of P(LOCF. Unadjusted Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from Unadjusted, Bonferroni or Dunnett’s and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 28: Response and Ppn LOCF. Bonferroni Significance\n\n\n\n\n\n\n\n\n\n\n\nFigure 29: Response and Ppn Fail. Dunnett Significance\n\n\n\n\n\n\n\n\n\n\nThis graph shows the final analysis at the end of individual simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 30: Response and Subject Allocation\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Response and Subject Allocation\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBlue bars showing the number of subjects allocated on each treatment arm.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 32: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\nControl to select the individual simulation results to be displayed\n\n\n\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 34: Response and Pr(MED relative to Control: Delta = 0.1) Probability\n\n\n\n\n\n\n\n\n\n\n\nFigure 35: Response and Pr(EDq relative to Control: Quantile = 0.9) Probability\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot.\n\n\n\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\nNote the ‘Interim’ control in both of the below figures.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 36: Response and Pr(MED relative to Control: Delta = 0.1) Probability with side panel allowing for selection of simulation, interim, and quantity of interest.\n\n\n\n\n\n\n\n\n\n\n\nFigure 37: Response and Subject Allocation with side panel allowing for selection of simulation and interim number.\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 39\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: e.g. Pr(Max), and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis). The graphs show for all the decision QOIs defined for the selected Target Dose the proportion of simulations that would have met the threshold values.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (unlike the examples above where the shape of the “existing stopping rules” line indicates that early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which decision QOI is evaluated and from which interim onwards stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 42: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\nFigure 43: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 45\n\n\n\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the decision QOI to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are successful/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates.\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 46\n\n\n\n\n\n\n\n\n\n\n\nFigure 47: Trace plot of an MCMC parameter.\n\n\n\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC.\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot.\n\n\n\n\n\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 48: Across scenario graph for selected arms with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 49: Across scenario graph for selected arms with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 50: Across scenario graph for showing QOIs per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 51: Across scenario graph for showing QOIs per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 52: Across scenario graph for showing the proportion of trials that result in success with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 53: Across scenario graph for showing the proportion of trials that result in success with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 54: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 55: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 56: Across scenario graph for showing the distribution of allocation per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 57: Across scenario graph for showing the distribution of allocation per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 58: Across scenario graph for showing the mean sample size of the study with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 59: Across scenario graph for showing the mean sample size of the study with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 60: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\n\n\nFigure 61: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 62: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.\n\n\n\n\n\n\n\n\n\n\n\nFigure 63: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#per-scenario-graphs",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#per-scenario-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "ContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Subject Allocation Across Simulations.\n\n\n\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 6: Response and Subject Allocation.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Response and Subject Allocation.\n\n\n\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean response and the 2.5%-97.5% boundaries of the estimated means over the simulations.\n\nThe blue bars show the mean number of subjects allocated.\nThe black line shows the true mean response being simulated.\nThe green dashed line shows the mean of the estimated response across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated mean response across the simulations (if less than 20 simulations have been run it simply shows the full spread).\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 8: Response and QOI Selection.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Response and QOI Selection.\n\n\n\n\n\n\nThese plots show the true simulated mean response and the mean and 95% spread of the mean fitted mean responses along with bars showing the proportion of simulations the different treatment arms are the selected target arm for each of the “Probability of being target” QOIs, such as Pr(Max), Pr(EDq relative to ..: Quantile=q) and Pr(MED relative to…: Delta=\\(\\delta\\)).\n\n\n\nThis set of graphs show the distribution over the simulations of the posterior probability estimates of quantities of interest for each dose. All the Posterior Probability, Predictive Probability, P-value and Probability of being Target QOIs that have been defined are available for plotting, via a drop down list.\nNote that in any one analysis the Probability of being Target will sum to 1 across the doses (and can sum to less than one in the case of the MED target if none of the doses look likely to be better than the CSD), whereas all the other QOIs are assessed individually for each dose.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(\\theta_d\\) - \\(\\theta_{Control}\\) &gt; 0.1) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.1) Across Simulations.\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of \\(P\\)(\\(P_d\\) - \\(P_{Control}\\) &gt; 0.15) Across Simulations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of \\(P\\)(MED relative to Control: Delta}=0.2) Across Simulations.\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 12: Posterior Mean Response at P(MED relative to Control: Delta}=0.1) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior Mean Response at P(MED relative to Control: Delta}=0.2) vs. Total Sample Size per Simulation Run\n\n\n\n\n\n\nThis graph shows a scatter plot of trial outcomes with the selected Target QOI as the y-axis and total number of subjects recruited as the x-axis. The trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as it is possible?\nWhen and with what mean response trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.\n\n\n\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial. The user can select whether time or subject enrollment is used as the x-axis.\n\n\n\nIf the design has used arm dropping, the following graphs are available.\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 18: Response and Proportion Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Response and Proportion Arm Dropped\n\n\n\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 20: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 22: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Cumulative Proportion of Trials where Arm Retained\n\n\n\n\n\n\n\n\n\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 24: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: Arm Retention Proportion\n\n\n\n\n\n\n\n\n\n\nThese graphs are available if frequentist analysis is enabled.\n\n\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 26: Distribution of P(LOCF. Bonferroni Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Distribution of P(LOCF. Unadjusted Adjusted Significance) Across Simulations\n\n\n\n\n\n\n\n\n\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from Unadjusted, Bonferroni or Dunnett’s and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 28: Response and Ppn LOCF. Bonferroni Significance\n\n\n\n\n\n\n\n\n\n\n\nFigure 29: Response and Ppn Fail. Dunnett Significance\n\n\n\n\n\n\n\n\n\n\nThis graph shows the final analysis at the end of individual simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 30: Response and Subject Allocation\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Response and Subject Allocation\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBlue bars showing the number of subjects allocated on each treatment arm.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 32: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Response and Subject Allocation with the side panel that allows for specification of the simulation for which the plot should be shown.\n\n\n\n\n\n\nControl to select the individual simulation results to be displayed\n\n\n\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 34: Response and Pr(MED relative to Control: Delta = 0.1) Probability\n\n\n\n\n\n\n\n\n\n\n\nFigure 35: Response and Pr(EDq relative to Control: Quantile = 0.9) Probability\n\n\n\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nThe green solid line shows the estimate of the mean response.\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the response.\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot.\n\n\n\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100).\nNote the ‘Interim’ control in both of the below figures.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 36: Response and Pr(MED relative to Control: Delta = 0.1) Probability with side panel allowing for selection of simulation, interim, and quantity of interest.\n\n\n\n\n\n\n\n\n\n\n\nFigure 37: Response and Subject Allocation with side panel allowing for selection of simulation and interim number.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#post-simulation-boundary-finding-graphs",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#post-simulation-boundary-finding-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "ContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Satisfying Final Futility Criteria by Threshold\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Satisfying Final Success Criteria by Threshold\n\n\n\n\n\n\n\nFigure 39\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: e.g. Pr(Max), and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis). The graphs show for all the decision QOIs defined for the selected Target Dose the proportion of simulations that would have met the threshold values.\nAs in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (unlike the examples above where the shape of the “existing stopping rules” line indicates that early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which decision QOI is evaluated and from which interim onwards stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 42: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\nFigure 43: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations Reaching Futility for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations Reaching Success for Early/Late Criterion Thresholds\n\n\n\n\n\n\n\nFigure 45\n\n\n\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the decision QOI to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are successful/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#mcmc-trace-plots",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#mcmc-trace-plots",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "ContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 46\n\n\n\n\n\n\n\n\n\n\n\nFigure 47: Trace plot of an MCMC parameter.\n\n\n\n\n\n\nIf an MCMC sample file has been output for one or more simulations (the default is to not output MCMC sample files due to their size), then for each of those simulations it is possible to view the MCMC trace of the sampled values for each of the parameters sampled in the MCMC.\nIf the design is adaptive, the user can select which interim (“update”) the samples are from, as well as which parameter’s samples to plot.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#across-scenario-graphs",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#across-scenario-graphs",
    "title": "Continuous and Dichotomous Output",
    "section": "",
    "text": "This graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 48: Across scenario graph for selected arms with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 49: Across scenario graph for selected arms with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 50: Across scenario graph for showing QOIs per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 51: Across scenario graph for showing QOIs per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 52: Across scenario graph for showing the proportion of trials that result in success with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 53: Across scenario graph for showing the proportion of trials that result in success with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 54: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 55: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 56: Across scenario graph for showing the distribution of allocation per arm with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 57: Across scenario graph for showing the distribution of allocation per arm with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 58: Across scenario graph for showing the mean sample size of the study with 5 scenarios and 5 design variants.\n\n\n\n\n\n\n\n\n\n\n\nFigure 59: Across scenario graph for showing the mean sample size of the study with 3 scenarios and 3 design variants.\n\n\n\n\n\n\n\n\n\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 60: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\n\n\nFigure 61: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.\n\n\n\n\n\n\n\n\n\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\nFigure 62: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.\n\n\n\n\n\n\n\n\n\n\n\nFigure 63: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#summary-per-scenario",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#summary-per-scenario",
    "title": "Continuous and Dichotomous Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after continuous or dichotomous simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn S uc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn F ut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inco nclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt; Dose&gt;&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn I ncorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation tabs provided in FACTS output for continuous or dichotomous trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nResponse\n\nContinuousDichotomous\n\n\n\nResponse columns available in FACTS output for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the mean response on this dose.\n\n\nSD Trt.: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this dose.\n\n\nMean Sigma\n1\nThe mean (over the simulations) of the estimate of the SD of the dose response across all the treatment arms (‘sigma’)\n\n\nSD Mean Sigma\n1\nThe SD (over the simulations) of the estimate of the SD of the dose response across all the treatment arms.\n\n\nTrue Mean Resp: &lt;Dose&gt;\nOne per arm\nThis is the true mean response from which the simulation data was sampled for each treatment arm.\n\n\nSD True Resp.: &lt;Dose&gt;\nOne per arm\nThis is the true SD of the dose response for each treatment arm\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit, based on the independent estimate of the dose response (omega_d), scaled by the longitudinal model's estimate of the proportion of the final effect observed at the visit.\n\n\nSD Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the SD ( over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Baseline Beta\n\nThis is the mean (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nSD Baseline Beta\n\nThis is the SD (over the simulations) of the estimate of “Beta” in the Baseline Adjusted dose response model.\n\n\nMean Baseline\n1\nThis is the mean (over the simulations) of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\nThis is the SD (over the simulations) of the estimate of the mean baseline score.\n\n\nTrue Mean Baseline\n1\nThis is the true mean from which baseline scores where simulated (including accounting for possible truncation of the baseline scores)\n\n\nTrue SD Baseline\n1\nThis is the true SD of the distribution from which baseline scores were simulated (including accounting for possible truncation of the baseline scores)\n\n\n\n\n\n\nResponse columns available in FACTS output for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Trt.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the estimate of the response on this dose.\n\n\nSD Trt.: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the response on this dose.\n\n\nTrue Mean Resp: &lt;Dose&gt;\nOne per arm\nThis is the true response rate from which the simulation data was sampled.\n\n\n\n\n\n\n\n\nObserved\n\nObserved columns available in FACTS output for continuous or dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Complete &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited per arm which have had their endpoint observed in this scenario.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Data at Specified Visit, Opportunity to Complete at Specified Visit) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\n&lt;Visit&gt;\nOne per arm per visit\nThis is the mean (over the simulations) of dropouts per arm per visit in this scenario.\n\n\n\n\n\nProbabilities\n\nProbabilities columns available in FACTS output for continuous or dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\nModel Parameters\n\nContinuousDichotomous\n\n\n\nModel Parameters columns available in FACTS output for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Sigma\n1\nThis is the mean (over the simulations) of the posterior estimate of sigma, the SD in the subject’s final responses.\n\n\nSD Mean Sigma\n1\nThis is the SD (over the simulations) of the estimate of sigma.\n\n\nMean Baseline Beta\n1\nThis is the mean (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nSD Baseline Beta\n1\nThis is the SD (over the simulations) of the estimate of the Beta parameter when Baseline Adjustment is Modeled.\n\n\nBAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\n\n\n\n\nModel Parameters columns available in FACTS output for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n’Virtual Subject Response &gt; Composite,\nThis is the same name as used for the results directory\n\n\nMean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nD*V\nThese are only calculated and written out if the ITP longitudinal model is being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt;\n&lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nBAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSD BAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nThis is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC Mean\n1\nThis is the SD (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nThis is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSD BAAC tau\n1\nThis is the SD (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#detailed-simulation-results",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#detailed-simulation-results",
    "title": "Continuous and Dichotomous Output",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 15‑1) displays the individual results for each simulation. This is the contents of the “simulations.csv” file, which is described below. The simulations results are partitioned into the same various results groupings as the summary results. These can be accessed from the “right click” menu, along with opening the results folder, opening the weeks file for a particular simulation and opening the simulated patients file for a particular simulation (where a “weeks” file or a “patients” file has been output).\n\n\n\n\n\n\nFigure 64: The simulation results pop out viewer showing simulation highlights.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nMean Sigma\n1\nThe mean of the estimate of sigma – the average standard deviation of the dose response\n\n\nSE Mean Sigma\n1\nThe standard error of the estimate of sigma – the average standard deviation of the dose response\n\n\nTrue Mean resp &lt;dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nTrue SD resp &lt;dose&gt;\nD\nThe true SD of the response for each treatment arm of the simulated subject responses\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nMean Beta\n1\nThe mean of the estimates of the coefficient of baseline adjustment\n\n\nSE beta\n1\nThe standard error of the estimates of the coefficient of baseline adjustment\n\n\nMean Baseline\n1\nThe mean of the estimate of the mean of the baseline score\n\n\nSE Mean Baseline\n1\nThe standard error of the estimate of the mean of the baseline score\n\n\nSD Baseline\n1\nThe mean of the estimate of the SD of the baseline score\n\n\nSE SD Baseline\n1\nThe standard error of the estimate of the SD of the baseline score\n\n\nTrue Mean Baseline\n1\nThe true mean of the baseline score (accounting for possible truncation)\n\n\nTrue SD Baseline\n1\nThe true SD of the baseline score (accounting for possible truncation)\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\nContents of the summary.csv file for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nTimestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTS that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation run.\n\n\nRandom Number Seed\n1\nThe random number seed\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.Subj 80%ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nPpn Overall Success\n1\nThe proportion of simulations that were successful (P(ES) + P(LS))\n\n\nPpn Overall Futility\n1\nThe proportion of simulations that were futile (P(EF) + P(LF))\n\n\nP(ES)\n1\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;dose&gt;\nD\nThe mean of the estimates of response of each treatment arm.\n\n\nSE Resp &lt;dose&gt;\nD\nThe standard error, over the simulations, of the estimate of response of each treatment arm.\n\n\nTrue Mean resp &lt;dose&gt;\nD\nThe true response for each treatment arm used when sampling the simulated subject responses.\n\n\nMean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used.\nThis is the mean (over the simulations) of the estimate of the mean response for a particular dose at a particular visit.\n\n\nSE Mean Longmod Resp: &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThis is the standard error (over the simulations) of the longitudinal model estimate of the response at intermediate visits (see above)\n\n\nMean Complete &lt;Dose&gt;\nD\nThe mean number of completers for each treatment arm\n\n\nMean Complete Info &lt;Dose&gt;\nD\nThe mean number of complete information (as specified by the interim information type on the Interims tab) for each treatment arm\n\n\nNo .Dropouts &lt;Dose&gt; &lt;Visit&gt;\nD*V\nThe mean number of dropouts per treatment arm per visit\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nPpn Correct Arm\n1\nThe proportion of the simulations where the correct arm has been selected, where an arm is deemed “correct” based on whether it was marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nPpn Incorrect Arm\n1\nThe proportion of the simulations where the incorrect correct arm has been selected, where an arm is deemed “incorrect” based on whether it wasn’t marked as “Should Succeed” for the relevant scenario on the VSR tab.\n\n\nQOI Columns\n\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary_freq_missingnesstype.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-summary_freq_missingnesstype.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of Summary_freq_{missingnessType}.csv",
    "text": "Contents of Summary_freq_{missingnessType}.csv\nThere is a frequentist summary file for each type of treatment of missing values.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContinuousDichotomous\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for continuous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nMean Theta &lt;dose&gt;\nD\nThe mean response per dose.\n\n\nSE Theta &lt;dose&gt;\nD\nThe standard error of the response per dose\n\n\nMean Beta (b aseline)\n1\nThe mean of the estimate of the baseline coefficient.\n\n\nSE Mean Beta (b aseline)\n1\nThe standard error of the estimate of the baseline coefficient\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nBias &lt;dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\nCoverage &lt;dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Dunnett &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Dunnett &lt;dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Bonf &lt;dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.\n\n\n\n\n\n\nContents of the summary_freq_{missingnessType}.csv file for dichotomous simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nMean Theta &lt;dose&gt;\nD\nThe mean response per dose.\n\n\nSE Theta &lt;dose&gt;\nD\nThe standard error of the response per dose\n\n\nPpn Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nBias &lt;dose&gt;\nD\nThe difference between the estimated mean response and the true (simulated) mean response per dose\n\n\nCoverage &lt;dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true mean response used to simulate subject responses.\n\n\nPpn Signif Dunnett\n1\nThe proportion of simulations where at least one of the Dunnett adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Dunnett &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Dunnett adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Dunnett &lt;dose&gt;\nD\nThe proportion of simulations where the Dunnett adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Bonf\n1\nThe proportion of simulations where at least one of the Bonferroni adjusted p-values is less than the user specified one-sided alpha.\n\n\nPpn Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the Bonferroni adjusted p-value is less than the user specified one-sided alpha.\n\n\nCoverage Bonf &lt;dose&gt;\nD\nThe proportion of simulations where the Bonferroni adjusted confidence interval contains the true mean response used to simulate subjects responses.\n\n\nPpn Signif Trend\n1\nThe proportion of simulations where the trend test p-value is less than the user specified one-sided alpha.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nRandom Number Seed\n1\n✔\n✔\nBase random number seed.\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc \nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) \nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nMean resp \nD\n✔\n✔\nThe estimated response of each treatment arm.\n\n\nSD Mean resp \nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm.\n\n\nTrue Mean resp \nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation.\n\n\nTrue SD resp \nD\n✔\n✔\nThe true SD of the response of each treatment arm for this simulation\n\n\nSigma\n1\n✔\n✔\nThe pooled estimate of the SD of the response across all the treatment arms\n\n\nSD_Sigma\n1\n✔\n✔\nThe SD of the pooled estimate of the SD of the response across all the treatment arms\n\n\nBeta\n1\n✔\n✔\nThe estimate of ‘Beta’ the baseline adjustment coefficient, if baseline adjustment is being used.\n\n\nSD_Beta\n1\n✔\n✔\nThe SD of the estimate of ‘Beta’ the baseline adjustment coefficient.\n\n\nMean Baseline\n1\n✔\n✔\nThe estimate of the mean baseline score.\n\n\nSE Mean Baseline\n1\n✔\n✔\nThe standard error of the estimate of the mean baseline score.\n\n\nSD Baseline\n1\n✔\n✔\nThe estimate of the SD of the baseline score.\n\n\nTrue Mean Baseline\n1\n✔\n✔\nThe true mean of the simulated baseline score\n\n\nTrue SD Baseline\n1\n✔\n✔\nThe true SD of the simulated baseline score\n\n\nMean Raw Response \nD\n✔\n✔\nThe observed mean response on each treatment arm (unadjusted by any modeling).\n\n\nSE Mean Raw Response \nD\n✔\n✔\nThe SE of the estimate of the mean raw response on each dose.\n\n\nComplete \nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis.\n\n\nComplete Information \nD\n✔\n✔\nThe number of subject who count as complete for the purposes of timing interims – whether complete or opportunity to complete and at which visit is as defined on the Interims tab. If interims are by enrolment then CompleteInformation is the number of subjects that have completed – final endpoint data is available.\n\n\n#Dropouts  \nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit.\n\n\nDR Param \n10\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 10 parameters, most less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the index “”.\n\n\nSd DR Param \n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nLongmod_   \nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. For linear regression the parameters reported are: - Alpha – per visit – the mean estimate of the constant offset in the change in response from this visit to the final visit - Beta – per visit – the mean estimate of the coefficient of change in response from this visit to the final visit - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. For time course hierarchical the parameters reported are: - Alpha – per visit – the mean estimate of the exponential coefficient of the proportion of the dose response and per-subject response seen at this visit.. - Lambda – per visit – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit. - Tau – the mean estimate of the SD of the per subject random effect For ITP the parameters are: - K – per model – the mean estimate of the ITP shape parameter - Tau – per model - the mean estimate of the SD of the per subject random effect - Lambda – per model – the mean estimate of the Sd of the residual error. - Omega – per treatment arm – the mean estimate of the mean treatment arm effect. For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp:  \nD * V\n✔\n✔\nThese are only calculated and written out if the Time Course Hierarchical or ITP longitudinal models are being used. This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses.\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time \nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nIn simulations file\nIn weeks file\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation\n\n\nLastInterim\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:1 = Early success2 = Late success3 = Late futility4 = Early futility5 = Success to futility flip-flop6 = Futility to success flip-flop7 = Inconclusive\n\n\nEarly Success Time\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time\n\n\nAlloc \nD\n✔\n✔\nThe number of subjects allocated to each arm\n\n\nPr(Alloc) \nD\n✔\n✔\nThe probability of allocation to the different arms following the interim\n\n\nMean resp \nD\n✔\n✔\nThe estimated response of each treatment arm\n\n\nSD resp \nD\n✔\n✔\nThe standard deviation of the estimate of response of each treatment arm\n\n\nMean resp (lower CI) \nD\n✔\n✔\nThe lower bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nMean resp (upper CI) \nD\n✔\n✔\nThe upper bound of the 95% Credible Interval of the estimate of response for each treatment arm\n\n\nTrue Mean resp \nD\n✔\n✔\nThe true mean response of each treatment arm for this simulation\n\n\nMean Raw Response \nD\n✔\n✔\nThe observed rate of response on each treatment arm (unadjusted by any modeling)\n\n\nComplete \nD\n✔\n✔\nThe number of completed subjects on each treatment arm at the time of the analysis\n\n\nComplete Information \nD\n✔\n✔\nThe number of subjects that have achieved the information criteria as specified on the Interims tab. May be the number enrolled, complete, or with the opportunity to complete. If complete or opportunity to complete, then the visit that should be complete is also specified.\n\n\n#Dropouts  \nD * V\n✔\n✔\nThe number of subjects dropped out on each treatment arm and each visit\n\n\nDR Param \n4\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled \\(\\alpha_1\\), \\(\\alpha_2\\), .. the subscripts corresponding to the column their value appears in here\n\n\nSd DR Param \n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter\n\n\nLongmod_   \nLM * LMP * V\n✔\n✔\nThe estimate of the mean value of each of the longitudinal model parameters. The number of models depends on the number of “model instances” the user has specified. Each value is reported per visit unless otherwise stated.For Beta-Binomial the parameters reported are:• Alpha0 & Beta 0 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 0• Prob01 – the mean probability of the Beta(Alpha0, Beta0) – the probability of 1 being the final result if the result at the visit is 0• Alpha1 & Beta 1 – The beta binomial alpha & beta parameters for the probability that the final result will be 1 if the result at the visit is 1• Prob11 – the mean probability of the Beta(Alpha1, Beta1) – the probability of 1 being the final result if the result at the visit is 1For logistic regression the parameters reported are: • Prob11 – the probability of 1 being the final result if the result at the visit is 1and • Prob01 – the probability of 1 being the final result if the result at the visit is 0For restricted Markov the parameters reported are:• Alpha0 – Alpha for state 0• AlphaS – Alpha for stable state• Alpha1 – Alpha for state 1• Prob0 – Transition probability to state 0• ProbS – Probability of remaining stable• Prob1 – Transition probability to state 1(values are for the transition to the next visit, so thre are no values for the final visit)If using a dichotomized continuous response, these columns will be for the selected continuous longitudinal model:For linear regression the parameters reported are:• Alpha – the mean estimate of the constant offset in the change in response from this visit to the final visit• Beta – the mean estimate of the coefficient of change in response from this visit to the final visit• Lambda – the mean estimate of the SD of the residual error term in the predicted final visit response based on the response at this visit.For ITP the parameters are:• K – single value per model – the mean estimate of the ITP shape parameter• Tau – single value per model - the mean estimate of the SD of the per subject random effect• Lambda – single value per model – the mean estimate of the Sd of the residual error.• Omega – per treatment arm not visit – the mean estimate of the mean treatment arm effect.For LOCF & Kernel Density there are no estimated parameters.\n\n\nLongmod Resp:  \nD * V\n✔\n✔\nThese are only calculated and written out if the ITP longitudinal model is being used for a dichotomized continuous endpoint.This is the mean response for a particular dose at a particular visit.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean comparator responses\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown\n\n\nArm Drop Time \nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit\n\n\nQOI Columns\n\n✔\n✔\nSee next section.\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns are grouped in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-simulations_freq_missingnesstype.csv-weeks_freq_missingnesstype.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv",
    "text": "Contents of Simulations_freq_{missingnessType}.csv, Weeks_freq_{missingnessType}.csv\nThese file contain the frequentist results after an interim analysis (weeks file) or final analysis (simulations file), apart for the first column, the columns are the same in the two types of file, and the same across the different methods for treating missing data.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation (simulations_freq_…csv only)\n\n\n# Weeks\n1\nThe week of the interim (weeks_freq_…csv only)\n\n\nTheta\nD\nUnadjusted estimate of response per treatment arm\n\n\nTheta SD\nD\nThe SD of the estimate of response per treatment arm\n\n\nTrt Effect\nD\nThe relative size of the treatment effect compared to control per arm (treatment effect for control is always 0). The direction of the comparison takes into account whether a response represents a subject’s condition improving or worsening, so that a positive treatment effect shows the response on the dose is better than on control.\n\n\nt-Stat\nD\nThe t-test statistic per treatment arm (for control this is always 0)\n\n\np-Value\nD\nThe unadjusted t-test p-value per arm (for control this is always 0)\n\n\nlower CI\nD\nThe unadjusted lower bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nupper CI\nD\nThe unadjusted upper bound of the alpha confidence interval of the estimated treatment effect per treatment arm (for control this is always 0).\n\n\nMin p-Value\n1\nThe smallest of the t-test p-values\n\n\np-Value Dunnett\nD\nThe Dunnett adjusted t-test p-value per arm (for control this is always 0).\n\n\nlower CI Dunnett\nD\nThe Dunnett adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper CI Dunnett\nD\nThe Dunnett adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Dunnett\n1\nThe smallest of the Dunnett adjusted p-values.\n\n\np-Value Bonf\nD\nThe Bonferroni adjusted t-test p-value per arm.\n\n\nlower-CI Bonf\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nupper-CI Bonf\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the estimated treatment effect (for control this is always 0).\n\n\nMin p-Value Bonf\n1\nThe smallest of the Bonferroni adjusted t-test value per arm.\n\n\nt-Stat trend\n1\nThe t statistic from the trend test.\n\n\np-Value trend\n1\nThe p-value from the trend test.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-patientsnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nBaseline\n1\nSubject baseline response, if simulated. If not simulated, then fixed at -9999.\n\n\nVisit \nV\nSubject’s response at each visit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nSubject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in.\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nLastVisit#\n1\nThe index of the last visit for which the subjects data was collected, if the subject did not complete all of its visits it could be because they dropped-out, the subject’s treatment arm was dropped (and there was no follow-up), or the study stopped early (and there was no follow-up).\n\n\nBaseline\n1\nAlways -9999 since there’s no baseline in dichotomous response trials.\n\n\nVisit \nV\nSubject’s response at each visit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/contanddichot.html#contents-of-mcmcnnnnn.csv",
    "title": "Continuous and Dichotomous Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nContinuousDichotomous\n\n\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nTheta \nD\nThe estimate of the mean response for each dose, based on the dose response model fitted.\n\n\nSigma\n1\nThe estimate of the SD of the endpoint.\n\n\nBeta\n1\nIf baseline is included in the simulated data and using the “Baseline Adjusted Model”, this is the value of “Beta”, the baseline coefficient.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod   \nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\n\n\nIf using a dichotomized continuous variable, then the parameters for the LM are the same as for the continuous model MCMC output listed in the Continuous tab.\nIf using a dichotomous longitudinal model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi \nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod   \nL*P*V\nIf the analysis includes longitudinal modelling then the samples of the parameters of the longitudinal models are output. The number of models depends on the number of different longitudinal fits the user has specified (one over all arms, one for control and one for all treatment arms, model all arms separately, etc.), the number of parameters the model has, and if these are fitted per visit (which depends on the model being fitted), then the number of visits.\n\n\n\nIf using a restricted markov model, then the following parameters are output in the MCMC file.\n\n\n\n\n\n\n\n\nColumn\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation.\n\n\nSample\n1\nThe index of the sample within the analysis.\n\n\nPi \nD\nThe estimate of the response rate for each dose, based on the dose response model fitted.\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nLongmod   \nL*P*V\nCurrently the values of Alpha0, Alpha1, AlphaS (the parameters to the Dirichlet distribution) are (unnecessarily – these values are not sampled) output, along with the values for P0, P1 and PS the transition probabilities of each model at each visit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Continuous/Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html",
    "href": "documentation/v71/userguides/core/vsr/continuous.html",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "",
    "text": "In FACTS Core with a continuous endpoint there are 2 different ways to specify the virtual subject response:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html#dose-response",
    "href": "documentation/v71/userguides/core/vsr/continuous.html#dose-response",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Dose Response",
    "text": "Dose Response\n\nDose response profiles can be added and deleted, and for each profile the user specifies:\n\nThe mean response for each treatment arm. If baseline is being simulated the user can select (on the Study tab) whether the response to be analyzed is the change from baseline or absolute response, and depending on that selection the response specified on this tab is either change from baseline or absolute response.\nThe standard deviation of the response – either through a common SD of response for all treatment arms, or by specifying the standard deviation for the response on each treatment arm separately.\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario.FACTS uses this value to report on the proportion of simulations that were successful and selected a ‘good’ treatment arm. The check box values do not effect the simulation code, just how output is reported.\n\nThe graph on the Explicitly Defined &gt; Dose Response tab shows the mean response specified for each treatment arm +/- 1.96 SD, and the control response + the default CSD level specified on the QOI tab.\nIf a 2D treatment arm model is being used, the doses are listed in “effective dose strength order” as was defined on the treatment arm tab.\n\n\nLoad Scenario Means From File\nIf the “Load scenario means from a file” option is selected, then in scenarios using this profile the simulations will use a range of dose responses instead of what is specified in the table.\nEach individual simulation uses one set of mean responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstance. Note that, to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\nAfter selecting the “.mvsr” file the graph shows the individual mean responses and the overall mean response over all the VSRs.\nThe format of the file is a simple CSV text file. Lines starting with a ‘#’ character are ignored so the file can include comment and header lines. There must be two columns per treatment arm giving the mean and SD of the change from baseline on each arm, the columns must be grouped first means then SDs and within each group they must be in dose index order. E.g.:\n#Cntrl, D1, D2, D3, Cntrl, D1, D2, D3\n-1, -1, -1, -1, 5, 5, 5, 5\n-1, -1.2, -1.4, -1.6, 5, 5, 5, 5\n-1, -1.8, -2.5, -2.5, 5, 5, 5, 5\n-1, -2, -2.5, -3, 5, 5, 5, 5\n-1, -2.5, -3.25, -3.25, 5, 5, 5, 5\n-1, -2.5, -3.25, -2.5, 5, 5, 5, 5",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html#longitudinal-vsr",
    "href": "documentation/v71/userguides/core/vsr/continuous.html#longitudinal-vsr",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Longitudinal VSR",
    "text": "Longitudinal VSR\nThe explicitly defined virtual subject longitudinal responses can be specified with any of 3 methods. No matter which longitudinal VSR simulation method is selected, it can be combined with any Dose Response VSR, and the dose response VSR is guaranteed to have the marginal distribution specified in the dose response VSR section. The longitudinal VSR determines how to correlate early endpoint values with the final endpoint value.\nThe three longitudinal VSRs available for continuous endpoints are:\n\nCorrelated\nHierarchical, which comes in two ‘flavors’:\n\nHierarchical (as in versions of FACTS prior to 6.1): the per subject random element ‘delta’ is scaled at visit ‘t’ by the response fraction ft at that visit.\nHierarchical MMRM (first available in FACTS 6.1): the per subject random element ‘delta’ is the same at all visits.\n\nITP\n\n\nCorrelated Simulation of Longitudinal Data\nThe Correlated method for generating longitudinal responses simulates the response observed at each visit by summing 3 elements – a fraction of the final response, a fraction of the ‘noise’ from the previous observation, and additional element of noise at the current visit.\n\nThe user specifies:\n\n\\(\\rho_{t}\\)\n\nthe correlation in the observation at visit \\(t\\) with visit \\(t - 1\\). Values should be in the range 0-1. The closer to 1, the greater the correlation between visit \\(t - 1\\) and visit \\(t\\). The \\(\\rho\\) at Visit 1 is not enterable, because there is no previous visit to correlate the first visit value with.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\varphi_{t}^{2}\\)\n\nthe fraction of the final standard deviation (SD) that will be observed at this visit. Values must be \\(&gt;0\\). An additional element of noise at visit \\(t\\) is sampled so that in combination with the noise carried forward from visit \\(t - 1\\), the overall variance in observations at visit \\(t\\) will have this fraction of the final variance.\n\n\nFor the first visit of patient \\(i\\), the response \\(y_{i,1}\\) assuming patient \\(i\\) was randomized to dose \\(j\\) is simulated as:\n\\[y_{i,1}\\ \\sim\\ N\\left( \\mu_{j,1},\\sigma_{j,1}^{2} \\right)\\]\nFor time points after the first visit, observed data is simulated as:\n\\[y_{i,t}\\ \\sim\\ \\mu_{j,t} + \\sqrt{1 - \\rho_{t}^{2}}N\\left( 0,\\sigma_{j,t}^{2} \\right) + \\rho_{t}\\left( y_{i,t - 1} - \\mu_{j,t - 1} \\right)\\frac{\\sigma_{j,t}}{\\sigma_{j,t - 1}} \\text{   for } t \\geq 2.\\]\nThere are 3 components of this equation. First, is the marginal mean of the response at the particular visit \\(t\\). This parameter is called \\(\\mu_{j,t}\\). \\(\\mu_{j,t}\\) is simply the dose response at the final endpoint times the proportion of the final effect that should be observed at visit \\(t\\).\n\\[\\mu_{j,t} = f_{t}\\mu_{j}\\]\nwhere \\(f_{t}\\) is input as the response fraction for each visit, and \\(\\mu_{j}\\) is the dose response for dose \\(j\\), which is input as the Response for each dose on the Dose Response tab.\nThe second component controls the adjustment of the variance based on the correlation of the endpoint to be sampled with the previous visit’s response. A strong correlation (\\(\\rho_{t}\\) close to \\(\\pm 1\\)) results in a variance reduction term \\(\\sqrt{1 - \\rho_{t}^{2}}\\) close to 0, which guarantees that the time \\(t\\) response is very close to the time \\(t - 1\\) response. The variance reduction term modifies a dose’s marginal visit variance \\(\\sigma_{j,t}^{2}\\). This dose by visit variance is calculated by squaring the marginal standard deviation for a dose’s final endpoint response \\(\\sigma_{j}\\) times the proportion of the total variance that is observed at visit \\(t\\), \\(\\phi_{t}\\), which is a user input on the Longitudinal VSR tab.\nSo, \\(\\sigma_{j,t}^{2} = \\phi_{t}^{2}\\sigma_{j}^{2}\\) where \\(\\phi_{t}\\) is the fraction of final SD specified in the Longitudinal VSR tab in FACTS, and \\(\\sigma_{j}\\) is the SD of the response specified in the Dose Response VSR tab in FACTS.\nThe final component of the model adjusts the mean of the visit \\(t\\) response that is to be simulated based on the residual of the time \\(t - 1\\) response. This component is:\n\\[\\rho_{t}\\left( y_{i,t - 1} - \\mu_{j,t - 1} \\right)\\frac{\\sigma_{j,t}}{\\sigma_{j,t - 1}}\\]\nValues of \\(\\rho_{t}\\) close to 1 lead to visit \\(t\\) responses with Z-scores similar to the visit \\(t - 1\\) responses, values of \\(\\rho_{t}\\) close to 0 lead to simulation of visit \\(t\\) responses with no regard to the previous visit’s residual value, and values of \\(\\rho_{t}\\) close to -1 lead to visit \\(t\\) responses with Z-scores that are -1 times the previous visit’s Z-score.\nThe specification of the visit level means \\(\\mu_{j,t}\\ \\)and variances \\(\\sigma_{j,t}^{2}\\) as response fractions and fractions of final SD, respectively, allows for each created longitudinal VSR to work with every created Dose Response VSR.\n\n\nHierarchical model\nThe hierarchical method for generating longitudinal responses simulates the response observed by sampling responses from a Normal distribution, where the mean is a combination of the mean final response of the treatment and a per-subject difference, scaled by a visit dependent coefficient, and the variance is a fraction of the variance of the final treatment effect.\nThis Hierarchical form of this model has a per-subject random effect parameter \\(\\delta_{i}\\) that is scaled down by the response scaling parameter \\(f_{t}\\). In the very similar Hierarchical (MMRM) model (described below), the random effect \\(\\delta_{i}\\) is not scaled by \\(f_{t}\\), so it provides a constant adjustment to all visits.\n\nIn the Hierarchical longitudinal subject data simulation model, the response variance is decomposed into two components. One is the within-subject variability (called intra-subject variability), and the other is across-subject variability (Called inter-subject variability). The parameter \\(\\omega\\) determines how much of the total variability is simulated in the inter- and intra-subject variabilities. More variability in the inter-subject simulation means that there is less variability in the intra-subject simulation, so the early visit endpoints will be more correlated with the final visit endpoint, and vice versa.\nThe parameters of the Hierarchical longitudinal subject data simulation model are:\n\n\\(\\omega\\)\n\nthe fraction of the variance of the final response (\\(\\sigma_{j}^{2}\\)) on the treatment arm that will be simulated in the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\phi_{jt}^{2}\\)\n\nthe fraction of the final endpoint variance that will be observed at this visit. Values must be \\(&gt;0\\) and must be such that \\(\\phi_{jt}^{2} - f_{t}^{2}\\omega\\) is \\(&gt;0\\).\n\n\nObservation \\(y_{it}\\) is the visit response at visit \\(t\\) for a subject \\(i\\) that was randomized from dose \\(j\\). The observation is generated from the distribution described by:\n\\[y_{i,t} = f_{t}\\left( \\mu_{j} + \\delta_{i} \\right) + N\\left( {0,\\ \\left( \\phi_{jt}^{2} - f_{t}^{2}\\omega \\right)\\ \\sigma}_{j}^{2} \\right)\\]\nwhere \\(\\sigma_{j}^{2}\\) is the variance of the final endpoint response for dose \\(j\\), \\(\\phi_{jt}^{2}\\) is the fraction of total variance observed at visit \\(t\\) on dose \\(j\\), \\(\\mu_{j}\\) is the final endpoint response mean for dose \\(j\\), \\(\\delta_{i}\\) is a subject level random effect, \\(f_{t}\\) is the fraction of the final endpoint response that is observed at visit \\(t\\), and \\(\\omega\\) is the proportion of overall variance due to intersubject (across subject) variability.\nThe prior for the patient random effect term of patient \\(i\\) who was randomized to dose \\(j\\) is\n\\[\\delta_{ij}\\ \\sim\\ N\\left( 0,\\ \\omega\\sigma_{j}^{2} \\right)\\]\nIf \\(\\omega\\) is close to 1, then most of the variability in the responses comes from differences in participants and the visit-to-visit correlation within a participant’s follow-up is high. If \\(\\omega\\) is close to 0, then there is little correlation between visits within a participant’s follow-up, and most of the overall variance comes from noise in the within patient responses, rather than differences across patients. In other words, large values of \\(\\omega\\) lead to early data that is more predictive of the final endpoint.\nNote that, in this model the response fraction \\(f_{t}\\) is multiplied by both the final endpoint mean and the patient level random effect. This results in the variance of the dose \\(j\\) response at visit \\(t\\) being \\[{\\left( \\phi_{jt}^{2} - f_{t}^{2}\\omega \\right)\\sigma}_{j}^{2} + f_{t}^{2}\\omega\\sigma_{j}^{2} = \\phi_{jt}^{2}\\sigma_{j}^{2}.\\]\nAdditionally, since \\(f_{t}\\) changes the proportion of the overall variance that comes from the random effect, the simulated correlation between visits decreases when values of \\(f_{t}\\) less than 1 are provided. See the MMRM version of the Hierarchical simulation model if this is undesirable.\nIf all \\(\\phi_{jt}^{2} = 1\\) and \\(f_{t} = 1\\), then the visits will have pairwise correlations equal to \\(\\omega\\). If the \\(\\phi_{jt}^{2}\\) or \\(f_{t}\\) are less than 1, then the visit pairwise correlations will depend on the input variance fractions \\(\\phi_{jt}^{2}\\) and \\(f_{t}\\).\n\n\nHierarchical (MMRM) model\nThe Hierarchical (MMRM) longitudinal patient simulation model is very similar to the Hierarchical simulation method, except that in the MMRM version the response fraction for a visit does not modify the patient level random effect. The user inputs for the Hierarchical (MMRM) model are nearly identical to the plain Hierarchical model.\n\n\\(\\omega\\)\n\nthe fraction of the variance of the final response on the treatment arm (\\(\\sigma_{j}^{2}\\)) that will be simulated in the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(f_{t}\\)\n\nthe fraction of the final mean response \\(\\mu_{j}\\) seen at visit \\(t\\). Values can lie outside the range 0-1.\n\n\\(\\phi_{jt}^{2}\\)\n\nthe fraction of the final endpoint variance that will be observed at this visit for dose \\(j\\), values must be &gt;0 and must be such that \\(\\phi_{jt}^{2} - \\omega\\) &gt; 0.\n\n\nThe Hierarchical MMRM method simulates responses at visit \\(t\\) for a subject \\(i\\) that was randomized to dose \\(j\\) from the distribution:\n\\[y_{i,t} = f_{t}\\mu_{j} + \\delta_{i} + N\\left(0,\\ \\left( \\phi_{jt}^{2} - \\omega \\right)\\ \\sigma_{j}^{2} \\right)\\].\nwhere \\(\\sigma_{j}^{2}\\) is the variance of the final endpoint response for dose \\(j\\), \\(\\phi_{jt}^{2}\\) is the fraction of total variance observed at visit \\(t\\) on dose \\(j\\), \\(\\mu_{j}\\) is the final endpoint response mean for dose \\(j\\), \\(\\delta_{i}\\) is a subject level random effect, \\(f_{t}\\) is the fraction of the final endpoint response that is observed at visit \\(t\\), and \\(\\omega\\) is the proportion of overall variance due to intersubject (across subject) variability.\nThe prior for the patient random effect term of patient \\(i\\) who was randomized to dose \\(j\\) is\n\\[\\delta_{ij}\\ \\sim\\ N\\left( 0,\\ \\omega\\sigma_{j}^{2} \\right)\\]\nIf \\(\\omega\\) is close to 1, then most of the variability in the responses comes from differences in participants and the visit-to-visit correlation within a participant’s follow-up is high. If \\(\\omega\\) is close to 0, then there is little correlation between visits within a participant’s follow-up, and most of the overall variance comes from noise in the within patient responses rather than differences across patients. In other words, large values of \\(\\omega\\) lead to early data that is more predictive of the final endpoint.\nNote that, in this model the response fraction \\(f_{t}\\) is multiplied by only the final endpoint mean and not the patient level random effect. This results in the variance of the dose \\(j\\) response at visit \\(t\\) being \\[{\\left( \\phi_{jt}^{2} - \\omega \\right)\\sigma}_{j}^{2} + \\omega\\sigma_{j}^{2} = \\phi_{jt}^{2}\\sigma_{j}^{2}.\\]\nThis total variance is the same as the non MMRM Hierarchical method, but only the \\(\\phi_{jt}^{2}\\) parameter effects the variance of early endpoint responses. If all \\(\\phi_{jt}^{2} = 1\\), then the visits will have pairwise correlations equal to \\(\\omega\\). If the \\(\\phi_{jt}^{2}\\) are less than 1, then the visit pairwise correlations will be larger than \\(\\omega\\), with the exact value depending on the input variance fractions \\(\\phi_{jt}^{2}\\).\n\n\nIntegrated Two Component Prediction (ITP) Simulation of Longitudinal Data\nThe ITP method for generating longitudinal responses simulates the response observed at each visit by summing 3 elements and scaling them by an exponential function. The 3 elements are: the mean final response, an element of inter-subject variability, and a residual variability at the current visit.\n\nThe user specifies\n\n\\(\\omega_{j}\\)\n\nfraction (for each dose) of the variance of the final response on the treatment arm (\\(\\sigma_{j}^{2}\\)) used for the inter-subject variance. The higher this value is, the more predictive a subject’s early observations are of their final outcome.\n\n\\(k_{j}\\)\n\nthe shape parameter (for each dose) of the exponential component governing the increase in the observed response. The values of \\(k\\) should be scaled to take into account the length of time (in weeks) to the intermediate and final visits. See below for more intuition on sensible values of \\(k\\).\n\n\nFor subject \\(i\\) at visit \\(t\\), who was randomized to dose \\(j\\), the response \\(y_{it}\\) is simulated as:\n\\[y_{it} = \\left( \\mu_{j} + s_{i} + \\epsilon_{it} \\right)\\left( \\frac{1 - \\text{exp}\\left( k_{j}x_{t} \\right)}{1 - \\text{exp}\\left( k_{j}x_{T} \\right)} \\right)\\]\nwhere \\(\\mu_{j}\\) is the mean of the final endpoint on dose \\(j\\), \\(s_{i}\\sim N\\left( 0,\\ \\omega_{j}\\sigma_{j}^{2} \\right)\\) is a subject specific random effect, each \\(\\epsilon_{it}\\sim N\\left( 0,{\\ \\sigma}_{j}^{2}\\left( 1 - \\omega_{j} \\right) \\right)\\) is a residual error, \\(k_{d}\\) is a shape parameter, \\(x_{t}\\) are the visit times that the \\(y_{it}\\) are observed, and \\(x_{T}\\) is the time of the final endpoint.\nThe ITP model implies that the variance of the observations shrinks towards 0 with the mean (so early visits have reduced expected responses and variances).\nThe shape parameter \\(k\\) determines the rate at which the final endpoint’s eventual effect is observed during a subject’s follow-up. A value of \\(k = 0\\) indicates that the proportion of effect observed moves linearly with time. A value of \\(k &lt; 0\\) means that the eventual final effect is observed earlier in follow-up and plateaus off as time moves towards the final endpoint. A value of \\(k &gt; 0\\) indicates that less of the total final endpoint effect is observed early in follow up, but as time approaches the final endpoint time the proportion of the effect observed increases rapidly. Values of \\(k\\) less than 0 tend to be more common than values of \\(k\\) greater than 0. See the figure below for a collection of possible shapes of the change in response using different values of \\(k\\).\nAdditionally, unlike the Correlated or Hierarchical simulation methods, the ITP method uses the actual visit time to simulate subject values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/continuous.html#baseline-vsr",
    "href": "documentation/v71/userguides/core/vsr/continuous.html#baseline-vsr",
    "title": "Virtual Subject Response - Continuous Endpoint",
    "section": "Baseline VSR",
    "text": "Baseline VSR\nIf simulation of baseline has been included on the Study &gt; Study Info tab, a new virtual subject response tab is available for specifying the baseline score.\n\nThe simulation of distribution of baseline scores is specified using a normal distribution with user specified mean and standard deviation, and optionally applied upper and lower bounds to reflect limitations on the score range or screening criteria. If the simulated baseline score is truncated, then the true mean and SD of the baseline are likely to be different from these values of the mean and SD which are before truncation.\nIf the response is chosen to be change from baseline on the Study Info tab, then the dose response VSR is specified as change from baseline, and the raw endpoint for a subject will actually be their baseline value plus their simulated change from baseline value. If the response is chosen to be Final endpoint value on the Study Info tab, then the dose response VSR specifies the direct distribution that the final endpoint will be sampled from, and changing the baseline distribution does not effect the final endpoint distribution.\nWhether the final endpoint is change from baseline or final endpoint value, it is possible to adjust the final response based on the simulated baseline value. To do so, the user selects “Use Baseline Adjustment for Subject Response” and supplies 3 parameters:\n\n\\(\\beta\\)\n\na coefficient that reflects the degree of influence of baseline on final score and the degree of variability in the final score due to baseline.\n\nc\n\na centering offset, typically the expected mean of the observed baseline scores\n\ns\n\na scaling element, typically set to the expected SD of the baseline.\n\n\nBe aware that performing a baseline adjustment for subject response can change the marginal distribution of the dose response VSR.\n\nExample\nIn the above screenshot a baseline of mean 25 and SD 10 has been specified for the distribution of the baseline values, so a centering of \\(c=25\\) and scaling of \\(s=10\\) is used. Wishing to simulate an overall SD of 5 in the final change from baseline and apportion two-thirds the variance to baseline, \\(\\beta\\) has been set as follows:\n\nThe desired final variance is 25 (\\(5^2\\)), divided into 1/3 dose response and 2/3 baseline effects.\nThe SD of the simulated response is set to \\(\\sqrt{\\left( 25*\\frac{1}{3} \\right)} = 2.89\\)\nThe SD of the scaled baseline score is 1, so to contribute half the final variance of 25, Beta is set to \\(\\sqrt{\\left( 25*\\frac{2}{3} \\right)} = 4.08\\)\nNote that when simulating a baseline effect in this way, limiting the range of baseline by specifying upper and lower cut-offs – which might be natural limits of the endpoint, or due to inclusion / exclusion criteria in the protocol – can significantly reduce the variance in the final endpoint due to the baseline effect.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/index.html",
    "href": "documentation/v71/userguides/core/vsr/index.html",
    "title": "Virtual Subject Response",
    "section": "",
    "text": "The Virtual Subject Response (VSR) tab allows the user to explicitly define virtual subject response profiles that dictate the distribution that patients’ endpoints should be simulated from. VSRs can be specified explicitly by specifying the control distribution and treatment effects, or by importing externally simulated patient responses.\nIf specifying the VSR in the “Explicitly Defined” tab you will always have the dose response VSR to specify, and if subjects have multiple visits, then a Longitudinal VSR must be specified as well. If using a continuous endpoint and simulating a baseline, then a baseline simulation VSR must be specified as well.\nA dose response VSR specifies how all arms in the study should have their final endpoint value simulated. For continuous endpoints this is the mean and standard deviation of the normal distribution. For dichotomous endpoints it is the probability of response, and for time-to-event endpoints it is the hazard rate for each arm.\nThe longitudinal VSR dictates how subject visits are correlated with the final endpoint value. Each endpoint type has different methods of simulating longitudinal correlation, which are explained in detail in the following sections.\nIt is common, and advisable, to create a variety of VSR scenarios. Each scenario is a combination of a dose response VSR, a longitudinal VSR (if it exists), and a baseline VSR (if it exists). Generally, at least one null VSR, and a set of alternative scenarios are created. In a null scenario the treatment arms have the same efficacy profile as the control arm. Alternative VSRs have treatments with a variety of treatment effects, usually with treatment arms simulated to be better than control.\nIf an external file is used to specify the subject responses to be simulated, it replaces the dose response, longitudinal, and baseline profiles specified in an explicitly defined VSR tab. An entire sequence of visit responses for a subjects is drawn from the uploaded patient responses file. This is elaborated on more in each endpoint’s VSR description.\nEach endpoint type (Continuous, Dichotomous, and Time-to-event) has its own method for specifying the dose response VSR and the visit to visit correlation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois.html",
    "href": "documentation/v71/userguides/core/qois.html",
    "title": "Quantities of Interest (QOI) Tab",
    "section": "",
    "text": "The quantities of interest tab allows the user to specify the Bayesian posterior quantities and frequentist p-values that are eligible to be used to make decisions in the trial as well as being output to the simulations results files.\nThere are 3 classes of QOI:\nTrial decisions at interim analyses or the final analysis are made based on decision quantities, but adaptations like adaptive allocation can be based on posterior probabilities, predictive probabilities, and target probabilities.\nNote that to creating a QOI for early stopping or final evaluation decisions will involve using 2 or 3 QOIs:\nThere are a number of pre-defined, default QOIs which simplifies the specification of the most commonly used decision quantities, and the importation of past FACTS designs. These are:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest (QOI) Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois.html#posterior-probabilities",
    "href": "documentation/v71/userguides/core/qois.html#posterior-probabilities",
    "title": "Quantities of Interest (QOI) Tab",
    "section": "Posterior Probabilities",
    "text": "Posterior Probabilities\nThese are Bayesian quantities to be calculated at each interim and at the final analysis.\n\n\n\n\n\n\nFigure 2: Add dichotomous posterior probabilities page.\n\n\n\nA Posterior Probability is specified by providing a value for each of the following:\n\nCompare:\n\nContinuous: Means\nDichotomous: Rates or Log-odds\nTime-to-Event: Hazard Ratio or Hazard Rates.\n\nCondition: “&gt;” or “&lt;” a comparison value.\nRelative to an absolute value or relative to the response on a specific dose.\nThe comparison can include a delta, which is the absolute value to be compared against if t the difference relative to the comparison arm is compared to. he comparison is absolute, or a value that\n\nThe QOI will be given a “name” derived from these details and a short or Alternative name that will be used in the output files to allow easier access from other software, e.g. from within R.\nIf the endpoint is TTE and the design includes a predictor endpoint, then the definition of the QOI includes the specification of which endpoint it applies to.\n\n\n\n\n\n\nSpecial Qualities of Posterior Probabilities in Time-to-Event\n\n\n\n\n\nThere are a couple of aspects of posterior probabilities that are slightly different for posterior probability QOIs.\nFirst, in FACTS Core TTE a Posterior Probability QOI (a Bayesian comparison of estimates of response) can be used to compare Hazard Ratios or Hazard Rates. In their descriptions Hazard Ratio QOIs use “HR” (e.g. “HRd &lt; 1”) and Hazard Rate QOIs use \\(\\lambda\\) (e.g. “\\(\\lambda_d\\) &lt; \\(\\lambda_{control}\\)”). If there are hazard rate QOIs defined, then FACTS restricts the hazard model (defined on the Design &gt; Hazard Model tab) to use just a single segment (so that there is just one lambda to compare). This is set in the “Com are” box of the QOI definition window:\n\n\n\n\n\n\nFigure 3: Specifying a posterior probability QOI based on hazard rate.\n\n\n\nAdditionally, in FACTS Core TTE when defining a Posterior Probability QOI (a Bayesian comparison of estimates of response) or a Target QOI (a Bayesian assessment of which treatment arm is most likely to fulfill a ‘target’ criteria) – the user can select whether the evaluation uses the Final Event endpoint, or the Predictor.\n\n\n\n\n\n\nFigure 4: Specifying a posterior probability QOI based on the predictor endpoint.\n\n\n\n\n\n\n\nNotes on setting Deltas\nFor the three endpoint types, delta’s are defined as:\n\nContinuous\n\nA CSD (Clinically Significant Difference) in the estimates of the mean response.\n\nDichotomous\n\nA CSD in the estimate of the response rates if Rates is selected in the QOI, and of log-odds of the response rate if Log-odds is selected in the QOI.\n\nTime-to-Event\n\nA CSHRD (Clinically Significant Hazard Ratio Difference) in the estimate of the Hazard Ratio\n\n\nA standard hypothesis test for demonstrating superiority to control uses a delta of 0. Testing for superiority with a non-zero delta is different, and the implications need to be carefully understood. Where the term CSD is used in this document, it should be taken to refer to CSHRD as well unless it is specifically stated otherwise.\nWhen setting a target treatment difference from control for the study to beat, it is also common to require a degree of confidence that the target has been beaten. To achieve posterior probabilities of &gt;50% that the target has been beaten, the estimated mean difference will have to be greater than the target difference.\nThus, when setting a delta, we are setting up two hurdles the study drug must beat, first the delta and then on top of that an additional margin to give a &gt;50% confidence that the margin has been beaten. Thus, it is necessary to avoid setting the target delta too large. A common mistake is to set the delta to the ‘expected difference’ (the value that might have been used in a conventional sample size calculation). In scenarios where the simulated response is equal to the ‘expected difference’ and hence the CSD this will give probabilities of being better than control by the delta of 50% on average, regardless of the sample size.\nIt is inadvisable to require a posterior probability of 50% that the response is better than the Control by the delta margin as this turns the test into one that simply depends on whether the point estimate of the response is better.\nIt is inadvisable to require posterior probability of less than 50% for success, as such criteria have the undesirable characteristic that they can be met in circumstances where it can be seen that if further data was gathered consistent with what has already been seen, it would lead the threshold no longer being met! The posterior distribution would shrink so that there was no longer sufficient of the tail above the CSD.\nIt is better therefore to use a delta that is less than the expected difference that it is hoped to achieve, somewhat like having a non-inferiority margin around the target. So, when the design is simulated with a response at the expected difference the target will be clearly exceeded. A useful default value to use for a delta is half the ‘expected difference’. This usually yields “comprehensible” probability thresholds for both success and futility.\nUsing a delta has benefits however – simply being better than control but for very small difference can be established simply by having large sample sizes. Being better than control by a delta establishes some confidence that there is some patient benefit. Use of a delta is also useful if otherwise success thresholds “well out in the tail” (e.g. &gt; 0.99) are required such as the equivalent of an early look in a conservative Group Sequential design. Though the use of a delta does mean that there is a less direct equivalence to a p-value.\n\n\nP-value Delta’s\nSeparately from the CSD/CSHRD delta, with a continuous or dichotomous endpoint (but not time-to-event), a frequentist super-superiority/non-inferiority delta can be specified.\nThese use the same selection of super-superiority/non-inferiority as the CSD\n\n\n\n\n\n\nFigure 5: The “Standard Evaluation Variables” options at the bottom of the QOI page.\n\n\n\nCurrently, unlike the CSD delta that is only applied to the default posterior probability QOI and MED target QOI, the p-value delta is applied to all the p-value QOIs and it cannot be overridden.\nThe value of the p-value delta is constrained to be 0 or positive. The ‘direction’ of the delta depends then on the direction of the endpoint (whether “higher/lower is better” or “a response is a positive/negative outcome”).\n\n\n\nSuper superiority and non-inferiority margin directions.\n\n\n\n\n\n\n\n\n\n\n\n\nHigher is better / Response is positive\n\n\nLower is better / Response is negative\n\n\n\n\n\n\nSuper-Superiority\n\n\nTrt – Control &gt; delta\n\n\nTrt – Control &lt; -delta\n\n\n\n\nNon-inferiority\n\n\nTrt – Control &gt; -delta\n\n\nTrt – Control &lt; delta\n\n\n\n\n\n\nP-value Comparisons with No Control Arm\nIf no control arm is included p-values, with a continuous or dichotomous endpoint, the treatment response is compared to a fixed value. The fixed value is specified as “Frequentist response/rate to compare to for p-value QOIs:” in the Standard Evaluation Variables section at the bottom of the QOIs tab. It is not currently possible to compare different p-value QOIs to different fixed responses or rates.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest (QOI) Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois.html#predictive-probabilities",
    "href": "documentation/v71/userguides/core/qois.html#predictive-probabilities",
    "title": "Quantities of Interest (QOI) Tab",
    "section": "Predictive Probabilities",
    "text": "Predictive Probabilities\nThere are two types of predictive probabilities –\n\nBayesian predictive probabilities, which are Bayesian predictions of frequentist outcomes and take into account the uncertainty around a set of parameters, and\nConditional Power, which aims to calculate the frequentist probability of success assuming a set of parameters to be true.\n\nThe primary difference between the Bayesian predictive probabilities and the conditional power calculations is that the Bayesian predictive probabilities are calculated taking the variability around the estimated treatment effect into account, while the conditional power calculations assume that the observed test statistic of past data is the exact effect that future data will be generated from.\nFor both Bayesian predictive probabilities and conditional power, we differentiate two types, predicting the outcome in the current trial and predicting the outcome in a future trial.\n\nBayesian predictive probabilities\n\nCurrent Trial Bayesian Predictive Probabilities\nIn the current trial, the outcome can be predicted under one of two assumptions:\n\nThat no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nThat the trial continues recruiting using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until complete.\n\nPredictive probabilities currently can only predict outcomes based on p-values. Note that since p-values are only calculated as comparisons against a control arm, predictive probabilities of success in the current trial are only available if the current trial includes a control arm.\n\n\n\n\n\n\nFigure 6: Add dichotomous predictive probability of current trial.\n\n\n\nThe user specifies how missingness is handled in the final analysis, the p-value test type – unadjusted, Bonferroni or Dunnett’s and the (one sided) alpha level for the significance test. The QOI assumes that the specified default p-value delta is used in the final test.\nThe predictive probability of the current trial at the maximum sample size is only available:\n\nIf the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation.\n\nThe predictive probability is calculated by simulating the remaining subjects, assuming their allocated at the current probabilities of allocation and simulating their final response based on the rate of response or (Normal) distribution of responses observed so far for each arm.\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\nThere is one simulation of the remaining data per MCMC loop, potentially significantly increasing the time to perform the overall simulation of the trial.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still perform the simulation of future subject outcomes based on posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\nCurrent Trial Bayesian Predictive Probabilities – Time-to-Event\nUnlike the Continuous and Dichotomous endpoints, to predict the probability of success at full enrollment with a Time-to-Event endpoint, it is also necessary to simulate the accrual rate (and hence how many events might be observed).\n\n\n\n\n\n\nFigure 7: Add time-to-event predictive probability of current trial.\n\n\n\nFor TTE, for a Predictive Probability of Success at Full Enrollment, there are new parameters to determine how accrual is modeled. There are 3 models for accrual\n\nFixed Rate, the parameters for this are:\n\nThe fixed (mean) accrual rate per week to simulate.\n\nEstimated from the Last ‘W’ Weeks accrual data, using a Poisson distribution, with a Gamma prior. The parameters for this are\n\nThe number of past weeks W to use the accrual data from.\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\nEstimated from all the accrual data from the start of the trial, using a Poisson distribution, with a Gamma prior, the parameters for this are:\n\nThe prior mean for the model of the accrual rate\nThe weight of the prior\n\n\n\n\nFuture Trial Bayesian Predictive Probabilities\nFor predictive probabilities for a future trial, a predictive probability of success in a subsequent phase 3 trial is estimated during the current trial. The probability of success in phase 3 is calculated for each study arm, based on the specification of the phase 3 trial given here of:\n\nwhether the aim is to show superiority or non-inferiority,\nthe sample size per arm,\nthe required one-sided alpha,\nand the super-superiority margin or non-inferiority margin (if any).\n\nGiven these criteria, FACTS calculates the predicted probability of success of the subsequent trial for that endpoint assuming the estimate of the response on that endpoint, integrated over the uncertainty in those estimates. The conventional expected power of the specified future trial is calculated for the treatment effect in each MCMC sample and then averaged. This predicted probability of success can then be used in the stopping criteria and final evaluation criteria for the current trial.\nThis QOI is an extension of the “probability of success in phase 3” in earlier versions of FACTS – the difference now is that multiple different possible future trials can be specified in different QOIs and used for different decisions.\n\n\n\n\n\n\nFigure 8: Add predictive probability of future trial non-inferiority.\n\n\n\nThis predictive probability has the following parameters that must be specified:\n\nWhether the future trial will be for Superiority or Non-inferiority.\nThe size of the future trial in terms of the number of subjects on each arm.\nThe (one sided) alpha level that will be used to determine the significance of the trial.\nThe Super-superiority margin (if any) or the non-inferiority margin. The default p-value delta is not used for this QOI it is specified as part of the QOI and can be different from the default.\n\nAs with all QOIs, the future trial predictive probability QOI will be given an alternative shorter name that can be used when accessing the output files from other software such as R.\nIf there is no data available on the arms involved in the Bayesian predictive probability calculation, FACTS will still calculate the predictive probability of the future trial being a success based on the estimated posterior distribution. The posterior distribution for the arms with no available data will be the same as the user specified prior distribution.\n\n\n\nConditional Power\nConditional power is currently available for Continuous and Dichotomous endpoints in Core and Staged Designs.\nWhen creating a Conditional Power QOI, the first specification that must be made is whether the conditional power should be calculated for the current trial or for a hypothetical future trial. The two selections are fundamentally different.\nThe Current Trial conditional power takes the information collected in the trial so far, and assumes that for the remainder of the current trial the data collected has an effect equal to the point estimate of the already collected data. The previously collected trial data is then combined with hypothetical future data and weighted based on the amount of information that has been and will be observed by the end of the trial. The result is the probability that the current trial will reject the null hypothesis at a provided alpha level under the assumption that the future patients are generated from a population having a treatment effect exactly equal to the past data’s test statistic.\nThe Future Trial conditional power also uses the information accrued in the trial so far to calculate the assumed treatment effect moving forward, but assumes that a new study would be started that does not use the data from the current trial in its analysis. The new study is assumed to be a 1:1 randomized study if there is a control arm, and a single arm study if no control, with a sample size per arm, objective, and alpha level specified at the time of QOI creation.\nIf a conditional power is ever calculated for an arm with no final response data or against a control arm with no final response data, the conditional power will be called missing (-9999 in FACTS).\n\nCurrent Trial Conditional Power\nWhen creating a current trial conditional power, the missingness strategy, multiplicity adjustment, final sample size, and alpha level for the significance test must be specified.\n\n\n\n\n\n\nFigure 9: Add a conditional power of the current trial QOI.\n\n\n\n\nHandle missingness using:\nMissingness handling for a continuous endpoint can be specified as:\n\nIgnore: subjects that are known dropouts are not included in the current or final sample size, and are not used to estimate the treatment effect.\nLast Observation Carried Forward (LOCF): subjects that are known dropouts have their last observed endpoint value assumed to be their final endpoint. Subjects with no early observed data are ignored. Subjects that have data to carry forward are considered complete, and are used to estimate the current treatment effect.\nBaseline Observation Carried Forward (BOCF): subjects that are known dropouts have their baseline value assumed to be their final endpoint. They are included in the current information as complete subjects and are used to estimate the current treatment effect. BOCF is only available for the continuous endpoint and when simulating baseline for subjects.\nFailure: subjects that are known dropouts are assumed to have a negative outcome at their final visit. If responses are good, the subject is a non-response, and if responses are bad, the subject is a responder. Failure is only available for dichotomous endpoints.\n\n\n\nTest Type\nThe test type can either be Unadjusted or Bonferroni. An unadjusted test will always use the specified alpha level in the significance test. The Bonferroni adjustment will divide the specified alpha value by the number of non-control arms enrolling in the study in the enrolment period leading up to the current analysis time.\n\n\nSample Size:\nThe current trial conditional power can be calculated at two different future time points.\n\nCurrent Enrollment: That no new subjects are recruited, but all those who have been recruited are followed up until they are complete.\nTrial Maximum: That the trial would continue recruiting subjects using the current allocation ratios until the trial maximum sample size is reached, and all subjects recruited are followed up until they have the opportunity to complete.\n\n\n\nOne-sided Alpha\nThe threshold that the current trial would have to be less than in order to be considered a success. The success/futility values specified in the design tab are not used in the conditional power calculation. This value may be adjusted by the “Test type:” input.\n\n\nSuper-Superiority (Non-inferiority) margin for p-value:\nThis value cannot be changed on the QOI pop-up. It’s instead modified at the bottom of the QOI page. The objective of the current trial cannot be different for each individual current trial conditional power QOIs.\n\n\nAdditional Notes\nCurrently, conditional power will always assume that there is no correction applied to combine the different p-values from the analysis timepoints, i.e. no combination test is used.\nThe conditional power of the current trial at the maximum sample size is only available if the allocation is either fixed, or fixed with arm dropping, but not adaptive allocation or deterministic allocation.\nConditional power for the current trial is calculated\n\nIgnoring the possibility of the trial stopping or dropping an arm at a future interim\nIgnoring the possibility of future subject drop-outs.\n\n\n\n\nFuture Trial Conditional Power\nConditional power of a future trial is reduced to a power calculation for a future trial, given the current frequentist estimates of treatment effect and standard deviation and a set of assumptions for the future trial, such as sample sizes, hypothesis to be tested, and significance levels to be used.\nThe test type of the future trial can be set to Superiority or Non-inferiority. If the superiority or non-inferiority margin is set to 0, then both types of tests simplify to traditional tests of superiority with no margin. If a non-zero margin is set, then the future trial must meet the success criteria indicated by the test type and the margin. The margin cannot be negative, but a super superiority test with a margin of -0.5 is equivalent to a non-inferiority test with a margin of 0.5.\nThe subjects per arm dictates how many subjects would be enrolled on each arm in the future study. If there is a control arm in the current trial, then the future trial is assumed to be randomized 1:1 between the active arm and the control. If there is no control arm in the current study, then the future trial is a single arm trial testing against the performance goal specified in the Freq. Comparison Response box in the QOI creation pop-up.\nThe One-sided Alpha is the significance level of the final analysis test of the future trial. The future trial is assumed to have 1 active arm, so there are no adjustments to the alpha level available.\nThe superiority margin or non-inferiority margin indicate the amount that the active arm must be better/not worse than the control arm by. If there is no control arm then this is the Freq Comparison response, which is the value that the active arms must be significantly better than in order to be declared a success. The frequentist margin for p-value QOIs on the QOI tab is not used for Future conditional power calculations – the future trial can have a different objective than the current trial.\n\n\n\n\n\n\nFigure 10: Add a conditional power of a future trial QOI.\n\n\n\n\n\nTechnical Aspects of Conditional Power Calculations\nThe conditional power calculations in FACTS are all calculated similarly to Jennison and Turnbull.\nFor continuous endpoint conditional power calculations, the conditional power of the future trial is calculated using a pooled standard deviation from all arms. For dichotomous conditional power calculations, each arm has its own standard deviation based on the MLE estimate of its proportion. These decision result in the conditional power tests matching how standard p-value QOIs are calculated for continuous and dichotomous endpoints.\nThe following sections will provide formulae to calculate the conditional power in the case where there is a control arm. Simplifications for when the arm is being compared to a fixed value rather than a control arm are trivial: information fractions are only a function of the active arm standard deviation and the test statistic does not have a second sample mean subtracted from the active arm.\nThe value of \\(\\delta\\), which can be a non-inferiority margin or a super superiority margin, is always positive coming out of FACTS. To make the math more concise we can define a couple of coefficients for the \\(\\delta\\) term that allow it to be used without re-writing formulas for different hypothesis test setups. If high values of the endpoint are good, then \\(s_1 = 1\\), and if low values of the endpoint are good, then \\(s_1=−1\\). If the specified \\(\\delta\\) is a non-inferiority margin, then \\(s_2 = 1\\), and if it’s a super superiority margin then \\(s_2=-1\\).\n\nContinuous Conditional Power for the Current Trial\nLet t be the interim index that the conditional power is being computed at, and T be the time of the analysis that the conditional power is being computed for. Then \\(Z_k\\) is the test statistic of the data collected up to the current interim analysis in the study, \\(I_k\\) is the information level at the time of the interim analysis, and \\(I_K\\) is the information level at the end of the study that the conditional power is being calculated for.\nLet arm 1 be the control and arm 2 be the active arm, \\(\\bar{x_{it}}\\) be the sample mean of arm \\(i\\) at time \\(t\\), \\(\\widehat{\\sigma_{i}^{2}}\\) be the sample variance of arm \\(i\\) at time \\(t\\), \\(n_{it}\\) be the number of subjects with complete known final data on arm \\(i\\) at interim analysis \\(t\\), and \\(n_{iT}\\) be the number of subjects with complete known final data on arm \\(i\\) at the time that conditional power is being calculated for. The pooled variance estimate is \\(\\widehat{\\sigma^{2}} = \\sum_{d = 1}^{D}\\widehat{\\frac{\\sigma_{d}^{2}}{n_{dt}}}\\) where D is the total number of arms in the study.\nThen,\n\\[I_{t} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1t}} + \\widehat{\\frac{\\sigma^{2}}{n_{2t}}} \\right)^{-1}\\]\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{1T}} + \\widehat{\\frac{\\sigma^{2}}{n_{2T}}} \\right)^{-1}\\]\n\\[Z_{t} = \\left( {\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{1}s_{2}\\delta \\right)\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the non-inferiority or super superiority margin.\nThen for a one-sided alpha level of \\(\\alpha\\), let \\(Z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Continuous Conditional Power for a Future Trial\nMost of the future trial conditional power calculation is the same as the above current trial conditional power. There are some simplifications.\n\\({\\overline{x}}_{it}\\) and \\(\\widehat{\\sigma_{i}^{2}}\\) are the same as in the current conditional power calculation. \\(I_t\\), the weight of the current trial Z-score, is set to 0. \\(I_T\\) is now the information at the end of the future trial, and is calculated as:\n\\[I_{T} = \\left( \\frac{\\widehat{\\sigma^{2}}}{n_{T}} + \\widehat{\\frac{\\sigma^{2}}{n_{T}}} \\right)^{- 1}\\]\nwhere \\(n_T\\) is the sample size per arm in the future trial and again \\(\\widehat{\\sigma^{2}}\\) is the pooled variance.\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - ({\\overline{x}}_{2t} - {\\overline{x}}_{1t} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for the Current Trial\nThe dichotomous conditional power calculations are similar in spirit to the continuous conditional power calculations. One substantial difference is in how the dichotomous conditional power tests handle a non-inferiority or super superiority margin, \\(\\delta\\). The method of calculating an appropriate test statistic for a frequentist hypothesis test is not obvious when the null scenario has a non-zero \\(\\delta\\).\nWhen there is no margin, the estimate for each treatment is simply based on the observed response proportion \\(\\widehat{p_{i}}\\) for arm \\(i\\), and the test statistic for a comparison of the control arm, \\(c\\), with dose \\(d\\) is the usual Wald test\n\\[Z_{d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}}}{\\sqrt{\\frac{\\widehat{p_{d}}(1 - \\widehat{p_{d}})}{n_{d}} + \\frac{\\widehat{p_{c}}(1 - \\widehat{p_{c}})}{n_{c}}}}\\]\nWhen there is a margin, FACTS uses the Farrington-Manning Likelihood Score test statistic to estimate quantities \\(\\widetilde{p_{d}}\\) and \\(\\widetilde{p_{c}}\\) based on the MLEs of the arm proportions governed by the constraint that \\(\\widetilde{p_{d}} - \\widetilde{p_{c}} = - s_{1}s_{2}\\delta\\). These constrained MLE estimates are used in the standard error of the test statistic. The FM test statistic is then,\n\\[Z_{FM,d} = \\frac{\\widehat{p_{d}} - \\widehat{p_{c}} + s_{1}s_{2}\\delta}{\\sqrt{\\frac{\\widetilde{p_{d}}(1 - \\widetilde{p_{d}})}{n_{d}} + \\frac{\\widetilde{p_{c}}(1 - \\widetilde{p_{c}})}{n_{c}}}}\\]\nSee the PASS documentation or SAS documentation for a complete description of the calculations that go into the FM test. In the SAS documentation, note that the FM test is the same as the Miettinen-Nurminen test without including the \\(\\frac{n}{n - 1}\\) variance correction. The FM test was used rather than the MN test because as \\(\\delta \\rightarrow 0\\), the FM test converges to the simple Wald test.\nOnce the test statistic has been resolved, the dichotomous conditional power for the current trial is calculated as follows. For calculating the conditional power for arm 2 compared to the control arm, called arm 1 without loss of generality, let \\(I_t\\) be the current information amount and \\(I_T\\) be the amount of information that the conditional power is being calculated for. Then,\n\\[I_{t} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1t}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2t}} \\right)^{- 1}\\]\n\\[I_{T} = \\left( \\frac{\\widetilde{p_{1}}\\left( 1 - \\widetilde{p_{1}} \\right)}{n_{1T}} + \\frac{\\widetilde{p_{2}}\\left( 1 - \\widetilde{p_{2}} \\right)}{n_{2T}} \\right)^{- 1}\\]\n\\[Z_{t} = \\left( \\widehat{p_{2}} - \\widehat{p_{1}} + s_{1}s_{2}\\delta \\right)*\\sqrt{I_{t}}\\]\nwhere \\(\\delta\\) is the super superiority or non-inferiority margin, and \\(n_{1t}\\) and \\(n_{2t}\\) are current number of completers on the control and active arm, and \\(n_{1T}\\) and \\(n_{2T}\\) are the number of completers that will be on the control and active arm at time that the conditional power is being calculated for. Additionally, if there is no non-inferiority or super superiority margin \\(\\delta\\), then all \\(\\widetilde{p_{*}}\\) values are equal to their corresponding \\(\\widehat{p_{*}}\\) values.\nFor a one-sided alpha level of \\(\\alpha\\), let \\(z_{1-\\alpha}\\) be the critical value corresponding to \\(\\alpha\\).\nIf high values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{Z_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of the current trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{{- Z}_{t}\\sqrt{I_{t}} - z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} - I_{t} \\right)}{\\sqrt{I_{T} - I_{t}}} \\right)\\]\n\n\nCalculation of Dichotomous Conditional Power for a Future Trial\nMost of the calculation quantities for the future trial conditional power are the same as the current trial conditional power. The only difference is that for the future trial conditional power we discard the influence of the current trial test statistic on the final analysis, so \\(I_t=0\\). Then the conditional power calculations become:\nIf high values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} + (\\widehat{p_{2}} - \\widehat{p_{1}} + s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]\nIf low values of the endpoint are good, the conditional power of a future trial is:\n\\[CP_{T} = \\Phi\\left( \\frac{- z_{1 - \\alpha}\\sqrt{I_{T}} - (\\widehat{p_{2}} - \\widehat{p_{1}} - s_{2}\\delta)\\left( I_{T} \\right)}{\\sqrt{I_{T}}} \\right)\\]",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest (QOI) Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois.html#p-values",
    "href": "documentation/v71/userguides/core/qois.html#p-values",
    "title": "Quantities of Interest (QOI) Tab",
    "section": "P-values",
    "text": "P-values\nA p-value QOI simply allows the user to specify which test type to be used (unadjusted, Bonferroni, Dunnett’s, or Trend Test), and how missing data is to be handled (ignored, LOCF, BOCF, and missing is failure). If a control arm is present, p-values are comparisons against the control arm. If there is no control arm present, p-value QOIs are comparisons against a fixed response or fixed rate that is specified at the bottom of the QOI tab (it cannot be set differently for separate QOIs).\nNote that for dichotomous endpoints p-value are calculated using the Test of Proportions, this test statistic asymptotically approaches a normal distribution, and at least 5 success and 5 failures should be observed for this to be reasonable.\nThe p-values are calculated including the default frequentist delta that has been specified at the bottom of the screen. The delta margin cannot be modified as part of the QOI, the value of the delta and nature of the test is the same for all the p-value QOIs.\nIn a TTE design with a predictor, the p-values are only calculated for the final event endpoint, not the predictors.\n\n\n\n\n\n\nFigure 11: Add a p-value QOI.\n\n\n\n\nP-values when there is no control arm\nIf there is no control arm (not currently an option in Time-to-Event), the p-value delta is instead interpreted as the objective response or response rate to compare to. The option to set the trial type is also disabled (“Superiority” is selected and the “non-inferiority option is greyed out). For a non-inferiority trial comparing to an objective response or response rate simply use the objective rate minus the non-inferiority delta (if a higher response is better) or the objective rate plus the non-inferiority delta (if a lower response is better).\nIt is currently only possible to have one objective rate to compare against.\nThe same objective rate will be used for the target p-value test in the predictive probabilities.\n\n\n\n\n\n\nFigure 12: The Core Dichotomous engine QOI tab when no control arm is included.\n\n\n\n\n\nFisher-Exact Test\nWhen specifying the QOIs for a dichotomous endpoint in a trial with a control arm, the bottom of the QOI tab allows the user to specify what statistical test to use when calculating p-values. Options are “Normal approximation” (default) and “Fisher exact test”. “Normal approximation” uses a t-test and should yield similar results to a Chi-Square test without continuity correction, while “Fisher exact test” uses an exact Fisher’s exact test for 2x2 tables.\nIf “Fisher exact test” is chosen as the test type, all p-value QOIs will use a Fisher’s exact test, except for future trial predictive probabilities, which will still use a t-test.\nIf “Fisher exact test” is chosen as the test type, only “Bonferroni” and “Unadjusted” are available as multiplicity corrections and any QOIs previously created using different multiplicity corrections will be deleted.\n“Fisher exact test” is not available for non-inferiority comparisons.\n\n\n\n\n\n\nFigure 13: The Core Dichotomous engine QOI tab showing the Fisher exact test selected on the bottom.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest (QOI) Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois.html#target-doses",
    "href": "documentation/v71/userguides/core/qois.html#target-doses",
    "title": "Quantities of Interest (QOI) Tab",
    "section": "Target Doses",
    "text": "Target Doses\nThe target dose QOIs are Bayesian posterior probabilities based on MCMC sampling of how often each dose meets the specified target criteria. There are 3 types of target specifiable\n\nMax – the dose with the maximum response, this has no parameters to specify, so is available as a pre-defined default QOI.\nMED – a Minimum Effective Dose, the lowest dose that has a response better than an absolute or relative target.\nEDq – an effective dose, the dose that achieves a specified proportion (quantile \\(q\\)) of the maximum improvement in response relative to an absolute value or the performance of a specific treatment arm.\n\n\n\n\n\n\n\nFigure 14: Add a probability of being target QOI.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest (QOI) Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois.html#decision-quantities",
    "href": "documentation/v71/userguides/core/qois.html#decision-quantities",
    "title": "Quantities of Interest (QOI) Tab",
    "section": "Decision Quantities",
    "text": "Decision Quantities\nThe QOIs described so far have defined values to be calculated across all doses. For a Success/Futility decision to be made it is necessary to specify the treatment arm whose QOI value is to be used in comparison to the success and/or futility criteria. This selection can be done by specifying a specific arm, but normally it is specified by using a “Probability of being Target” selection criteria. A simple and common example is “The probability of the response being better than that of the Control of the Treatment arm with the maximum response”:\n\n\n\n\n\n\nFigure 15: Add a decision quantity QOI for the posterior probability of benefit over the control arm.\n\n\n\n\n\n\n\n\n\nFigure 16: Add a decision quantity QOI for the minimum effective dose.\n\n\n\nA decision QOI consists of (1) a QOI that has been calculated for each dose, and (2) a method of choosing a dose to use the QOI value of. Choosing the dose can be done either by using a target Dose QOI like Pr(Max), Pr(EDq…), etc, by choosing the dose with the highest or lowest value of a QOI, or by explicitly choosing a dose level in advance.\nAs an example using a target QOI, you can imagine evaluating a decision QOI that is specified to choose the probability of being better than Control by 2 units Pr(\\(\\theta_d - \\theta_0 &gt; 2\\)) based on the arm with the highest ED90 EDq relative to control; Quantile 0.9.\nInstead of a Target Dose QOI, it is also possible to specify a specific dose, or to use the terms “Max probability over all doses” and “Min probability over all doses” which mean that the dose with the minimum or maximum value of the per dose QOI is used. This enables:\n\nDecisions QOIs testing specific doses, for example by defining a Posterior probability QOI that compares all doses against the lowest dose, and then a Decision QOI using that posterior probability with the highest dose selected as the target dose it is possible to based decisions on the posterior probability that the highest dose is better than the lowest dose.\nA Decision QOI using “Max probability over all doses” allows a decision to be based on whether a probability or p-value is greater than a threshold at any dose, or less than a threshold at all doses.\nA Decision QOI using “Min probability over all doses” allows a decision to be based on whether a probability or p-value is less than a threshold for any dose or greater than a threshold at all doses.\n\nThere is one special case: if the QOI chosen is the Trend Test p-value, then because this is not actually a per-dose value, only one “Target dose” can be selected: “Overall Significance”.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest (QOI) Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/qois.html#standard-evaluation-variables",
    "href": "documentation/v71/userguides/core/qois.html#standard-evaluation-variables",
    "title": "Quantities of Interest (QOI) Tab",
    "section": "Standard Evaluation Variables",
    "text": "Standard Evaluation Variables\nThese 2 parameters are used across some of the default QOIs and hence specified outside the normal QOI dialog boxes.\n\nThe CSD value\nand whether absolute or relative to the Control arm\n\nthese are used in both the default “Pr(CSD)” and the “MED relative to Control” QOIs.\n\n\n\n\n\n\nFigure 17: Standard Evaluation Variables for specifying the clinically significant difference (CSD) and the default QOI comparison type.\n\n\n\nNote that the CSD value here is designed to be usually entered as a positive value, as in earlier versions of FACTS. Its sign is automatically adjusted if “lower score means subject improvement” or if the trial is a non-inferiority trial.\nThese adjustments are not made for other user entered QOIs. The directions of comparison both for the definition of the probability to be calculated and the comparison of the resulting probability with a threshold are under the user’s control, as are whether delta’s are negative or positive. This allows the user to define QOIs in whatever fashion is natural to them and their team.\n\nThe direction of comparison for default QOIs\nNote that as a result of being able to specify whether a higher or lower response represents improvement, and whether the aim is superiority or non-inferiority, the CSD or the NIM must always be a positive value. FACTS will automatically determine which direction is appropriate (e.g. if lower values are subject improvement, the engine will realize a CSD will need to be subtracted from the control score before comparing with the estimate of response on a treatment arm).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Quantities of Interest (QOI) Tab"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/dichotomous.html",
    "href": "documentation/v71/userguides/core/longitudinalmodels/dichotomous.html",
    "title": "Longitudinal Models for Dichotomous Endpoints",
    "section": "",
    "text": "LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {\\(y_{it}\\)} is the set of observed responses from early visits, and \\(y_{i t_m}\\) is the last observed value of \\(y_{it}\\), then the LOCF model for the final endpoint \\(Y_i\\) is\n\\[Y_i \\mid \\{y_{it}\\} = y_{i t_m}\\]\n\n\nBeta Binomial\nThe Beta Binomial longitudinal model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(\\pi_{t y_{it}})\\]\nwhere \\(\\pi_{t y_{it}}\\) is the probability that a patient is a response at the final endpoint given its early observed endpoint at time \\(t\\) is \\(y_{it}\\),\n\\[\\pi_{t y_{it}} = \\Pr(Y_i = 1 \\mid y_{it}) \\sim \\text{Beta}(\\alpha_{t {y_it}}, \\beta_{t y_{it}})\\]\nWe use the set cardinality operator \\(\\mid \\ldots \\mid\\) to obtain the posterior distributions of \\(\\alpha_t\\) and \\(\\beta_t\\) as:\n\\[\\alpha_{t0} = \\alpha_{\\mu 0} + \\left| Y_i = 1, y_{it} = 0 \\right| \\] \\[\\alpha_{t1} = \\alpha_{\\mu 1} + \\left| Y_i = 0, y_{it} = 0 \\right| \\] \\[\\beta_{t0} = \\beta_{\\mu 0} + \\left| Y_i = 1, y_{it} = 1 \\right| \\] \\[\\beta_{t1} = \\beta_{\\mu 1} + \\left| Y_i = 0, y_{it} = 1 \\right| \\]\ni.e. a prior value \\((\\alpha_{\\mu 0}, \\alpha_{\\mu 1}, \\beta_{\\mu 0}, \\beta_{\\mu 1})\\) plus the number of subjects for which the final response is known to be 1 for \\(\\alpha_{tx}\\) (or 0 for \\(\\beta_{tx}\\)) and the response at time \\(t\\) is \\(x\\).\nThe \\(\\alpha_{tx}\\) and \\(\\beta_{tx}\\) parameters are independently estimated using only patients in their model instance, and may or not have identical priors \\(\\alpha_{\\mu *}\\) and \\(\\beta_{\\mu *}\\) depending on the Model Priors selection in FACTS. A common non-informative prior for the \\(\\pi_{t0}\\) and \\(\\pi_{t1}\\) parameters is \\(\\text{Beta}(1,1)\\).\n\n\nLogistic regression\nThe Logistic regression longitudinal model works similarly to the Beta Binomial imputation model. The difference is in the method of modeling the probability of a transition from an interim visit to the final visit \\(\\Pr(Y_i = 1 \\mid y_{it})\\). Like the Beta Binomial model, the logistic regression model imputes a patient’s final endpoint given their most recent observed response.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(\\pi_{t y_{it}})\\]\nwhere \\(\\pi_{t y_{it}}\\) is the probability of a response at the final endpoint time given that its early observed endpoint at time \\(t\\)$ is \\(y_{it}\\). Then, we define the parameter\n\\[\\theta_{ty_{it}} = \\text{logit}\\left( \\pi_{ty_{it}} \\right) = \\log\\left( \\frac{\\pi_{ty_{it}}}{1 - \\pi_{ty_{it}}} \\right)\\].\nThe priors on \\(\\theta_{t0}\\) and \\(\\theta{t1}\\) are:\n\\[\\theta_{t0} \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\] \\[\\theta_{t1} \\sim \\text{N}(\\mu_1, \\sigma_1^2)\\]\nThe model computes the posterior distribution of \\(\\theta_{t0}\\) and \\(\\theta_{t1}\\) using all patients on arms belonging to the model instance that have observed endpoint values at time \\(t\\) and the final endpoint time \\(T\\).\nThe priors on \\(\\theta_{t0}\\) and \\(\\theta_{t1}\\) may be shared across model instances and/or visits depending on the selection made in the Model Priors section of the FACTS Logistic regression Longitudinal model page.\nA possible starting place for non-informative priors in this model would be: \\(\\mu=0\\), \\(\\sigma=2\\). A weakly informative set of priors that an early response makes a final response more likely could be \\[\\theta_{t0} \\sim \\text{N}(-.75, 1.25^2)\\] and \\[\\theta_{t1} \\sim \\text{N}(0.75, 1.25^2)\\]\n\n\nRestricted Markov Model (Absorbing Markov Chain)\n\n\n\n\n\n\nUsing the Restricted Markov Model\n\n\n\nThe restricted markov model is special in the sense that it can be used if and only if the “Use longitudinal modeling” check box is checked, the “Enable Special Longitudinal Options” check box is checked, and “Use restriced Markov model” is selected. When these conditions are met the Virtual Subject Response tab changes and the Design &gt; Longitudinal tab only has the Restricted Markov option.\n\n\nThe Absorbing Markov Chain model assumes that at each visit a subject is in one of three states – responder (1), stable (S), or failure (0). The responder and failure states are absorbing, meaning that once a patient has entered one of those states they must remain in that state for the remainder of the visits. Patients in the stable state may move to a responder or a failure in subsequent visits.\nUnlike most other longitudinal models in FACTS, the Restricted Markov Model estimates the probability of a result at a visit based on the visit right before it, rather than predicting directly to the final endpoint from the early visit.\n\\[\\Pr(y_{it} = n \\mid y_{i, t-1} = S) \\sim \\text{Dirichlet}(\\{\\alpha_{0,t}, \\alpha_{1,t}\\, \\alpha_{S,t}\\}) \\text{ for } t\\ge 2\\]\nWhere n can be 0, 1, or S, denoting the probability of going to the Fail state, the Responder state, or the Stable state at visit \\(t\\) from the Stable state at visit \\(t-1\\). \\(t\\) must be greater than or equal to \\(2\\), because we do not impute the first visit – a subject missing visit 1 and all visits after does not contribute to the longitudinal model or dose response model.\nThe priors for the \\(\\alpha\\) parameters are specified in terms of the prior number of transitions from Stable at \\(t-1\\) to each different state at time \\(t\\). For example, if the prior value for the parameter \\(\\alpha_{1,3}\\) is \\(2\\), we are putting apriori information into the Dirichlet distribution suggesting that \\(2\\) patients transitioned from Stable at visit 2 to Responders at visit 3. Specified priors can either be common or different across model instances based on user specification.\nThe parameters defining the posterior distribution of the state probabilities are available in closed form as:\n\\[\\alpha_{0,t} = \\gamma_{0,t} + \\left|y_{it}=0, y_{i, t-1} = S\\right|\\] \\[\\alpha_{S,t} = \\gamma_{S,t} + \\left|y_{it}=S, y_{i, t-1} = S\\right|\\] \\[\\alpha_{1,t} = \\gamma_{1,t} + \\left|y_{it}=1, y_{i, t-1} = S\\right|\\]\nTo create a dichotomous endpoint, the user specifies in the Study &gt; Study Info &gt; Design Options section whether patients remaining in a stable state at the final visit should be dichotomized as responders or failures.\n\n\nDichotomous Endpoint: Dichotomized Continuous Longitudinal Model\nThe user may select (on the Study tab) to assume that the dichotomous final endpoint is generated by observing continuous longitudinal data and then dichotomizing the final endpoint based on whether it is greater than or less than a provided threshold. If the user selects this option, then they may select any of the continuous longitudinal models specified in the Continuous Longitudinal Models section. The engine will impute incomplete subjects according to the continuous model, resulting in a continuous imputed final endpoint. The imputed dichotomous final endpoint is then simply the dichotomized version of the continuous imputation.\nAll priors and methods are identical to the continuous longitudinal models mentioned above.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Longitudinal Models",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/tte.html",
    "href": "documentation/v71/userguides/core/longitudinalmodels/tte.html",
    "title": "Longitudinal (predictor) Models for Time-to-event Endpoints",
    "section": "",
    "text": "For all predictors (\\(Z\\)) for time-to-event endpoints, the engine estimates both a marginal distribution (normal mean and variance for continuous, probability of response for dichotomous, and a piecewise exponential hazard model for time to event predictors) and a working model relating the predictor to the final endpoint. The marginal distribution is used to impute predictors for subjects lacking an observed predictor value and may also be used for stopping (see section on stopping). The working model is used to impute final endpoints for subjects lacking a final endpoint. For subject missing both a predictor and final endpoint, the predictor is imputed first from its marginal distribution, and then the final endpoint is imputed conditionally on the predictor \\(Z\\).\n\nContinuous Predictor\nWithin each dose (including control and active comparator), the marginal distribution of \\(Z\\) is a normal distribution with mean \\(\\theta_{Zd}\\) and standard deviation \\(\\sigma_Z\\). The standard deviation is common across the doses, but the means \\(\\theta_{Zd}\\) are allowed to vary across the same range of dose response models as the final endpoint (NDLM, Logistic, etc.). The prior specification for these predictor dose response models is identical in structure to the final endpoint, although the user selects a separate set of parameter values. The dose response for the predictor does not need to match the dose response for the final endpoint.\nThe working model assumes the final event time T is related to the predictor \\(Z\\) by assuming \\(T\\mid Z \\sim \\text{Exp}(\\lambda_d e^{\\beta Z})\\), where \\(\\lambda_d\\) varies by dose and has separate priors \\(\\lambda_d \\sim \\text{Gamma}(\\alpha_d, \\beta_d)\\) for each dose. The coefficient in the exponent \\(\\beta\\) (no subscript) is constant across doses with prior \\(\\beta \\sim \\text{N}(m, s)\\). \n\n\nDichotomous Predictor\nA dichotomous predictor is handled similarly to a continuous predictor, with a marginal distribution having a predictor dose response model. However, in this case the predictor dose response relates the log-odds rather than the probability of response itself. The working model for dichotomous is identical to the working model for a continuous predictor, with \\[T\\mid Z \\sim \\text{Exp}(\\lambda_d e^{\\beta Z})\\]. In this situation the working model is simpler to understand, as \\[T\\mid (Z=0) \\sim \\text{Exp}(\\lambda_d)\\] and \\[T \\mid (Z=1) \\sim Exp(\\lambda_{d\\beta})\\].\n\n\nTime to Event Predictor\nThe time to event predictor is qualitatively different than the continuous or dichotomous predictors. Instead of the predictor adjusting the hazard, the time to event predictor is viewed as an offset. The final endpoint is viewed as a sum of a predictor time \\(Z_1\\) and a post-predictor time \\(Z_2\\), where \\(Z_1\\) and \\(Z_2\\) are independent random variables and the final endpoint is thus \\(Z_1 + Z_2\\).\nFor the working model, \\(Z_1 \\sim \\text{PWExp}(\\lambda_{1s}*\\theta_{1d})\\) and \\(Z_2 \\sim \\text{Exp}(\\lambda_{2d})\\), with priors \\(\\theta_{1s} \\sim \\text{Gamma}(\\alpha_{1s}, \\beta_{1s})\\), \\(\\theta_{2d} \\sim \\text{Gamma}(\\alpha_{2d}, \\beta_{2d})\\) (with \\(Z_1\\)’s control hazard model potentially being piecewise exponential). For imputation, a subject missing both the biomarker and final endpoint times has both \\(Z_1\\) and \\(Z_2\\) imputed, with the final endpoint imputed as the sum. For a subject with a predictor time but no final endpoint, \\(Z_2\\) is imputed and added to the observed predictor time to impute the final endpoint.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Longitudinal Models",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html",
    "href": "documentation/v71/userguides/core/study/dichotomous.html",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "",
    "text": "Figure 1: The study tab for a dichotomous trial.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html#design-options",
    "href": "documentation/v71/userguides/core/study/dichotomous.html#design-options",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Design Options:",
    "text": "Design Options:\nIn the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\nEnable adaptive features\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\nUse longitudinal modeling\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\nEnable Special Longitudinal Options\nIf “Use Longitudinal Modeling” is selected, then special longitudinal options can be turned on.\n\nEndpoint is dichotomized continuous-valued response\nIf this option is selected, then the endpoint is actually simulated as a dichotomized continuous response – e.g. a “responder analysis” where a subject with a change from baseline above (or below) a certain threshold is a ‘responder’ and otherwise is not.\nWhen using a dichotomized response, the longitudinal analysis is on the underlying continuous response rather than the dichotomous responder endpoint. After imputing subject values based on intermediate subject data they are dichotomized.\n\n\nUse restricted Markov model\nWhen using the restricted Markov model a subject can be in one of 3 states “Success”, “Failure” or “Stable.” Once the subject has become a “Success” or “Failure” they are [fixed in that state](## “Success and Failure are called”absorbing states.”), and it will be their final outcome.\nAll subjects are initially in the “Stable” state and may or may not transition to another state during follow-up. If a subject finishes follow-up in the “Stable” state, they must be classified as failure or success.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html#study-information",
    "href": "documentation/v71/userguides/core/study/dichotomous.html#study-information",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "Study Information:",
    "text": "Study Information:\nThe study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\nRecruit Subjects\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\nResponse:\nIndicate whether a response indicates the subject improving or worsening (and thus whether a higher or lower response rate is a good thing). The value provided here informs the direction of frequentist hypothesis tests and the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\nSchedule of Post-Baseline Visits\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/dichotomous.html#d-treatment-arm-model",
    "href": "documentation/v71/userguides/core/study/dichotomous.html#d-treatment-arm-model",
    "title": "Study Tab - Dichotomous Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens.\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html",
    "title": "Hazard Model and Predictor Model",
    "section": "",
    "text": "The two main differences in the design tabs available for a time-to-event endpoint rather than continuous or dichotomous are way that the control arm hazard model is defined and the use of a predictor model."
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html#fixed-priors",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html#fixed-priors",
    "title": "Hazard Model and Predictor Model",
    "section": "Fixed priors",
    "text": "Fixed priors\nIf “Enable hierarchical data modeling for the control arm” is not selected, then independent gamma distributions with parameters specified in Hazard Rate tab’s table are used. The gamma parameters have been reparameterized so that the mean hazard rate and a weight (in terms of number of events) are provided instead of the traditional \\(\\alpha\\) and \\(\\beta\\) parameters.\n\n\n\n\n\n\nFigure 1: Specifying a fixed prior for a piecewise exponential model with 2 time segments."
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html#hierarchical-prior",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html#hierarchical-prior",
    "title": "Hazard Model and Predictor Model",
    "section": "Hierarchical Prior",
    "text": "Hierarchical Prior\nIf “Enable hierarchical data modeling for the control arm” is selected, then the independent priors specified per segment are augmented by additional data specified on the “Hierarchical Priors” tab.\n\n\n\n\n\n\nFigure 2: Specifying the sufficient statistics and hierarchical models hyper-parameters for the bayesian augmented control model.\n\n\n\nThe additional data comes in the form of sufficient statistics from an outside data source. The sufficient statistics are, for each segment in each study, the number of events on the control arm and the control arm exposure time in subject weeks. The information from the prior study can be ‘down-weighted’ by reducing, pro-rata, the number of events and exposure time before entering them. By supplying the summary statistics from previous trials and specifying the parameters for prior distributions for the parameters of a hierarchical model, the gamma priors are updated and become the priors applied to the data collected on the virtual subjects.\nThe hyper-parameters are the mean and standard deviation of Normal distribution for the log hazard ratios of the event rates of the historical studies and the current study. The prior distribution for the mean hyper-parameter is a Normal distribution, for which the user specifies the mean and standard deviation. The prior distribution for the standard deviation hyper-parameter is an Inverse-Gamma distribution for which the user specifies either as a mean and a weight, or via Alpha and Beta parameters (depending on the user selection on Settings &gt; Options &gt; Gamma Distribution Parameters).\n\n\n\n\n\n\nSetting the priors for Hierarchical model hyper parameters\n\n\n\nUnless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.\nIn this case the following would be reasonable:\n\nSet the prior mean value for Mu as the mean of the log-hazard ratios of the event rates of the control arm and the historic studies (usually this will be 0)\nSet the prior SD for Mu equal to at least the largest log hazard ratio of the event rates for the historic studies.\nSet the mean for tau to the same value as the prior SD for Mu.\nSet the weight for tau to be &lt; 1.\n\nOne can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small relative to the number of studies, then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.\nTo give some prior preference towards pooling or separate analysis the weight for tau has to be large, relative to the number of historic studies. To have a design that is like pooling the historic studies, the mean for tau needs to be small – say 10% or less of the value suggested above. For there to be no borrowing from the historic studies the value for tau needs to be large – say 10x or more the value suggested above.\nThe best way to understand the impact of the priors is try different values and run simulations."
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html#continuous-dichotomous-predictors",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html#continuous-dichotomous-predictors",
    "title": "Hazard Model and Predictor Model",
    "section": "Continuous & Dichotomous Predictors",
    "text": "Continuous & Dichotomous Predictors\nFor the predictor model with a continuous or dichotomous predictor, there are two tabs – the Dose Response and the Relationship to Endpoint tabs.\nThe dose response model for the predictor is the model used to estimate the marginal distribution of the predictor response at each dose as a normal mean and variance (for a continuous endpoint) or mean and variance of the log-odds (for a dichotomous endpoint). The dose response options are as per the continuous and dichotomous endpoint standard dose response options (except use of BAC for the predictor is not supported). The predictor dose-response model can be selected and specified completely independently of the dose response model for the time-to-event endpoint (the model of the log hazard ratio). See the FACTS Core Design Guide for details.\n\n\n\n\n\n\nFigure 3: The dichotomous predictor model showing a Simple NDLM dose response. The NDLM model for the predictor response works exactly like the NDLM dose response model.\n\n\n\n\nRelationship to Endpoint Model\nThe relationship to endpoint model is:\n\n\\(T_{i} \\sim \\text{Exp}(\\lambda_{d}\\ e^{\\beta Z_{i}})\\) for a continuous predictor where \\(Z_i\\), is the observed value of the predictor for the \\(i\\)th subject\n\\(T_{i} \\sim \\text{Exp}(\\lambda_{d}\\ e^{\\beta Z_{i}})\\) for a dichotomous predictor where \\(Z_i=0\\) or \\(1\\) is the observed dichotomous predictor for the ith subject\n\nThat is, for the ith subject, the time to their event is taken to follow an exponential distribution with mean \\(\\lambda_d\\ e^{\\beta Z_i}\\). The parameters of the model are estimated from the values of the predictor, and the time to event for the subjects for whom these have been observed. Then the time to event is imputed for subjects for whom predictor values have been observed, but not events.\n\n\n\n\n\n\nFigure 4: The Relationship to Endpoint tab for a continuous or dichotomous predictor.\n\n\n\nOn the Relationship to Endpoint tab, the user specifies the prior distributions for the relationship to endpoint model.\nFor a continuous predictor, \\(\\lambda_d\\) is the dose dependent hazard rate when the predictor is at its ‘center’ value. For a dichotomous predictor \\(\\lambda_d\\) is the dose dependent hazard rate when the predictor is 0. The \\(\\lambda\\)s have independent gamma priors for each dose, specified by their expected mean and a weight in equivalent number of events seen. Thus, a weight of 1 would be very weakly informative with any normal number of events. A weight of 0.1 would be uninformative with even a small number of events. A way to think about the use of a weight of \\(&gt;1\\) is, if weight of \\(n&gt;1\\) is used, then after \\(n\\) actual events are observed the posterior mean estimate of \\(\\lambda\\) will be the average of the observed mean and the prior mean.\nThe \\(\\beta\\) parameter has a normal prior, specified by its prior mean and standard deviation. For a continuous predictor, this is the log scaling factor of the hazard rate for the correlation of the event rate to the predictor. For a dichotomous predictor, it is the log scaling factor of the hazard rate for subjects who have had response on the predictor. For a continuous predictor with the center and scale values set so that the value of \\(Z\\) will vary approximately between \\(-1\\) and \\(1\\), and a dichotomous predictor where the predictor value is \\(0\\) or \\(1\\), a prior distribution of \\(N(0,5)\\) for \\(\\beta\\) would mean that \\(e^{\\beta Z}\\) will take values between \\(22,000^{-1}\\) to 22,000 and the prior could be deemed to be essentially uninformative. Large values (&gt;&gt;5) for the SD of the prior for \\(\\beta\\) can lead to numeric overflow in the simulator. If the values of Z do not largely fall in the range \\([-1, 1]\\), it is suggested that the prior for \\(\\beta\\) be constructed to limit the coefficient \\(\\exp{(Z\\beta)}\\) to lie within its plausible range. If the prior for \\(\\beta\\) is left uninformative and \\(Z\\) can take values \\(&gt;&gt; 1\\) it can result in inflation in the uncertainty in the estimates of the hazard ratios of subject’s time to final event from a few extreme imputed time-to-event values for subjects whose times-to-event are imputed, particularly if their predictor value is imputed too."
  },
  {
    "objectID": "documentation/v71/userguides/core/design/hazardmodel.html#tips-on-prior-strength",
    "href": "documentation/v71/userguides/core/design/hazardmodel.html#tips-on-prior-strength",
    "title": "Hazard Model and Predictor Model",
    "section": "Tips on prior strength",
    "text": "Tips on prior strength\nIf the main use of the predictor is to improve the information for the final analysis, unless the trial has a particularly small sample size, usually weak priors can be used so the model is driven by the data observed in the trial.\nIf the model is to be used to improve the information at interim analyses, and even at the first interim there will be some events observed to allow an initial model to be fitted (even though it might have a very diffuse posterior), again weak priors can be used.\nIf the model is to be used to allow interim analyses before any events are likely to have been seen, then informative priors are necessary.\nWhen there is a need to set informative priors, one way to establish the values to use is to estimate them from simulations using external data file based on actual data.\nMethod\n\nCreate a simple, fixed trial with a sample size (per arm) of the weight of the prior desired, with non-informative priors for the predictor model.\nCreate an external file of actual data, or data sampled from the prior predictor-time-to-event model with at least as many samples per arm as the sample size in step 1.\nRun 100 simulations and take the averages of the posterior estimates of the predictor model parameters to use as informative values in the actual design."
  },
  {
    "objectID": "documentation/v71/userguides/core/design/interims.html",
    "href": "documentation/v71/userguides/core/design/interims.html",
    "title": "Interims",
    "section": "",
    "text": "Interim analyses allow for decision making throughout the lifecycle of an adaptive trial in FACTS. Interim analyses can adjust allocation probabilities, drop arms, or allow for early success/futility of the trial. Interims can either be specified with calendar frequency – occurring every specified number of weeks or specified to occur after a specified amount of information has been collected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Interims"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/interims.html#continuous-and-dichotomous-endpoint",
    "href": "documentation/v71/userguides/core/design/interims.html#continuous-and-dichotomous-endpoint",
    "title": "Interims",
    "section": "Continuous and Dichotomous Endpoint",
    "text": "Continuous and Dichotomous Endpoint\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have actually completed a specified visit\nthe number of subjects who have had the opportunity to complete a specified visit (includes drop-outs)\n\nWhen specifying an interim analysis schedule, it can be done either based on time or based on one of the information categories above.\nIf specifying interims based on time the first interim analysis timing must be based on information, and each subsequent interim is triggered after the provided amount of time has elapsed. If accrual completes before the first interim threshold is reached, and the first interim was defined in terms of the number of subjects enrolled, then the interims by time start at full accrual. If the first interim is defined in any other terms (by events, subjects complete or subjects with opportunity to complete) then interims only start when this is reached (which might be never).\nIf specifying interims based solely on information, the table on the “Interims” tab determines when the analyses will be triggered. Each interim is defined individually by the number of patients/observations that have satisfied some criteria. If information is If information is defined as Subjects Enrolled, then interim are triggered immediately upon enrollment of the subject satisfying the criteria. If information is defined as completers or opportunity to complete, then interims are triggered immediately upon the visit being reached that satisfies the specified criteria. Successive interims must be in terms of the same or more observations at the same or later visit, and either Visit or Subject must increase. Different types of information cannot be mixed to trigger interim analyses except in using time to trigger interims after the first based on information.\n \nIf interims are governed by time, there is the option as to whether interims should continue after full accrual, or discontinue.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Interims"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/interims.html#time-to-event-endpoint",
    "href": "documentation/v71/userguides/core/design/interims.html#time-to-event-endpoint",
    "title": "Interims",
    "section": "Time-to-Event Endpoint",
    "text": "Time-to-Event Endpoint\nInformation can be defined in terms of:\n\nnumber of subjects that have been recruited\nthe number of subjects who have observed their predictor endpoint\nthe number of subjects who have had the opportunity to observe their predictor endpoint (includes drop-outs)\nspecified numbers of events observed\nspecified number of predictor events observed\n\nOutside of the new types of information, the time-to-event triggers work in exactly the same way that continuous and dichotomous triggers do.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Interims"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html",
    "href": "documentation/v71/userguides/core/design/doseresponse.html",
    "title": "Dose Response Models",
    "section": "",
    "text": "Dose response models in FACTS may be more accurately called final endpoint models. They create and model a relationship across the doses specified in the Treatment Arms tab. Often, but not always, the dose strength, called “Effective Dose Strength” in the Study &gt; Treatment Arms tab of FACTS, is used in the dose response models to determine the order of doses, and which doses are more related to others.\nThe dose response models can be simple, and model the doses largely independently, as is done with the Independent Dose Model or the Independent Beta Binomial Model (dichotomous only). They can have logistic style models with interpretable parameters, like the 3-parameter logistic or the \\(E_{max}\\) model (called Sigmoidal in FACTS). The dose response model can also be a model that creates a smooth, spline like, model over the doses using a normal dynamic linear model (NDLM), a monotonic NDLM, or a 2nd order NDLM.\nFor all endpoints, we model the response at each dose, d, in terms of \\(\\theta_d\\) on a continuous scale, allowing a consistent and rich range of dose response models to be used for all endpoint types. Transformations (see below) of the dichotomous and time-to-event responses are used to achieve this.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#independent-dose-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#independent-dose-model",
    "title": "Dose Response Models",
    "section": "Independent Dose Model",
    "text": "Independent Dose Model\nThe “Independent Dose Model” providing a simple pair-wise comparison between the study drug arms and the control arm and/or the active comparator arm. The doses are modelled as independent and normally distributed with a prior of:\n\\[\\theta_d \\sim \\text{N}(\\mu_d, \\nu_d^2)\\]\nWhere \\(\\mu_d\\) and \\(\\nu_d^2\\) are specified in FACTS and can either be the same or vary across arms.\nThis model is useful:\n\nWhen there is only one or two experimental arms\nWhen the study drug arms don’t differ by dose but in other ways, so there is no ordered sequence of treatments – e.g. each arm is the study drug in combination with a different additional drug.\nFor simulating simple trial designs\nFor simulating a ‘conventional’ or ‘strawman’ design to compare more complex designs against\n\nOtherwise for trials with multiple related arms that differ by dose, it is usually more efficient to use a dose-response model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#independent-beta-binomial-model-dichotomous-only",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#independent-beta-binomial-model-dichotomous-only",
    "title": "Dose Response Models",
    "section": "Independent Beta-Binomial Model (Dichotomous Only)",
    "text": "Independent Beta-Binomial Model (Dichotomous Only)\nThis is a “no model” option similar to the above, but only available for the Dichotomous endpoint. Unlike any other dose response model, this model uses the beta distribution to model the probability of response directly rather than fitting a model to the log-odds of the probability.\nThe final endpoint response \\(Y_i\\) is modeled as:\n\\[Y_i \\sim \\text{Bernoulli}(P_d)\\] where \\(P_d\\) is the probability that a patient is a response at the final endpoint for subjects randomized to dose \\(d\\). With posterior\n\\[P_d \\sim \\text{Beta}(\\alpha_d + \\text{responders}_d, \\beta_d + \\text{non-responders}_d)\\]\nWhere \\(\\alpha_d\\), \\(\\beta_d\\) are the priors for the arm \\(d\\), \\(\\text{responders}_d\\) is the number of responders on arm \\(d\\) and \\(\\text{non-responders}_d\\) is the number of non-responders on arm \\(d\\).\nThis model has the advantages of an easier to understand prior, and better estimation of \\(P_d\\) when the number of responders and non-responders is small (either is &lt; 5) compared to the log-odds model. As it’s a independent dose model, it is well suited to analyzing single arm, two arm or three arm trials, or trials where the experimental arms are unrelated, and so there is no basis on which to borrow information between them. With trials with multiple arms of the same treatment at different doses it is usually more advantageous to fit a dose response model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#simple-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#simple-ndlm",
    "title": "Dose Response Models",
    "section": "Simple NDLM",
    "text": "Simple NDLM\nThe Normal Dynamic Linear Model (NDLM) estimates the final endpoint as a smoothed curve across doses included in the model. Doses are tied directly to their nearest neighbor, and the prior expectation for a dose is that it is equal to its neighboring doses, tending to a constant dose response across doses in the absence of data. The model is defined as follows:\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately.\nThe dose response of the first dose, \\(d'\\), has a prior of:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nwhere \\(\\mu_{d'}\\) and \\(\\tau_{d'}^2\\) are specified directly in FACTS. Subsequent dose response estimates \\(\\theta_{d'+1}, \\ldots, \\theta_D\\) have priors centered at the previous dose response with variances based on the distance between the dose \\(d\\) strength and the dose \\(d-1\\) strength. Specifically,\n\\[\\theta_d \\sim N\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\text{ for } d=d'+1, \\ldots, D\\]\nwhere for dose strengths \\(\\nu_d\\) and \\(\\nu_{d-1}\\), \\(\\tau_{d-1}^2\\) is defined as \\[\\tau^2_{d-1}=\\tau^2\\left(\\nu_d-\\nu_{d-1}\\right)\\]\nThe prior distribution for the “drift” parameter, which controls the amount of smoothing is:\n\\[\\tau^{2}\\sim IG\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) and \\(\\tau_n\\) are specified in the Dose Response tab in FACTS under Model Parameters. See here for help with specifying an inverse gamma distribution with center and weight.\nIn the continuous case the residual error around the estimated dose response is\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nwhere \\(\\sigma_\\mu\\) and \\(\\sigma_n\\) are specified on the Dose Response tab in FACTS under Error Parameters.\nThe Simple NDLM is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Simple NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a ‘null’ scenario where the response on all the doses is the same as control, the simple NDLM reduces type-1 error. The estimate of \\(\\tau^2\\) tends towards zero and the estimate of the dose response tends to a flat line. The smoothing of the estimates of the response reduces the random fluctuations in the estimates from dose to dose and is a very effective counter to the problem of type-1 error inflation due to the multiplicity of doses being tested. A small prior for the mean value of \\(\\tau\\) will accentuate this effect. The price to be paid for this benefit is a tendency to underestimate large changes in response or where there is a peak in the response. This over-smoothing can reduce power. A prior for the mean of \\(\\tau\\) centered at large values will minimize over-smoothing, and a very large prior would simply remove any smoothing effect at all. A standard prior for the mean of \\(\\tau\\) would be the expected average change in response from dose to dose, scaled to take into account the differences in the specified effective dose strengths.\nUsually, the choice of prior for \\(\\tau^2\\) is tuned based on simulation results, trading off reducing type-1 error in the null case against loss of power and reduced estimate of maximum response in the effective scenario with the largest jump in response or steepest peak.\nAside: When using the NDLM model or any of its alternatives (2\\(^{nd}\\) order or Monotonic NDLM), and including doses in a design that receive no subject allocation (ie. Allocation ratio set to 0), some important characteristics of the model should be considered in the interpretation of results. First, the NDLM model “borrows” information from neighboring doses, and therefore including unallocated intermediate doses will increase uncertainty in the fitted response. Information must “pass through” these intermediate doses, and uncertainty should be expected to increase as the number of intermediate unallocated doses between two allocated doses increases. Secondly, the NDLM model estimates the response at unallocated doses, but without subject data at these doses the uncertainty in the estimate can be considerably larger than doses with subject data. This high degree of uncertainty can lead to a vestigial probability that an unallocated dose is the target dose (Pr(EDq), Pr(MED), Pr(Max)) even though the mean response estimate and probabilities at neighboring doses with subject data would not suggest this to be the case.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#monotonic-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#monotonic-ndlm",
    "title": "Dose Response Models",
    "section": "Monotonic NDLM",
    "text": "Monotonic NDLM\nThe Monotonic NDLM endpoint model is similar to the NDLM in the sense that it smooths dose response estimates using neighboring doses, but is constrained to be either monotonically increasing or monotonically decreasing across doses. The direction of monotonicity depends on the value specified in the “Monotonic:” selection in the Model Parameters section of the Monotonic NDLM Dose Response page.\nThe use of this model is generally limited to endpoints where a strong presumption of increasing/decreasing response with dose is reasonable – such as a biomarker or a toxicity endpoint.\nLet doses \\(d = d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control or control is included in the dose response model, and \\(d'=2\\) if the control arm is modeled separately. The following model is the monotonically positive NDLM:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{d'},\\tau^2_{d'}\\right)\\]\nand\n\\[\\theta_d \\sim N^+\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\] where \\(\\tau_{d-1}^2\\) is defined as in the NDLM, and \\(X \\sim \\text{N}^+(\\mu, \\sigma^2)\\) refers to a positive truncated normal distribution with density function:\n\\[f_{X}(x) = \\frac{1 - \\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&gt;0\\]\nThe result of this dose-response model is that the curve is monotonically increasing, in that \\(\\theta_d&gt;\\theta_{d-1}\\).\nThe monotonically decreasing NDLM is similar except: \\[\\theta_d \\sim N^-\\left(\\theta_{d-1},\\tau^2_{d-1}\\right) \\mbox{ for } d = d', \\ldots, D,\\]\nwhere \\(X \\sim \\text{N}^-(\\mu, \\sigma^2)\\) refers to a negative truncated normal distribution:\n\\[f_{X}(x) = \\frac{\\Phi\\left( - \\frac{\\mu}{\\sigma} \\right)}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(x - \\mu)^{2} \\right\\} \\text{ for } x&lt;0\\]\nThe result of this dose-response model is that the curve is monotonically decreasing, in that \\(\\theta_d&lt;\\theta_{d-1}\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#second-order-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#second-order-ndlm",
    "title": "Dose Response Models",
    "section": "Second Order NDLM",
    "text": "Second Order NDLM\n\n\n\n\n\n\nNote\n\n\n\nThe second order NDLM described in this section is the version utilized in FACTS version 4.0 and later. The model labelled “Second Order NDLM” in versions before 4.0 is was maintained as the model labelled “Legacy 2nd Order NDLM” until the release of FACTS 7.1, at which time it was removed.\n\n\nThe second order NDLM serves as a “smoother” similar to the NDLM, however it tends to smooth toward a linear fit with a slope (the NDLM itself tends to shrink each dose’s fit toward its neighbors, while the second order NDLM prefers any trend in the neighbors).\nLet doses \\(d=d', \\ldots, D\\) be doses in the dose response model and \\(\\theta_d\\) be the estimated dose response for dose \\(d\\). The initial dose \\(d'=1\\) if there is no control arm or control is included in the dose response model, and \\(d'=2\\) if the control arm is modelled separately. The initial dose \\(d'\\) is modeled:\n\\[\\theta_{d'} \\sim N\\left(\\mu_{0},\\tau^2_{0}\\right)\\]\nwhere \\(\\mu_0\\) and \\(\\tau_0^2\\) are specified directly in FACTS.\nIn the case of a time-to-event endpoint, the initial dose \\(d'\\) is the control arm, and has a \\(\\theta_{d'}= 0\\) by definition, so no prior distribution is needed.\nThe prior distribution for the dose response for the dose directly after the initial dose is specified on the difference between the \\(d'\\) and \\(d'+1\\) level doses:\n\\[\\theta_{d'+1} - \\theta_{d'} \\sim N\\left(\\mu_{1},\\tau^2_{1}\\right)\\]\nSuccessive doses are then modeled based on differences in slope between the dose and the two doses below them. Let:\n\\[\\theta_{d} = \\theta_{d - 1} + \\Delta_{d}\\zeta_{d} + \\frac{\\Delta_{d}}{\\Delta_{d - 1}}\\left( \\theta_{d - 1} - \\theta_{d - 2} \\right)\\]\nfor doses \\(d=d'+2,\\ldots,D\\), where \\(\\Delta_d=\\nu_d-\\nu_{d-1}\\) and \\(\\Delta_{d-1}=\\nu_{d-1}-\\nu_{d-2}\\). The priors for the dose response smoothing terms \\(\\zeta_d\\) are:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau_2^2)\\] The smoothing is determined by the parameter \\(\\tau_2\\). Small values of \\(\\tau_2\\) lead to more smoothing, while large values of \\(\\tau_2\\) lead to little information sharing between doses (above dose 1). Note that “small” and “large” is dependent on the scale of the doses. The prior on the smoothing parameter is:\n\\[\\tau_{2}^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau_2\\), and \\(\\tau_n\\) is the prior weight. See here for help with specifying an inverse gamma distribution with center and weight.\nNote that that in this formulation, \\(\\tau_2^2\\) can be thought of as the deviation from linearity. The linear scaling of deviations thus leads to a conditional variance that scales as the square of the dose difference:\n\\[\\text{Var}[\\theta_d \\mid \\theta_{d-1}, \\theta_{d-2}]=\\tau_2^2\\cdot (\\nu_d-\\nu_{d-1})^2\\]\nThe second order NDLM, like the simple NDLM, is generally a good model to use when the shape of the likely dose response is unknown, but we would like to borrow information between a dose and its neighboring doses. No monotonicity or pre-determined shape is enforced by the Second Order NDLM. The variance in dose response from dose to dose is updated based on the observed data, but as the number of doses in a trial is typically small, a moderately informative prior is often used.\nIn a ‘null’ scenario where the response on all the doses is the same as control the Second Order NDLM, like the Simple NDLM, tends to reduce type-1 error. As the estimate of \\(\\tau^2\\) tends to zero the estimate of the dose response tends to a line (with non-zero slope if appropriate).\nThe second order NDLM has more power than the Simple NDLM when the dose response is smooth, but will tend to shrink estimates to the control by more than the Simple NDLM if there are marked peaks or steps in the response. Because it borrows from two neighboring doses, it is better at estimating a response for a dose where there is no data than the simple NDLM can, and can be used with burn-in’s where not all doses have been allocated to, but doesn’t cope well if there are consecutive doses where there is no data.\nHowever, unlike the simple NDLMs where the variance structure is preserved if doses are added to or dropped from the design, this is not true for the 2nd order NLDM. Thus, if using the 2nd order NDLM and the doses that are available to the model are changed, then the parameters for the prior for \\(\\tau_2^2\\) may need to be re-visited.\nThe simple NDLM is likely to be preferred where there are fewer doses (4-5), and the second order NDLM is often preferred when there are 7 or more (it will depend on the expected smoothness of the response – the smoother it is the more useful the second order NDLM will be).\nAs with the simple NDLM, the choice of prior for \\(\\tau_2^2\\) can be tuned based on simulation results: balancing having a smaller prior that limits type-1 error in the null case, against having a larger prior that yields more power and better estimate of maximum response in the effective scenarios with the large jumps in response or pronounced peaks.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#parameter-logistic",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#parameter-logistic",
    "title": "Dose Response Models",
    "section": "3-Parameter Logistic",
    "text": "3-Parameter Logistic\nThe 3-parameter logistic dose response model estimates efficacy as a smooth curve using an intuitive parameterization. The dose response for a dose \\(d\\) with effective dose strength \\(\\nu_d\\) is:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}}\\]\nWhere the \\(a\\) parameters have the following description:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated change in dose response for a dose with strength ∞ when compared to a dose of strength 0.\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum (\\(a_2\\))\n\n\nThe shape of the 3-parameter logistic curve can vary wildly based on the parameter values, but always starts at \\(a_1\\) at dose strength 0 and monotonically increases to \\(a_1+a_2\\) as the effective dose strength goes to infinity.\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nIn the continuous case, the prior distribution for the error term in the 3-Parameter Logistic is Inverse Gamma:\n\\[\\sigma^{2}\\sim IG\\left( \\frac{\\sigma_{n}}{2},\\frac{\\sigma_{\\mu}^{2}\\sigma_{n}}{2} \\right)\\]\nAn advantage of the 3-Parameter Logistic model is that it is has a small number of parameters that have defined, intuitive interpretations. Since there are few parameters it’s generally easy to estimate the parameters even on small amounts of data. Because of the limited flexibility of the logistic curve, the 3-parameter logistic model should only be used when there is evidence that it is appropriate. See the Hierarchical Logistic and Sigmoid (Emax) models for dose response models with a similar pattern, but slightly more flexibility in shape.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-logistic",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-logistic",
    "title": "Dose Response Models",
    "section": "Hierarchical Logistic",
    "text": "Hierarchical Logistic\nThe hierarchical logistic model is an extension of the 3-parameter logistic with the form:\n\\[\\theta_{d} = a_{1} + \\frac{a_{2}v_{d}}{v_{d} + a_{3}} + \\zeta_{d}\\]\nwhere \\(\\zeta_d\\) is a random intercept term that modifies \\(a_1\\) differently for each dose under the constraint that all \\(\\zeta_d\\) must sum to 0.\nThe additional term \\(\\zeta_d\\) is a per dose ‘off-curve’ effect. It allows for a secondary non-parametric dose effect over and above the logistic model. It has been used in circumstances in which there is actually some falling away of effect at the highest dose. This is an example mean fit from 100 simulations. The model fit and error bars are in green, and the true dose responses are shown by the black line.\n\n\n\n\n\n\nFigure 1: An example of a fit from the hierarchical logistic model averaged over 100 simulations. The black line shows the truth, and the green line with error bars shows the model fits.\n\n\n\n\\(\\zeta_d\\) is modelled as:\n\\[\\zeta_d \\sim \\text{N}(0, a_4^2)\\]\nconditioned that\n\\[\\sum_{d}^{}\\zeta_{d} = 0\\]\nAnd \\(a_4^2\\) has an inverse gamma prior:\n\\[a_{4}^{2}\\sim IG\\left( \\frac{\\Lambda_{n}}{2},\\frac{\\Lambda_{\\mu}^{2}\\Lambda_{n}}{2} \\right)\\]\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\]\nA typical recommended value for the center of the prior distribution of \\(\\alpha_4\\) is the largest expected likely deviation in true response from the underlying logistic with a weight of 1. The effect of this prior is then checked through simulation of a range of plausible dose response profiles, with a range of designs to check the sensitivity of the final estimate of response to this prior. Additionally, see here for help with specifying an inverse gamma distribution with center and weight.\nIn this model, compared to the ‘plain’ 3-parameter logistic above, it is important that the true ED50 lies within the expected dose range, and that the prior for the ED50, \\(a_3\\), has the majority of its probability mass in the available dose range. For example, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be:\n\\[a_{3}\\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\nUsing a weaker prior, such as \\(\\text{N}^+(\\nu_D, \\nu_D^2)\\) leads to a more linear fit. With just this change to the prior for \\(a_3\\) the average of the estimated of the mean response changes from the graph above to:\n\n\n\n\n\n\nFigure 2: An example of hierarchical logistic model fits using a prior that results in a flatter dose response model fit.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#sigmoid-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#sigmoid-model",
    "title": "Dose Response Models",
    "section": "Sigmoid Model",
    "text": "Sigmoid Model\nA sigmoid model (\\(\\text{E}_{\\text{max}}\\)) provides a coherent parameterization of the dose response trend similar to the 3-parameter logistic, but with one extra shape parameter, \\(a_4\\).\nThe model formula is:\n\\[\\theta_{d} = a_{1} + \\frac{(a_{2} - a_{1})v_{d}^{a_{4}}}{{a_{3}}^{a_{4}} + v_{d}^{a_{4}}}\\]\nThe interpretation of the four parameters is:\n\n\\(a_1\\)\n\nthe estimated dose response for a dose of strength 0\n\n\\(a_2\\)\n\nthe estimated dose response for a dose of strength \\(\\infty\\) (slight difference from Logistic models)\n\n\\(a_3\\)\n\nthe estimated ED50, the dose that has 50% of the dose response maximum attainable effect (\\(a_2-a_1\\))\n\n\\(a_4\\)\n\ncontrols the slope of the dose response model at the ED50. A larger value of \\(a_4\\) corresponds to a steeper slope. A value of \\(a_4=1\\) makes the Sigmoid model equivalent to a Three Parameter Logistic model with \\(a_2\\) equal to \\(a_1 + a_2\\) from the Sigmoid model. A value of \\(a_4\\) approaching 0 corresponds to a dose response model that is nearly flat at \\(\\frac{a_{1} + a_{2}}{2}\\). By differentiation, it can be seen that the slope where the effective dose \\(\\nu_d=a_3\\) is \\((a_{2} - a_{1})\\frac{a_{4}}{4a_{3}}\\).\n\n\nThe following independent prior distributions are assumed:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_4, \\lambda_4^2)\\]\nThe advantage of this model is that it is considerably more flexible than the logistic, but still more efficient to estimate than an NDLM. This model (if applicable) is ideal if the target is an Effective Dose (ED) such as an ED90. Estimating an ED requires estimating the response on control, the maximum response, and the gradient which is requires a lot of data to do well if using no model or a smoothing model such as one of the NDLMs. With the 4-parameter Sigmoid model (or 3-parameter logistic – but that has a more limited applicability due to being significantly less flexible in shape), it’s usually not an issue to estimate the EDx well.\nThe caveats to using this model are:\n\nWhilst being much more flexible than the logistic, it still has model assumptions, in particular monotonicity, that must be met for this to be a reasonable model to be used.\nThe curve is only well estimated if the true ED50 lies within the doses tested.\nLike the hierarchical logistic model above, the prior for \\(a_3\\) should be weakly informative, with the majority of its probability mass in the available dose range. So similarly, if the range of the effective dose strengths is from 0 to \\(\\nu_D\\) then a typical ‘weakly informative’ prior for \\(a_3\\) would be: \\[a_3 \\sim N^{+}\\left( \\frac{\\nu_{D}}{2},\\left( \\frac{\\nu_{D}}{2} \\right)^{2} \\right)\\]\n\n\n\n\n\n\n\nFigure 3: Example of a sigmoid model with \\(\\alpha_1=5\\), \\(\\alpha_2=10\\), \\(\\alpha_3=3\\), and \\(\\alpha_4=5\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#u-shaped-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#u-shaped-model",
    "title": "Dose Response Models",
    "section": "U-Shaped Model",
    "text": "U-Shaped Model\nThe U-shaped model when the allows for an initial increase (decrease) in the dose response with dose, followed by a leveling out of the mean, then a decrease (increase). Whether the U-Shaped model increases then decreases or decreases then increases is specified by the user on the Dose Response tab. An example curve for this model is given in the figure below.\nThe dose-response curve can be characterized in four different regions of doses. For the first region, which contains the lowest doses, \\(0&lt;\\nu_d&lt;p_{min}\\), the dose-response curve is increasing (decreasing):\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( \\frac{\\nu_{d}}{p_{\\min}} \\right)^{\\alpha}\\]\nThe next region is the plateau, where the dose-response curve is constant. For \\(p_{min} &lt; \\nu_d &lt; p_{min}+p_{width}\\): \\[\\theta_d=\\theta_0 + S\\cdot\\delta\\] For the third region, the dose-response curve is decreasing (increasing). For \\(p_{min}+p_{width} &lt; \\nu_d &lt; p_{min}+p_{width} + w_{width}\\),\n\\[\\theta_{d} = \\theta_{0} + S \\cdot \\delta \\cdot \\left( 1 - \\frac{\\nu_{d} - \\left( p_{\\min} + p_{width} \\right)}{w_{width}} \\right)^{\\beta}\\]\nFor the final region, the dose-response curve is again constant, at the same level as the zero-dose. For \\(\\nu_d &gt; p_{min}+p_{width} + w_{width}\\), \\[\\theta_d = \\theta_0\\]\nThe parameters of the model are described below:\n\n\\(S\\) is \\(1\\) or \\(-1\\), as determined by the Model is increasing/decreasing radio buttons. \\(S=1\\) if Model is Increasing is selected, indicating that the model starts increasing at low doses.\n\\(\\theta_0\\) represents the zero-strength dose response. Its prior is: \\[\\theta_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\]\n\\(\\delta\\) represents the maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[\\delta \\sim \\text{N}^+(\\mu_\\delta, \\sigma_\\delta^2)\\]\n\\(p_{min}\\) represents the lowest dose at which the maximal change in response from the zero-strength is achieved. It is restricted to be positive, and has a prior of: \\[p_{min} \\sim \\text{N}^+(\\mu_{min}, \\sigma_{min}^2)\\]\n\\(p_{width}\\) represents the width of the plateau – the region where all doses achieve maximal change in response from the zero-strength dose. It is restricted to be positive, and has a prior of: \\[p_{width} ~ \\sim \\text{N}^+(\\mu_{width}, \\sigma_{width}^2)\\]\n\\(w_{width}\\) represents the width of the region for doses beyond the plateau, where response is returning to the zero-strength dose level. It is restricted to be positive, and has a prior of: \\[w_{width} ~ \\sim \\text{N}^+(\\mu_{w}, \\sigma_{w}^2)\\]\n\\(\\alpha\\) determines the rate of change of the dose response curve for doses below the plateau. Values less than \\(1\\) indicate a rapid initial change from baseline, with slower change as the plateau is reached. Values greater than \\(1\\) indicate a slow initial change from baseline, with more rapid change as the plateau is reached. To help avoid identifiability issues, \\(\\alpha\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). \\(\\alpha\\)’s prior is: \\[\\alpha \\sim \\text{LN}^*(\\mu_\\alpha, \\sigma_\\alpha^2)\\] where \\(\\text{LN}^*()\\) represents the lognormal distribution with truncation constraints at \\(10^{-1}\\) and \\(10^{1}\\).\n\\(\\beta\\) determines the rate of change of the dose response curve for doses beyond the plateau. Values less than \\(1\\) indicate a slow initial change from the plateau level, with more rapid change subsequently. Values greater than \\(1\\) indicate a rapid initial change from the plateau level, with slower change subsequently. To help avoid identifiability issues, \\(\\beta\\) is restricted to be between \\(10^{-1}\\) and \\(10^{1}\\). The prior on \\(\\beta\\) is: \\[\\beta \\sim \\text{LN}^*(\\mu_\\beta, \\sigma_\\beta^2)\\]\n\nThe U-shaped model has seven free parameters, and thus it is inadvisable to use such a model for dose-finding trial designs that contain only a few dose levels. When a small number of doses are available, it may help to constrain the values of \\(\\alpha\\) and \\(\\beta\\) by utilizing small standard deviations in the priors.\n\n\n\n\n\n\nFigure 4: An example dose response curve for the U-shaped model. In this example, \\(\\alpha&lt;1\\) and \\(\\beta &gt; 1\\).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#plateau-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#plateau-model",
    "title": "Dose Response Models",
    "section": "Plateau Model",
    "text": "Plateau Model\nThe plateau model is a special case of the U-shaped model, in which \\(p_{width}=\\infty\\). That is, there is no return to baseline for high doses. This model eliminates three parameters from the U-Shaped model, since \\(p_{width}\\), \\(w_{width}\\), and \\(\\beta\\) are not used. See the figure below to see an example dose response model for the plateau model that mimics the beginning of the U-Shaped model above.\n\n\n\n\n\n\nFigure 5: An example dose response curve for the Plateau model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#parameter-exponential-logistic-dichotomous-only",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#parameter-exponential-logistic-dichotomous-only",
    "title": "Dose Response Models",
    "section": "3 Parameter Exponential Logistic (Dichotomous Only)",
    "text": "3 Parameter Exponential Logistic (Dichotomous Only)\nThe 3-parameter exponential logistic model has the following structure:\n\\[\\theta_d = a_1 + a_2 \\nu_d^{a_3}\\]\nWhere \\(\\nu_d\\) is the effective dose strength of dose \\(d\\). This is a logistic model for the dichotomous endpoint because \\(\\theta_d\\) is the log odds ratio of the probability of the response, \\(P_d\\) at dose \\(d\\).\nThe exponent parameter \\(\\alpha_3\\) allows the change in slope at the lower end of the curve to be asymmetric in comparison to the change in curve at the upper end of the curve.\nThe priors for the parameters are:\n\\[a_1\\sim \\text{N}(\\Lambda_1, \\lambda_1^2)\\] \\[a_2\\sim \\text{N}(\\Lambda_2, \\lambda_2^2)\\] \\[a_3\\sim \\text{N}^+(\\Lambda_3, \\lambda_3^2)\\] The interpretations of the parameters defining this model are:\n\n\\(a_1\\)\n\nthe dose response for a dose with strength 0\n\n\\(a_2\\)\n\nthe slope associated with the exponentiated dose strength\n\n\\(a_3\\)\n\na shape parameter modifying the effective dose strength through exponentiation.\n\n\nThe figure below shows an example of two different 3-parameter exponential logistic model fits. Notably, the fit shown in green has an \\(a_3\\) parameter greater than \\(1\\), which leads to faster increases of the response rate model as the effective dose strength increases.\n\n\n\n\n\n\nFigure 6: Two different examples of 3-parameter exponential logistic model fits. The green model has an \\(\\alpha_3\\) parameter greater than 1, which leads to faster increases of the sigmoid model as the effective dose strength increases.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-model",
    "title": "Dose Response Models",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nLike the Independent Dose Model, the hierarchical model treats the arms as if they are an unordered set of treatments, but unlike the Independent Dose Model the Hierarchical Model shares information between doses included in the model. The hierarchical model is a way of encouraging the estimates of arm responses to be similar, and the degree of this encouragement can be controlled using parameters of the prior distribution. The hierarchical model assumes that the arm parameters are drawn from the same normal distribution:\n\\[\\theta_d \\sim \\text{N}(\\mu, \\tau^2)\\]\nWhere \\(d\\) is the set of doses included in the model. The prior distributions for \\(\\mu\\) and \\(\\tau^2\\) are\n\\(\\mu \\sim \\text{N}(\\Lambda_\\mu, \\lambda_\\mu^2)\\)\nand\n\\[\\tau^{2}\\sim \\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nwhere \\(\\tau_\\mu\\) is a central value for \\(\\tau\\), and \\(\\tau_n\\) is the prior weight. \\(\\tau^2\\) governs the amount of information shared between doses in the model. The estimates of the dose parameters can be encouraged to be close together by choosing a small value for \\(\\tau_\\mu\\) and a large value for \\(\\tau_n\\). See here for a tool to help understand the inverse gamma distribution specified by center and weight parameters.\nThe control arm can be included in the hierarchical model if it is desired to encourage the experimental arms to be similar to the control arm, or excluded from it if the experimental arms are encouraged to be similar to each other but not necessarily to the control arm. This is not true with time-to-event data, when the control arm can only be excluded from the hierarchical model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#linear-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#linear-model",
    "title": "Dose Response Models",
    "section": "Linear Model",
    "text": "Linear Model\nThe linear model uses a strong assumption of exact linearity between the responses and the dose strengths. (For dichotomous models, the linearity is on the log-odds scale, and the linearity is on the log hazard ratio scale for TTE models.) The model is\n\\[\\theta_d=\\alpha+\\beta\\nu_d\\] for all doses \\(d\\) in the model. Both \\(\\alpha\\) and \\(\\beta\\) are given normal prior distributions:\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\]\nThe linear model is appropriate when there is strong scientific knowledge that the shape of the response curve should be linear as a function of dose strength. Note that if an arm has a dose strength between two other arms in the model, its response will always be estimated to be between the responses of those two arms. As a result, the linear model may not be appropriate in combination with certain decision quantities such as Pr(Max), because some doses are guaranteed to have probability zero of being the best arm. If the control arm is included in the linear model and it has the smallest dose value, then all experimental arms will always have exactly the same probability of being better than control.\nWe recommend creating scenarios where the true parameters deviate from linearity and simulating them to test the robustness of a design based on the linear model.\n\n\n\n\n\n\nNote\n\n\n\nFor dichotomous models, the linear model replaces the 2-parameter logistic model available in FACTS versions before 6.4. The two models are identical.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-linear-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-linear-model",
    "title": "Dose Response Models",
    "section": "Hierarchical Linear Model",
    "text": "Hierarchical Linear Model\nA more flexible version of the linear model is the hierarchical linear model. This model encourages the arm responses to have a linear-like shape, but also includes a term allowing parameters to deviate from linearity when appropriate. The arm parameters satisfy the relationship\n\\[\\theta_d = \\alpha + \\beta \\nu_d + \\zeta_d\\] where the \\(\\alpha\\) and \\(\\beta\\) parameters are as in the linear model, with prior distributions\n\\[\\alpha \\sim \\text{N}(\\Lambda_\\alpha, \\lambda_\\alpha^2)\\] \\[\\beta \\sim \\text{N}(\\Lambda_\\beta, \\lambda_\\beta^2)\\] and the \\(\\zeta_d\\) parameters are the deviations from linearity. These deviations are assumed to have a normal distribution constrained so that their average value is zero:\n\\[\\zeta_d \\sim \\text{N}(0, \\tau^2) \\text{ with } \\sum_d\\zeta_d=0.\\]\nThe prior distribution for \\(\\tau^2\\) is\n\\[\\tau^{2}\\sim\\text{IG}\\left(\\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nIf \\(\\tau^2\\) is small, which can be encouraged by choosing \\(\\tau_\\mu\\) to be small and \\(\\tau_n\\) to be large, then the dose parameter estimates will lie close to a line. See here for help understanding FACTS’s parameterization of the inverse gamma distribution.\nThe hierarchical linear model is a good choice when, for example, one expects a linear relationship between the dose strengths and the response but one is not prepared to assume exact linearity. It can also be a good way of encouraging, but not requiring, monotonicity in the response.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#d-continuous-factorial-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#d-continuous-factorial-model",
    "title": "Dose Response Models",
    "section": "2D Continuous Factorial Model",
    "text": "2D Continuous Factorial Model\nThe 2D Continuous Factorial Model fits a regression on the dose response that uses the numeric dose strength as a continuous valued covariate. The dose response (with \\(\\eta_r\\) and \\(\\zeta_c\\) denoting dose strength of the row level and column level, respectively) is modeled as:\n\\[\\theta_{rc} = \\alpha_0 + \\alpha_1 \\zeta_c + \\alpha_2 \\eta_r + \\alpha_3\\zeta_c \\eta_r\\] With priors\n\\[\\alpha_0 \\sim \\text{N}(\\mu_0, \\sigma_0^2)\\] \\[\\alpha_1 \\sim \\text{N}(\\mu_1, \\sigma_1^2)\\] \\[\\alpha_2 \\sim \\text{N}(\\mu_2, \\sigma_2^2)\\] \\[\\alpha_3 \\sim \\text{N}(\\mu_3, \\sigma_3^2)\\]\nThen, \\(\\alpha_0\\) is the response at the control combination, \\(\\alpha_1\\) is the linear coefficient of the response to the column factor strengths \\(\\zeta_c\\), and \\(\\alpha_2\\) is the linear coefficient of the response to the row factor strengths \\(\\eta_r\\).\nThe user has the option to simplify the model and exclude the interaction term \\(\\alpha_3\\), which is the coefficient of the product of the two factor strengths.\nNote that despite being called “continuous” this dose response model applies to all endpoints. It’s called continuous because the coefficients act on the dose strengths as continuous measures of strength.\n\n\n\n\n\n\nFigure 7: 2D Continuous Factorial Dose Response model specification page.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#d-discrete-factorial-model",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#d-discrete-factorial-model",
    "title": "Dose Response Models",
    "section": "2D Discrete Factorial Model",
    "text": "2D Discrete Factorial Model\nThe 2D Discrete Factorial Model fits a regression on the dose response that uses the dose strength as a factor covariate rather than a numeric dose strength. Each dose is an independent level within each row or column factor, so any dose is equidistant from all other doses\n\\[\\theta_{rc} = \\alpha + \\gamma_r + \\beta_c\\]\nWith priors\n\\[\\alpha \\sim \\text{N}(\\mu_\\alpha, \\sigma_\\alpha^2)\\]\n\\[\\beta_c \\sim \\text{N}(\\mu_{\\beta_c}, \\sigma_{\\beta_c}^2)\\] \\[\\gamma_r \\sim \\text{N}(\\mu_{\\gamma_r}, \\sigma_{\\gamma_r}^2)\\]\nThe parameters associated with lowest level of each factor, \\(\\gamma_0\\) and \\(\\beta_0\\), are constrained to be \\(0\\).\n\n\n\n\n\n\nFigure 8: 2D Discrete Factorial Dose Response model specification page.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#d-ndlm",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#d-ndlm",
    "title": "Dose Response Models",
    "section": "2D NDLM",
    "text": "2D NDLM\nThe 2-D NDLM Model takes the idea of smoothing across doses that was used in the standard NDLM model, but smooths across row factors and column factors separately.\n\nThe Base Model, with Control Included\nThe treatment effect for the combination of level \\(r\\) in the row factor and level \\(c\\) in the column factor is denoted as \\(\\theta_{rc}\\), and \\(Y_{rc}\\) is the observed data in that cell. The borrowing parameters are denoted as \\(\\phi\\) for the row factor smoothing, and \\(\\tau\\) for the column factor smoothing. The dose strengths are denoted as \\(\\nu_r\\) for the row factors, and \\(\\omega_c\\) for the column factors. Let \\(\\Delta \\nu_r = \\nu_r - \\nu_{r-1}\\) and \\(\\Delta \\omega_c = \\omega_c - \\omega_{c-1}\\) (for \\(r&gt;0\\) and \\(c&gt;0\\)). For notational convenience at the grid edge, let \\(\\theta_{-1, c} = 0\\), \\(\\theta_{r,-1}\\), \\(\\Delta\\nu_0\\equiv\\infty\\), and \\(\\Delta\\omega_0\\equiv\\infty\\).\nThe 2-D NDLM Model with control included in the model can then be specified as:\n\\[\\theta_{0,0} \\sim \\text{N}(\\mu_0, \\tau_0^2)\\] \\[\\theta_{rc} \\sim \\text{N}(\\mu_{rc}, \\tau_{rc}^2)\\] where\n\\[\\tau_{rc}^{2} = \\left( \\frac{1}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{1}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)^{- 1}\\]\n\\[\\mu_{rc} = \\tau_{rc}^{2}\\left( \\frac{\\theta_{r - 1,c}}{\\mathrm{\\Delta}\\nu_{r}\\phi^{2}} + \\frac{\\theta_{r,c - 1}}{\\mathrm{\\Delta}\\omega_{c}\\tau^{2}} \\right)\\]\nwith priors\n\\[\\tau^{2}\\sim\\text{IG}\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\n\\[\\phi^{2}\\sim\\text{IG}\\left( \\frac{\\phi_{n}}{2},\\frac{\\phi_{\\mu}^{2}\\phi_{n}}{2} \\right)\\]\nNote: that not all combinations of \\(r\\) and \\(c\\) will have data associated with them. However, FACTS will still evaluate the prior at all combinations. Thus, in the situation depicted below where the grayed-out point indicates a treatment combination with no allocation, \\(\\theta_{1,2}\\) is not modeled conditioned only on \\(\\theta_{1,1}\\). \\(\\theta_{0,1}\\) also informs on \\(\\theta_{1,2}\\) via \\(\\theta_{0,2}\\).\n\n\n\n\n\n\nFigure 9: 2D dosing grid example with no data for (0,2).\n\n\n\n\n\nFix smoothing ratio for row factor and column factor\nOptionally, the user might want to specify the row factor borrowing as a function of the column factor borrowing:\n\\[\\phi\\equiv k \\cdot \\tau\\] where \\(k\\) is a user-specified constant. This can be performed by selecting “Fixed smoothing ratio” in the \\(\\phi^2\\) prior specification area.\n\n\nControl not in model, no zero-level doses\nIf neither treatment arm allows zero-level doses (e.g. like one would expect if the column factor represented doses and the row factor represented frequency), then a prior for the lowest dose would instead be needed as input from the GUI:\n\\[\\theta_{1,1} \\sim \\text{N}(\\mu_1, \\tau_1^2)\\]\n\n\n\n\n\n\nFigure 10: 2D NDLM Dose Response model specification page.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-priors",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#hierarchical-priors",
    "title": "Dose Response Models",
    "section": "Hierarchical Priors",
    "text": "Hierarchical Priors\nIf a hierarchical prior is used for either the Control or Active Comparator arm, then FACTS will estimate a prior for that arm using external data that the user provides. This prior is estimated through a hierarchical model, and then the in-study data collected on subjects randomized to that arm are used to estimate the posterior distribution for the arm.\nTo specify a hierarchical prior, the user specifies the sufficient statistics from each historical study. These are:\n\nContinuous: the mean response and the SD of the response of the control or active comparator arm and the number of subjects observed.\nDichotomous: the observed number of responders and the number of subjects observed in the study.\nTime-to-Event: the number of events and the amount of exposure within each bin in the piecewise model.\n\nThe information from the historical studies can be ‘down-weighted’ by decreasing the effective information in the sample size. For continuous, this can be done by decreasing the sample size by a percentage. For dichotomous, both the number of responders and the number of subjects would be decreased by a percentage. For time-to-event, multiplying the number of events and the exposure by the same fraction will reduce the information in the study without changing the reported hazard rate.\n\n\n\n\n\n\nFigure 11: The Hierarchical Priors tab for a continuous study when both the Control and Active Comparator are given hierarchical priors.\n\n\n\nThe hierarchical models for the control or active comparator rates are very similar across the endpoints. They are briefly described below.\n\nContinuousDichotomousTime-to-Event\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\theta_{0t} \\sim \\text{N}(\\mu_0, \\tau_0^2) \\text{ for } \\tau=0,1,2,\\ldots,T\\] where \\(\\theta_{0t}\\) is the mean for the control arm in trial \\(t\\) (\\(t=0\\) for the current trial and \\(t=1,2,\\ldots,T\\) for previous trials). A user needs to specify appropriate priors for the hyper-parameters:\n\\[\\mu_0 \\sim \\text{N}(m_0, \\sigma_0^2)\\] \\[\\tau_0^2 \\sim \\text{IG}(a_0, b_0)\\]\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\theta_{0t} \\sim \\text{N}(\\mu_0, \\tau_0^2) \\text{ for } \\tau=0,1,2,\\ldots,T\\] where \\(\\theta_{0t}\\) is the log-odds for the control arm in trial \\(t\\) (\\(t=0\\) for the current trial; \\(t=1,2,\\ldots,T\\) for previous trials). A user needs to specify appropriate priors for the hyper-parameters:\n\\[\\mu_0 \\sim \\text{N}(m_0, \\sigma_0^2)\\] \\[\\tau_0^2 \\sim \\text{IG}(a_0, b_0)\\]\nThe prior distributions for the hyper-parameters of the hierarchical model. The hyper-parameters are the mean and standard deviation of Normal distribution for the log hazard ratios of the event rates of the historical studies and the current study.\n\n\nThe model used for incorporating data from previous trials is as follows:\n\\[\\lambda_{st}=\\lambda_s \\exp(\\gamma_t) \\text{ for } t=0,1,2,\\ldots,T\\] where \\(\\lambda_{st}\\) is the hazard rate for the control arm in segment \\(s\\) (\\(s=1,2,\\ldots,S\\)) for previous trial \\(t\\) (\\(t=1,2,\\ldots,T\\)) and \\(\\lambda_{s0}\\) is the hazard rate for the current control arm in segment \\(s\\); \\(\\lambda_s\\) is a base hazard for segment \\(s\\); and \\(\\gamma_t\\) is the log hazard ratio between that base rate and the \\(\\lambda_{st}\\) values.\nThe following hierarchical model is used\n\\[\\gamma_t \\sim \\text{N}(\\mu_\\gamma, \\tau_\\gamma^2) \\text{ for } t=0,1,2,\\ldots,T\\] Users specify priors for the hyper-parameters:\n\\[\\mu_\\gamma \\sim \\text{N}(m_\\gamma, t_\\gamma^2)\\] \\[\\tau^2 \\sim \\text{IG}(a_\\gamma, b_\\gamma)\\]\nThe formulation above is not identifiable as changes in \\(\\lambda_s\\) can be compensated for by changes in the \\(\\gamma_t\\) values (thus, one can use different combination of \\(\\lambda_s\\) and \\(\\gamma_t\\), but acquire the same set of values \\(\\lambda_{st}\\) and thus the same likelihood). To avoid this difficulty, we use the above formulation but fix \\(\\gamma_0 = 0\\). In addition to preserving the identifiability of the structure, this constraint allows \\(\\lambda_s\\) to have the interpretation of being the hazard rate for the current control arm, and thus the prior on \\(\\lambda_s\\) from the main dose response may be used as the prior for \\(\\lambda_s\\).\n\n\n\n\nSetting Priors for Hierarchical Model Hyper Parameters\nUnless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.\nIn this case the following would be reasonable:\n\nSet the prior mean value for Mu as the mean of the mean responses on the control arm in the historic studies\nSet the prior SD for Mu equal to the average SD of response divided by the square root of the number of studies.\nSet the center for tau to the same value as the prior SD for Mu.\nSet the weight for tau to be &lt; 1.\n\nOne can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small (relative to the number of studies), then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.\nTo give some prior preference towards pooling or separate analysis the weight for tau has to be large (relative to the number of historic studies) – to have a design that is like pooling all the historic studies the mean for tau needs to be small (say 10% or less of the value suggested above). For there to be no borrowing from the historic studies the value for tau needs to be large (say 10x or more the value suggested above).\nThe best way to understand the impact of the priors is try different values and run simulations.\n\n\nBayesian Augmented Control (BAC) Example:\nIt is easiest to study the impact of BAC in a simplified version of the trial being designed, for example use a 2 arm fixed design with ‘no model’, no adaptation, no longitudinal modeling, etc.\nFor instance, in a continuous setting with an expected sample size of 50 on control, a mean response of 5 and an SD in the response of 2, looking at the effect of having 4 prior, similarly sized studies:\n\n\n\nExternal Study Sufficient Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of subjects\n\n\nMean Response\n\n\nSD of Response\n\n\n\n\n\n\nStudy 1\n\n\n50\n\n\n4.76\n\n\n2\n\n\n\n\nStudy 2\n\n\n50\n\n\n4.93\n\n\n2\n\n\n\n\nStudy 3\n\n\n50\n\n\n5.07\n\n\n2\n\n\n\n\nStudy 4\n\n\n50\n\n\n5.24\n\n\n2\n\n\n\n\nFor simplicity we have just varied the means of the studies – they are all consistent with a normally distributed estimate, with mean of 5 and a standard error of \\(\\frac{2}{\\sqrt{50}}\\).\nBy simulating a fixed trial with a sample size of 100 and fixed 1:1 allocation between control and a study arm we can easily observe the effects of a BAC on the control arm. Creating 6 VSR profiles with sigma = 2, and the same mean response for control and the study arm in each profile of: 4.53, 4.76, 5.07, 5.24 and 5.47 we can look at the bias of the final estimate and the shrinkage in the SD of the estimate that could be brought about by using BAC:\n\n\n\nQuick simulation study of how the hierarchical model for BAC effects estimates of the control rate under different true control rate scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Mean\n\n\nRaw mean\n\n\nRaw SD\n\n\nEstimate inc BAC\n\n\nSD inc BAC\n\n\nBias\n\n\nEffective additional Subjects\n\n\n\n\n\n\n4.53\n\n\n4.55\n\n\n0.28\n\n\n4.64\n\n\n0.255\n\n\n2.1%\n\n\n11.1\n\n\n\n\n4.76\n\n\n4.78\n\n\n0.28\n\n\n4.83\n\n\n0.250\n\n\n1.0%\n\n\n13.5\n\n\n\n\n4.93\n\n\n4.95\n\n\n0.28\n\n\n4.96\n\n\n0.248\n\n\n0.2%\n\n\n14.3\n\n\n\n\n5.07\n\n\n5.09\n\n\n0.28\n\n\n5.07\n\n\n0.248\n\n\n-0.4%\n\n\n14.2\n\n\n\n\n5.24\n\n\n5.26\n\n\n0.28\n\n\n5.20\n\n\n0.250\n\n\n-1.1%\n\n\n13.2\n\n\n\n\n5.46\n\n\n5.49\n\n\n0.28\n\n\n5.38\n\n\n0.255\n\n\n-1.9%\n\n\n10.7\n\n\n\n\nNote it is important to use the column “SD Mean resp 1” from the simulations file to see the SD of the estimate and not use the SD of the estimates in the summary table.\nThe small shrinkage in the SD is more impressive when viewed in terms of how many additional subjects would have to be recruited to achieve the same shrinkage in the raw estimate. These are useful savings for small additional bias.\nThe “effective additional subjects” was calculated: \\[\\left( \\frac{\\text{True sigma}}{\\text{AVG(SD Mean resp)}} \\right)^{2} - \\left( \\frac{\\text{True sigma}}{\\text{AVG(SE Mean Raw Response)}} \\right)^{2}\\] where in this example \\(\\text{True sigma}\\) was 2.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/doseresponse.html#time-to-event-missingness",
    "href": "documentation/v71/userguides/core/design/doseresponse.html#time-to-event-missingness",
    "title": "Dose Response Models",
    "section": "Time-to-Event Missingness",
    "text": "Time-to-Event Missingness\nFor a time-to-event endpoint, as is the convention in this endpoint, subjects are observed up to the time they have an event (or the end of follow up), so no subject drops out after having had an event. Subjects that do drop-out are included in the analysis as not having had an event up to the last observation of the subject.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Dose Response Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/frequentistanalysis.html",
    "href": "documentation/v71/userguides/core/design/frequentistanalysis.html",
    "title": "Frequentist Analysis",
    "section": "",
    "text": "On the Frequentist Analysis tab the user can specify that some standard frequentist analyses be performed at trial analyses. The frequentist analysis tab is completely separate from, and independent of, any p-value QOIs that have been defined. The analyses specified on this tab cannot be used for simulated trial decisions - they are for storing in output only.\nEach specified analysis can be conducted using a variety of ways of handling missingness. Select all ways of handling missingness that are desired:\n\nMissing data replaced by last observation carried forward (LOCF)\nMissing data replaced by baseline observation carried forward (BOCF). This is only available if the endpoint is continuous and Baseline is being simulated.\nMissing data is ignored (a “per-protocol” analysis).\nMissing data is treated as a failure. This is only available if the endpoint is dichotomous.\n\nIf the trial has interim analyses, then for the simulations for which frequentist weeks files are to be output (specified on the Simulation tab) the standard frequentist analyses will be performed. If the trial has p-values QOIs, those QOIs are calculated every interim in all simulations.\nHaving the frequentist analysis include Dunnett’s adjusted p-values is a separate option (that applies to all the analysis type requested) because of the significant run-time overhead this can entail. Dunnett’s adjustment is available for continuous and dichotomous frequentist analyses.\nThe frequentist analysis tabs for the continuous and dichotomous engines also have trend tests, and allow the user to specify contrast coefficients to conduct those tests.\nNote that the reported frequentist estimates of the treatment effect take the specified direction of response on the Study tab (whether a response indicates subject improving or worsening) into account. They are adjusted so that a treatment that is estimated to be better than the control always has a positive treatment effect.\n\n\n\n\n\n\nFigure 1: The frequentist analysis tab for a continuous endpoint.\n\n\n\n\nContinuous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing unadjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\n(p-value)[## “1-sided p-value is reported in all the frequentist results. This is done in order to be consistent with comparisons with 1-sided α-values elsewhere.”],\nconfidence interval for the mean difference,\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nIf selected, using Dunnett-adjusted dose-placebo comparisons based on a two-sample t-test calculate the:\n\ntest statistic,\np-value,\nconfidence interval for the mean difference\nfor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were) .\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nIf neither placebo nor an active comparator are simulated, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out.\nIf high values of the endpoint are good, P-values are calculated using a one-sided t-test testing \\[H_0: \\mu_T &lt; \\mu_C\\] against \\[H_1: \\mu_T \\ge \\mu_C\\] with \\(\\mu_T\\) being the true treatment response mean and \\(\\mu_C\\) being the true control response mean. If low values of the endpoint are good, then the signs of the hypotheses are flipped.\n\n\nDichotomous Endpoints\nAt the end of each simulated trial the following frequentist values will be calculated for each missingness handling method checked:\n\nUsing the methodology described by Agresti, Mee and Nurminem for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nIf checked, using Dunnett-adjusted dose-placebo comparisons for comparing the difference of proportions:\n\ntest statistic,\np-value,\n95% confidence interval for the difference in proportions,\nmarginal probabilities of significance.\n\nUsing the general trend test calculate the t-test statistic and p-value using user supplied contrast coefficients.\n\nP-values are calculated by user choice either using a normal approximation (see previous section on p-value calculations for continuous endpoints) or a Fisher’s exact test, which is a conservative test to test the null hypothesis that the outcome is independent of the treatment assignment. For the normality approximation to have good asymptotic coverage, the expected number of successes and failures under the null hypothesis should be greater or equal to 5 for both groups. In cases where this is not guaranteed, Fisher’s exact test is recommended when strict type 1 error control is required.\nIf neither placebo nor active comparator are specified, a difference from 0 is assessed for each dose arm and Dunnett-adjusted calculations are not carried out.\n\n\nTime-to-Event Frequentist Analysis\nFor each simulated trial, the following frequentist analyses will be performed:\n\nDose-placebo comparisons based on log-rank test and the Cox proportional hazards model. Summaries include:\n\nThe log-rank and Wilcoxon test statistics and the corresponding p-values,\nEstimated hazard ratio and its confidence interval from Cox model,\nFor each dose its marginal probabilities of significance (the number of times it was significantly separated from placebo independent of whether any other doses were, based on the hazard ratio inference from the Cox model).\n\nMedian survival times based on the Kaplan-Meier method.\nFor the predictor, descriptive statistics are computes – a 7 number summary for a continuous predictor, percentages for the dichotomous predictor, and median time, average hazard, and a cox model with predictor as covariate are computed for a time to event predictor.\n\nThe following variations for adjusting the alpha level for multiple comparisons should be available using the user-specified 1-sided alpha: unadjusted, and Bonferonni adjusted.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Frequentist Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/glossary.html",
    "href": "documentation/v71/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page provides definitions of terms, acronyms, and abbreviations that are commonly used across FACTS documentation.\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nName\nDefinition\n\n\n\n\n3+3\n3+3 The conventional 3-person cohort phase 1 design for Oncology trials with fixed dose escalation and de-escalation rules based only on the toxicities observed in the last cohort.\n\n\nActive Comparator\nA treatment arm where subjects are treated with some existing therapy which the user either wishes to compare the novel treatment to (in addition to control), or wishes to use to check that the study has been properly designed to show a difference from control.\n\n\nBaseline\nThe subject’s baseline is their endpoint value before treatment on the trial is started. In FACTS, the subject’s baseline is measured at their first visit at follow-up time 0, any prior visits (e.g. for screening to see if the patient is eligible for the trial) are not included in the simulation.\n\n\nbCRM\nA “bi-variate” form of the CRM that analyses both an efficacy and a toxicity endpoint.\n\n\nCap\nA limit on the number of subjects recruited. In FACTS N-CRM users can specify a cap on the overall number of subjects to be recruited in the trial (the ‘Overall Cap’) and specify stopping rules to define when the trial should stop before it reaches cap.\n\n\nControl\nIs the treatment arm with which the novel treatment(s) are principally being compared. Control may be placebo, or some existing standard of care, or therapy, against which the novel treatment has to be benchmarked in order to determine its likely usefulness.\n\n\nCore\nFACTS Core: A mode of FACTS for designing trials where multiple treatments, (possibly different doses of a novel treatment) are tested against a control and optionally an active comparator.\n\n\nCRM\nContinual Reassessment Method – a dose escalation design where the dose-toxicity is estimated using a simple Bayesian model, and the resulting estimates used to control the dose escalation and estimate the Maximum Tolerated Dose (MTD).\n\n\nDE\nDose Escalation: a mode of FACTS where subjects are treated in cohorts and dose escalation is determined by the number of toxicities observed.\n\n\nDichotomized endpoint\nA dichotomous endpoint is created by measuring a continuous endpoint and scoring a subject as a responder if their score is above or below a specified threshold. FACTS allows the underlying continuous endpoint to be modeled longitudinally in order to provide a better prediction of whether a subject will be a responder at their final visit, or not.\n\n\nDose Response Model\nA model used in the statistical analysis of the final response as a function of the treatment dose strength. FACTS includes both parametric and non-parametric models, including ‘no model’.\n\n\nED\nEnrichment Designs: a mode of FACTS for designing trials where the same treatment is testing in different settings for example different sub-populations or different but related indications.\n\n\nEndpoint\nAn endpoint is a measure of the subject’s health that is being analyzed in order to learn about the effects of the treatments being studied in the trial.\n\n\nFACTS\nFixed and Adaptive Clinical Trial Simulator.\n\n\nFinal Endpoint\nThe value, or state, of a subject’s endpoint at the last visit in the follow-up schedule.\n\n\nGroup\nThe very neutral term ‘groups’ is used in ED, because the ED design could be used in a number of different settings. The groups could be for instance different patient sub-populations with the same disease, different disease sub-populations such as different sarcoma types, or even different but related diseases that the same treatment might be effective against (migraine, dental pain and post-operative pain for instance). The key characteristic is that there is a need to study each independently (with their own control) but also look across the different groups for some purpose – this might be for analysis where there is an expectation of commonality in response on control, or in the difference from control on the treatment arms; or operationally where there is limited budget, drug supply or time across the groups; or for the purposes of decision making – where the treatment has to be successful in more than one group to be worth considering for further development.\n\n\nGUI\nGraphical User Interface, the visual part of the FACTS application that the user interacts with.\n\n\nHistorical Control\nA ‘historic control’ arm is used when no control arm is randomized to in the study, and the response on the arms where the novel treatment administered are compared to combined data from control arms from other already complete studies.\n\n\nInterim Visit\nA visit between the baseline visit and final visit, at which a subject’s endpoints are measured.\n\n\nIntermediate Endpoint\nThe value of a subject’s endpoint at a visit after their baseline visit, but before their final visit. Intermediate endpoint values can be used to estimate, using a longitudinal model, what the subject’s final endpoint will be (or would have been) thus providing additional information for analyses. Multiple imputation is used ensure that these estimated responses are included in the analysis with all due uncertainty. Sometimes called an early endpoint.\n\n\nLongitudinal Model\nAn analysis model that models the relationship between a subject’s interim endpoint values and their final endpoint value. In FACTS, all longitudinal models are simply multiple imputation models that can be used to impute a subjects final endpoint value when it is not available.\n\n\nMethod\nIn the FACTS documentation we try to reserve the term ‘method’ for the algorithms used in the simulation (as opposed to the analysis) part of the program. In the analysis part we use the term ‘Model’, see below.\n\n\nModel\nIn the FACTS documentation we try to reserve the term ‘model’ for the statistical models used in the analysis of the trial data (in the ‘design’ section of the FACTS user interface). Where mathematical algorithms are used for other purposes in FACTS (for instance in the generation of the simulated data) we try to use the term ‘method’. It is common for users to confuse the data generation and the trial implementation components of FACTS, and using distinct terminology may help to reduce this.\n\n\nMTD\nThe dose most likely to be the Maximum Tolerated Dose (MTD) – the dose with the highest Pr(MTD).\n\n\nMTD+\nThe dose most likely to be the MTD+ – the dose with the highest Pr(MTD+).\n\n\nMultiple Imputation\nWhen the Bayesian statistical models are fit to simulated data within FACTS, if longitudinal modeling has been included, then multiple imputation is used for missing subject final endpoint values. This includes subjects whose final endpoint data are missing due to the subject having dropped out and subjects who have not yet had enough follow-up time to observe their final visit response yet. Imputed final endpoint values are separately sampled at each iteration of the MCMC from the posterior distribution of the longitudinal model, given the subject’s interim endpoint values.\n\n\nOSD\nFor those designs with both a toxicity and efficacy endpoint the OSD is the Optimum Selected Dose, this will be the MED if the MED is below the MTD, otherwise it will be the MTD.\n\n\nOSD+\nFor those designs with both a toxicity and efficacy endpoint the OSD+ is the Optimum Selected Dose, this will be the MED+ if the MED+ is below the MTD+, otherwise it will be the MTD+.\n\n\nPr(MTD)\nA dose’s probability of being MTD is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable toxicity band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. This estimate of MTD is constrained to select one of the available doses.\n\n\nPr(MTD+)\nA dose’s probability of being the MTD+ is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. Unlike Pr(MTD), Pr(MTD+) includes estimating whether a dose below or a dose above the range of those being tested is more likely to have a toxicity in the acceptable band than any of the doses in the range.\n\n\nProfile\nA profile is a specification of one aspect of a scenario. Examples of these aspects are Accrual Rate, VSR, Dropout Rate, and others. A scenario is made up of one profile of each of the required types for the type of trial being simulated. FACTS allows the user to specify multiple profiles of each type and then presents all the possible combinations of profiles as scenarios that can be used to drive simulations.\n\n\nQOI\nQuantity Of Interest - A value to be calculated because it is of interest to the proceeding of the simulated trials. Quantities may be of interest because they are to be used in an interim decision, to adapt allocation, for final evaluation, or to ensure it is value is written out in the FACTS results files for analysis outside of FACTS.\n\n\nResponse\nA synonym of endpoint value. May be the value of a subjects final endpoint, or a change in a subject’s endpoint compared to their baseline state.\n\n\nRestricted Markov\nA form of the Dichotomous endpoint where, rather than the subject’s endpoint being either 0 or 1 and able to switch between them from visit to visit, their endpoint value is ‘stable’ until it becomes either ‘response’ or ‘failure’. Once their state has become ‘response’ or ‘failure’ it can then not change. For example ‘failure’ could be “subject has resumed smoking” in a smoking cessation trial, or death in an oncology trial.\n\n\nScenario\nA scenario is the complete specification of the unknown external factors that determine the data observed on the trial and its timing. The exact factors depend on the type of trial being simulated, but typically include:\n\nthe distribution of the final change from base line, or probability of response or rate of events in the different treatment groups\nthe properties of subjects’ early responses and the correlation with their final outcome\nthe rate at which subjects are recruited into the trial\nthe rate at which subjects drop out of the trial.\n\n\n\nSPEC\nThe Design Engine Specification document, describes the system algorithms, and meaning of parameters in a more technical context. SPEC documents have been deprecated as of FACTS 7.1.1\n\n\nSubject\nAn entity recruited onto a clinical trial for the purposes of learning about the properties of a treatment. Depending on the type of trial they might be patients or they might be healthy volunteers.\n\n\nTreatment Arm\nAn arm being studied in a clinical trial. A treatment arm may refer to different doses of the same treatment or completely separate therapies. Subjects, upon entering a study, are randomized to a treatment arm.\n\n\nUG\nThe User Guide document - describes in detail how to use a FACTS engine.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Glossary"
    ]
  },
  {
    "objectID": "documentation/v71/glossary.html#overview",
    "href": "documentation/v71/glossary.html#overview",
    "title": "Glossary",
    "section": "",
    "text": "This page provides definitions of terms, acronyms, and abbreviations that are commonly used across FACTS documentation.\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nName\nDefinition\n\n\n\n\n3+3\n3+3 The conventional 3-person cohort phase 1 design for Oncology trials with fixed dose escalation and de-escalation rules based only on the toxicities observed in the last cohort.\n\n\nActive Comparator\nA treatment arm where subjects are treated with some existing therapy which the user either wishes to compare the novel treatment to (in addition to control), or wishes to use to check that the study has been properly designed to show a difference from control.\n\n\nBaseline\nThe subject’s baseline is their endpoint value before treatment on the trial is started. In FACTS, the subject’s baseline is measured at their first visit at follow-up time 0, any prior visits (e.g. for screening to see if the patient is eligible for the trial) are not included in the simulation.\n\n\nbCRM\nA “bi-variate” form of the CRM that analyses both an efficacy and a toxicity endpoint.\n\n\nCap\nA limit on the number of subjects recruited. In FACTS N-CRM users can specify a cap on the overall number of subjects to be recruited in the trial (the ‘Overall Cap’) and specify stopping rules to define when the trial should stop before it reaches cap.\n\n\nControl\nIs the treatment arm with which the novel treatment(s) are principally being compared. Control may be placebo, or some existing standard of care, or therapy, against which the novel treatment has to be benchmarked in order to determine its likely usefulness.\n\n\nCore\nFACTS Core: A mode of FACTS for designing trials where multiple treatments, (possibly different doses of a novel treatment) are tested against a control and optionally an active comparator.\n\n\nCRM\nContinual Reassessment Method – a dose escalation design where the dose-toxicity is estimated using a simple Bayesian model, and the resulting estimates used to control the dose escalation and estimate the Maximum Tolerated Dose (MTD).\n\n\nDE\nDose Escalation: a mode of FACTS where subjects are treated in cohorts and dose escalation is determined by the number of toxicities observed.\n\n\nDichotomized endpoint\nA dichotomous endpoint is created by measuring a continuous endpoint and scoring a subject as a responder if their score is above or below a specified threshold. FACTS allows the underlying continuous endpoint to be modeled longitudinally in order to provide a better prediction of whether a subject will be a responder at their final visit, or not.\n\n\nDose Response Model\nA model used in the statistical analysis of the final response as a function of the treatment dose strength. FACTS includes both parametric and non-parametric models, including ‘no model’.\n\n\nED\nEnrichment Designs: a mode of FACTS for designing trials where the same treatment is testing in different settings for example different sub-populations or different but related indications.\n\n\nEndpoint\nAn endpoint is a measure of the subject’s health that is being analyzed in order to learn about the effects of the treatments being studied in the trial.\n\n\nFACTS\nFixed and Adaptive Clinical Trial Simulator.\n\n\nFinal Endpoint\nThe value, or state, of a subject’s endpoint at the last visit in the follow-up schedule.\n\n\nGroup\nThe very neutral term ‘groups’ is used in ED, because the ED design could be used in a number of different settings. The groups could be for instance different patient sub-populations with the same disease, different disease sub-populations such as different sarcoma types, or even different but related diseases that the same treatment might be effective against (migraine, dental pain and post-operative pain for instance). The key characteristic is that there is a need to study each independently (with their own control) but also look across the different groups for some purpose – this might be for analysis where there is an expectation of commonality in response on control, or in the difference from control on the treatment arms; or operationally where there is limited budget, drug supply or time across the groups; or for the purposes of decision making – where the treatment has to be successful in more than one group to be worth considering for further development.\n\n\nGUI\nGraphical User Interface, the visual part of the FACTS application that the user interacts with.\n\n\nHistorical Control\nA ‘historic control’ arm is used when no control arm is randomized to in the study, and the response on the arms where the novel treatment administered are compared to combined data from control arms from other already complete studies.\n\n\nInterim Visit\nA visit between the baseline visit and final visit, at which a subject’s endpoints are measured.\n\n\nIntermediate Endpoint\nThe value of a subject’s endpoint at a visit after their baseline visit, but before their final visit. Intermediate endpoint values can be used to estimate, using a longitudinal model, what the subject’s final endpoint will be (or would have been) thus providing additional information for analyses. Multiple imputation is used ensure that these estimated responses are included in the analysis with all due uncertainty. Sometimes called an early endpoint.\n\n\nLongitudinal Model\nAn analysis model that models the relationship between a subject’s interim endpoint values and their final endpoint value. In FACTS, all longitudinal models are simply multiple imputation models that can be used to impute a subjects final endpoint value when it is not available.\n\n\nMethod\nIn the FACTS documentation we try to reserve the term ‘method’ for the algorithms used in the simulation (as opposed to the analysis) part of the program. In the analysis part we use the term ‘Model’, see below.\n\n\nModel\nIn the FACTS documentation we try to reserve the term ‘model’ for the statistical models used in the analysis of the trial data (in the ‘design’ section of the FACTS user interface). Where mathematical algorithms are used for other purposes in FACTS (for instance in the generation of the simulated data) we try to use the term ‘method’. It is common for users to confuse the data generation and the trial implementation components of FACTS, and using distinct terminology may help to reduce this.\n\n\nMTD\nThe dose most likely to be the Maximum Tolerated Dose (MTD) – the dose with the highest Pr(MTD).\n\n\nMTD+\nThe dose most likely to be the MTD+ – the dose with the highest Pr(MTD+).\n\n\nMultiple Imputation\nWhen the Bayesian statistical models are fit to simulated data within FACTS, if longitudinal modeling has been included, then multiple imputation is used for missing subject final endpoint values. This includes subjects whose final endpoint data are missing due to the subject having dropped out and subjects who have not yet had enough follow-up time to observe their final visit response yet. Imputed final endpoint values are separately sampled at each iteration of the MCMC from the posterior distribution of the longitudinal model, given the subject’s interim endpoint values.\n\n\nOSD\nFor those designs with both a toxicity and efficacy endpoint the OSD is the Optimum Selected Dose, this will be the MED if the MED is below the MTD, otherwise it will be the MTD.\n\n\nOSD+\nFor those designs with both a toxicity and efficacy endpoint the OSD+ is the Optimum Selected Dose, this will be the MED+ if the MED+ is below the MTD+, otherwise it will be the MTD+.\n\n\nPr(MTD)\nA dose’s probability of being MTD is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable toxicity band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. This estimate of MTD is constrained to select one of the available doses.\n\n\nPr(MTD+)\nA dose’s probability of being the MTD+ is the probability that it is the dose with the highest probability of having a toxicity rate in the acceptable band, and (if a threshold has been specified) does not have a probability of excess or unacceptable toxicity above the threshold. Unlike Pr(MTD), Pr(MTD+) includes estimating whether a dose below or a dose above the range of those being tested is more likely to have a toxicity in the acceptable band than any of the doses in the range.\n\n\nProfile\nA profile is a specification of one aspect of a scenario. Examples of these aspects are Accrual Rate, VSR, Dropout Rate, and others. A scenario is made up of one profile of each of the required types for the type of trial being simulated. FACTS allows the user to specify multiple profiles of each type and then presents all the possible combinations of profiles as scenarios that can be used to drive simulations.\n\n\nQOI\nQuantity Of Interest - A value to be calculated because it is of interest to the proceeding of the simulated trials. Quantities may be of interest because they are to be used in an interim decision, to adapt allocation, for final evaluation, or to ensure it is value is written out in the FACTS results files for analysis outside of FACTS.\n\n\nResponse\nA synonym of endpoint value. May be the value of a subjects final endpoint, or a change in a subject’s endpoint compared to their baseline state.\n\n\nRestricted Markov\nA form of the Dichotomous endpoint where, rather than the subject’s endpoint being either 0 or 1 and able to switch between them from visit to visit, their endpoint value is ‘stable’ until it becomes either ‘response’ or ‘failure’. Once their state has become ‘response’ or ‘failure’ it can then not change. For example ‘failure’ could be “subject has resumed smoking” in a smoking cessation trial, or death in an oncology trial.\n\n\nScenario\nA scenario is the complete specification of the unknown external factors that determine the data observed on the trial and its timing. The exact factors depend on the type of trial being simulated, but typically include:\n\nthe distribution of the final change from base line, or probability of response or rate of events in the different treatment groups\nthe properties of subjects’ early responses and the correlation with their final outcome\nthe rate at which subjects are recruited into the trial\nthe rate at which subjects drop out of the trial.\n\n\n\nSPEC\nThe Design Engine Specification document, describes the system algorithms, and meaning of parameters in a more technical context. SPEC documents have been deprecated as of FACTS 7.1.1\n\n\nSubject\nAn entity recruited onto a clinical trial for the purposes of learning about the properties of a treatment. Depending on the type of trial they might be patients or they might be healthy volunteers.\n\n\nTreatment Arm\nAn arm being studied in a clinical trial. A treatment arm may refer to different doses of the same treatment or completely separate therapies. Subjects, upon entering a study, are randomized to a treatment arm.\n\n\nUG\nThe User Guide document - describes in detail how to use a FACTS engine.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Glossary"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/execution.html",
    "href": "documentation/v71/userguides/core/execution.html",
    "title": "Execution Tab",
    "section": "",
    "text": "The Accrual sub-tab provides an interface for specifying accrual profiles. Accrual profiles define the mean recruitment rate week by week during the course of the trial. Virtual subjects are simulated from a Poisson process in which the expected number of subjects per week is allowed to change week by week.\nAccrual profiles are shown as a list on the left of the screen, as depicted below. These accrual profiles may be renamed by double-clicking on them and typing a new profile name. After creating a profile, the user must create at least one recruitment region. Early in the trial design process, detailed simulation of the expected accrual pattern is typically not necessary and a single region with a simple mean accrual rate is sufficient.\nTo model the expected accrual rates more precisely over the course of the trial, the user may specify multiple regions for each accrual profile and separately parameterize them. Regions are added via the table in the center of the screen Figure 1. Within this table, the user may modify:\n\nthe peak mean weekly recruitment rate,\nthe start date (in weeks from the start of the trial) for this recruitment region,\nwhether the region will have a ramp up phase and if so when the ramp up will be complete (in weeks from the start of the trial).\nWhether the region will have a ramp down, and if so when the ramp down start and when the ramp down will complete (in weeks from the start of the trial).\n\nRamp up/ramp down define simple linear increase/decreases in mean recruitment rate from the start to the end of the ramp. Note that simulation of accrual is probabilistic, but ramp downs are defined in terms of time, so even if ramp downs are planned so that at the average accrual rate they will occur as the trial reaches cap, there is a risk in simulations when accrual has been slower than average, that ramp downs occur before the full sample size is reached. It is advisable to have at least one region that doesn’t ramp down to prevent simulations being unable to complete.\nA graph of the recruitment rate of the highlighted region is shown as well. As the recruitment parameters are changed, the graph will update to show the time at which full accrual is reached. An accrual profile that does not reach full accrual is invalid and cannot be used to run simulations.\n\n\n\n\n\n\nFigure 1: Execution &gt; Accrual tab.\n\n\n\nIn the screenshot above you can see the two step ramp up in accrual from two regions – each starting at different offsets into the trial.\nNote that the accrual profile graph is only the mean expectation; actual accrual is simulated using exponential distributions for the intervals between subjects, derived from the mean accrual profile specified here. Thus some simulated trials will recruit more quickly than this and some more slowly.\nThere are commands to import and export region details from/to simple external XML files. When importing, the regions defined in the external file are added to the regions already defined, they don’t replace them.\nThis is an example of a very simple region file defining just one region:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;regions&gt;\n&lt;region&gt;\n&lt;name&gt;Region 1&lt;/name&gt;\n&lt;rate&gt;5&lt;/rate&gt;\n&lt;start&gt;0&lt;/start&gt;\n&lt;ramp-up /&gt;\n&lt;ramp-down /&gt;\n&lt;/region&gt;\n&lt;/regions&gt;\n\n\nIf “Deterministic” accrual has been specified on the Study &gt; Study Info tab, then on the Accrual tab, rather that specifying an accrual profile from which subject recruitment times are simulated, the user loads a file of specific accrual dates for every subject.\n\n\n\n\n\n\nFigure 2: Execution &gt; Deterministic Accrual tab.\n\n\n\nThe user specifies a “.dat” file to load that contains the subject accrual dates in weeks from the start of the trial.\nThe required file format is a text file with comma separate values. One row per subject, with 3 fields on each row:\n\nthe subject ID, (an integer)\nthe ID of the region where the subject was recruited (an integer)\nand the subjects randomization date (in weeks from the start of the trial – this is a ‘real’ number allowing fractions of a week to be specified)\n\nThe file must contain sufficient entries to allow the maximum number of subjects specified on the Study &gt; Study Info tab to be recruited.\nAfter successfully loading a file, the FACTS GUI shows a plot of the resulting weekly accrual rate",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/execution.html#deterministic-accrual",
    "href": "documentation/v71/userguides/core/execution.html#deterministic-accrual",
    "title": "Execution Tab",
    "section": "",
    "text": "If “Deterministic” accrual has been specified on the Study &gt; Study Info tab, then on the Accrual tab, rather that specifying an accrual profile from which subject recruitment times are simulated, the user loads a file of specific accrual dates for every subject.\n\n\n\n\n\n\nFigure 2: Execution &gt; Deterministic Accrual tab.\n\n\n\nThe user specifies a “.dat” file to load that contains the subject accrual dates in weeks from the start of the trial.\nThe required file format is a text file with comma separate values. One row per subject, with 3 fields on each row:\n\nthe subject ID, (an integer)\nthe ID of the region where the subject was recruited (an integer)\nand the subjects randomization date (in weeks from the start of the trial – this is a ‘real’ number allowing fractions of a week to be specified)\n\nThe file must contain sufficient entries to allow the maximum number of subjects specified on the Study &gt; Study Info tab to be recruited.\nAfter successfully loading a file, the FACTS GUI shows a plot of the resulting weekly accrual rate",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Execution"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/successfutility.html",
    "href": "documentation/v71/userguides/core/design/successfutility.html",
    "title": "Success/Futility Criteria",
    "section": "",
    "text": "The Success/Futility Criteria tab is where users specify the decision rules for determining study success or failure. The Final Analysis criteria always exist, and should, in general, be specified for every simulated trial. If simulating an adaptive trial, then interim analysis decision rules are also specified here.\n\nDecisions in FACTS Core\nThere are \\(7\\) possible decisions that can be made in a FACTS Core design, each with a numeric identifier that FACTS uses in the .csv output to denote decisions. The Outcome column contains the decision made, and the number 1-7 map to decisions as follows:\n\n\nEarly Success\n\nEarly success is achieved if and only if the trial meets the success condition at an interim analysis, and does not meet the futility criteria at the final analysis. The final analysis criteria must not be met whether or not subjects were selected to follow-up after an early success decision.\n\n\n\nLate Success\n\nLate success is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then meets the final analysis success criteria.\n\n\n\nLate Futility\n\nLate success is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then meets the final analysis futility criteria. Late futility is not automatically the complement of late success; the futility rule must be specified as the complement of the success rule to make it true.\n\n\n\nEarly Futility\n\nEarly futility is achieved if and only if the trial meets the futility condition at an interim analysis, and does not meet the success criteria at the final analysis. The final analysis criteria must not be met whether or not subjects were selected to follow-up after an early success decision.\n\n\n\nSuccess to futility flip-flop\n\nSuccess to futility flip-flop is achieved if and only if the trial meets the success condition at an interim analysis, but meets the futility condition at the final analysis. Success to futility flip-flops can be achieved whether or not subjects are followed up after the early success decision.\n\n\n\nFutility to success flip-flop\n\nFutility to success flip-flop is achieved if and only if the trial meets the futility condition at an interim analysis, but meets the success condition at the final analysis. Futility to success flip-flops can be achieved whether or not subjects are followed up after the early futility decision.\n\n\n\nInconclusive\n\nInconclusive is achieved if and only if the trial enrolls to the maximum sample size, collects full follow-up, and then does not meet the final success or final futility criteria.\n\n\n\nEvery simulated trial will result in exactly one of these decisions. Non-adaptive trials will result in either Late Success, Late Futility, or Inconclusive. An adaptive trial that does not stop at an interim analysis will result in Late Success, Late Futility, or Inconclusive. An adaptive trial that stops enrolling for early success at an interim analysis will end up as an Early Success or a Success to Futility flip-flop. An adaptive trial that stops enrolling for early futility will result in Early Futility or Futility to Success flip-flop.\n\n\nFinal Evaluation\nOn the Final Evaluation tab, the user can specify rules for judging the study for final futility or final success at its end.\nThe left column of the Final Evaluation tab contains the specification of the trial final futility rule, and the right column contains the specification of the final success rule.\nTo add a decision rule, click the “Add…” button within the appropriate column, select a decision quantity QOI, a comparison inequality sign, and a threshold. Final success and final futility criteria can each have multiple components to them, and the selection at the bottom of the column called “Combine criteria using:” dictates if success or futility should be declared if every single criteria is met (AND) or if any criteria is met (OR).\nThe success and futility rules need not be complementary - there can be trials that do not meet either criteria at the final analysis. These trials would be considered inconclusive. It is allowable, although generally not recommended, to specify overlapping success and futility rules. If a trial were to satisfy both the success and futility criteria at the final analysis it would be considered a final futility.\nThe Final Evaluation Criteria are always applied at the time of the final analysis, even if the study stops early for success or futility, and whether or not “Continue follow-up if study stopped for success” or “Continue follow-up if study stopped for futility” are selected. As a result, setting final analysis criteria that are easier to satisfy than interim analysis criteria may result in the labelling of trials that meet the interim analysis criteria but do not satisfy the final analysis criteria as flip-flops.\n\n\n\n\n\n\nFigure 1: The final evaluation tab within the success futility tab.\n\n\n\n\n\nInterim Analysis Criteria\nOn the success/futility criteria of a design with “Enable adaptive features” checked on the Study &gt; Study Info page, the user can specify rules for judging the study for futility or success at an interim and at the final evaluation. If the trial has no interims there will be just a tab for the Final Evaluation criteria. If the trial has interim analyses, then there can be tabs that define different early success/futility criteria at the different interims.\nAt the top of the main tab is a control to allow tabs to be created for different interims. In a newly created adaptive design, FACTS will create a tab for interim 1.\nIf early success/futility criteria are specified for an interim, then they will be taken to apply to all subsequent interims until the next one for which criteria are supplied, then those criteria will apply until the next interim for which criteria are applied and so on. There will be no stopping for success or futility until the first interim for which early stopping criteria have been defined.\nIt is possible to specify overlapping early success and early futility criteria at an interim, but it is not possible to stop for both success and futility at the same time. The FACTS design engines will stop the trial but the user should not rely on one or other outcome taking precedence. It is not considered good practice to have success and futility rules that could both be true, so FACTS does not guarantee a “tie break” rule.\nIn the output files there are columns labeled “Success ” and “Futile ” indicating whether any decision criteria became true, and “Success Combined” and “Futile Combined” indicating if all the criteria for a success or futility determination have been met. If both sets of criteria are met simultaneously, FACTS will only flag one of “Success Combined” and “Futile Combined” as being met, corresponding to how the outcome of the trial has been flagged.\n\n\n\n\n\n\nFigure 2: An interim evaluation tab within the success futility tab. The selected interim evaluation tab controls the interim analysis decisions at the 3rd interim analysis and all later interims.\n\n\n\nHaving created a tab to define the early success/futility criteria at an interim, the user has options to delete the tab or to copy criteria that have already been entered on another tab before editing them.\nThe user specifies:\n\nWhether early stopping for futility or success is to be allowed by selecting the “Futility criteria” or “Success criteria”\nThe stopping rules are specified, by selecting the QOI to be tested, the direction of the comparison and the threshold value for the criteria to be met, as in the Final Evaluation tab above.\nThe user can specify whether, if multiple criteria have been specified, they all need to be meet (Criteria combined using “AND”) or only any one of them needs to be met (Criteria combined using “OR”). These are specified independently for stopping for success and for futility.\nIf stopping is allowed, the user specifies what the minimum amount of information (using the information type specified on the Interims tab). This can be specified both in overall terms and information on specific doses. If multiple minimum-information criteria are specified they must all be met for the QOI criteria to be evaluated.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Success/Futility Criteria"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/index.html",
    "href": "documentation/v71/userguides/core/design/index.html",
    "title": "Design Overview",
    "section": "",
    "text": "The FACTS core engine allows for the design and simulation of fixed and adaptive clinical trials, especially focused on, but not limited to, Bayesian designs with multiple active arms.\n\nSub-tabs of the Design Tab\nTrials designed in the core engines are comprised of a number of elements:\n\nThe dose response model: the user must specify how the doses are related to eachother in the primary analysis, though there is a simple ‘no model’ option that estimates the mean treatment effect of each arm independently. A fixed trial uses the dose response model for the final Bayesian analysis of the data; an adaptive trial uses the same model both for the final analysis and at the interim updates.\nThe longitudinal (predictor) model: whether the trial is adaptive or fixed, the user may select to whether to use a longitudinal model (similarly, a predictor model in time to event). In a fixed trial the longitudinal model can be used to multiply impute final values for subjects that have dropped out. In an adaptive trial it is also used at the interim updates to multiply impute final values for subjects who have been recruited but do not yet have final values. In a fixed trial with no subject dropouts using a longitudinal model would have no effect on the outcome, analysis, or conduct of the trial.\nAllocation rules: in a fixed trial the user just specifies the proportion of subjects to be recruited to each arm, and the same can be done in an adaptive trial (i.e. an adaptive trial does not have to adapt the allocation), but an adaptive trial has a range of options that the user can use to adapt how subjects are allocated to the different treatment arms as the trial progresses.\nEarly stopping rules: in an adaptive trial the user can select the criteria and specify the thresholds at which trial should be stopped at any interim where the conditions are satisfied. Early stopping is optional, and even in an adaptive design the user can opt to always recruit the maximum permitted number of subjects. In a fixed trial there are no interim analyses and hence no opportunity to stop early.\nFinal evaluation criteria: the same Bayesian evaluation criteria are available whether the trial is fixed or adaptive. The user selects which criteria to use and what thresholds will constitute success or failure. The success and failure criteria do not have to be complements of eachother, and any analysis that doesn’t completely satisfy either the success or futility criteria is called, “inconclusive.”\nFrequentist analysis: frequentist p-values can be calculated comparing each dose to the control arm (or a fixed value if there is no control). P-values can be used as decision making quantities at interim updates or final analyses. p-value cannot benefit from the dose reponse models or longitudinal models, which are specific to the Bayesian model in FACTS.\n\n\n\nEvaluation of Bayesian Posterior Estimates\nAt every interim and final analysis there is a Bayesian model fit to the data observed up to that point in the trial. The Bayesian model contains a dose response model and, often, a longitudinal model. In the absence of a longitudinal model, the posterior is calculated as:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)}\\]\nwhere \\(\\phi\\) is the set of parameters of the selected response model, \\(p(\\phi)\\) is the prior for those parameters, \\(y_i\\) is the final response for each subject and \\(n\\) is the number of subjects with complete data.\nWith a longitudinal model, this becomes:\n\\[p(\\omega|Y) \\propto \\prod_{i = 1}^{n}{p(y_{i}|\\phi)p(\\phi)\\prod_{i = 1}^{n}{\\prod_{j = 1}^{L}{p(y_{ij}|\\psi)p(\\psi)}}}\\]\nwhere \\(\\psi\\) is the set of parameters of the selected longitudinal model, \\(p(\\psi)\\) is the prior for those parameters, \\(y_{ij}\\) is the response for each subject \\(i\\) at each visit \\(j\\) and \\(L\\) is the number of visits.\nThe posterior is evaluated using MCMC with individual parameters updated by Metropolis Hastings (or Gibbs sampling where possible), using only the \\(y_i\\) and \\(y_{ij}\\) data available at the time of the update.\nThe likelihood and priors for each of the dose response models are provided in the description of the Dose Response tab, and the likelihood and priors for the multiple imputation models are provided in the description of the Longitudinal Models tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html",
    "href": "documentation/v71/userguides/core/design/allocation.html",
    "title": "Allocation",
    "section": "",
    "text": "The options on allocation tab depend on whether an adaptive or non-adaptive design has been selected on the ‘Study &gt; Study Info’ tab, and if adaptive whether subjects are recruited sequentially or in cohorts.\nIf the design is non-adaptive then the only allocation option is blocked with fixed allocation ratios.\nIf the design is adaptive with Continuous or Deterministic recruitment, then there are 4 allocation options.\nIf the design is adaptive with cohort recruitment then there are 3 allocation options, all of which can be combined with early stopping:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#randomization-ratio-and-blocking",
    "href": "documentation/v71/userguides/core/design/allocation.html#randomization-ratio-and-blocking",
    "title": "Allocation",
    "section": "Randomization Ratio and Blocking",
    "text": "Randomization Ratio and Blocking\nIn the Randomization Ratio and Blocking section of the Allocation tab the user inputs the components of randomization blocks that enroll from the onset of the study and until any arm is dropped. These blocks work like the Fixed Allocation tab blocks.\nOnce an arm is dropped the “Upon arm drop:” option in the Setup section of the allocation tab will determine how randomization proceeds.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#arm-dropping-criteria",
    "href": "documentation/v71/userguides/core/design/allocation.html#arm-dropping-criteria",
    "title": "Allocation",
    "section": "Arm Dropping Criteria",
    "text": "Arm Dropping Criteria\nThe user may select any combination of Bayesian “Per Dose” QOIs: Posterior Probabilities or Predictive Probabilities, or a Target Dose QOI. For each criteria selected (at least one must be selected), the user supplies a direction of comparison and probability threshold. If at any interim, any arm meets the specified arm dropping criteria, then that arm becomes a candidate for dropping. After all candidates are identified for dropping, the Setup rules determine which, if any, of the candidates will be dropped.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#setup",
    "href": "documentation/v71/userguides/core/design/allocation.html#setup",
    "title": "Allocation",
    "section": "Setup",
    "text": "Setup\nA variety of rules are specified in the Setup section of the Allocation tab.\n\nMax number of arms that can be dropped during the study\nThe maximum number of arms that can be dropped – this can be set to from 0 to the number of active arms, the Control and Active Comparator are never dropped. This figure is the ‘maximum number of arms that can be dropped over the duration of the trial’. At any interim any number between 0 and the ’maximum number of arms can be dropped as long as this limit is not exceeded on the overall number of arms that would have been dropped.\nIf the maximum number of arms that can be dropped is set to the number of study drug arms, then if all arms are dropped this is recorded as an “early termination for futility”.\n\n\nPrune from lowest/highest dose\nArm dropping can be restricted to ‘pruning from the lowest dose’ and/or ‘pruning from the highest dose’. If ‘prune from the lowest dose’ is selected, then until the lowest dose meets the arm dropping criteria the second lowest dose cannot be dropped and so on. At an interim when the lowest dose does meet the arm dropping criteria, and the second dose does too, then the second dose can be dropped as well (and third and fourth …), it does not have to wait until the next interim. Pruning from the highest dose does the same thing, except that no dose can be dropped unless every larger dose will also be dropped.\nIf no pruning is selected or pruning from both ends is selected, then the user must select to prioritize the higher or lower dose to cover the instance when more doses are candidates for dropping than are allowed to drop by the maximum threshold. If “Prioritize dropping the lowest dose” is selected then when forced to choose between two or more doses to drop, the one with the lowest effective dose strength will be dropped.\n\n\nUpon arm drop\nFinally, specify what is to be done with the unused subjects that would have been allocated to an arm that has now been dropped. There are three options:\n\nMaintain study size, maintain combined block size of treatments\n\nSubjects that would have been allocated to arms that have been dropped are now randomized between all the remaining active treatment arms (excluding control and active comparator) using their original allocation ratios. For example, if the allocation ratio was 2:1:1:1 between Control and D1, D2 and D3, then after dropping D1 the block size remains size 5 (2 to Control and 1 to each of D2 and D3) with the \\(5^{th}\\) slot being allocated 1:1 between the remaining two study arms D2 & D3. Thus, the overall proportion of subjects on Control (and Active Comparator) is maintained.\n\nMaintain study size, reduce combined block size of treatments\n\nSubjects that would have been allocated to any arms that have been dropped are randomized between all remaining arms (including control) using a reduced total block size. Thus, if the allocation ratio was 2:1:1 between Control and D1 and D2, then after dropping D1 the block size drops to total size three: 2 to Control and 1 to D2.\n\nDecrease the study size, reduce combined block size of treatments\n\nSubjects that would have been allocated to any arms that have been dropped are no longer recruited, and the total sample size of the study is reduced; the remaining subjects are recruited using a reduced block. If interims are specified by number of subjects recruited, these interim sizes are reduced proportionally. As an example of this, if an arm that gets 20% of the randomization is dropped when there are 100 subjects enrolled, then if the next interim is scheduled at 200 subjects enrolled, it will actually be performed at 180 subjects enrolled.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#early-stopping-only",
    "href": "documentation/v71/userguides/core/design/allocation.html#early-stopping-only",
    "title": "Allocation",
    "section": "Early Stopping Only",
    "text": "Early Stopping Only\nIf allocation is to be fixed the “Early Stopping Only” option should be selected. Then on this tab the fixed allocation ratios and block size are specified – for the first cohort and then for all subsequent cohorts. In each case the user specifies the number of each arm to allocate in each cohort and the sum must equal the size of the cohort.\n\n\n\n\n\n\nFigure 7: Cohort allocation tab with early stopping, but no adaptive allocation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/design/allocation.html#adaptive-allocation-1",
    "href": "documentation/v71/userguides/core/design/allocation.html#adaptive-allocation-1",
    "title": "Allocation",
    "section": "Adaptive Allocation",
    "text": "Adaptive Allocation\nIf allocation is to be adaptive the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort by sampling from allocation vector” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation ratios for the remaining subjects in the cohort as for adaptive allocation (see Adaptive Allocation above).\n\n\n\n\n\n\n\n\nFigure 8: Cohort allocation tab adaptive allocation.\n\n\n\n\nAdaptively allocate to best dose\nIf allocation is to be adaptive, but at each cohort just Control and the “best dose” are to be allocated to, the “Adaptive Allocation” option should be selected and then under “Subsequent Cohort Allocation” the “Allocate cohort to single treatment (plus control)” should be selected. Then on this tab\n\nThe fixed allocation ratios and block size are specified for the first cohort. The user specifies the number of each arm to allocate and the sum must equal the size of the cohort.\nThe user then specifies\n\nThe fixed allocation to control\nThe QOIs to be used to calculate the allocation weighting to calculate for the non-Control arms, (see Adaptive Allocation above) and all the subjects not allocated to control will be allocated to this arm.\n\n\n\n\n\n\n\n\nFigure 9: Cohort allocation tab with adaptive allocation allocating to only the best dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Allocation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/tte.html",
    "href": "documentation/v71/userguides/core/study/tte.html",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "",
    "text": "Figure 1: The study tab for a time-to-event trial.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/tte.html#design-options",
    "href": "documentation/v71/userguides/core/study/tte.html#design-options",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Design Options:",
    "text": "Design Options:\n\nEnable adaptive features\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\nInclude Predictor\nWhether an early predictor is to be included in the design. An early predictor can be a continuous or dichotomous outcome, or a precursor event. The user specifies whether higher score, a response, or precursor event is a positive or negative outcome for the subject (this is used in the comparison with the CSD – not in the relation between the predictor and the final event, that is solely down to the parameters entered on the virtual subject response tabs). For continuous or dichotomous predictors, the user specifies after what time (weeks after randomization) the predictor is observed. For all predictors a clinically significant difference can be specified which can be used to specify early stopping or final evaluation conditions on the predictor. If the predictor is continuous, the value is delta on the mean difference between a study arm and control, if dichotomous, the value is the difference in rate and if the predictor is a precursor event then the value is a clinically significant hazard ratio.\nIf an early predictor is included, then a ‘Predictor’ tab is added to the Virtual Subject Response tabs, and a Predictor tab is added to the Design tabs, with a predictor Dose Response sub-tab and Relationship to Endpoint sub-tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/tte.html#study-information",
    "href": "documentation/v71/userguides/core/study/tte.html#study-information",
    "title": "Study Tab - Time-to-Event Endpoint",
    "section": "Study Information",
    "text": "Study Information\n\nMaximum number of subjects\nThe maximum number of subjects that can be recruited into the study and the maximum number of events to be observed (if there is a time-to-event predictor, the maximum events to be observed can be specified to be the predictor events).\n\n\nMaximum number of events refers to\nDetermine what counts as an event when determining when the trial should be stopped due to reaching the number of events specified in the “Maximum number of events” box. The options are “Final events” and “Predictor events.”\n\n\nMaximum number of events\nThe maximum number of events (type of event specified above) allowed in the trial. When this value is reached the trial moves on to the final analysis stage.\n\n\nMax follow-up per subject (wks)\nThe maximum follow-up time per subject. No subject will be followed for longer than this. If the study does not stop early it will stop when every subject recruited has reached their maximum follow-up time or has had an event. With this option all subjects have the same maximum follow-up.\n\n\nFollow-up after full enrollment (wks)\nThe follow-up time after full enrolment. If the study does not stop early then it will stop the specified time after full enrolment or when every subject has had an event. With this option subjects recruited earlier may be follow-up for longer than subjects recruited later in the trial and the overall amount of exposure is maximized.\n\n\nEvents indicate\nWhether an event indicates a success (time to recovery) or a failure (death or progression) for a subject. The value specified here also dictates the direction of frequentist tests and predictive probabilities.\n\n\nRecruit subjects\nDictate whether subjects are recruited sequentially or deterministically.\nIf recruited sequentially, the user recruitment will be simulated stochastically using a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html",
    "href": "documentation/v71/userguides/core/study/continuous.html",
    "title": "Study Tab - Continuous Endpoint",
    "section": "",
    "text": "Figure 1: The study tab for a continuous trial.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html#design-options",
    "href": "documentation/v71/userguides/core/study/continuous.html#design-options",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Design Options:",
    "text": "Design Options:\nIn the design options section of the Study tab the user gets check boxes for whether they want to enable adaptive features, use longitudinal modelling, or include simulation of baseline. These options have the following effects on the trial simulation.\n\nEnable adaptive features\nWhether the design is adaptive or fixed. If “adaptive features” are enabled, some adaptive specific parameters and tabs are added to the GUI, such as the tabs for defining interims, early stopping criteria, and adaptive allocation.\n\n\nUse longitudinal modeling\nWhether longitudinal modeling is going to be used. If longitudinal modeling is not selected, some longitudinal specific parameters and tabs are hidden. If use longitudinal modeling is selected, FACTS expects that the early endpoint data will be used, it cannot be ignored. If intermediate data is not intended to be used in the modeling, then it should not be simulated in FACTS.\n\n\nInclude simulation of baseline\nWhether to include simulation of subject’s baseline score, and if so, whether the response to be modeled is change from baseline or final endpoint value.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html#study-information",
    "href": "documentation/v71/userguides/core/study/continuous.html#study-information",
    "title": "Study Tab - Continuous Endpoint",
    "section": "Study Information:",
    "text": "Study Information:\nThe study information section allows the user to specify how subject accrual should be simulated, and whether a larger endpoint value indicates improvement or not.\n\nRecruit Subjects\nSubject accrual can be configured to be done continuousl, in cohorts, or deterministically.\nIf recruited continuously, the user recruitment will be simulated stochastically with a Poisson process, using the parameters specified on the Execution &gt; Accrual tab.\nIf recruited in cohorts the user specifies:\n\nThe size of the first cohort\nThe size of subsequent cohorts\nThe maximum number of cohorts\nThe time (in weeks) to recruit each cohort\n\nThe “Maximum number of subjects” and “Maximum trial duration” fields are automatically updated to reflect the cohort parameters entered.\nIf recruited deterministically, the user specifies the recruitment date of every subject recruited by uploading a file of dates on the Execution &gt; Accrual tab.\n\n\nResponse:\nIndicate whether a larger endpoint value indicates subjects improvement or if a lower endpoint value is better. The value provided here informs the direction of frequentist hypothesis tests and with the evaluation of QOIs like Pr(Max) and predictive probabilities.\n\n\nSchedule of Post-Baseline Visits\nIf “Use longitudinal modeling” is not checked, then the post baseline visit specification is simple. The only required entry is the time it takes to observe a subject’s final endpoint.\nIf “Use longitudinal modeling” is checked, the follow-up period for subjects is slightly more involved. Visits are specified one at a time, by entering the required week value for the visit and then clicking ‘Add’, or by specifying a regularly spaced sequence: select ‘Auto-Generate’, enter the number of visits, the week of the first visit and the number of weeks between each visit and then click ‘Generate’.\nIndividual visits can be deleted by selecting them in the list and clicking ‘Delete’. The default visit names can be edited by clicking the visit name and typing. The week and the index cannot be changed. Should it be necessary to change the week of a visit, the incorrect visit must be deleted and a new one with the correct week number added.\n\n\n\n\n\n\nFigure 2: The control for setting up the visit schedule.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/study/continuous.html#d-treatment-arm-model",
    "href": "documentation/v71/userguides/core/study/continuous.html#d-treatment-arm-model",
    "title": "Study Tab - Continuous Endpoint",
    "section": "2D Treatment Arm Model",
    "text": "2D Treatment Arm Model\nIf the user checks the “Use 2D treatment arm model” option, the screen changes to allow the treatment strengths to be specified on 2 axes, and to specify which of the treatment combinations thus created will actually included as arms in the trial.\n\n\n\n\n\n\nFigure 4: The Study &gt; Treatment Arms tab when using a 2D treatment arm model.\n\n\n\nThe user now specifies strengths as “Column Factors” and “Row factors”. For example, the Column factors could be different dose strengths and the row factors could be different dosing regimens, or different treatments that the investigational treatment could be combined with. Because of the way the factors are shown in FACTS graphs it is better to use the factor with the greater number of values as the column factor. From the point of view of the analysis however the factors are treated symmetrically. In FACTS we have adopted the neutral term “factor” to avoid the impression that these designs can only apply to doses and dosing regimens[^3].\nFACTS then shows all the potential combinations of column and row factors in the list “Select doses to be used in trial” in which the user selects which of the factor combinations that will be present as arms in the trial. FACTS constructs the “effective strengths” opf the combinations by simply adding together the strengths of the two factors, but while the strengths of the factors (like the strengths of “doses” in the 1D setting) cannot be simply changed (because of the problems of managing the ordering of the factors), strengths of combinations can be directly edited, and indeed must be edited to resolve any “ties”. All combinations must have unique strengths so that they can be ranked in order.\nIf a control treatment arm is included in the trial then a “Control” factor is included in the Column factors and the user selects which combination of this factor and a Row factor is the Control combination. This combination is given the effective strength 0 and moved to the head of the list of doses. If the “include a control treatment arm” option is checked, there must be a combination that acts as control dose. If the option is not checked there will be no control dose.\nFACTS displays the resulting ordered list of combinations in the “Doses in trial” column.\nThere is also a “Show grid” option that will display a small graphic of the 2D layout of these “Doses in trial”:\n\n\n\n\n\n\nFigure 5: The dosing grid for a 2D dosing model with 2 row factor levels and 4 column factor levels.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Study",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/index.html#how-many-longitudinal-models",
    "href": "documentation/v71/userguides/core/longitudinalmodels/index.html#how-many-longitudinal-models",
    "title": "Longitudinal Models",
    "section": "How many longitudinal models?",
    "text": "How many longitudinal models?\nWhen specifying a longitudinal model in FACTS, the user must clarify whether the longitudinal model is shared across all doses, or if different arms are to estimate different functions connecting early endpoints to the final endpoint.\nThe options that may be selected for the number of model instances are:\n\n“Single model for all arms” – just one set of longitudinal parameters is estimated using all the subject data regardless of their treatment.\n“Model control separately” – two sets of longitudinal parameters are estimated, one using data from just those subjects on the control arm, and the other using all the subject data from all the other arms.\n“Model comparator separately” (only available if there is an active comparator arm) - two sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, and the other using all the subject data from all the other arms (control and active treatments).\n“Model control and comparator separately” (only available if there is an active comparator arm) - three sets of longitudinal parameters are estimated, one using data from just those subjects on the active comparator arm, one using data from just those subjects on the control arm, and the third using all the subject data from all the other arms.\n“Model all arms separately” – a set of longitudinal parameters is independently estimated separately for each arm, each set estimated just using the data available from the subjects on that arm.\n\nThe fewer models used, the more data is pooled, the more precisely the model parameters can be estimated, and the more informative the intermediate data can be. Pooling data assumes that subjects in different groups have the same longitudinal profile, although they may still have different magnitude or probability of response (estimated by Dose Response Model).\nIf the response profile could be different on different treatment arms – for example, if subjects on control recover constantly over time, but those on an effective dose of the study drug recover very quickly early on, and more gradually later in their follow-up - a longitudinal model based on pooled data would tend to over-estimate the final endpoint of subjects on an effective dose of the study drug when only their early data was available. In a case like that, it would be better to use separate models. If the rapid early response was likely on all study drug treatment arms, then possibly use “model control separately.” If, however, the rapid response will only be seen on some study drug arms then use “model all arms separately”.\nIn addition to declaring how many longitudinal models should be estimated, when there is more than one model instance the user must declare if the prior distributions should be specified differently across each model instance or not. The options that may be selected for prior specification (for all LMs but linear regression) are:\n\nSame priors across all model instances\n\n\nEach instance of the model has identical prior distributions on its parameters. Estimation is still performed independently on each model instance.\n\n\nSpecify priors per model instance\n\n\nEach instance of the model has its own priors that may vary across instances.\n\nThe linear regression longitudinal model has a different parameter set estimated for each visit, so its prior specification rules are slightly different. See the linear regression section below for specifics about prior specification specific to this model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Longitudinal Models"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/longitudinalmodels/continuous.html",
    "href": "documentation/v71/userguides/core/longitudinalmodels/continuous.html",
    "title": "Longitudinal Models for Continuous Endpoints",
    "section": "",
    "text": "LOCF (Last Observation Carried Forward)\nThe simplest possible longitudinal model. If {\\(y_{it}\\)} is the set of observed responses from early visits, and \\(y_{i t_m}\\) is the last observed value of \\(y_{i t}\\), then the LOCF model for the final endpoint \\(Y_i\\) is\n\\[Y_i\\mid \\{y_{it}\\} = y_{it_m}\\]\nIn the continuous engine \\(t_m\\) can be any earlier observed visit including the baseline value.\n\n\nLinear Regression\n\n\n\n\n\n\nShiny App\n\n\n\nThe following shiny application for a tool that helps visualize and set priors for the linear regression longitudinal model.\nSee here.\n\n\nThe linear regression model fits a simple linear model from the data at each visit with the final visit\n\\[Y_i \\mid y_{it} \\sim \\alpha_t + \\beta_t y_{it} + \\text{N}(0,\\lambda_t^2)\\]\nThe parameter \\(\\alpha_t\\) is the intercept of the model for visit t, and the parameter \\(\\beta_t\\) is a multiplicative modifier (slope) of the response observed longitudinal at visit \\(t\\) to adjust the prediction of the final endpoint.\nImputation of the final endpoint value for a subject using the linear regression longitudinal model is based on only the latest observed visit’s endpoint value.\nThe default setting of “Same priors across all model instances and visits,” implies that each parameter \\(\\alpha\\), β, and λ have the same prior for all t. Estimation of those parameters is still done independently for each model instance. The one prior across all model instance are formulated as:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_\\mu, \\alpha_\\sigma^2)\\] \\[\\beta_t \\sim \\text{N}(\\beta_\\mu, \\beta_\\sigma^2)\\] \\[\\lambda_{t}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nThe above prior formulation may not be desirable if specifying priors that are not extremely diffuse – especially on the \\(\\beta\\) parameters. Instead, selecting “Specify priors per visit across all model instances,” will share the prior specification across all instances of the model, but allows for different priors to be put on the parameters associated with each visit. The user inputted prior parameters are now subscripted with \\(t\\) to denote the visit they correspond to. These priors apply to all model instances:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_{\\mu_t}, \\alpha_{\\sigma_t}^2)\\] \\[\\beta_t \\sim \\text{N}(\\beta_{\\mu_t}, \\beta_{\\sigma_t}^2)\\] \\[\\lambda_{t}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n_t}}{2},\\frac{\\lambda_{\\mu_t}^{2}\\lambda_{n_t}}{2} \\right)\\]\nIt is also possible to specify priors “Per model instance and visit,” in which every visit has separate priors, and those differing priors vary across model instances. This is the most flexible prior specification method. The user inputted prior parameters are now subscripted by both t for visit and i for model instance.\n\\[\\alpha_{ti} \\sim \\text{N}(\\alpha_{\\mu_{ti}}, \\alpha_{\\sigma_{ti}}^2)\\] \\[\\beta_{ti} \\sim \\text{N}(\\beta_{\\mu_{ti}}, \\beta_{\\sigma_{ti}}^2)\\] \\[\\lambda_{ti}^{2} \\sim \\text{IG}\\left( \\frac{\\lambda_{n_{ti}}}{2},\\frac{\\lambda_{\\mu_{ti}}^{2}\\lambda_{n_{ti}}}{2} \\right)\\]\nA potential starting place for non-informative prior values would be\n\n\\(\\alpha\\)\n\nmean of 0, SD \\(\\ge\\) largest expected response\n\n\\(\\beta\\)\n\nmean of either 0 or \\(\\frac{\\text{final visit time}}{\\text{early visit time}}\\), SD \\(\\ge\\) largest expected ratio of final visit to first visit\n\n\\(\\lambda\\)\n\nmean of expected SD of the endpoint (‘sigma’), weight of 1. The variability of the prediction from the longitudinal model (based on an observed intermediate response) should be less than that based solely on the treatment allocation, thus this is a weakly pessimistic prior on the effectiveness of the longitudinal model that should be quickly overwhelmed by the data.\n\n\nThis model is easy to understand and can be used even if there is only one visit, but doesn’t get more powerful if there are more visits. Its power will depend on the degree of correlation between the intermediate visit response and the final response.\n\n\nTime Course Hierarchical\nThe Time Course Hierarchical models the relationship between subjects’ early responses and their final response. It incorporates a per-subject offset from the mean response and a scaling factor from each visit to the final endpoint, but with no model of the change from visit-to-visit.\nThe response at the \\(t_{th}\\) visit for the \\(i^{th}\\) subject, having been randomized to the \\(d^{th}\\) dose is modeled as:\n\\[y_{it} \\sim e^{\\alpha_t}(\\theta_d + \\delta_i) + \\text{N}(0, \\lambda_t^2)\\]\nThe imputed final response (visit \\(T\\)$) for the \\(i^{th}\\) subject, having been randomized to the \\(d^{th}\\) dose is modeled as:\n\\[Y_{iT} \\sim \\theta_d + \\delta_i + \\text{N}(0, \\lambda_T^2)\\]\n(i.e. \\(\\alpha_T\\) is 0).\nThe model parameters can be interpreted as follows:\n\n\\(\\theta_d\\)\n\nthe estimated mean response at the final visit in dose \\(d\\) from the dose response model.\n\n\\(\\delta_i\\)\n\nthe estimated patient level random effect around the mean final response (\\(\\theta_d\\)) for the dose \\(d\\) that patient \\(i\\) is randomized to.\n\n\\(\\alpha_t\\)\n\na scaling parameter that determines the proportion of the final response that is observable at visit \\(t\\). A value of \\(\\alpha_t=0\\) indicates that the expected value of early visit \\(t\\) is equal to the estimated final visit mean \\(\\theta_d\\). A value of \\(\\alpha_t= −0.69315\\) indicates that the expected value of early visit \\(t\\) is 50% of the estimated final visit mean \\(\\theta_d\\).\n\n\\(\\lambda_t^2\\)\n\nthe variance of the endpoint around the estimated mean response at visit \\(t\\).\n\n\nThe prior for \\(\\alpha_t\\) is a normal distribution with a user specified the mean and standard deviation:\n\\[\\alpha_t \\sim \\text{N}(\\alpha_\\mu, \\alpha_\\sigma^2)\\]\nThe prior for the \\(\\delta_i\\) terms is a normal distribution with a mean of 0 and variance τ2.\n\\[\\delta_i \\sim \\text{N}(0, \\tau^2)\\]\n\\(\\tau^2\\) is estimated as part of the longitudinal model, and has an inverse gamma prior distribution with prior central value \\(\\tau_\\mu\\) and weight (in terms of “equivalent number of observations”) \\(\\tau_n\\):\n\\[\\tau^{2} \\sim \\text{IG}\\left( \\frac{\\tau_{n}}{2},\\\\\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\]\nThe prior for the \\(\\lambda_t^2\\) terms is an inverse gamma distribution with prior central value \\(\\lambda_\\mu\\) and weight (in terms of “equivalent number of observations”) \\(\\lambda_n\\):\n\\[\\lambda_{t}^{2}\\sim\\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be\n\n\\(\\alpha_t\\)\n\nmean of -2, SD of 2, … so the prior ~70% interval for \\(\\alpha_t\\) is between -4 and 0 (+/1 1 SD) and thus the prior 70% interval for \\(e^{\\alpha_t}\\) to be between 0.02 and 1.\n\n\\(\\tau\\)\n\nmean set to the expected SD of the endpoint (‘sigma’), with a weight of 1.\n\n\\(\\lambda_t\\)\n\nmean set to the expected SD of the endpoint (‘sigma’), with weight of 1.\n\n\nWe would expect \\(\\tau^2 + \\lambda^2 \\approx \\sigma^2\\), thus to specify a prior mean of \\(\\sigma\\) for each with a weight of 1 is a weakly pessimistic prior that should be quickly overwhelmed by the data.\nThis model is useful if there is thought to be a significant per-subject component to the response that should be manifest at the intermediate visits, and sufficient visits for the per-subject component to be estimated.\n\n\nKernel Density\nThe Kernel Density Model longitudinal model is a non-parametric re-sampling approach that is ideal for circumstances where the relationship between the interim time and the final endpoint is not known or not canonical.\nThe procedure is as follows. Assume an interim value for patient \\(i\\) at time \\(t\\), \\(Y_{it}\\). Patient \\(i\\) does not have an observed final endpoint at time \\(T\\), so one is to be imputed. Let \\((X_{1t},X_{1T}), \\ldots, (X_{nt}, X_{nT})\\) be the set of values for the previous subjects for whom there exists an interim value \\(X_{*t}\\) and final value \\(X_{*T}\\).\nTo impute a value of \\(Y_{iT}\\) given \\(Y_{it}\\), a pair \\((X_{kt},X_{kT})\\) is selected with probability based on the pair’s time \\(t\\) visit response’s proximity to the observed \\(Y_{it}\\):\n\\[\\Pr\\left(\\text{Selecting}\\left( X_{kt},\\\\X_{kT} \\right) \\right) = \\frac{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}{\\sum_{k = 1}^{n}{\\exp\\left( - \\frac{1}{2h_{X_{t}}^{2}}\\left( Y_{it} - X_{kt} \\right)^{2} \\right)}}\\]\nThen, a value of \\(Y_{iT}\\) is imputed from the following distribution, which uses the selected pair’s final endpoint response \\(X_{kT}\\):\n\\[Y_{iT} \\sim \\text{N}(X_{kT}, h_{X_T}^2)\\]\nThe bandwidths \\(h_{X_t}\\) and \\(h_{X_T}\\) are selected based on the criterion given by Scott (1992). That is,\n\\[h_{X_{j}} = \\sigma_{X_{j}} \\left( 1 - \\rho^{2} \\right)^{\\frac{5}{12}} \\left( 1 + \\frac{\\rho^{2}}{2} \\right)^{- \\frac{1}{6}}{\\\\n}^{- \\frac{1}{6}}\\text{   for } j = t \\text{ and } T\\]\nwhere \\(\\sigma_{X_j}\\) is the standard deviation of the observed responses at time \\(j\\), \\(n\\) is the number of pairs \\((X_{*t},X_{*T})\\) that were chosen between, and \\(\\rho\\) is the correlation coefficient between \\(X_t\\) and \\(X_T\\) in the pairs \\((X_{1t},X_{1T}), \\ldots, (X_{nt}, X_{nT})\\).\nThe Kernel Density model does not take prior distributions as input. So long as each early visit has more subjects with an observed early response and final response than the value entered in “Minimum number of participants with an early visit and final visit needed to estimate kernel bandwidths for that early visit:” then this algorithm runs without regard for user input.\nIf any visit has fewer subjects with early data and final data than the specified minimum number of participants, then instead of calculating the values of \\(h_{X_t}\\) or \\(h_{X_T}\\) the input values of “Fixed kernel bandwidth \\(h_x\\):” and “Fixed kernel bandwidth \\(h_y\\):” are used.\nFor \\(h_x\\) and \\(h_y\\), possible starting values are the expected SD of the endpoint (‘sigma’). The default value for the minimum number of subjects with complete early and final visits is 6, but this value can be set to anything greater than 0 that the user desires.\nThe Kernel Density model is effective and flexible with no model assumptions, but its computational overhead is large. Simulations may take \\(\\sim 10\\) times longer to run, and having no prior model there has to be ‘in trial’ data before it can come into play.\n\n\nITP\nThe ITP (Integrated Two-component Prediction) model fits an observation for patient \\(i\\) on dose \\(d\\) at visit \\(t\\) as:\n\\[y_{idt} = \\left( \\theta_{d} + s_{id} + \\epsilon_{idt} \\right)\\left( \\frac{1 - \\text{exp}\\left( kx_{idt} \\right)}{1 - \\text{exp}(kX)} \\right)\\]\nwhere \\[\\epsilon_{idt} \\sim \\text{N}(0, \\lambda^2)\\] \\[s_{id} \\sim \\text{N}(0, \\tau^2)\\]\nand \\(\\theta_d\\) is the mean estimate of the final endpoint for dose d using all complete and partial data and assuming an independent dose response model on the doses. \\(s_{id}\\) is a subject specific random effect, \\(k\\) is a shape parameter, \\(x_{idt}\\) is the time \\(y_{idt}\\) is observed, \\(X\\) is the time to final endpoint, and each \\(\\epsilon_{idt}\\) is a residual error.\nThe ITP model is similar to the Time Course Hierarchical above in that both models estimate subject specific component of the response. The biggest difference between the two is that in the ITP models the response changes over time as a parametric function based on the parameter \\(k\\), rather than having a separately estimated \\(e^{\\alpha_t}\\) for each visit.\nThe shape parameter \\(k\\) determines the rate at which the final endpoint’s eventual effect is observed during a subject’s follow-up. A value of \\(k=0\\) indicates that the proportion of effect observed moves linearly with time. A value of \\(k&lt;0\\) means that the eventual final effect is observed earlier in follow-up and plateaus off as time moves towards the final endpoint. A value of \\(k&gt;0\\) indicates that less of the total final endpoint effect is observed early in follow up, but as time approaches the final endpoint time the proportion of the effect observed increases rapidly. Values of \\(k\\) less than 0 tend to be more common than values of \\(k\\) greater than 0. See the figure below for a visualization of the different response-over-time curves that can be estimated with the ITP model.\n\n\n\n\n\n\nFigure 1: Examples of a variety of ITP models with the shape parameter k ranging from -1 to 1.\n\n\n\nThe priors for the parameters in the ITP model are: \\[k \\sim \\text{N}(\\mu_k, \\sigma_k^2)\\] \\[\\theta_d \\sim \\text{N}(\\mu_{\\theta_d}, \\sigma_{\\theta_d}^2)\\] \\[\\tau^2 \\sim \\text{IG}\\left( \\frac{\\tau_{n}}{2},\\frac{\\tau_{\\mu}^{2}\\tau_{n}}{2} \\right)\\] \\[\\lambda^2 \\sim \\text{IG}\\left( \\frac{\\lambda_{n}}{2},\\frac{\\lambda_{\\mu}^{2}\\lambda_{n}}{2} \\right)\\]\nA reasonable starting place for prior values would be:\n\n\\(\\theta_d\\)\n\nmean of the expected mean improvement from baseline on the Control arm, with SD greater than or equal to the expected treatment effect size.\n\n\\(k\\)\n\na mean of 0 and SD of 2, if subject improvement is expected to be rapid and then diminishing, the prior mean might be -0.5 or -1, if subject improvement is expected to be initially slow or non-existent then increasing, the prior mean might be 0.5 or 1.\n\n\\(\\tau\\)\n\nmean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\n\n\\(\\lambda\\)\n\nmean set to the expected SD of the endpoint (“sigma”), with a weight of 1.\n\n\nThe ITP model implies that the variance of the observations shrinks towards 0 with the mean (so early visits have reduced expected responses and variances). The ITP model may result in biased estimates of \\(\\theta_d\\) and/or the variance terms \\(\\tau^2\\) and \\(\\lambda^2\\) if the mean-variance relationship assumed by the ITP model is not present in the observed data. If the model assumptions are correct this can be a very effective longitudinal model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Design",
      "Longitudinal Models",
      "Continuous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html",
    "href": "documentation/v71/userguides/core/vsr/tte.html",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "",
    "text": "The Virtual Subject Response tab allows the user to explicitly define virtual subject response profiles, and/or to import virtual externally simulated patient responses. When simulations are executed, they will be executed for a specific scenario – where a scenario is a combination of one each of predictor VSR, control hazard rate, dose response VSR, accrual rate, and dropout rate. If an external file is used to specify the subject responses to be simulated, this effectively replaces the predictor, control hazard and dose response profiles in a scenario.\nUnlike other endpoints where just a response (mean change from base line or rate) is specified, specification of subject responses for a time-to-event endpoint is done by first specifying a piecewise exponential event rate for the control population and then hazard ratios for the treatment arms. For simplicity, this means of specifying the simulated event rates is also used when no control arm is present and the comparison is with historic control rates.\nIn FACTS Core TTE, there is also the ability to include the simulation of a ‘predictor’ endpoint. Predictor endpoints can be a continuous measure, dichotomous outcome, or a precursor event. The interface for specifying how the predictor endpoint data is to be simulated is different in each case.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined",
    "href": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nWith no predictor, there are two screens for defining virtual subject responses; the first is used to define the control hazard rate and the second to define the hazard ratio of the events on each arm to control.\n\nControl Hazard Rates\nThe user may create a number of different control hazard rate profiles to simulate from. The different profiles will be used to define different simulation scenarios in combination with other profiles defining the other properties that have to be simulated.\nThe hazard rate to simulate in a profile is specified as a piecewise exponential. The follow-up time can be divided into different time segments and a different event rate simulated in each segment.\nSegment boundaries and hazard rates are always entered using “weeks” as the time unit. While this might not always be the most convenient, it allows FACTS to use the same time unit everywhere.\nDifferent segments in the follow-up period are specified by adding segment ends. Adding a ‘segment end’ adds a segment interval to the list to allow the event rate for that interval to be specified. To delete an interval, select the interval starting with the breakpoint to be deleted and click the “Delete” button.\nThe graph can show the hazard rate, the cumulative probability of not having an event or the probability of not having seen an event over time, using the event rates specified. This is useful for checking that the segments and event rates have been entered correctly.\n\n\n\n\n\n\nFigure 1: The tab for specifying the control arm hazard rate for an explicitly defined VSR.\n\n\n\n\n\nDose Response\nThe user may create a number of different dose response profiles to simulate from. The different profiles will be used to define different simulation scenarios in combination with other profiles defining the other properties that have to be simulated.\nWithin each profile the user specifies:\n\nThe hazard ratio compared to control for each treatment arm (except control itself, where of course the ratio is 1).\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\n\nOn the graph the different hazard ratios for the profile are plotted, along with the ‘target’ – this is the default CSHRD or NIHRD offset from the QOI tab, the direction of the offset is dependent on whether events are ‘good’ or ‘bad’ and whether the aim of the trial is to show superiority on non-inferiority.\n\n\n\n\n\n\nFigure 2: Specify the hazard ratio per dose\n\n\n\n\n\nLoading Scenario Control Hazard Rates and Scenario Hazard Ratios from file\nIf the “Load scenario hazard rates from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses across the simulations.\nEach individual simulation uses one set of responses from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from a single version of the ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ loaded from an ‘.mvsr’ file then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\nFor a TTE endpoint the user must supply 2 files – one for the Control Hazard Rates and one for the Hazard Ratios in each group. MVSR hazard rates are only combined with MVSR control hazard rates. The lines from each file are paired up for each simulation, so the first control hazard rate is used with the first dose response hazard ratio, the second control hazard rate is used with the second dose response hazard ratio and so on. There must be the same number of lines in each file.\n\n\n\n\n\n\nFigure 3: Load a scenario’s control hazard rates from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the different control hazard rates over time.\n\n\n\n\n\n\nFigure 4: Load a scenario’s treatment hazard ratios from an external file.\n\n\n\nAfter selecting the “.mvsr” file the graph shows the different individual hazard ratios for each dose.\nThe VSR parameters are provided in two separate files, (the number of lines in the files should be the same for the two files). The formats are:\n\nControl Hazard Rate File: Each line should contain columns [L1, L2, … , LS] giving the true control hazard rates (\\(\\lambda\\)) for each of the S segments. (Note: FACTS will treat the hazard rate as per week).:\nHazard Ratio File: Each line should contain columns [HR1, HR2, … , HRD] giving the true Mean Hazard Ratios for each of the D dose arms. (Note: HR1 = 1 by definition, but the column of 1’s to be included here for completeness.)\n\nThe use of MVSR files has not been extended to the case where a predictor is being used.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#external-files",
    "href": "documentation/v71/userguides/core/vsr/tte.html#external-files",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External Files",
    "text": "External Files\nAs well as simulating subject responses within FACTS they can be simulated externally and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data can be done from the External Files sub-tab depicted in Figure 5 below.\nTo import an external file, the user must first add a profile to the table. After adding the profile, the user must click “Browse” to locate the file of externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 5: The external files sub-tab for uploading patient data to be sampled from.\n\n\n\n\nRequired Format of Externally Simulated Data\nThe supplied data should have the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nUncensored time to event in weeks\nThis column is a placeholder for predictor data. It may be filled out or empty when there is no predictor. It will be ignored.\n\nThe GUI requires that the file name has a “.dat” suffix. The file need not have column headers, but if it does the first column name must start with a pound sign (#) which tells FACTS to ignore that row.\nThe following shows values from an example file with a dichotomous predictor. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n\n\n\n\n\n\n\n\n\n#Patient ID\nDose Index\nUncensored Time to Event (weeks)\nOptional Predictor value\n\n\n\n\n1\n1\n8.87\n0\n\n\n2\n1\n9.34\n0\n\n\n3\n2\n6.78\n-9999\n\n\n4\n2\n10.23\n0\n\n\n5\n2\n9.96\n0\n\n\n6\n2\n5.6\n1\n\n\n7\n1\n37.01\n20.36\n\n\n8\n1\n28.67\n0\n\n\n9\n1\n39.70\n0\n\n\n\nIn the above, column headers have been included to make it clearer to read but they are not required.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-event-rate-predictor",
    "href": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-event-rate-predictor",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined (Event Rate | Predictor)",
    "text": "Explicitly Defined (Event Rate | Predictor)\nThe first method (described in this section) may in some circumstances be the more natural way to think of the data (we might want to simulate for instance that subjects with a reduction in tumor size have a longer survival time), however it gives rise to data which does not have an exponential distribution. To retain an exponential distribution in the simulated event rate it is necessary to use the second method where the control rate and treatment arm hazard ratios are specified first and then the probability of the observed predictor derived from that (described in the next section).\n\nContinuous Predictor\nIf a predictor with a continuous endpoint is being used then there is a new tab in the VSR section to allow the specification of profiles that define how the predictor endpoint is to be simulated.\n\nPredictor\nFirstly, the predictor endpoint to be simulated is specified in the same way as a FACTS Core continuous endpoint. A number of profiles can be specified, in each one the mean change from baseline of the predictor for each treatment arm is specified, along with the variability in the change. The variability is specified as the SD of a Normal error in the observed change, a single value can be specified for all arms, or separate values for each arm.\nThe mean hazard rate for the simulated subjects now depends on the value of the predictor \\(Z\\), the baseline hazard rate for the time segment \\(\\mu_{s}\\), the log hazard ratio for the dose \\(\\theta_{d}\\), and with parameters \\(b_{Zd}\\), center and scale for the predictor:\n\\[h_{sdZ} = \\mu_{s}exp\\left( \\theta_{d} \\right)\\exp\\left( b_{Zd}\\frac{Z - center}{scale} \\right)\\]\nNote that in this form of the predictor, the observed event rate is affected by the predictor, it will not be the same as specified in the dose-response profile, also the observed times to event will not be exponentially distributed. The impact of this is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a control hazard rate of 0.01, and dose response hazard ratio of 1, when a continuous predictor is added with SD 2, and mean response of 1 on control and 3 on the treatment arm, with center specified at 2, scale at 3 and \\(b_{z}\\) at 0.2, simulations of 1000 subjects’ times-to-events on each of control and treatment arm give (ignoring censoring) a HR of between 1.05 and 1.21.\nNote that if events are bad and higher predictor scores indicate subject improvement (lower hazard rate) then the \\(b_{z}\\) coefficients need to be negative. The same is true if events are good and lower predictor scores indicate subject improvement.\n\n\n\n\n\n\nFigure 6: Specifying the continuous predictor’s VSR and how it effects the hazard rate for an arm.\n\n\n\nOn the graph showing the predictor response to be simulated, the ‘target’ line shows the offset of the predictor CSD from the predictor response on the Control arm.\n\n\nControl Hazard Rates\nWith a continuous predictor, the control hazard rate tab does not change. It’s the same as the non-predictor tab.\n\n\nDose Response\nThe manner of specifying the dose response does not change – multiple profiles may be specified and in each profile the hazard ratio for each treatment arm is specified. However, the overall hazard ratio simulated will depend on the combination of the control hazard rate, the dose response hazard ratio and the effect of the predictor on the final event rate.\nThis leads to two different ways for the predictor to be brought into the simulation. First, the hazard ratios \\(\\theta_{d}\\) may all be 1, while the mean of the predictor may change across doses. This would indicate that dose makes no difference given a fixed value of the predictor, but that the different doses achieve their effect by changing the distribution of the predictor values themselves. If the \\(\\theta_{d}\\) values differ from 1, then this indicates that there is a dose effect even conditional on a fixed value of the predictor (thus, for example, a control subject with Z=1 has a different hazard than a treatment subject with Z=1).\nThe resulting hazard ratio is plotted in the graph at the bottom of the tab. The predictor profile and control hazard rate profile to use in generating the graph can be selected in the controls to the right of the graph.\n\n\n\n\n\n\nFigure 7: Specify the hazard ratio per arm. The true simulated hazard ratio will be a combination of this value and the values provided in the predictor tab.\n\n\n\n\n\n\nDichotomous Predictor\nWhen simulating the event rate conditional on the predictor, the predictor endpoint to be simulated is specified in the same way as a for a simple dichotomous endpoint, the control hazard rate is specified separately for each dichotomous predictor value, and the Dose Response for the time-to-event endpoint has a hazard ratio that depends on the dichotomous predictor’s value.\nSo, the effect of the predictor on the background event rate is seen on the control hazard rate tab, and on the treatment arm hazard ratios on the dose response tab.\nOn those tabs the user specifies\n\nseparate control hazard rates for subjects who have a predictor response and those who do not\nseparate hazard ratios for each dose for subjects who have a predictor response and those who do not.\n\nThe hazard rates and ratios apply from the moment a subject is recruited (they do not change after the dichotomous predictor is assessed) and do not depend on the predictor being observed (which could be prevented if the event happens first).\n\n\n\n\n\n\nFigure 8: Specifying the dichotomous predictor’s probability of response for each arm.\n\n\n\nThe graph shows the specified response rate for each treatment arm and the target rate on control plus CSD.\n\nControl Hazard Rate with Dichotomous Predictor\n\n\n\n\n\n\nFigure 9: Specify the control arm hazard rate for each potential value of the dichotomous predictor.\n\n\n\nAs usual, multiple control hazard rate profiles can be created and the hazard rate on the control arm specified over different time segments. What differs from the case where there is no predictor is that, if a dichotomous predictor (with event rate simulated dependent on the predictor) is being used, separate hazard rates are specified for subjects depending on whether or not they will have the dichotomous predictor response or not.\n\n\nDose Response with a Dichotomous Predictor\n\n\n\n\n\n\nFigure 10: Specify the hazard ratio for active arms given the value of the dichotomous predictor.\n\n\n\nAs usual, multiple dose response profiles can be created, and in each profile the hazard ratio to simulate for each treatment arm compared to the control arm is specified. What differs from the case where there is no predictor, is that, if a dichotomous predictor is being used, separate hazard ratios are specified for subjects depending on whether or not they will have the dichotomous predictor response or not.\nAs with the continuous predictor, the observed event rate is affected by the predictor (it will not be the same as specified in the dose-response profile). Similarly, the observed times to event will not be exponentially distributed. The graph shows the effective combined hazard ratio for a given combination of control hazard rate, predictor rates and dose response. The controls for selecting which control hazard rate and which predictor rates to use are to the right of the graph.\nThe impact is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a predictor response rate on control of 0.1 and control hazard rates of 0.01 (with a predictor response) and 0.02 (with no predictor response), and treatment arm with a predictor response rate of 0.2 and hazard ratios of 0.9 (with a predictor response) and 0.8 (with no predictor response), simulations of 1000 subjects times to events on each of control and treatment arm give (ignoring censoring) a HR of between 0.75 and 0.90.\n\n\n\nTime-to-Event Predictor\n\nPredictor\nThe predictor endpoint to be simulated is specified in a similar manner to specifying the simulation of a time to event endpoint. A number of profiles can be specified; in each one the hazard rate on the control arm is specified over one, or more, time segments, and the overall hazard ratio of the time to the predictor event of each treatment arm to the control arm is specified.\nThe simulation of the time to the final event is in terms of the event rate (over one, or more, time segments) on the control arm after the predictor event, and then the hazard ratio of the time to the final event after the predictor event of each treatment arm to the control arm.\nOn those tabs the user specifies\n\nseparate control hazard rates for subjects post predictor event\nhazard ratios for each treatment arm the time to final event, after the predictor event has occurred.\n\n\n\n\n\n\n\nFigure 11: Specifying the dichotomous predictor’s control hazard rate and active arm hazard ratios for each arm.\n\n\n\nMultiple predictor profiles can be created.\nFor each profile the control hazard rate can be specified over an arbitrary set of time segments (i.e the time segments can vary from profile to profile, can be different from the observation times (if any) and different from the time segments used in the analysis model.\nThe hazard ratio of the control arm to itself has to be 1 and cannot be modified. For the other treatment arms, the time to predictor event is specified by specifying the hazard ratio on that treatment arm, to the control arm.\nThe graph can be used to show the hazard rate, probability of event or probability of not having the event on each arm.\n\n\nControl Hazard Rates\nSpecifying the simulation of the control hazard rate with a TTE predictor is the same as specifying the control hazard rate without a predictor. The difference is that with a TTE predictor, the hazard rate specified here is only simulated after the predictor has been seen.\n\n\n\n\n\n\nFigure 12: Specify the control arm’s event rate after the predictor endpoint event has occured.\n\n\n\n\n\nDose Response\n\n\n\n\n\n\nFigure 13: Specify the hazard ratio for the active arms on the hazard rate after the predictor event is observed. The primary endpoint event time can also be set to equal the predictor event time with some probability.\n\n\n\nAs usual, multiple hazard ratio profiles can be created, and in each profile the hazard ratio to simulate each treatment arm compared to the control arm specified. What differs from the case where there is no predictor is that if a TTE predictor is being used,\n\nthe hazard ratios are specified for the occurrence of the final event having observed the predictor event\na probability can be specified that the post predictor event to endpoint event time is 0.\n\nAs with the continuous predictor, the observed event rate is affected by the predictor, it will not be the same as the hazard ratio for endpoint post event predictor. The observed times to event will not be exponentially distributed. The impact is best understood by simulating the patient responses and performing rough analyses (e.g. in R).\nFor example, with a predictor hazard rate on control of 0.1 and post predictor hazard rates of 0.01 and probability that the post predictor event time is 0 of 0.1, and treatment arm with a predictor hazard ratio of 0.9 and post predictor hazard ratio of 0.8 and probability that the post predictor event time is 0 of 0.1, simulations of 1000 subjects times to events on each of control and treatment arm give (ignoring censoring) a HR of between 0.74 and 0.88.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-predictor-event-rate",
    "href": "documentation/v71/userguides/core/vsr/tte.html#explicitly-defined-predictor-event-rate",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "Explicitly Defined (Predictor | Event Rate)",
    "text": "Explicitly Defined (Predictor | Event Rate)\nTo simulate event times and an associated predictor with a possible correlation between the two in a way that preserves the exponential distribution of the event times, use this tab to specify the simulation of subjects’ time to event first and then the probability of the observed predictor derived from that. This is only supported for Dichotomous and TTE predictors.\nThe “Explicitly Defined (Predictor | Event Rate)” tab allows the specification of profiles that define how the predictor endpoint is to be simulated in relation to the control hazard rate and hazard ratios for the final events. This allows the simulated final observed final events to still have an exponential distribution.\nOnce the time-to-event for a subjects has been simulated, a simple user specified transformation of the time-to-final-event provides the expected value of the predictor’s distributions.\n\nDichotomous\nWhen using a dichotomous predictor, control hazard rates and dose response hazard ratios are specified as when there is no predictor.\n\nPredictor\n\n\n\n\n\n\nFigure 14: Specify the distribution of the predictor per arm given the uncensored endpoint value for a patient.\n\n\n\nAs usual, multiple profiles can be defined. To simulate the dichotomous endpoint given the event rate, the predictor values are simulated by drawing from the Bernoulli distribution with probability given by the inverse logit(\\(\\alpha + \\beta Y\\)), where \\(\\alpha\\) and \\(\\beta\\) are specified here and \\(Y\\) is the subject’s final time to event (in weeks). A single set of values for \\(\\alpha\\) and \\(\\beta\\) can be specified, or separate values per treatment arm can be specified. The expected response rate is shown in the plot at the bottom of the predictor tab.\n\n\n\nTime-to-Event\nWhen simulating a time-to-event predictor, control hazard rates and dose response hazard ratios are specified as when there is no predictor.\n\nPredictor\n\n\n\n\n\n\nFigure 15: Specify the distribution of the predictor event per arm given the uncensored endpoint value for a patient.\n\n\n\nMultiple profiles can be defined. To simulate predictor event endpoint given the event rate of the primary event, the predictor values are simulated by drawing from the Exponential distribution with rate given by (\\(\\lambda_{z}\\exp(\\ \\beta Y)\\)), where \\(\\lambda_z\\) and \\(\\beta\\) are specified here and \\(Y\\) is the subject’s time to event (in weeks). A single set of values for \\(\\lambda_z\\) and \\(\\beta\\) can be specified, or separate values per treatment arm can be specified.\nThe arm specific hazard ratios of the predictor given the final endpoint event rate of a specified scenario is shown in the plot at the bottom of the predictor tab. The final endpoint scenario can be changed using the dropdown boxes the right of the figure.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/tte.html#external",
    "href": "documentation/v71/userguides/core/vsr/tte.html#external",
    "title": "Virtual Subject Response - Time-to-Event Endpoint",
    "section": "External",
    "text": "External\nAs well as simulating subject responses with predictors within FACTS they can be simulated externally and imported into FACTS where the supplied responses are sampled from when simulating the trial. The specification of a file containing subject response data (which must be in the required format) can be done from the External Files sub-tab depicted below (Figure 6‑13).\nTo import an external file, the user must first add a profile to the table. After adding the profile, the user must click “Browse” to locate the file of externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 16: The external files sub-tab for uploading patient data to be sampled from.\n\n\n\n\nRequired Format of Externally Simulated Detail\nThe supplied data should have the following columns:\n\nPatient id, these must be positive integers and unique\nArm Index (1 = Control, 2 = Treatment1, 3 = Treatment2, …)\nUncensored time to event in weeks\nUncensored Predictor value, one of the following depending on the predictor type:\n\nContinuous: “NN.NN” change from baseline\nDichotomous: 0 for no response, 1 for response\nTTE: uncensored time to event in weeks\n\n\nThe GUI requires that the file name has a “.dat” suffix. The file need not have column headers, but if it does the first column name must start with a pound sign (#).\nThe following shows values from an example file with a dichotomous predictor. Unlike other endpoints there is only one line per subject, as there is no need to record the subject’s state at interim visits.\n\n\n\n\n\n\n\n\n\n#Patient ID\nDose Index\nUncensored Time to Event (weeks)\nUncensored Predictor value\n\n\n\n\n1\n1\n8.87\n0\n\n\n2\n1\n9.34\n0\n\n\n3\n2\n6.78\n1\n\n\n4\n2\n10.23\n0\n\n\n5\n2\n9.96\n0\n\n\n6\n2\n5.6\n1\n\n\n7\n1\n37.01\n0\n\n\n8\n1\n28.67\n0\n\n\n9\n1\n39.70\n0\n\n\n\nIn the above, column headers have been included to make it clearer to read but they are not required.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/dichotomous.html",
    "href": "documentation/v71/userguides/core/vsr/dichotomous.html",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "",
    "text": "In FACTS Core with a dichotomous endpoint there are 4 different ways to specify the virtual subject response:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/dichotomous.html#dose-response",
    "href": "documentation/v71/userguides/core/vsr/dichotomous.html#dose-response",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Dose Response",
    "text": "Dose Response\n\nDose response profiles can be added and deleted, and for each profile the user specifies:\n\nThe response rate for each treatment arm.\nA check box that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\n\nThe graph on the tab shows the mean response rate specified and the target.\nIf a 2D treatment arm model is being used, the doses are listed in “effective dose strength order” as was defined on the treatment arm tab.\n\nLoad Scenario Rates From File\nIf the “Load scenario means from a file” option is selected then in scenarios using this profile the simulations will use a range of dose responses. Each individual simulation uses one set of response rates from the supplied file, each row being used in an equal number of simulations. The summary results are thus averaged over all the VSRs in the file. The use of this form of simulation is somewhat different from simulations using a single rate or single external virtual subject response file. When all the simulations are simulated from one ‘truth’ then the purpose of the simulations is to analyse the performance of the design under that specific circumstance. When the simulations are based on a range of ‘truths’ then the summary results show the expected probability of the different outcomes for the trial over that range of possible circumstances. Note that to give different VSRs different weights of expectation, the more likely VSRs should be repeated within the file.\n\nAfter selecting the “.mvsr” file the graph shows the individual response rates and the mean response rate over all the VSRs.\nThere is a check box per dose that allows the user to specify whether a specific arm “should succeed” in that scenario: so that FACTS can report on the proportion of simulations that were successful and a ‘good’ treatment arm selected.\nThe format of the “.mvsr” file is a simple CSV text file. Lines starting with a ‘#’ character are ignored, so the file can include comment and header lines. There must be one column per treatment arm, and they must be in dose index order. Each value is the underlying response rate to simulate. E.g.:\n#Cntrl, D1, D2, D3, D4\n0.05, 0.1, 0.15, 0.25, 0.5\n0.05, 0.1, 0.15, 0.23, 0.45\n0.05, 0.1, 0.15, 0.21, 0.4\n0.05, 0.1, 0.15, 0.19, 0.35\n0.05, 0.1, 0.15, 0.17, 0.3\n0.05, 0.1, 0.15, 0.15, 0.25",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/vsr/dichotomous.html#longitudinal",
    "href": "documentation/v71/userguides/core/vsr/dichotomous.html#longitudinal",
    "title": "Virtual Subject Response - Dichotomous Endpoint",
    "section": "Longitudinal",
    "text": "Longitudinal\nThe tab for defining dichotomous longitudinal response profiles allows the user to specify the overall transition probabilities between responder and non-responder.\nIf no special longitudinal options are selected on the Study Info page, then this default simulation method that uses transition probabilities to simulate sequential dichotomous endpoints is used.\nThe transition probabilities method for generating longitudinal responses simulates the response observed at each visit by using the probability that a subject becomes or remains a ‘1’ from one visit to the next – and all subjects start with a response of 0.\n\nThe user specifies for each visit,\n\nthe probability of a subject whose response was a ‘0’ at the previous visit having a response of ‘1’ at this visit\nthe probability of a subject whose response was a ‘1’ at the previous visit having a response of ‘1’ at this visit\n\nHowever, these probabilities imply a particular probability that a subject has a response of ‘1’ at the final visit, so they need to be modified for each arm in each dose response profile to give the desired final probability of response. This is done by numerically determining for each final response rate, a single value which when added to all the specified transition probabilities in the log-odds space yield the desired probability of final response.\nLet \\(Q_{td}\\) be the probability of transitioning from 0 to 1, and \\(R_{td}\\) be the probability of transitioning from 1 to 1 for each visit (t) and dose (d).\nFor the 1st visit:\n\\[P\\left( y_{1d} = 1 \\right) = Q_{1d}\\]\nwhich is like considering the imaginary 0th visit to have been a non-response. For subsequent visits:\n\\[P\\left( y_{td} = 1 \\middle| y_{t - 1,d} = 0 \\right) = Q_{td}\\]\n\\[P\\left( y_{td} = 1 \\middle| y_{t - 1,d} = 1 \\right) = R_{td}\\]\nThe dose response (\\(P_{d}\\)) is first specified in the VSR &gt; Explicitly Defined &gt; Dose Response tab, and then longitudinal components (\\(q_{t}\\) and \\(r_{t}\\)) are specified separately. FACTS then makes an adjustment to the longitudinal components \\(q_{t}\\) and \\(r_{t}\\) to calculate \\(Q_{td}\\) and \\(R_{td}\\) for each dose while maintaining the value for \\(P_{d}\\) specified in the Dose Response tab.\nThe matrices Q and R are calculated by applying offsets \\(f_{d}\\) in log odds space to the longitudinal values:\n\\[Q_{t,d} = \\frac{e^{q'_{td}}}{1 + e^{q'_{td}}}, \\ \\ q'_{td} = \\ln\\left( \\frac{q_{t}}{1 + q_{t}} \\right) + f_{d}\\]\n\\[R_{t,d} = \\frac{e^{r'_{td}}}{1 + e^{r'_{td}}}, \\ \\ r'_{td} = \\ln\\left( \\frac{r_{t}}{1 + r_{t}} \\right) + f_{d}\\]\nwhere \\(f_{d}\\) is calculated iteratively for each dose to ensure that Q and R give the correct final probability of response for each dose. As a result,\n\\[P_{d} = x_{Td}\\]\nwhere\n\\[x_{1d} = Q_{1d}\\]\nand\n\\[x_{t} = x_{t - 1}R_{td} + \\left( 1 - x_{t - 1} \\right)Q_{td}\\]\n\nExample\nWith 3 visits and probabilities of \\(0\\rightarrow 1\\) of \\(0.2\\) and of \\(1\\rightarrow 1\\) of \\(0.9\\) at each visit, the probability of a final response is 0.438. This 0.438 is fixed, and cannot be changed without changing the transition probabilities.\n\n\n\n\n\n\n\n\n\n\nIndex\nVisit\nProb 1-&gt;1 (\\(r_t\\))\nProb 0-&gt;1 (\\(q_t\\))\nCumulative Pr(Resp)\n\n\n\n\n1\nVisit 1\n\n0.2\n0.2\n\n\n2\nVisit 2\n0.9\n0.2\n\\(0.2*0.9 + (1-0.2)*0.2 = 0.34\\)\n\n\n3\nVisit 3\n0.9\n0.2\n0.438\n\n\n\nIf a response profile calls for the probability of a final response to be simulated with a probability of 0.8, a fixed offset in log-odds is found which, when applied to all the transition probabilities, results in the desired final probability of a response. Replicating this by hand to an accuracy of 4 significant digits yields an offset of \\(f_d = 1.173\\), which gives:\n\\[\\text{logit}^-1(\\text{logit}(0.2) + 1.173) = 0.4469\\] \\[\\text{logit}^-1(\\text{logit}(0.9) + 1.173) = 0.9668\\]\nThen,\n\n\n\n\n\n\n\n\n\n\nIndex\nVisit\nProb 1-&gt;1 (\\(r_t\\))\nProb 0-&gt;1 (\\(q_t\\))\nCumulative Pr(Resp)\n\n\n\n\n1\nVisit 1\n\n0.4469\n0.4469\n\n\n2\nVisit 2\n0.9668\n0.4469\n\\(0.4469*0.9668 + (1-0.4469)*0.4469 = 0.6792\\)\n\n\n3\nVisit 3\n0.9668\n0.4469\n0.438",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Virtual Subject Response",
      "Dichotomous"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html",
    "href": "documentation/v71/userguides/core/simulation/tte.html",
    "title": "Time to Event Output",
    "section": "",
    "text": "To enable swift visualization and analysis of the simulation results, FACTS has a number of pre-defined graphs it can display. Full and detailed simulation results are available in ‘csv’ format files that can be loaded into other analysis tools allowing any aspect of the simulation to be explored. These files are described in Section 16, below.\nTo view the graphs of the results of the simulations of a particular design variant in a particular scenario, select a row of scenario results by clicking on it and then click on the ‘View Graph’ button and select “Show Per Scenario Graphs”.\nTo view graphs showing the results of the simulations of possibly all the design variants and all the scenarios click on the ‘View Graph’ button and select “Show Across Scenario Graphs”. This launches a graph display that displays multiple graphs in a trellis plot.\n\n\n\n\n\n\nFigure 1: Select the scenarios to include in Across Scenario graphs.\n\n\n\nThe graph display supports copying an image of the graph to the clipboard, to facilitate pasting them into documents and presentations. Right clicking on a graph brings up a short menu that allows the image of the graph to be copied to the clipboard or saved in ‘.png’ format to a file.\nMany graphs have a number of controls to allow the graph to be tailored, standard graph controls available on most graphs are:\n\nSet Y axis – this displays a dialog boxing allowing the user to fix the minimum and maximum of each of the Y axes and the number of ‘tick’ marks. (Not displayed if the ‘y’ value must lie in the interval 0-1.\nSpace doses evenly – if the x-value is ‘dose’, then data for each dose can either be spaced equally or proportionate to the doses’ effective dose strength.\n\n\n\n\nThe mean probability is plotted as a large dot.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#allocation-box-and-whisker-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#allocation-box-and-whisker-plot",
    "title": "Time to Event Output",
    "section": "Allocation Box and Whisker Plot",
    "text": "Allocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 2: Distribution of Subject Allocation Across Simulations.\n\n\n\nThis graph displays a box and whisker plot of the number of subjects recruited into each arm. These plots show:\n\nThe distribution over all simulations of the number of subjects allocated to each arm shown as a box and whisker plot. If the design is not adaptive, the number allocated will be the same in every simulation and the box and whiskers collapse to a single line.\nThe mean number of events observed in each arm across the simulations shown as a red triangle.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#events-boxplot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#events-boxplot",
    "title": "Time to Event Output",
    "section": "Events Boxplot",
    "text": "Events Boxplot\n\n\n\n\n\n\nFigure 3: Distribution of Number of Events Across Simulations\n\n\n\nThis graph displays a box and whisker plot of the number of events observed in each arm. These plots show:\n\nThe distribution over all simulations of the number of events observed in each arm shown as a box and whisker plot.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-subject-allocation",
    "href": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-subject-allocation",
    "title": "Time to Event Output",
    "section": "Hazard Ratio and Subject Allocation",
    "text": "Hazard Ratio and Subject Allocation\n\n\n\n\n\n\nFigure 4: Hazard Ratio and Subject Allocation\n\n\n\nThis graph shows for each treatment arm, the mean subject allocation, mean number of events observed and the mean estimated hazard ratio. Specifically:\n\nThe blue bars show the mean number of subjects without events and the brown bars mean number of subjects who had events.\nThe black line shows the true Hazard Ratio being simulated (but without allowing for any effect of the predictor)\nThe green dashed line (drawn if a response model is fitted), shows the mean of the estimated hazard ratios across the simulations\nThe green ‘error bars’ show the spread of the central 95% of the estimated hazard ratios (if less than 20 simulations have been run it simply shows the full spread).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-target-selection-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#hazard-ratio-and-target-selection-graphs",
    "title": "Time to Event Output",
    "section": "Hazard Ratio and Target Selection graphs",
    "text": "Hazard Ratio and Target Selection graphs\n\n\n\n\n\n\nFigure 5: Hazard Ratio and \\(Pr\\)(MED relative to Control: Delta=-0.2) Selection\n\n\n\nThese plots show the true simulated hazard ratio (without allowance for the effect of the predictor) and the mean and 95% spread of the mean fitted hazard ratios along with bars showing the proportion of simulations the different treatment arms are the finally selected target. The target displayed is selected by a control on the graph, from any of the defined Target QOIs.:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#predictor-response-and-allocation",
    "href": "documentation/v71/userguides/core/simulation/tte.html#predictor-response-and-allocation",
    "title": "Time to Event Output",
    "section": "Predictor: Response and Allocation",
    "text": "Predictor: Response and Allocation\n\n\n\n\n\n\nFigure 6: Predictor Hazard Ratio and Subject Allocation\n\n\n\nThese plots show the true simulated predictor response and the mean and 95% spread of the mean fitted predictor response model, with the blue bars showing the mean number of subjects allocated to each arm across the simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#per-dose-qois",
    "href": "documentation/v71/userguides/core/simulation/tte.html#per-dose-qois",
    "title": "Time to Event Output",
    "section": "Per Dose: QOIs",
    "text": "Per Dose: QOIs\n\n\n\n\n\n\nFigure 7: Distribution of P(Succ. Future Trial: N=250; Sup. \\(\\alpha=0.025\\), \\(\\delta=0\\) Adjusted Significance) Across Simulations\n\n\n\nThese plots show a box plot showing the distribution of the values for any of the defined QOIs for each dose.\nWhich QOI is displayed can be selected by the user from a drop down list on the graph. Any of the Posterior Probability, Predictive Probability, P-value and Target QOI’s can be selected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#target-hazard-ratio-scatter-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#target-hazard-ratio-scatter-plot",
    "title": "Time to Event Output",
    "section": "Target Hazard Ratio Scatter Plot",
    "text": "Target Hazard Ratio Scatter Plot\n\n\n\n\n\n\nFigure 8: Posterior Mean Hazard Ratio at Pr(Max) vs Total Sample Size per Simulation Run\n\n\n\nThis graph shows a scatter plot of trial outcomes with the estimate hazard ratio as the y-axis and total number of subjects recruited as the x-axis. A drop down list on the graph allows the user to select which target QOI is used to supply the hazard ratio:\nThe trials are plotted with a symbol that indicated the final outcome:\n\nA light blue circle indicates a trial that stopped early for success\nA dark blue circle indicates a trial that was a late success\nA dark red square indicates a trial that was a late futility\nA light red square indicates a trial that stopped early for futility\nA light pink diamond indicates a trial that stopped early for success but where the final analysis was futility\nA dark pink diamond indicates a trial that stopped early for futility but where the final analysis was success\nA yellow cross indicates a trial that completed full enrolment but was inconclusive at the end.\n\nThese graphs are not particularly useful if there is no early stopping.\nIf the design does allow early stopping this graph shows\n\nThe overall distribution of sample sizes – in particular is stopping evenly distributed from the moment stopping is possible, or is there are large cohort of trials that stop as soon as its possible?\nWhen and with what response rate trials stop incorrectly, in particular having trials stopping and the decision being reversed suggests stopping is allowed to soon or the criteria is not conservative enough.\n\nThere are two further variants of this graph, these have alternative y-axes: the hazard ratio at the EDx or at the MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#cumulative-operating-characteristics-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#cumulative-operating-characteristics-plot",
    "title": "Time to Event Output",
    "section": "Cumulative Operating Characteristics Plot",
    "text": "Cumulative Operating Characteristics Plot\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Duration\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Subjects\n\n\n\n\n\n\n\nFigure 9\n\n\n\nThere are two graphs, one that shows the cumulative proportion of durations across all simulations, and the other shows the cumulative proportion of subjects across all simulations.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#time-course-for-stopping",
    "href": "documentation/v71/userguides/core/simulation/tte.html#time-course-for-stopping",
    "title": "Time to Event Output",
    "section": "Time course for stopping",
    "text": "Time course for stopping\nThere are two graphs, one that shows the distribution over time for stopping for futility and one for stopping for success.\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Futility Over Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Success Over Time\n\n\n\n\n\n\n\nFigure 10\n\n\n\nThese plots show as cumulative curves the proportion of the simulations that stopped at different time points from the start of the trial.\nThe x-axis is configurable, the user can select cumulative stopping to be plotted relative to time, sample size or number of events observed.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#arm-dropping-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#arm-dropping-graphs",
    "title": "Time to Event Output",
    "section": "Arm Dropping Graphs",
    "text": "Arm Dropping Graphs\nIf the design has used arm dropping, the following graphs are available.\n\nHazard ratio and Ppn Arms Dropped\n\n\n\n\n\n\nFigure 11: Hazard Ratio and Proportion Arm Dropped\n\n\n\nThis graph shows the true (simulated) mean response and the average fitted response with 2.5% & 97.5% quantiles.\nThe histogram bars show for each arm the proportion of simulations for which the arm was dropped.\n\n\nTime Course for Arm Dropping\nThis graph shows, for each arm, the cumulative proportion of simulations when it was dropped. The user can select whether time or number of subjects allocated to the arm is used as the x-axis.\n\n\n\n\n\n\nFigure 12: Cumulative Proportion of Trials where Arm Dropped\n\n\n\n\n\nTime Course for Arm retention\nThis graph shows, for each arm, the cumulative proportion of simulations when the arm was retained. The user can select whether time or number of subjects allocated to the arm is used as the x-axis. This is simply the inverse of the arm dropping graph above.\n\n\n\n\n\n\nFigure 13: Cumulative Proportion of Trials Where Arm Retained\n\n\n\n\n\nArm Retention Proportion\nThis graph shows, for each arm, the simple proportion of simulations where the arm was retained.\n\n\n\n\n\n\nFigure 14: Arm Retention Proportion",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#frequentist",
    "href": "documentation/v71/userguides/core/simulation/tte.html#frequentist",
    "title": "Time to Event Output",
    "section": "Frequentist",
    "text": "Frequentist\nThese graphs are available if frequentist analysis is enabled.\n\nFrequentist P(significance)\nThese graphs are the box and whisker plots of various p-values that are automatically calculated (that is to say they do not need to be defined as QOIs for them to be calculated) for the final analysis. The user can select from Unadjusted, Bonferroni and Dunnett's and LOCF, BOCF (if baseline is simulated) or PP (per-protocol) treatment of missing values.\n\n\n\n\n\n\nFigure 15: Distribution of \\(P\\)(Log Rand Adjusted Significance) Across Simulations\n\n\n\n\n\nFrequentist: Response and Significance\nThis graph plots the mean and 2.5-97.5 percentile of the mean across the simulations or the simple frequentist estimate of the response for each arm, along with a histogram plot of the ppn of times the response on each arm was significant. The user can select from the Unadjusted, Bonferroni or Log Rank, Log Rank Bonferroni, Wilcox and Wilcox Bonferroni p-values.\n\n\n\n\n\n\nFigure 16: Hazard Ratio and Ppn Log Rank Significance",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#per-sim-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#per-sim-graphs",
    "title": "Time to Event Output",
    "section": "Per Sim Graphs",
    "text": "Per Sim Graphs\nThis set of graphs includes a control that allows the user to select which simulation to graph the results from. Each graph shows the output from only 1 simulated trial.\n\n\n\n\n\n\nFigure 17: The plot window when selecting a Per Sim graph. Note the Simulation control in the bottom left allowing for the selection of which simulated trial the graph should be made for.\n\n\n\n\nHazard Ratio and Subject Allocation\nThis graph shows the final analysis at the end of individual simulations.\n\n\n\n\n\n\nFigure 18: Hazard Ratio and Subject Allocation\n\n\n\nOn each graph\n\nThe black line shows the ‘True’ Hazard Ratio being simulated, (not adjusted to include any predictor effects)\nThe blue bars show the mean number of subjects without events and the brown bars mean number of subjects who had events.\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\n\n\n\nPosterior Quantities\nThis graph shows the final analysis at the end of individual simulations, including the distribution of a selected QOI.\n\n\n\n\n\n\nFigure 19: Hazard Ratio and \\(Pr\\)(\\(HR_d\\) - 1 &lt; -0.2) Probability\n\n\n\nThis graph includes a control that allows the user to select which simulation to graph the results from and which QOI to plot. On each graph\n\nThe black line shows the ‘True’ mean response being simulated.\nThe brown circles and arms show the mean raw data and its 95% CI for each arm.\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\nBrown bars showing the final value of the selected QOI for each arm at the end of the simulated trial.\n\nThe user can select from any of the Posterior Probability, Predictive Probability or Target Probability QOIs to plot, including QOI’s based on the predictor.\n\n\nPredictor Response and Allocation\n\n\n\n\n\n\nFigure 20: Predictor Hazard Ratio and Subject Allocation (Week: 80)\n\n\n\nIf the simulations include a predictor this graph is available.\nThis graph includes a control that allows the user to select which simulation to graph the results from. On each graph\n\nThe black line shows the ‘True’ Predictor response being simulated\nIf a response model has been fitted:\n\nThe green solid line shows the estimate of the mean hazard ratio\nThe green dashed lines show the boundaries of the 95% credible interval for the estimate of the hazard ratio.\n\nOtherwise if the Independent Dose model has been fitted:\n\nThe light green circle shows the mean estimate of the hazard ratio for each arm\nThe vertical green arms show the extent of the 95% credible interval for the estimate of the hazard ratio for each arm.\n\nBlue Bars showing number of subjects allocated to each treatment arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#per-interim-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#per-interim-graphs",
    "title": "Time to Event Output",
    "section": "Per Interim Graphs",
    "text": "Per Interim Graphs\nThis is an identical set of graphs to the Per Sim graphs, the difference is that in addition to a control to select which simulation to graph the results from, there is a control to select which interim within the simulation to graph the results from. These graphs are only available for those simulations for which ‘weeks’ files have been output (by default the first 100). Note the control in the bottom left that allows for specification of Simulation number as well as Interim number.\n\n\n\n\n\n\nFigure 21: The plot window when selecting a Per Interim graph. Note the Simulation control in the bottom left allowing for the selection of which simulated trial and which interim analysis the graph should be made for.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#post-simulation-boundary-finding-graphs",
    "href": "documentation/v71/userguides/core/simulation/tte.html#post-simulation-boundary-finding-graphs",
    "title": "Time to Event Output",
    "section": "Post Simulation Boundary Finding Graphs",
    "text": "Post Simulation Boundary Finding Graphs\n\nExplore Success/Futility Eval Criteria\n\n\n\n\n\n\n\n\n\n\n\n(a) Proportion of Simulations that would be futile by futility threshold.\n\n\n\n\n\n\n\n\n\n\n\n(b) Proportion of Simulations that would be successful by success threshold.\n\n\n\n\n\n\n\nFigure 22\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have been a success/failure at final evaluation. Using the two drop down controls the user can select the target dose to use: Max, EDx or MED, and set and lower/upper limits to explore for the threshold (setting the range used on the x-axis).\nFor the given target the proportion of trials that would meet each of the criteria over the range of threshold values is plotted. As in the examples above, the plots will be somewhat jagged if only a small number of simulations have been run. These graphs can be used to select thresholds that can be expected to yield a certain level of type-1 or power, but the user must remember these will only be approximate (depending on the number of simulations) but can be useful to understand the designs sensitivity to the thresholds and to set initial thresholds early on in the design / simulation process that will get close to the desired type-1 error and power from the outset.\n\n\nExplore Early Success/Futility Eval Criteria\n\n\n\n\n\n\n\n\n\n\n\n(a) Cumulative Proportion of Simulations Satisfying Futility Criterion by Interim\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative Proportion of Simulations Satisfying Success Criterion by Interim\n\n\n\n\n\n\n\nFigure 23\n\n\n\nThese graphs can be used to explore what proportion of simulated trials of a particular scenario would have stopped early for success/futility. NOTE these graphs require weeks files to have been output, they are also most use where the design has been simulated with interims but no early stopping (as in the examples above where the shape of the “existing stopping rules” line indicates that no early stopping occurred in these simulations).\nUsing the two drop down controls, the user can select which stopping criteria is evaluated and from which interim stopping will be permitted. Lines are then displayed for the proportion of simulations that would have stopped by each interim for a fixed set of thresholds.\nTypically these can be used to see at what threshold (and starting at what interim) stopping for success or futility introduces an unacceptably level of ‘incorrect’ early stopping – stopping for futility in successful scenarios and stopping for success in futile scenarios and whether at below/above these levels there may be a useful probability of correct stopping.\n\n\nExplore Arm Dropping Criteria\n\n\n\n\n\n\nFigure 24: Cumulative Proportion of Simulations Where Selected Arm Satisfies Criterion by Interim\n\n\n\nThis graph is similar to the Explore Early Success/Futility Criteria graph but for arm dropping criteria. The user has an additional control to select which dose the proportion of trials in which the arm would be dropped is displayed.\n\n\nSuccess/Futility Stopping Contours\n\n\n\n\n\n\n\n\n\n\nFigure 25\n\n\n\nThese graphs allow early stopping and final evaluation criteria to be considered jointly. Like the explore early stopping criteria plots, these graphs require weeks files to have been output and will work best if interims were evaluated but no trials actually stopped early.\nThe user selects the criterion to plot, the first interim at which early stopping is allowed, and the upper/lower limit of the threshold to consider.\nFACTS will then plot contours where (final evaluation threshold, early stopping threshold) yield the same proportion of trials that are a success/futile. Contours are only plotted where final evaluation threshold &lt; early stopping threshold for success and vice versa for futility. Early stopping criteria should not be less strict than final criteria.\nAgain the example graphs shown are based on only 100 simulations with weeks files, so any threshold derived will only very approximately yield these success/futility rates.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#selected-arms",
    "href": "documentation/v71/userguides/core/simulation/tte.html#selected-arms",
    "title": "Time to Event Output",
    "section": "Selected Arms",
    "text": "Selected Arms\nThis graph shows a bar chart for each scenario and variant selected. Each chart shows how often each arm was ‘selected’ by the target QOI specified on the Study &gt; Variants tab. Each bar uses a stacked bar to show the proportion of times that arm was:\n\n“Successful” –the arm was correctly selected (marked as “Should succeed” on the VSR tab) and the trial was successful.\n“Should not succeed” – the arm was incorrectly selected (not marked as “Should succeed” on the VSR tab) and the trial was successful.\nUnsuccessful – the arm was selected and the trial failed.\n\n\n\n\n\n\n\nFigure 26: Across scenario graph for selected arms with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#qoi-box-plots",
    "href": "documentation/v71/userguides/core/simulation/tte.html#qoi-box-plots",
    "title": "Time to Event Output",
    "section": "QOI Box Plots",
    "text": "QOI Box Plots\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the values of a selected QOI for each arm. There is a drop down control to allow the selection of the QOI to be displayed. Any Posterior probability, Predictive probability, p-value or target QOI can be selected.\n\n\n\n\n\n\nFigure 27: Across scenario graph showing QOIs per arm with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#ppn-success",
    "href": "documentation/v71/userguides/core/simulation/tte.html#ppn-success",
    "title": "Time to Event Output",
    "section": "Ppn Success",
    "text": "Ppn Success\nThis grouped bar chart shows a bar for the proportion of successful simulations for each variant, grouped by scenario.\n\n\n\n\n\n\nFigure 28: Across scenario graph for showing the proportion of trials that result in success with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#response",
    "href": "documentation/v71/userguides/core/simulation/tte.html#response",
    "title": "Time to Event Output",
    "section": "Response",
    "text": "Response\nThis graph shows a dose response plot for each scenario and variant selected. Each plot shows the mean estimate over the simulations and the 95%-ile interval of the mean estimates over the simulations. The graph also shoes the “true response” i.e. the mean response being simulated.\n\n\n\n\n\n\nFigure 29: Across scenario graph for showing the average estimated dose response model vs. the true dose responses with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#allocation",
    "href": "documentation/v71/userguides/core/simulation/tte.html#allocation",
    "title": "Time to Event Output",
    "section": "Allocation",
    "text": "Allocation\nThis graph shows a box and whisker plot for each scenario and variant selected. Each plot shows the distribution of the number of subjects allocated to each arm over the simulations.\n\n\n\n\n\n\nFigure 30: Across scenario graph for showing the distribution of allocation per arm with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#sample-size",
    "href": "documentation/v71/userguides/core/simulation/tte.html#sample-size",
    "title": "Time to Event Output",
    "section": "Sample Size",
    "text": "Sample Size\nThis graph shows the mean sample size for each scenario at different maximum sample sizes (the different variants).\n\n\n\n\n\n\nFigure 31: Across scenario graph for showing the mean sample size of the study with 4 scenarios and 3 design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#interim-vs-final-scatter-plot",
    "href": "documentation/v71/userguides/core/simulation/tte.html#interim-vs-final-scatter-plot",
    "title": "Time to Event Output",
    "section": "Interim vs Final Scatter Plot",
    "text": "Interim vs Final Scatter Plot\nThis is an interactive plot that shows the outcome of each simulation over all the scenarios of a selected variant.\nThe x-axis shows the value of the selected decision QOI at the specified interim, and the y-axis shows the final value of a (possibly different) selected decision QOI. As well as selecting a decision QOI for the final decision, the graph can use the actual final decision in the simulations – the value of the QOI used in the first criteria is used for the decision is used for the y-axis.\nNote the graph is based just on the values at the selected interim, what might happen at other interims is ignored.\nThresholds can be set to specify early and late success/futility. Each scenario is then coloured to show its outcome – and stacked bar graphs are displayed to show the proportion of each outcome for each scenario. The thresholds can be changed and the graph re-drawn. If the selected QOI uses a p-value the checks for success are that it is “&lt;” than the threshold, (and vice-versa for futility), otherwise the checks for success are that it is “&gt;” the threshold (and vice-versa for futility).\nThe graph can only display the results for simulations for which cohorts files have been output, and for simulations that reached the specified interim. Thus the graph works best if interims have been specified, but the simulations run without any early stopping enabled – so each simulation has the results for every interim.\nWith 1,000 simulations per scenario and cohorts files for each simulation, the graph will take several seconds to re-draw. To allow several parameters to be set before redrawing (rather than having to wait for the redraw after each change) the graph is only redrawn after the ‘Redraw’ button is clicked.\n\n\n\n\n\n\nFigure 32: Across scenario graph showing an interim QOI plotted against a final analysis QOI with the decision dictating the point color. Additionally, the bars on the right hand side represent the proportion of trials that made each decision based on the decision rules specified in the plot controls.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#receiver-operating-characteristics",
    "href": "documentation/v71/userguides/core/simulation/tte.html#receiver-operating-characteristics",
    "title": "Time to Event Output",
    "section": "Receiver Operating Characteristics",
    "text": "Receiver Operating Characteristics\nThis graph plots the relative proportion of successful simulations in the scenarios compared to the proportion of successes in one specific scenario. The intended use of this is to yield a “Receiver Operating Characteristics” (ROC) curve. This is done as follows:\n\nSelect the decision QoI that is to be used to determine final success/futility. (The graph only supports the simple case where just one is being used).\nSelect a scenario that represents the ‘Null’ case – where determining success is a type-1 error.\nIf multiple variants have been simulated, select which variant to plot.\n\nFACTS computes for the null scenario, for a range of decision threshold values, what proportion of the ‘Null’ scenario sims would have been successful (giving the estimated type-1 error rate), these proportions form the x-values for each point. The corresponding threshold values are shown on the x-axis at the top of the graph, the type-1 error rate on the x-axis at the bottom of the graph.\nFACTS then computes for each of the other scenarios in turn the proportion of sims that would have been successful and selected a “should succeed” dose so it is necessary for doses to have been specified as “should succeed” on the Virtual Subject Response profiles (giving the estimate of power for that scenario – where ‘power’ also requires correct dose selection), at each of the threshold values, these proportions are then shown on the y-axis of the graph.\nThus for each scenario we can see the estimated power of the design to determine success for a given type-1 error rate in the specified Null scenario.\n\n\n\n\n\n\nFigure 33: Across scenario graph showing ROC curves of the power of one selected design variant under different scenarios as a function of the type I error in the selected null scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#summary-per-scenario",
    "href": "documentation/v71/userguides/core/simulation/tte.html#summary-per-scenario",
    "title": "Time to Event Output",
    "section": "Summary per Scenario",
    "text": "Summary per Scenario\nBy default, only a subset of all columns of output are provided when simulations are complete. This set of columns is called “Highlights.” To get access to further columns, you can right click anywhere on the simulation output table and click “Summary Results: ….” for whichever set of columns you would like to see. There is a “Summary Results: All” option that will provide a pop up containing every column available for summary results.\nThe following sections break down which columns are available by sorting to which column subset.\n\nHighlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\nThe columns that are displayed by default in the simulations tab after time-to-event simulations have been run.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn Overall Success\n1\nThis is the proportion of simulations that stopped for success, either early success or late success (as defined below).\n\n\nPpn Early Success\n1\nThis is the proportion of simulations that stopped early for success (and did not regress to futility in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn Late Success\n1\nThis is the proportion of simulations that did not stop early but were successful in the final analysis.\n\n\nPpn Overall Futility\n1\nThis is the proportion of simulations that stopped for futility, either early futility or late futility (as defined below).\n\n\nPpn Late Futility\n1\nThis is the proportion of simulations that did not stop early but were futile in the final analysis.\n\n\nPpn Early Futility\n1\nThis is the proportion of simulations that stopped early for futility (and did not regress to success in the final analysis – though they might have regressed to ‘inconclusive’).\n\n\nPpn S uc-&gt;Fut Flipflop\n1\nThis is the proportion of simulations that stopped early for success but regressed to futility in the final analysis.\n\n\nPpn F ut-&gt;Suc Flipflop\n1\nThis is the proportion of simulations that stopped early for futility but regressed to success in the final analysis.\n\n\nPpn Inco nclusive\n1\nThis is the proportion of simulations that did not stop early and were neither successful nor futile in the final analysis.\n\n\nEarly Success Time\n1\nThis is the average time to an early decision to stop for success (i.e. the time excluding final follow up) over those simulations that did decide early to stop for success.\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nMean Duration\n1\nThe mean over the simulations of the total duration from the start of enrolment to end of follow up.\n\n\nPPn Arms Drop: &lt; Dose&gt;&gt;\nOne per arm\nThis is the number of times (over the simulations) that each arm was dropped.\n\n\nMean LP Enrolled\n1\nThis is the mean, in weeks (over the simulations) of the duration of the trial from first patient first visit to Last Patient First Visit (i.e. the duration of accrual).\n\n\nPpn Correct Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nPpn I ncorrect Arm\n1\nThe proportion of simulations that met the success criteria and selected (by the target QOI specified on the Study &gt; Variant tab) one of the arms not marked as “should succeed” on the Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response tab\n\n\nVersion\n1\nThe FACTS version number at the time the simulations were run.\n\n\n\n\n\nAllocation\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns.\n\nThe allocation tabs provided in FACTS output for time-to-event trials.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nSD Mean Subj.\n1\nThis is the standard deviation across the simulations of the number of subjects recruited.\n\n\n80%-ile Subj\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nMean Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the number of subjects recruited into each arm in this scenario.\n\n\nSD Alloc.: &lt;Dose&gt;\nOne per arm\nThis is the SD of the means (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\n\n\n\nResponse\nThe following columns provide summaries of the estimated hazard ratios based on the bayesian model incorporating the dose response model and the predictor model, if one is specified.\n\nResponse columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Trt. &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of the hazard ratio of the treatment arms to control. The HZ for the control arm is always 1.\n\n\nSD Trt .: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of the treatment hazard ratio for each arm. The SD of the treatment response for the control arm is always 0.\n\n\nTrue Resp: &lt;Dose&gt;\nOne per arm\nThis is the true Hazard Ratio from which the simulation data was sampled. When using VSR with Event Rate | Predictor, these rates will not be the same as the Dose Response HRs entered on the VSR &gt; Explicitly Defined (ER | P) &gt; Dose Response tab.\n\n\n\n\n\nObserved\nBy right clicking and selecting the Observed columns, a pop-out will appear that provides the following columns. These columns relate to the raw data observed in the trial.\n\nObserved columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Total Events\n1\nThe mean (over the simulations) of the number of events observed in each trial.\n\n\nMean Events &lt;Dose&gt;\nOne per arm\nThe mean (over the simulations) of the number of events observer in each arm in each trial.\n\n\nMean Events &lt;Dose&gt; &lt;S egment&gt;\nOne per arm per segment\nThe mean (over the simulations) of the number of events observer in each arm and segment in each trial.\n\n\nMean Complete Info &lt;Dose&gt;\nOne per arm\nThis is the mean (over the simulations) of information observed per arm as defined on the Interims tab (Subjects enrolled, Complete Predictor Data, Opportunity to Complete Predictor, Events, Predictor Events) in this scenario.\n\n\nMean Dropouts &lt;Dose&gt;\nOne per arm\nIf dropouts are simulated this is the mean (over the simulations) of the number of subjects in each arm that dropout before an event is observed.\n\n\nMean Exposure &lt;Dose&gt; &lt;S egment&gt;\nOne per arm\nThe mean (over the simulations), of the total exposure of the subjects observed on each arm and segment.\n\n\n\n\n\nProbabilities\nBy right clicking and selecting the allocation columns, a pop-out will appear that provides the following columns. These columns provide summaries of the Quantities of Interest values.\n\nProbabilities columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\n&lt;QOI&gt; &lt;Dose&gt;\nOne per arm per QOI\nFor each Posterior Probability and Predictive Probability QOI defined on this endpoint, this is the mean over the simulations of the estimate of the probability of the QOI for each dose. (P-value QOIs are reported in the frequentist results table).\nFor each Target QOI this is the proportion of simulations where this dose was selected at the end of the trial as the dose with the greatest probability of meeting the target condition.\nThe probability that each dose is the target at the end of a simulated trial is its marginal probability (the number of times it was the dose closest to the target in the MCMC sampling of the analysis at the end of the trial). The target of having the Max response on this endpoint, or some fraction of it (EDq) is always identifiable, so the Ppn(target) for these QOIs will sum to 1 across the doses.\nAn MED target is not guaranteed to be identifiable, if no dose meets the CSD criteria in any MCMC sample so all doses have a 0 probability of having a response greater than Control (or AC) by the CSD then no dose is the MED. So the sum of each Ppn(target) QOI across the doses should sum to between 0 and 1 inclusive.\nDecision QOI’s are not reported here separately, but their component QOIs – the vector of values and target QOI used to select from them are.\n\n\n\n\n\nHierarchical Prior Parameters\nBy right clicking and selecting the Hierarchical Prior Parameters columns, a pop-out will appear that provides the following columns.\n\nHierarchical Prior Parameters columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean BAC Mu\n1\nThe average (over the simulation) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nSD BAC Mu\n1\nThe average (over the simulations) of the standard deviation of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nMean BAC Tau\n1\nThe average (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nSD BAC Tau\n1\nThe standard deviation (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Control (hierarchical Prior) distribution of mean Control responses, if a Hierarchical Prior is being used for Control is being used, otherwise the column contains -9999.\n\n\nMean BAAC Mu\n1\nThe average (over the simulation) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nSD BAAC Mu\n1\nThe average (over the simulations) of the standard deviation of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nMean BAAC Tau\n1\nThe average (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator is being used, otherwise the column contains -9999.\n\n\nSD BAAC Tau\n1\nThe standard deviation (over the simulations) of the posterior estimate of the standard deviation of the Bayesian Augmented Active Comparator (hierarchical Prior) distribution of mean Active Comparator responses, if a Hierarchical Prior is being used for Active Comparator, otherwise the column contains -9999.\n\n\n\n\n\nPredictor\nBy right clicking and selecting the Predictor columns, a pop-out will appear that provides the following columns. These colums provide summaries of the bayesian model estimates relating to the predictor endpoint.\n\nPredictor columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nMean Pred. Resp.: &lt;Dose&gt;\nOne per arm\nThe mean (over the simulations) of the mean estimate of the predictor response for each arm. The response reported is:\nContinuous predictor: The mean change from baseline\nDichotomous predictor: The response rate\nTTE predictor: The hazard ratio\nNo predictor: 0\n\n\nSD Pred. Resp.: &lt;Dose&gt;\nOne per arm\nThe SD (over the simulations) of the mean estimate of the predictor response for each arm.\n\n\nMean Pred. Sigma\n1\nThis is the average (over the simulations) of the estimate of sigma (SD of response) of a continuous predictor, if one was being used. Otherwise the column contains -9999.\n\n\nSD Pred Sigma\n1\nThis is the standard deviation (over the simulations) of the estimate of the sigma (SD of the response) of a continuous predictor, if one was being used. Otherwise the column contains -9999.\n\n\nTrue P redictor Resp: &lt;Dose&gt;\nOne per arm\nThis is the true predictor response from which the simulated subject predictor responses were sampled. For a continuous predictor this is the mean of the response, for a dichotomous predictor it is the predictor response rate and for a time-to-event predictor it is the hazard ratio of the predictor, otherwise the column contain -9999.\n\n\nTrue P redictor Sigma: &lt;Dose&gt;\nOne per arm\nThis is the true standard deviation of the predictor response from which the simulated subject predictor responses were sampled when the predictor is a continuous measure, otherwise the column contains -9999\n\n\nMean Pred. Lambda: &lt;Dose&gt;\nOne per arm\nThis is the average (over the simulations) of the estimate of Lambda for each dose in the endpoint predictor model – the estimated mean time to the final event for each dose either where the predictor is zero for a continuous or dichotomous predictor, or for the time after observing the event of time-to-event predictor. Otherwise the column contains 0.\n\n\nSD Pred. Lambda: &lt;Dose&gt;\nOne per arm\nThis is the standard deviation (over the simulations) of the estimate of Lambda for each dose (see above). Otherwise the column contains 0.\n\n\nMean Pred. Beta\n1\nThis is the average (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor. Otherwise the column contains -9999.\n\n\nSD Pred. Beta\n1\nThis is the standard deviations (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor. Otherwise the column contains -9999.\n\n\n\n\n\nModel Parameters\nBy right clicking and selecting the Model Parameters columns, a pop-out will appear that provides the following columns.\n\nModel Parameters columns available in FACTS output for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis column gives the name of the scenario, concatenating together the profile names from the following tabs:\n‘Execution &gt; Accrual’,\n‘Execution &gt; Dropout Rate’,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Dose Response’,\n’Virtual Subject Response &gt; Explicitly Defined &gt; Control Hazard rates,\n‘Virtual Subject Response &gt; Explicitly Defined &gt; Predictor’,\nThis is the same name as used for the results directory\n\n\nTrue P redictor Baseline Hazard Lambda: &lt;s egment&gt;\nOne per VSR pr edictor hazard rate segment\nThe true hazard rate for the predictor (if the predictor is itself an event), in each of the predictor VSR time segments.\n\n\nLambda: &lt;s egment&gt;\nOne per Design, Hazard Model segment\nThe mean (over the simulations) of the mean estimate of lambda – the event rate (events per week) in each time segment of the hazard model.\n\n\nSE Lambda: &lt;s egment&gt;\nOne per Design, Hazard Model segment\nThe standard error (over the simulations) of the mean estimate of lambda for each time segment of the hazard model.\n\n\nTrue Lambda: &lt;s egment&gt;\nOne per VSR control hazard segment\nThe true hazard rate for each segment of the virtual subject response hazard rate profile.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#detailed-simulation-results",
    "href": "documentation/v71/userguides/core/simulation/tte.html#detailed-simulation-results",
    "title": "Time to Event Output",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 15‑1) displays the individual results for each simulation. This is the contents of the “simulations.csv” file, which is described below.\n\n\n\n\n\n\nFigure 34: The simulation results pop out viewer showing simulation highlights.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary.csv",
    "title": "Time to Event Output",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContents of the summary.csv file for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nProject\n1\nThe name of the facts file\n\n\nScenario\n1\nThe name of the scenario\n\n\nT imestamp\n1\nThe time the simulations were run\n\n\nVersion\n1\nThe version of FACTC that was used to run the simulations\n\n\nNSim\n1\nThe number of simulation runs.\n\n\nNo. Subj\n1\nThe mean, over the simulations, of the total number of subjects recruited in the trial.\n\n\nSE Subj.\n1\nThe standard error of the total number of subjects recruited into the trials.\n\n\nNo.subj 80-ile\n1\nThe 80th percentile, over the simulations, of the total number of subjects recruited in the trial.\n\n\nP(ES)\n\n1\n\nThe proportion of simulations that stopped early for success.\n\n\nP(LS)\n1\nThe proportion of simulations that did not stop early, and were successful on final evaluation.\n\n\nP(LF)\n1\nThe proportion of simulations that did not stop early, and were futile on final evaluation.\n\n\nP(EF)\n1\nThe proportion of simulations that stopped early for futility.\n\n\nSFFF\n1\nThe proportion of simulations that stopped early for success, but at final analysis were futile (Success-Futility ‘flip-flops’).\n\n\nFSFF\n1\nThe proportion of simulations that stopped early for futility, but at final analysis were successful (Futility- Success ‘flip-flops’).\n\n\nUndec.\n1\nThe proportion of simulations that did not stop early, and met neither the success or futility final evaluation criteria and hence are counted as ‘undecided’.\n\n\nUnused\n3\nThree unused columns for other outcome types.\n\n\nEarly Success Time\n1\nThis is the mean early decision time (over the simulations stopped early for success), it is the time from the start of the trial to the interim where early success was declared. It does not include the subsequent follow up time.\n\n\nMean Alloc &lt;dose&gt;\nD\nThe mean (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nSE Alloc &lt;dose&gt;\nD\nThe standard error (over the simulations) of the number of subjects allocated to each treatment arm.\n\n\nMean Resp &lt;dose&gt;\nD\nThe mean of the estimates of hazard ratio of each treatment arm. (This is always 1 for the control arm)\n\n\nSE Resp &lt;dose&gt;\nD\nThe standard error, over the simulations, of the estimate of hazard ratio of each treatment arm. (This is always 0 for the control arm).\n\n\nTrue Mean resp &lt;dose&gt;\nD\nThe true hazard ratio for each treatment arm used to derive the simulated subject event times (the HR for the control arm is always 1).\n\n\nMean p redictor resp &lt;dose&gt;\nD\nThe mean (over the simulations) of the estimated predictor response per arm – this is the estimated mean response for a continuous predictor, the estimated response rate for a dichotomous predictor and the estimated hazard ratio for a time to event predictor.\n\n\nSE p redictor resp &lt;dose&gt;\nD\nThe standard error (over the simulations) of the estimated predictor response per arm.\n\n\nMean P redictor Sigma\n1\nThe mean (over the simulation) of the estimated standard deviation in the predictor response when modeling a continuous predictor.\n\n\nSE P redictor Sigma\n1\nThe standard error (over the simulations) of the estimated standard deviation in the predictor response when modeling a continuous predictor.\n\n\nTrue Mean p redictor resp &lt;dose&gt;\nD\nThe true predictor response rate used to drive the simulation of the subjects’ predictor outcomes – this is the mean of the response for a continuous predictor, the response rate for a dichotomous predictor and the hazard ratio for a time to event predictor.\n\n\nTrue Sigma p redictor &lt;dose&gt;\nD\nThe true sigma used to drive the simulation of the subjects’ predictor outcomes when the predictor is a continuous measure.\n\n\nMean P redictor Lambda &lt;dose&gt;\nD\nThe mean (over the simulations) of the estimate of lambda – the estimated mean time to the final event in the endpoint predictor model – the estimated mean time to the final event for each dose when the predictor is zero (continuous or dichotomous) or for the time after observing the predictor event.\n\n\nSE P redictor Lambda &lt;dose.\nD\nThis is the standard error (over the simulations) of the estimate of lambda for each dose.\n\n\nMean P redictor Beta\n1\nThis is the mean (over the simulations) of the estimate of Beta – the coefficient that relates the predictor to the endpoint for a continuous or dichotomous predictor.\n\n\nSE p redictor Beta\n1\nThis is the standard error (over the simulations) of the estimate of Beta.\n\n\nMean TTE P redictor Baseline Hazard Lambda &lt;s egment&gt;\nS\nThis is the mean (over the simulations) of the estimate of the event rate on the control arm in each hazard model time segment.\n\n\nSE TTE P redictor Baseline Hazard Lambda &lt;s egment&gt;\nS\nThis is the standard error (over the simulations) of the estimate of the event rate on the control arm in each hazard model time segment.\n\n\nBAC Mean\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Mean\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\nIf a hierarchical prior for Control is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nSE of BAC Tau\n1\nIf a hierarchical prior for Control is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Mean\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the mean (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nSE of BAAC Tau\n1\nIf a hierarchical prior for Active Comparator is being used this is the standard error (over the simulations) of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nEst. Lambda &lt;s egment&gt;\nS\nThe mean, over the simulations, of the estimate of the event rate on the control arm in each time segment of the hazard model.\n\n\nSE lambda &lt;s egment&gt;\nS\nThe mean, over the simulations, of the SD of the estimate of the event rate on the control arm in each time segment of the hazard model.\n\n\nMean Total Events\n1\nThe mean, over the simulations of the total number of events observed in the trials.\n\n\nSE Total Events\n\nThe standard error, over the simulations, of the total number of events observed in the trials.\n\n\nNo. Events &lt;dose&gt;\nD\nThe mean, over the simulations, of the number of events observed on each arm.\n\n\nSE No. Events &lt;dose&gt;\n\nThe standard error, over the simulations, of the number of events observed on each arm.\n\n\nMean Exposure &lt;dose&gt;\n\nThe mean, over the simulations, of the total exposure of the subjects observed on each arm.\n\n\nSE Exposure &lt;dose&gt;\n\nThe standard error, over the simulations, of the total exposure of the subjects observed on each arm.\n\n\nMean Duration\n1\nThe mean, over the simulations, of the overall duration of the trials, from first person first visit, to last person last visit.\n\n\nPpn Arms Dropped &lt;dose&gt;\nD\nThe proportion of the simulations in which each arm was dropped. These will be zero if arm dropping is not selected as the means of adaptive allocation.\n\n\nMean LPFV time\n1\nThe mean, over the simulations, of the accrual period of the trials, from first person first visit, to last person first visit (LPFV).\n\n\nQOI Columns\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns in the following order:\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nPosterior Probabilities\nThe summary file contains the mean (over the simulations) of the posterior probability for each dose.\n\n\nPredictive Probabilities\nThe summary file contains the mean (over the simulations) of the predictive probability for each dose.\n\n\nP-Values\nThe summary file contains the mean (over the simulations) of the p-value for each dose.\n\n\nTarget Probabilities\nThe summary file contains the proportion of times (over the simulations) each dose had the greatest probability of being the target.\n\n\nDecision QOIs\nThe summary file contains the mean (over the simulations) of the final value of the decision QOI at the target.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary_freq.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-summary_freq.csv",
    "title": "Time to Event Output",
    "section": "Contents of Summary_freq.csv",
    "text": "Contents of Summary_freq.csv\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\n\nContents of the summary_freq.csv file for time-to-event simulations.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPpn LR Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Log Rank test) is less than the user specified one-sided alpha.\n\n\nPpn LR Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Log Rank test) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn LR Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Log Rank test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn LR Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Log Rank test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn Wilcox Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Wilcoxon test) is less than the user specified one-sided alpha.\n\n\nPpn Wilcox Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Wilcoxon test) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn Wilcox Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Wilcoxon test) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn Wilcox Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Wilcoxon test) is less than the Bonferroni correct user specified one-sided alpha.\n\n\nMean HR &lt;dose&gt;\nD\nThe mean Hazard Ratio per dose.\n\n\nSE HR &lt;dose&gt;\nD\nThe standard error of the Hazard Ratio per dose\n\n\nPpn HR Signif\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Cox model) is less than the user specified one-sided alpha.\n\n\nPpn HR Signif &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Cox model) is less than the user specified one-sided alpha.\nThe marginal probability of significance per dose (using unadjusted p-values.\n\n\nPpn HR Signif Bonf\n1\nThe proportion of simulations where at least one of the unadjusted p-values (from the Cox model) is less than the Bonferroni corrected user specified one-sided alpha.\n\n\nPpn HR Signif Bonf &lt;dose&gt;\nD\nFor each treatment arm, the proportion of simulations where the unadjusted p-value (from the Cox model) is less than the Bonferroni correct user specified one-sided alpha.\n\n\nBias &lt;dose&gt;\nD\nThe difference between the mean response and the true (simulated) response per dose\n\n\nCoverage &lt;dose&gt;\nD\nThe proportion of simulations where the unadjusted confidence interval for the response contains the true response rate used to simulate subject responses.\n\n\nMean KM med &lt;dose&gt;\nD\nThe mean Kaplan-Meier estimate of the median survival time per dose\n\n\nSE KM med &lt;dose&gt;\nD\nThe standard error of the Kaplan-Meier estimate of the median survival time per dose\n\n\nPredictor Cols\n\nThe appropriate output columns for the chosen type of predictor. See next tables for specifics of columns provided for each predictor type.\n\n\n\n\nContinuous Predictor:\n\nExtra columns on the summary_freq.csv file when using a continuous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox Mean HR &lt;dose&gt;\nD\nThe mean value of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Cox SE HR &lt;dose&gt;\nD\nThe standard error of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Avg. Min &lt;dose&gt;\nD\nThe average of the minimum value for the predictor per dose.\n\n\nPredictor Avg. 10-percentile &lt;dose&gt;\nD\nThe average of the 10th percentile values for the predictor per dose.\n\n\nPredictor Avg. 25-percentile &lt;dose&gt;\nD\nThe average of the 25th percentile values for the predictor per dose.\n\n\nPredictor Avg. Median &lt;dose&gt;\nD\nThe average of the median values for the predictor per dose.\n\n\nPredictor Avg. 75-percentile &lt;dose&gt;\nD\nThe average of the 75th percentile values for the predictor per dose.\n\n\nPredictor Avg. 90-percentile &lt;dose&gt;\nD\nThe average of the 90th percentile values for the predictor per dose.\n\n\nPredictor Avg. Max &lt;dose&gt;\nD\nThe average of the maximum value for the predictor per dose.\n\n\nPredictor Mean &lt;dose&gt;\nD\nThe mean of the mean value of the predictor per dose.\n\n\nPredictor SE &lt;dose&gt;\nD\nThe standard error of the mean value of the predictor per dose.\n\n\n\n\n\nDichotomous Predictor:\n\nExtra columns on the summary_freq.csv file when using a dichotomous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox Mean HR &lt;dose&gt;\nD\nThe mean value of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Cox SE HR &lt;dose&gt;\nD\nThe standard error of the cox model coefficient – the predictor as a covariate predicting final endpoint per dose.\n\n\nPredictor Mean Response Rate &lt;dose&gt;\nD\nThe mean of the predictor response rate per dose.\n\n\nPredictor SE Response Rate &lt;dose&gt;\nD\nThe standard error of the predictor response rate per dose.\n\n\n\n\n\nTime-to-Event Predictor\n\nExtra columns on the summary_freq.csv file when using a time-to-event predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Mean KM Median &lt;dose&gt;\nD\nThe mean of the Kaplan-Meier estimate of the median time to the predictor event per dose.\n\n\nPredictor SE KM Median &lt;dose&gt;\nD\nThe standard error of the Kaplan-Meier estimate of the median time to the predictor event per dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations.csv-and-weeksnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of simulations.csv and weeksNNNNN.csv",
    "text": "Contents of simulations.csv and weeksNNNNN.csv\nThe simulations.csv file holds the FACTS summary of the final analysis for each simulation (one per row).\nThe weeksNNNNN.csv file holds the FACTS summary of every analysis for the NNNNNth simulation. It contains a row for each interim in the trial and a row for the final analysis (Interim number 999).\nThe final analysis occurs after all the planned data is collected. If the trial does not stop early, the final analysis occurs after full follow-up for all subjects. If the trial does stop early at an interim analysis, then the timing of the final analysis depends on the status of the check boxes on the Interims tab. If follow-up is collected after the particular interim decision, then an analysis after full follow-up of the subjects recruited up to the point of the interim when the stopping decision was taken. If there is no follow-up intended after the interim analysis decision, then the final analysis occurs immediately (at the same time as the interim analysis), and the final analysis criteria are checked.\nThe first line is a header line, starting with a ‘#’, containing the FACTS GUI version number, the name of the FACTS file, the name of the scenario, and the time stamp of the start of the simulation.\nThe second line is a header line, starting with a ‘#’, containing the following column headings.\nMost of the columns are common to the two file types, but the weeks file does not contain columns for the ‘final’ values of the evaluation criteria.\n\nSimulations and weeks file column names for a time-to-event endpoint.\n\n\n\n\n\n\n\n\n\nColumn Title\nNu mber of col umns\nIn s imula tions file\nIn w ee ks fi le\nDescription\n\n\n\n\n# Weeks (Duration)\n1\n\n✔\nThe week of the analysis\n\n\n#Sim\n1\n✔\n\nThe number of the simulation.\n\n\nLastInt erimNumber\n1\n✔\n\nThe index of the last interim performed the index of the interim immediately before the final interim (index ‘999’). Note this is not necessarily the interim when the trial stopped if the design includes follow-up after stopping.\n\n\n#Subjects\n1\n✔\n✔\nThe number of subjects recruited in the simulation.\n\n\nOutcome\n1\n✔\n✔\nA flag categorizing final study outcome:\n\n= Early success\n= Late success\n= Late futility\n= Early futility\n= Success to futility flip-flop\n= Futility to success flip-flop\n= Inconclusive\n\n\n\nEarly Success\n1\n✔\n✔\nThis is the time to the early success decision (-9999 if there was no early success decision). The from the start of the trial to the interim where the early success conditions were first met. It does not include the subsequent follow up time.\n\n\nAlloc &lt;dose&gt;\nD\n✔\n✔\nThe number of subjects allocated to each arm.\n\n\nPr(Alloc) &lt;dose&gt;\nD\n✔\n✔\nThe probability of allocation to the different arms following the interim.\n\n\nMean resp &lt;dose&gt;\nD\n✔\n✔\nThe estimated hazard ratio of each treatment arm.\n\n\nSD resp &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of the hazard ratio of each treatment arm.\n\n\nTrue Mean resp &lt;dose&gt;\nD\n✔\n✔\nThe true mean response (hazard ratio) of each treatment arm for this simulation.\n\n\nMean Predictor Resp &lt;dose&gt;\nD\n✔\n✔\nThe estimated response of the predictor at each treatment arm.\nCts: mean change from baseline\nDich: response rate\nTTE: hazard ratio with control\n\n\nSD Predictor Resp &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of the predictor response.\n\n\nMean Predictor Sigma\n1\n✔\n✔\nThe mean of the estimate of the sigma of the predictor response (the sd of the response of a continuous predictor)\n\n\nSD Predictor Sigma\n1\n✔\n✔\nThe SD of the estimate of the sigma of the predictor response (the sd of the response of a continuous predictor)\n\n\nTrue Predictor Mean resp &lt;dose&gt;\nD\n✔\n✔\nThe true mean predictor response being simulated for each treatment arm (mean of a continuous predictor, response rate of a dichotomous predictor and the hazard ratio of a time-to-event predictor)\n\n\nTrue Predictor Sigma &lt;dose&gt;\nD\n✔\n✔\nThe true sigma of the predictor response of each treatment arm, if the predictor is a continuous endpoint\n\n\nMean Predictor Lambda &lt;dose&gt;\nD\n✔\n✔\nThe mean of the estimate of Lambda for each dose in the endpoint predictor model – the estimated mean time to the final event for each dose where the predictor is zero for a continuous or dichotomous predictor or the time after observing the event of a time-to-event predictor.\n\n\nSD Predictor Lambda &lt;dose&gt;\nD\n✔\n✔\nThe standard deviation of the estimate of Lambda for each dose in the endpoint predictor model (see above).\n\n\nMean Predictor Beta\n1\n✔\n✔\nThe mean of the estimate of the Beta coefficient in the endpoint predictor model.\n\n\nSD Predictor Beta\n1\n✔\n✔\nThe standard deviation of the estimate of the Beta coefficient in the endpoint predictor model.\n\n\nMean TTE Predictor Baseline Hazard Lambda &lt;seg&gt;\nS\n✔\n✔\nThe mean estimate of the hazard rate of the predictor event on the control arm, when using a time-to-event predictor, for each time segment of the predictor control hazard model.\n\n\nMean TTE Predictor Baseline Hazard Lambda &lt;seg&gt;\nS\n✔\n✔\nThe standard deviation of the estimate of the hazard rate of the predictor event on the control arm, when using a time-to-event predictor, for each time segment of the predictor control hazard model.\n\n\nNum Events &lt;dose&gt;\n&lt;segment&gt;\nS * D\n✔\n✔\nThe number of events observed on each arm in each control hazard model time segment.\n\n\nTotal Exposure &lt;dose&gt;\n&lt;segment&gt;\nS * D\n✔\n✔\nThe total exposure (in weeks) of the subjects on each arm in each control hazard model time segment.\n\n\nSeed1, Seed2\n2\n✔\n✔\nThe random number seeds at the start of the interim.\nDue to a change in the random number generator to one that uses seeds far larger than two 32-bit integers these values are not currently being written out.\n\n\nDR Param &lt;param&gt;\n4\n✔\n✔\nThe estimate of the mean value of each of the response model parameters. The response models require up to 4 parameters, some less. The model parameters are labeled α1, α2, .. the subscripts corresponding to the column their value appears in here.\n\n\nSd DR Param &lt;param&gt;\n4\n✔\n✔\nThe standard deviation of the estimate of the mean value of the corresponding response model parameter.\n\n\nMean Lambda &lt;seg&gt;\nS\n✔\n✔\nThe mean estimate of the hazard rate on the control arm, for each time segment of the analysis model.\n\n\nSd Lambda &lt;seg&gt;\nS\n✔\n✔\nThe standard deviation of the estimate of the hazard rate on the control arm, for each time segment of the analysis model.\n\n\nTrue Lambda\nS\n✔\n✔\n\n\n\nNum Events &lt;dose&gt;\nD\n✔\n✔\nThe number of events observed on each treatment arm.\n\n\nBAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Control (Hierarchical Prior) distribution of mean control responses.\n\n\nBAAC Mean\n1\n✔\n✔\nThis is the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Mean SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the mean of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau\n1\n✔\n✔\nThis is the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nBAAC Tau SD\n1\n✔\n✔\nThis is the SD of the posterior estimate of the SD of the Bayesian Augmented Active Comparator (Hierarchical Prior) distribution of mean active comparator responses.\n\n\nDuration\n1\n✔\n✔\nThe time in weeks to the analysis shown.\n\n\nArm Drop Time &lt;dose&gt;\nD\n✔\n✔\nFor each treatment arm, the time in weeks when it was decided to drop the arm.\n\n\nLPFV\n1\n✔\n✔\nThe time in weeks to the last patient first visit.\n\n\nQOI Columns\n\n\n\n\n\n\n\n\nQOI Columns\nThe QOI columns depend on the QOIs that have been defined for this design. The columns are grouped in the following order:\n\n\n\n\n\n\n\nQOI Type\nDescription\n\n\n\n\nPosterior Probabilities\nThe posterior probability for each QOI for each dose.\n\n\nPredictive Probabilities\nThe predictive probability for each QOI for each dose.\n\n\nP-Values\nThe p-value for each QOI for each dose.\n\n\nTarget Probabilities\nThe probability of being the target for each QOI for each dose.\n\n\nDecision QOIs\nThe value of the decision QOI at the target.\n\n\nSuccess Futile\nA flag for each QOI decision criteria indicating if the decision QOI was evaluated and compared to a threshold at the interim (weeks file) or final analysis (simulation file). The flag value is -1 if the decision QOI was not evaluated, 0 if it did not meet the threshold and 1 if it did.\n\n\nSuccess / Futile Combined\nFlags indicating if the interim or final analysis determined success or futility taking all the factors defined for Success/Futility into account: 0 if the conditions were not met, 1 it the conditions were met. If success and futility conditions have been defined that are not mutually exclusive and both sets of combined conditions are met, FACTS will pick one of the outcomes as met but not the other. In order to discourage defining Success and Futility rules that can both be true FACTS does not guarantee which outcome will be selected.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations_freq.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-simulations_freq.csv",
    "title": "Time to Event Output",
    "section": "Contents of Simulations_freq.csv",
    "text": "Contents of Simulations_freq.csv\n\nContents of the simulations_freq.csv file for a time-to-event simulation.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n# Trial\n1\nThe number of the simulation.\n\n\nLR Stat &lt;dose&gt;\nD\nUnadjusted log-rank test statistic per arm\n\n\nLR pval &lt;dose&gt;\nD\nThe unadjusted log-rank p-value per arm\n\n\nLR adj_pval &lt;dose&gt;\nD\nThe Bonferroni adjusted log-rank p-value per arm\n\n\nWilcoxon stat &lt;dose&gt;\nD\nThe Wilcoxon test statistic per arm\n\n\nWilcoxon pval &lt;dose&gt;\nD\nThe unadjusted Wilcoxon p-value per arm\n\n\nWilcoxon adj_pval &lt;dose&gt;\nD\nThe Bonferroni adjusted Wilcoxon p-value per arm\n\n\nHR &lt;dose&gt;\nD\nThe Cox model hazard ratio per arm\n\n\nHR pval &lt;dose&gt;\nD\nThe unadjusted Cox model p-value per arm\n\n\nHR lower CI &lt;dose&gt;\nD\nThe unadjusted lower bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nHR upper CI &lt;dose&gt;\nD\nThe unadjusted upper bound of the alpha confideneeeddce interval of the hazard ratio estimate.\n\n\nHR adj pval &lt;dose&gt;\nD\nThe Bonferroni adjusted Cox model p-value per arm\n\n\nHR adj lower CI &lt;dose&gt;\nD\nThe Bonferroni adjusted lower bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nHR adj upper CI &lt;dose&gt;\nD\nThe Bonferroni adjusted upper bound of the alpha confidence interval of the hazard ratio estimate.\n\n\nKM med &lt;dose&gt;\nD\nThe Kaplan Meier estimates of the median survival time per arm.\n\n\nPredictor Cols\nD\nThe appropriate output columns for the chosen type of predictor. See next tables for specifics of columns provided for each predictor type.\n\n\n\n\nContinuous predictor\n\nExtra columns on the simulations_freq.csv file when using a continuous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox HR &lt;dose&gt;\nD\nThe Cox model Hazard Ratio per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pred\n1\nThe Cox model predictor coefficient\n\n\nPredictor Cox HR pval &lt;dose&gt;\nD\nThe Cox model p-value per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pval pred\n1\nThe Cox model predictor p-value\n\n\nPredictor Cox HR lower CI &lt;dose&gt;\nD\nThe Cox model lower bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR lower CI pred\n1\nThe Cox model lower bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Cox HR upper CI &lt;dose&gt;\nD\nThe Cox model upper bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR upper CI pred\n1\nThe Cox model upper bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Min &lt;dose&gt;\nD\nThe minimum value of the predictor value for each treatment arm.\n\n\nPredictor 10-percentile &lt;dose&gt;\nD\nThe 10th percentile value of the predictor value for each treatment arm.\n\n\nPredictor 25-percentile &lt;dose&gt;\nD\nThe 25th percentile value of the predictor value for each treatment arm.\n\n\nPredictor Median &lt;dose&gt;\nD\nThe mediam value of the predictor value for each treatment arm.\n\n\nPredictor 75-percentile &lt;dose&gt;\nD\nThe 75th percentile value of the predictor value for each treatment arm.\n\n\nPredictor 90-percentile &lt;dose&gt;\nD\nThe 90th percentile value of the predictor value for each treatment arm.\n\n\nPredictor Max &lt;dose&gt;\nD\nThe maximum value of the predictor value for each treatment arm.\n\n\nPredictor Mean &lt;dose&gt;\nD\nThe mean of the predictor value for each treatment arm\n\n\nPredictor SD &lt;dose&gt;\nD\nThe SD of the mean of the predictor value for each arm\n\n\n\n\n\nDichotomous predictor\n\nExtra columns on the simulations_freq.csv file when using a dichotomous predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor Cox HR &lt;dose&gt;\nD\nThe Cox model Hazard Ratio per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pred\n1\nThe Cox model predictor coefficient\n\n\nPredictor Cox HR pval &lt;dose&gt;\nD\nThe Cox model p-value per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR pval pred\n1\nThe Cox model predictor p-value\n\n\nPredictor Cox HR lower CI &lt;dose&gt;\nD\nThe Cox model lower bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR lower CI pred\n1\nThe Cox model lower bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Cox HR upper CI &lt;dose&gt;\nD\nThe Cox model upper bound of the alpha confidence interval of the hazard ratio estimate per arm (where the model includes the predictor as a covariate predicting final endpoint).\n\n\nPredictor Cox HR upper CI pred\n1\nThe Cox model upper bound of the alpha confidence interval of the predictor coefficient.\n\n\nPredictor Response Rate &lt;dose&gt;\nD\nThe predictor response rate per arm.\n\n\n\n\n\nTime-to-event Predictor\n\nExtra columns on the simulations_freq.csv file when using a time-to-event predictor.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nPredictor KM Median &lt;dose&gt;\nD\nThe Kaplan Meier estimate of the median time to the predictor event per arm",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-patientsnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of PatientsNNNNN.csv",
    "text": "Contents of PatientsNNNNN.csv\n\nThe patients file output when simulating a time-to-event tria..\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\n#Subject\n1\nThe subject id number, starting at 1.\n\n\nRegion\n1\nWhich region the subject was recruited in\n\n\nDate\n1\nThe date, in weeks from the start of the trial, of the subject’s baseline visit and randomization.\n\n\nDose\n1\nThe index number (1, …) of the treatment arm the subject belongs to.\n\n\nDuration\n1\nThe time of observation of the subject (in weeks)\n\n\nOutcome\n1\nWhether an event was observed (1) or not (0)\n\n\nPredictor\n1\nThe value of the observed predictor\n\n\nPred Outcome\n1\nA flag indicating whether the predictor was observed or not\n\n\nDropout\n1\nA flag indicating whether the subject dropped out (1) or not (0).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "href": "documentation/v71/userguides/core/simulation/tte.html#contents-of-mcmcnnnnn.csv",
    "title": "Time to Event Output",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (including the burnin) and the samples from all the analyses in the simulation are included. The first two columns are the analysis index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\nMCMC file format for a time-to-event simulation.\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nAnalysis\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nHR &lt;dose&gt;\nD\nThe estimate of the hazard ratio for each dose, based on the dose response model fitted.\n\n\nLambda &lt;seg&gt;\nS\nThe estimate of the control hazard rate in each segment of the control hazard rate model\n\n\nA &lt;1-8&gt; / Tau\nP\nIf the dose response model estimates parameters then the samples of these are listed next. The number and name of the parameters will vary depending on the model being fitted. Check the Design &gt; Dose Response tab for a listing of the parameters.\n\n\nPredictor …\n\nThe parameters of the predictor endpoint, these vary by the type of endpoint that the predictor has – they will be consistent with the parameters that would be output if the predictor was the only endpoint in a single endpoint design.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation",
      "Time-to-Event"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html",
    "href": "documentation/v71/userguides/core/simulation/index.html",
    "title": "Simulation",
    "section": "",
    "text": "The Simulation tab allows the user to execute simulations for each of the scenarios specified for the study. The user may choose the number of simulations, whether to execute locally or on the Grid, and modify the random number seeds, Figure 11‑1.\nIn the Simulation tab the user can provide simulation configuration parameters like the number of simulations to run, whether the simulations can be run on the Grid, the parallelization strategy, the random number seed used in the simulations, and the number of certain output files that should be kept during the simulation execution.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#number-of-simulations",
    "href": "documentation/v71/userguides/core/simulation/index.html#number-of-simulations",
    "title": "Simulation",
    "section": "Number of simulations",
    "text": "Number of simulations\nThis box allows the user to enter the number of simulations that they would like FACTS to run for each scenario listed in the table at the bottom of the simulation tab. There is no set number of simulations that is always appropriate.\n\n10 simulations\n\nYou might want to run 10 simulations if you just want to look at a few simulated trials and assess how the decision rules work and if FACTS is simulating what you expected based on what you input on the previous tabs. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\n\n100 simulations\n\nYou might want to run 100 simulations if you want to look at many individual trials to make sure that what you want to happen is nearly always happening. You can also start to get a very loose idea about operating characteristics like power based on 100 sims. 100 simulations is also usually sufficient to spot big problems with the data analysis such as poor model fits or significant bias in the posterior estimates.\n\n1,000 simulations\n\nYou might want to run 1,000 simulations if you want estimates of operating characteristics like power, sample size, and Type I error for internal use or while iterating the design. This generally isn’t considered enough simulations for something like a regulatory submission. With 1,000 simulations the standard error for a typical type I error calculation is on the order of \\(0.005\\).\n\n10,000 simulations\n\nYou might want to run 10,000 simulations per scenario if you are finalizing a design and are preparing a report. This is generally enough simulations for a regulatory submission, especially in non-null simulation scenarios. The standard error for a typical type I error calculation using 10,000 simulations is on the order of \\(0.0015\\).\n\n&gt; 10,000\n\nYou might want to run more than 10,000 simulations if you want to be very certain of an operating characteristic’s value - like Type I error. And plan to use the measurement of the quantity for something important like a regulatory submission. The standard error of a Type I error calculation with 100,000 simulations, e.g., is on the order of \\(0.0005\\).\n\n&gt; 100,000\n\nYou probably don’t want to run more than 100,000 simulations per scenario. Maybe your finger slipped an hit an extra 0, or you thought there were 5 zeroes in that number when there were actually 6. If the simulated trial is adaptive, this is going to take a while.\n\n\nEach time the FACTS application opens, the “Number of Simulations” will be set to the number of simulations last run for this design. Not all scenarios must be run with the same number of simulations. If completed results are available, the actual number of simulations run for each scenario is reported in the ‘Num Sims’ column of the results table. The value displayed in the “Number of Simulations” control is the number of simulations that will be run if the user clicks on the ‘Simulate’ button.\nNote also that if a scenario uses an external VSR file or directory of external files, the number of simulations will be rounded down to the nearest complete multiple of the number of VSR lines or external files. If the number of simulations requested is less than the number of VSR lines or external files, then just the requested number of simulations are run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#start-at-simulation",
    "href": "documentation/v71/userguides/core/simulation/index.html#start-at-simulation",
    "title": "Simulation",
    "section": "Start at Simulation",
    "text": "Start at Simulation\nThe “Start at simulation” option allows for the simulation of a particular trial seen in a previous set of simulations without having to simulate all of the previous trials in that previous set to get to it.\nThe initial random seed for FACTS simulations is set in the simulation tab. The first thing that FACTS does is to draw the random number seeds to use at the start of each simulation. Thus, it is possible to re-run a specific simulation out of a large set without re-running all of them. For example, say the 999th simulation out of a set displayed some unusual behavior, in order to understand why, one might want to see the individual interim analyses for that simulation (the “weeks” file), the sampled subject results for that simulation (the “Subjects” files) and possibly even the MCMC samples from the analyses in that simulation. You can save the .facts file with a slightly different name (to preserve the existing simulation results), then run 1 simulation of the specific scenario, specifying that the simulations start at simulation 999 and that at least 1 weeks file, 1 subjects file and the MCMC samples file (see the “MCMC settings” dialog) are output.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#parallelization-packet-size",
    "href": "documentation/v71/userguides/core/simulation/index.html#parallelization-packet-size",
    "title": "Simulation",
    "section": "Parallelization Packet Size",
    "text": "Parallelization Packet Size\nThe parallelization packet size option allows simulation jobs to be split into runs of no-more than the specified number of trials that are run in parallel. If more simulations of a scenario are requested than can be done in one packet, the simulations are broken into the requisite number of packets, run, and combined and summarized when they are all complete. The final results files will look just as though all the simulations were run as one job or packet.\nThe packet size must be a perfect divisor of the number of simulations. This is usually easy since common numbers of simulations are multiples of 100, but don’t try to use a prime number for Number of Simulations or you’re stuck with only 2 packet size options.\nBy default (if the check box with Choose Parallel Packet Size is not checked) the number of simulations per packet depends on the number of simulations per scenario. If the number of simulations is less than 1000, then each scenario is packaged as a single packed and simulated. If the number of simulations per scenario is greater than or equal to 1000, the default packet size is 10 and all simulations are decomposed into packets of size 10.\n\n\n\n\n\n\nPacketization with VSRs using .mvsr files\n\n\n\nIf an external file is used to create explicit VSRs (a .mvsr file), then the packet size should be a multiple of the number of rows in that MVSR file. Each packet will get passed the entire .mvsr file to run. If there are multiple .mvsr files with differing numbers of lines then only the VSR scenarios that have a .mvsr file that has a number of rows that is a divisor of the packet size will be run. The rest will error. The packet size can then be modified to get each of the .mvsr specified VSR files to be run.\nCare should be taken when packetizing a scenario that includes an external data file to supply the virtual subject responses; in this situation, a of copy of the external file is included in each packet which can cause the packetisation process to run out of memory as the packets are being created. In this case, use a smaller number of larger packets, such as packets that are 1/10th of the total number of simulations.\n\n\nWhen running simulations, FACTS will create and run as many packets in parallel as there are execution threads on the local machine. In general, the overhead of packetization is quite low, so a packet size of 10 to 100 can help speed up the overall simulation process. Threads used to simulate scenarios that finish quickly can pick up packets for scenarios that take longer. The progress bar updates as simulation packets complete, so the smaller the packet size, the more accurately FACTS can report the overall progress of the simulation execution.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#random-seed",
    "href": "documentation/v71/userguides/core/simulation/index.html#random-seed",
    "title": "Simulation",
    "section": "Random Seed",
    "text": "Random Seed\nRandom number generation plays a huge role in FACTS’s virtual patient generation and statistical analyses. In order to exactly reproduce a statistical set of results, it is necessary to start the random number generation process from an identical “Random Seed”. Using the same random seed in the same version of FACTS guarantees that simulated trials will always be reproducible. Changing the design parameters or the version of FACTS may or may not remove this reproducibility depending on the change.\nEven a small change in the random seed will produce very different simulation results.\nIn addition to setting the seed, the user can choose whether they want the “Same seed for all scenarios” or “Different seed” for different scenarios. If “Same seed for all scenarios” is selected, the subjects generated for each simulated trial will match for the different scenarios. This induces a correlation among the simulation output for different scenarios. This can be good if you’re trying to compare operating characteristics for different scenarios. If “Different seed” is selected, then each scenario has its own seed that samples a different set of subjects than any other scenario. This uncorrelates the simulation output across scenarios, which can be advantageous if the absolute value of the operating characteristics are more valuable to you than the comparison of operating characteristics across scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#mcmc-settings",
    "href": "documentation/v71/userguides/core/simulation/index.html#mcmc-settings",
    "title": "Simulation",
    "section": "MCMC Settings",
    "text": "MCMC Settings\nTo set advanced settings for simulation, the user may click the “MCMC Settings” button, which will display a number of additional specifiable parameters for simulation in a separate window.\n\n\n\n\n\n\nFigure 2: Advanced MCMC settings.\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the MCMC chain to reach its stationary distribution. Burn-in samples are output in MCMC files if the files are output.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest.\n\nThe third parameter controls the number of MCMC samples taken after each imputation of missing data using the longitudinal model. The default value is 1. This parameter only has an effect if Bayesian imputation is being used to impute missing or partially observed data. Increasing the value of this parameter allows the parameter estimates to converge somewhat to a potentially new stationary distribution for each new set of imputed data. If the imputed data is only a small percentage of the overall data this is likely unnecessary. As a rough guide, if it at some early interims &gt; 5% of the data being analyzed will be imputed, a value in the range 2 to 10 is recommended to avoid underestimating the uncertainty. A higher number should be used the greater the proportion of imputed data.\nThe last two parameters concern the output of the MCMC samples to a file. It is possible to have the design engine output the sampled values from the MCMC in all of the interims of the first N simulated trials of each scenario by specifying the “Number of MCMC files to output” to be greater than 0. The resulting files, ‘mcmcNNNN.csv’, will be in the results directory with all the other results files for that scenario. These files include the burn-in samples from the MCMC chains.\nThe final parameter in MCMC Settings is the thinning parameter. This parameter will only keep every \\(N^{th}\\) sample taken during MCMC where \\(N\\) is the thinning parameter. Thinning MCMC samples can reduce the autocorrelation of consecutive MCMC iterations, which increases the effective samples per retained sample, but also results in needing many more MCMC iterations to reach the same number of retained samples. Generally, we do not recommend thinning for standard simulation runs.\n\n\n\n\n\n\nWarning about thinning\n\n\n\nUnlike other software that performs MCMC, when you choose to thin by a value, FACTS does not increase the number of MCMC iterations it performs in order to retain the value specified in “Number of Samples”. So if you leave “Number of Samples” at its default value, \\(2500\\), and thin by \\(10\\), you will be left with \\(250\\) retained samples. You should adjust for this by increasing the “Number of Samples” if you choose to thin.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#results-output",
    "href": "documentation/v71/userguides/core/simulation/index.html#results-output",
    "title": "Simulation",
    "section": "Results Output",
    "text": "Results Output\nThe results output section of the Simulation tab allows for the specification of how many output files should be generated for files that are individually created for each simulation. Summary files (summary.csv) that have 1 line per scenario are always created. Simulations files (simulations.csv) that have 1 line per simulation are always created. Weeks files (weeksXXXXX.csv), patients files (patientsXXXXX.csv), and frequentist weeks files (weeks_freq_{missingness}_XXXXX.csv) are not created for every single simulation. Instead, the number of simulation specific output files can be set per type. This limits the amount of output files that FACTS will save.\nSee the endpoint specific descriptions of the output files for descriptions of what the previously mentioned output files report. See here for core continuous or dichotomous output files, and see here for core time-to-event output files.\nSome plots in FACTS that are created based on weeks files, and if very few weeks files are saved, the plots will not be as accurate or descriptive.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/core/simulation/index.html#facts-grid-simulation-settings",
    "title": "Simulation",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nA user with access to a computational grid may choose to run simulations on the grid instead of running them locally. This frees the user’s computer from the computationally intensive task of simulating so that they can continue other work or even shutdown their PC or laptop. In order to run simulations on the grid, it must first be configured. This is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#right-click-menu",
    "href": "documentation/v71/userguides/core/simulation/index.html#right-click-menu",
    "title": "Simulation",
    "section": "Right Click Menu",
    "text": "Right Click Menu\nClicking the Right-hand mouse button on a row in the simulations tab brings up a short cut menu:\n\n\n\n\n\n\nFigure 4: The menu that appears when you right click on the table within the simulation tab.\n\n\n\nThese will respectively:\n\nOpen a new Windows directory browser window showing the contents of the simulation results for that scenario.\nOpen a window that displays the individual simulation results for that scenario. The results initially displayed are the ‘highlights’ columns, similarly to the summary results (see below) the results columns are collected into sub-groups, windows of these subgroups can be opened from the Right Click menu of the Simulation Results highlights window.\nOpen a window that displays the frequentist analysis summary results. This option is only available if one or more frequentist analyses have been selected on the Design &gt; Frequentist Analysis tab. (If more than one analysis has been requested – using different treatments of missing data there will be separate options in the menu to display each summary).\nOpen R loading in the result files for that scenario as separate dataframes.\nOpens the FACTS graph control displaying the graphs for that scenario.\nOpens the FACTS graph control that displays the trellis plot of graphs of selected scenarios for selected design variants.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#open-in-r",
    "href": "documentation/v71/userguides/core/simulation/index.html#open-in-r",
    "title": "Simulation",
    "section": "Open in R",
    "text": "Open in R\nIf aggregated results files have been created then the Open in R button will start R and load the aggregated ‘.csv’ files.\nIf there are no aggregated files then the results files of the currently selected scenario are loaded. R can also be opened in this fashion by right clicking on a row in the simulation results table.\nWhen FACTS starts R it writes out an R auto run startup script that loads the csv files into R as separate dataframes.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#aggregation",
    "href": "documentation/v71/userguides/core/simulation/index.html#aggregation",
    "title": "Simulation",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 5: Window that appears when aggregating simulation results.\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nValues in columns that are independent of dose will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. WeeksNNNNN.csv files are aggregated into a single agg_[pivot_]weeks.csv file. PatientsNNNNN.csv files are aggregated into a single agg_patients.csv file, but they are never pivoted because each row already refers to a single dose. Similarly the various frequentist results at the summary, simulation and weeks level are aggregated (if they’ve been output).\nRegionIndex.csv is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nRecruitment Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g. External Subjects Profile if there are no external scenarios)\n\n\nDropouts Profile\n\n\n\nLongitudinal Rates Profile\n\n\n\nDose Response Profile\n\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nP(TS)\nProportion of trial success (early success + late success)\n\n\nP(TF)\nProportion of trial futility (early futility + late futility)\n\n\nSim\nSimulation number. Only present in weeks and patients files.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/simulation/index.html#design-report",
    "href": "documentation/v71/userguides/core/simulation/index.html#design-report",
    "title": "Simulation",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/analysis.html",
    "href": "documentation/v71/userguides/core/analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "The analysis tab allows the user to supply a specific data set for analysis by the design specified in the Design tab of the “.facts” file.\nClicking on the “Use Design to Analyze Data” button, will create an empty “subject.csv” file in the main simulation results directory and an ‘Analysis’ sub-directory there for running the analysis and saving the outputs.\nAlternatively, clicking on the “Import Data to Analyze” launches a file browser, allowing the user to select a ‘.csv’ file to load as the data to analyze. This is a shortcut for first clicking on the “Use Design to Analyze Data” button, and then clicking on the “Select File to Create New Analysis” button on the subject data tab.\nAfter enabling data analysis, the analysis screen is shown with no data loaded. By clicking on the “Subject Data” tab the user is now able to enter data values directly, or to load a ‘.csv’ file already containing data:",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/analysis.html#the-subject.csv-file-format",
    "href": "documentation/v71/userguides/core/analysis.html#the-subject.csv-file-format",
    "title": "Analysis",
    "section": "The subject.csv file format",
    "text": "The subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nThe format of the file is the same as the ‘patientsNNNN.csv’ output file (continuous/dichotomous or time-to-event), and the column values described above.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/core/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "href": "documentation/v71/userguides/core/analysis.html#converting-arrival-date-value-from-days-to-weeks",
    "title": "Analysis",
    "section": "Converting arrival date value from days to weeks",
    "text": "Converting arrival date value from days to weeks\nFrom FACTS 7.0 the value in the Date field is interpreted as being in weeks (rather than days as in previous versions). If you have existing data, a simple conversion tool is provided “Convert Date from Days to Weeks” that simply divides all of thete values by 7. Having run the conversion, you then need to save the modified data. The values of the Date field will only make a difference to TTE analyses.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: The data as provided to the analysis tab with dates in days.\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: The same dates, but converted to weeks.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Core Designs",
      "Analysis"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html",
    "href": "documentation/v71/userguides/crm.html",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system.\n\n\n\nThis document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nThis document has been updated for the version 7.1 release of Dose Escalation FACTS.\n\n\n\nPlease cite FACTS wherever applicable using this citation.\n\n\n\nAn overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-purpose",
    "href": "documentation/v71/userguides/crm.html#sec-purpose",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "",
    "text": "This document describes how use the FACTS Dose Escalation (DE) CRM design engine. It is intended for all end users of the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-scope",
    "href": "documentation/v71/userguides/crm.html#sec-scope",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "",
    "text": "This document covers the FACTS Dose Escalation N-CRM design engine user interface.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS 6.3 and later installed on Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-context",
    "href": "documentation/v71/userguides/crm.html#sec-context",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "",
    "text": "This document has been updated for the version 7.1 release of Dose Escalation FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-citing",
    "href": "documentation/v71/userguides/crm.html#sec-citing",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-definitions",
    "href": "documentation/v71/userguides/crm.html#sec-definitions",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "",
    "text": "An overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-7.1-changes-to-n-crm",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS 7.1 Changes to N-CRM",
    "text": "FACTS 7.1 Changes to N-CRM\nIn FACTS 7.1 there were new features added to N-CRM:\n\nIt is now possible to backfill to the current escalation dose (also known as “frontfilling”).\nIt is now possible to specify a third queue concept – maximum number of patients in their DLT period on the current MTD estimate.\nIt is now possible to define the concept of “near” target/MTD as part of stopping rules, for both fine-grained and regular dosing.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-7.0-changes-to-n-crm",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS 7.0 Changes to N-CRM",
    "text": "FACTS 7.0 Changes to N-CRM\nIn FACTS 7.0 the only change was on the Analysis tab, when entering subject data manually, the FACTS GUI now ensures that a cohort number is entered for every subject.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.5-changes-to-n-crm",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS 6.5 Changes to N-CRM",
    "text": "FACTS 6.5 Changes to N-CRM\nIn FACTS 6.5 there was a new feature added to N-CRM:\n\nIt is now possible to generate a design report – a Word document describing design - once the design has been simulated. In FACTS 6.5 there was two small changes to the functionality:\nWhen deriving toxicity/efficacy priors from quantiles at the lowest and highest doses, the “middle” probability option now refers to the median dose rather than the reference dose and is now optional. When pre-6.5 FACTS files are loaded in FACTS 6.5, the “middle” probability option will be ignored and replaced by 0.5.\nWhen deriving toxicity/efficacy priors from specific quantiles the specification of at least two dose levels is now required whereas previously the specification of at least three dose levels was required.\n\nIn FACTS 6.5 there were some improvements in the simulated behavior:\n\nDesigns which include efficacy, the “Maximum cohorts used to determine MTD” parameter on the Allocation Rule tab is now observed, in FACTS 6.4 and earlier it was ignored.\nIn N-CRM designs using open enrollment without accrual pausing, FACTS will now output entries in the cohort file for subjects that have been lost following stopping rules being met.\nIn N-CRM designs using open enrollment, FACTS will no longer prematurely apply stopping conditions when there are still subjects whose outcomes have been observed, but not yet processed.\nIn N-CRM designs using open enrollment, FACTS will now report the selected MTD/MED/OSD and associated state to be the dose where the stopping conditions were first met, unless the MTD has subsequently decreased post stopping rules being met. This is to prevent a dose above the one selected when the stopping conditions were met being reported as the MTD when it is very likely that there is insufficient data on this higher dose to justify its selection. If rather than reporting the MTD at the point when the stopping rules where met, you would like the trial to resume if the dose selected as MTD has changed (and this the stopping rules possibly no longer met), ensure that the ”Pause accrual and wait for completers” option is selected on the “Stopping Criteria” tab. This allows the trial to resume if the recruitment cap has not been met.\nFACTS Command Line mode and FLFLL (FACTS Linux File Loader Lite) now correctly handle the processing of N-CRM designs whose prior was derived using the “Legacy Prior” option.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.4-changes-to-n-crm",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS 6.4 Changes to N-CRM",
    "text": "FACTS 6.4 Changes to N-CRM\nIn FACTS 6.4 there were no changes to the Dose Escalation simulators.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.3-changes-to-n-crm",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS 6.3 Changes to N-CRM",
    "text": "FACTS 6.3 Changes to N-CRM\nIn FACTS 6.3 a number of changes were made to improve facilities in N-CRM, or improve the way existing facilities were implemented. These were:\n\nNew run-in options: the existing run-in scheme is available as “simple run-in”, “custom run-in” allows a specific sequence of doses and number of subjects to test at each dose to be specified, “small cohort pre-escalation” allows a run that uses a smaller cohort size but follows the dose escalation rules and over dose control.\nNew “backfill” options in open enrolment. Backfill allows subjects that become available at a time when they can’t be allocated to the current dose (because the maximum number of subjects without final results have already been allocated to the current dose).\nImproved handling of “maximum subjects without final results” in open enrolment. In earlier versions of FACTS this was a “global” maximum, which led to a suboptimal allocation pattern and overly cautious rejection of subjects that became available. The new model applies a maximum “per dose” so that once the trial has escalated to a new dose strength, any subjects without final results on lower doses do not block allocation to the new dose, in addition it is possible to specify two different maximums – one for when a dose has just been escalated to but has not been “cleared” (typically smaller and more cautious), and one when a dose has been cleared but we continue to allocate to it because it is the target dose (typically larger and more confident). This method is such an improvement that we recommend moving any design using open enrolment to this new version of FACTS.\nImproved Ordinal Toxicity model – the way the likelihood is calculated has been improved – reducing the uncertainty in the model fit. Any design using an ordinal model will need to re-calibrate the prior if you move the design to FACTS 6.3. If you have a design already complete, or in execution we recommend you remain using the earlier version of FACTS for that trial.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.2-changes-to-n-crm",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS 6.2 Changes to N-CRM",
    "text": "FACTS 6.2 Changes to N-CRM\nIn FACTS 6.2 features available separately in the other FACTS CRM engines (CRM (Toxicity), bCRM & CRM Ordinal) were all incorporated into N-CRM. This allowed these features to be used in conjunction with N-CRM’s target toxicity band methodology, overdose control and open enrollment features, as well as in conjunction with each other for the first time.\nThe new features are:\n\nFrom CRM (Toxicity) the option to specify that the data is coming from ‘two groups’ and for the toxicity experienced in the two groups to be modelled with a joint model [CRM 2 Sample]. This allows a trial where there are two patient populations (such as adults and children) or where there are two versions of the treatment to be simulated.\nFrom bCRM the option to model a second binary efficacy endpoint [bCRM] and the for dose allocation to proceed in two stages – the first to establish an MTD and the second to establish an MED.\nFrom CRM Ordinal the option for the toxicity endpoint to be modelled not as binary endpoint, but one with different categories of toxicity, and with a joint model applied to the different categories [CRM Ordinal]. The endpoint can be to model either 3 or 4 categories of toxicity:\n\ncategory 1 is “no toxicity”,\ncategory 2 is “mild toxicity”,\ncategory 3 is “toxicity”\ncategory 4 (if included) is “severe toxicity”\n\n\nAll decision making is made in terms of the probability of observing a category 3 (or worse) toxicity.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "href": "documentation/v71/userguides/crm.html#facts-6.1-changes-to-n-crm",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS 6.1 Changes to N-CRM",
    "text": "FACTS 6.1 Changes to N-CRM\nIn FACTS 6.1 N-CRM has the new feature of ‘design variants’ that allow the user to easily simulate and evaluate an N-CRM design at different sample sizes. This change includes 4 elements:\n\nUnder the ‘Study’ tab the user can now specify the number of design variants, and for each variant the maximum study size in Cohorts.\nOn the simulation tab FACTS will display a copy of each simulation scenario for each variant.\nThe simulation results now include the Ppn of trials that stopped for each stopping reason: stopping because all doses are too toxic (the toxicity estimates exceed the overdose criteria), because a stopping rule was met or because the study cap was reached.\nThere are now a set of cross variant graphs that show trellis plots of the key summary graphs by design variant and scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#overdose-control",
    "href": "documentation/v71/userguides/crm.html#overdose-control",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Overdose Control",
    "text": "Overdose Control\nOverdose control can be specified on the Study &gt; Toxicity tab. Overdose control specifies a limit on the probability that a dose has a toxicity rate above a certain level. After fitting the Bayesian logistic regression model, all doses for which the posterior probability that their toxicity rate lies above the specified level exceeds the specified limit are ineligible for allocation. Because the Bayesian Logistic regression is monotonic, this means that after every analysis either all doses are permitted for allocation or there will be a dose level above which no dose is permitted for allocation.\n\n\n\n\n\n\nFigure 1: Setting the overdose control limit\n\n\n\nThe overdose control is specified in terms of the “toxicity bands” (concept of allowing ranges for the target toxicity, excess toxicity, unacceptable toxicity and under-dosing explained in more detail in this section) and can either be in terms of the “excess and unacceptable toxicity bands” or just the “unacceptable toxicity band”. The “excess and unacceptable toxicity band” is every toxicity rate above the upper bound of the target band. Care should be taken when setting the permitted threshold for this joint band. If set below 0.5, it will likely exclude doses whose mean expected toxicity rate is within the target band with the risk that this makes the escalation decision in the design too cautious. Initially it might be recommended to just use the “unacceptable band” for specifying the overdose control. This allows an overdose control that is more strict – for example: “exclude any dose where the probability that the toxicity rate is above 0.6, is greater than 20%“. The lower bound for the unacceptable band can be set wherever desired, its only role is in defining this band for overdose control. It is also possible to specify that the limit changes over the course of the trial, allowing the overdose control to become stricter as more information becomes available. For example, one could reduce the permitted probability of a dose having a toxicity rate in the unacceptable band from 50% to 25% in steps of 2.5% after every cohort.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#dose-escalation-rules",
    "href": "documentation/v71/userguides/crm.html#dose-escalation-rules",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Dose Escalation Rules",
    "text": "Dose Escalation Rules\nThe dose escalation could be solely controlled by the overdose control (as originally proposed in (Neuenschwander, Branson, and Gsponer 2008)), however this means that the escalation behavior is very dependent on the interplay between the prior and the observed data. Usually, teams prefer to have a fixed set of rules in place ensuring the escalation behavior is sufficiently cautious. FACTS has an option to just use overdose control or to use a combination of overdose control and a set of fixed escalation rules. In the latter case, the following rules can be set in the Design &gt; Allocation Rule &gt; Allocation tab:\n\n\n\n\n\n\nFigure 2: Dose escalation rules\n\n\n\nWe introduce the notion of whether a dose has been “cleared”. A dose is cleared once we have sufficient data on it (usually, but not necessarily, the results of one cohort, but if the cohort size is small, for example 2 subjects, perhaps more than one cohort will be required). This can be supplemented by a rule that if the observed raw toxicity rate at the dose exceeds a certain limit, then the dose is not counted as cleared (this rule is usually unnecessary if overdose control limits have been set). Once a dose has been cleared, it stays cleared, meaning there is “maximum cleared dose”. The number of dose increments or the factor of dose strength above the current cleared dose that can be allocated to is then specified. For example, with doses of 12.5, 25, 50, 100, 150, 200, 250, we might allow escalation at two dose increments a time. In the figure below, you see the combination of settings used to achieve this behaviour alongside the “Fastest Possible Dose Escalation” plot on the right:\n\n\n\n\n\n\nFigure 3: Escalation by number of dose increments\n\n\n\nAlternatively, we can specify the permitted escalation as a ratio, for example we might allow the dose strength to be at most tripled at each escalation, which, with the example dose strengths, makes the initial escalation more cautious:\n\n\n\n\n\n\nFigure 4: Escalation by dose strength factor\n\n\n\nThe escalation rules can be adjusted so that instead of a single increment rule, there are different increments depending on the dose, or depending on the number of observed toxicities. To modify our earlier example, we can allow escalation by 2 dose levels while no toxicities have been observed, but limit it to only one dose level once one or more toxicities have been observed:\n\n\n\n\n\n\nFigure 5: Escalation increment varying by number of toxicities\n\n\n\nLastly escalation can be relative to the highest cleared dose, or relative to the last dose allocated.\nTo summarize the allocation procedure:\n\nThe current maximum cleared dose is identified.\nThe current data is analyzed using the Bayesian Logistic Regression model.\nThe overdose rules are evaluated and all doses exceeding the overdose control limit are excluded from this escalation selection.\nFrom the remaining doses, the dose best meeting the target MTD or target toxicity interval objective based on the model is selected as the “target dose” (TD).\nIf the TD is at or below the current maximum cleared dose, the next cohort is allocated to the TD.\nIf the TD is within the escalation rules of the current maximum cleared dose, the next cohort is allocated to the TD.\nOtherwise, the next cohort is allocated to the highest dose above the current maximum cleared dose as allowed by the escalation rules.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#initial-run-in",
    "href": "documentation/v71/userguides/crm.html#initial-run-in",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Initial Run-in",
    "text": "Initial Run-in\nThe purpose of defining a run-in is to define a fixed allocation behavior to be followed up to the first toxicity being observed. The specified number of subjects to allocate to each dose in the run-in and which doses to test are specified. This scheme is followed until a toxicity is observed or we reach the end of this fixed scheme.\nThree forms of run-in specification are available:\n\nSimple: allocates a small cohort to every defined dose in ascending order (unless fine grain doses - see this section – have been specified, in which case the escalation rules are followed).\nCustom: allocates a defined number of subjects (possibly varying by dose) to selected doses in ascending order.\nSmall cohort pre-escalation: allocates a small cohort, but follows the escalation rules assuming just a single small cohort is required to clear a dose.\n\nAll run-in schemes can be modified in a number of ways:\n\nSpecifying a maximum dose at which the run-in stops if no toxicities are observed until that dose.\nIf ordinal toxicities are being simulated, the run-in may should at the first observed category 2 toxicity (rather than a category 3 toxicity)\nWhether the subjects used in the run-in should be counted towards the trial sample size or not.\nWhen a toxicity is observed the standard behavior is to allocate to the minimum of: the last dose tested in the run-in, the current TD or the highest dose that can be allocated to by the overdose rules. This can be replaced by expanding the allocation on the current dose to make it a full cohort as specified in Study &gt; Study Info tab (this option is particularly useful in conjunction with stopping for a category 2 toxicity).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#two-groups",
    "href": "documentation/v71/userguides/crm.html#two-groups",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Two Groups",
    "text": "Two Groups\nFACTS has the option to model the subjects in the trial as belonging to two different groups, these can be either:\n\nTwo groups distinguished by a baseline property of the subjects, for example adults and paediatrics.\nTwo groups separated by a difference in treatment (and selected randomly), for example the study drug alone or in combination with an additional drug.\n\nThere are options for when group 2 starts enrolling:\n\nThey can be recruited sequentially – group 1 then group 2.\nThey can be recruited in parallel\nThe second group can be started when the allocation to the first group reaches a particular dose\nThe second group can be started when the number of subjects allocated to group 1 reaches a particular threshold.\n\nA joint model is fitted to the two groups.\nThe first group is modeled:\n\\[\nlogit(p_{1j}) = \\alpha + \\beta \\hat{x}_j\n\\]\nThe second group is modeled:\n\\[\nlogit(p_{2j}) = (\\alpha + a) + (\\beta + b) \\hat{x}_j\n\\] With separate priors and some optional constraints on \\(a\\) and \\(b\\). Dose escalation and stopping are judged independently for the two groups.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-efficacy",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-efficacy",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Efficacy",
    "text": "Efficacy\nFACTS has the option to additionally model an efficacy endpoint. There are currently two limitations in simulation:\n\nOnly a binary efficacy endpoint can be simulated\nThe efficacy endpoint is assumed to be available at the same time as the toxicity endpoint.\n\nThe efficacy and toxicity endpoints are modelled separately. There are options to specify early stopping rules for finding the MTD, and to specify a cap on the sample size that can be spent finding the MTD. Once these rules are met, then allocation is towards the Minimum Efficacious Dose (MED) – if this is below the MTD. If the estimated MED lies at or above the estimated MTD, the allocation is at the estimated MTD.\nIf while allocating to the estimated MED further toxicity results change the estimate of the MTD, and if there is now insufficient information on the MTD as specified by the early stopping rules for finding the MTD, allocation switches back to allocating to the estimated MTD, if the sample size cap for finding the MTD allows.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-fgd",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-fgd",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Fine Grain Dosing",
    "text": "Fine Grain Dosing\nIn some settings, e.g. when the drug is delivered in solution by IV or when manufacturing allows any dose in a range from say 100mg to 400mg in steps of 10mg, dose strengths need not be restricted to just a small number of pre-defined levels. FACTS has a feature that allows this to be simulated, not with a continuous range of doses, but with “fine grain” dosing.\nFACTS supports the specification of a range of doses from a minimum to a maximum with doses either equally spaced or spaced with equal ratio. Using dose ratio makes most sense it you want to use the dose strength whilst believing the effect will be roughly log-dose. Using dose ratios, it’s necessary to accept FACTS reporting dose strengths only close to those desired. As an example, if the main doses followed a dose doubling scheme: 12.5, 25, 50, etc., one might use fine grain dosing with dose space ratios of approximately the 4th root of 2 (1.1892). The resulting doses are 12.5, 14.865, 17.677, 21.022, 24.999, 29.729, 35.354, 42.043, 49.998, etc., which means there are three dose levels between each of the original doses.\nThere are two alternatives:\n\nUse nominal dose strengths 1, 2, 3, 4, … (i.e. assuming the dose spacing is linear in expected effect) and label the doses according to their actual strength.\nUse a fixed dose interval (e.g. 12.5 resulting in doses of 12.5, 25, 37.5, 50, 62.5, etc.) so the lower doses (of the original scheme) have fewer (or no) intermediate doses and the higher doses have many more. The dose escalation rules can be specified in terms of dose strength ratio to achieve the required escalation, for example allowing dose escalation with a dose strength ratio of 2 will result in the initial escalation using doses 12.5, 25, 50, 100, etc.\n\nAs well as possibly adjusting the dose escalation step size to accommodate the new dose levels on the Design &gt; Allocation Rule tab, there are two other rules that may need modification:\n\nTo count a dose as “cleared”, we might now count cohorts on nearby doses to count towards the required clearing total. This is specified as the “Max ratio of dose strengths considered as near” (if dose allocation rules apply to ratio of dose strength) or “Delta in dose strength considered as near” (if dose allocation rules apply to dose strength) on the Design &gt; Allocation Rule tab.\n\nFor example, if we have doses at roughly 4th root of 2 intervals, we might count any dose within a ratio of 1.2 as “near” so that any cohorts allocated to immediate neighbor doses count towards clearing a dose.\nAlternatively, if we have doses every 12.5mg from 12.5 to 400, counting any dose within a ratio of 1.1 will mean that from dose 125 and above, immediate neighbor doses (within 12.5) count towards clearing a dose, and from dose 250 and above, doses within 25mg (two immediate neighbor doses) count towards clearing a dose.\n\nThe concept of “near doses” in fine grained dosing allows us to skip certain doses in the escalation phase, which might make sense if there is reason to believe that doses of similar dose strengths behave similarly and don’t provide enough additional information to justify assigning more cohorts to.\n\n\n\n\n\n\nFigure 6: Doses from 12.5 to 400mg, with fixed spacing of 12.5. Showing dose escalation by dose doubling.\n\n\n\nWhen requiring a certain number of cohorts to have been allocated to the estimated MTD before the trial can stop / to allow the trial to stop, we might now count cohorts on doses near the estimated MTD as counting towards that total. This is set on the Design &gt; Stopping Criteria tab. In considering which doses are near, the same logic as on the Design &gt; Allocation Rule tab regarding Dose Strength or Ratio of Dose Strength will be used.\n\nIf Dose Strength is used, then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength. For example, by +/- 12.5 mg:\n\n\n\n\n\n\nFigure 7\n\n\n\nIf ratio of Dose Strength is used then on the Design &gt; Stopping Criteria tab, we can include cohorts on nearby doses to count towards the required number of cohorts on the estimated MTD by specifying “nearby” in terms of dose strength ratio. For example, by +/- 10%:\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nNote that with Fine Grain dosing, if a band is specified for a dose to count as cleared, then the maximum cleared dose will be the maximum dose within that band, and if incrementing relative to the Maximum cleared dose, then the maximum permitted increment will be relative to the maximum dose within the cleared band.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-oe",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-oe",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Open Enrollment",
    "text": "Open Enrollment\nOpen enrollment (Broglio et al. 2015) can be used instead of cohort enrollment. Cohort enrolment enrolls a fixed number of subjects to a given dose, then waits until their treatment and follow up is complete (so their final status – whether they suffered a DLT (Dose Limiting Toxicity) or not – is known) before deciding on the next dose to allocate to and then recruiting the next cohort. This likely means that subjects become available for inclusion in the trial, but have to be turned away as the trial waits for the current cohort to complete. Open enrollment attempts to address this by allowing subjects to be enrolled whilst the current “cohort” is completing. However, this may come with some risk – more than a cohort’s worth of subjects may now be exposed to a new dose before we have any estimate of its DLT rate. To allow this risk to be managed, open enrollment introduces two new concepts:\n\nWhen allocating to an uncleared dose, a cap can be set on the number of subjects that can be allocated to that dose who have not yet got their final results (“OE cap 1”). For example, if this number is set to 3 (to be the same as a common cohort size), after 3 subjects have been recruited and allocated to the current dose, no more subjects will be allocated to this dose until at least one of these subjects has completed. Until then, potential subjects that become available will be turned away unless backfilling is enabled (see next point below). But unlike cohort enrolment, as soon as the first of the subjects on the dose completes, a subject that comes available could now be allocated to the dose, depending on further rules explained below (frontfilling) – unless of course that subject’s result has changed the estimated MTD. Note that the trial won’t escalate beyond the current dose until the required number of subjects to clear the dose have completed. By default, the trial won’t allocate more than the number of subjects required to clear the dose until the dose is cleared, meaning if 3 subjects are required to clear a dose and 3 subjects have been allocated to this dose, even when 1 or 2 of these subjects have their final results and a new subjects is enrolled, they won’t be allocated to this dose. If this is regarded as over cautious, it can be modified by enabling frontfilling, allowing 3 subjects without final results simultaneously. In the above example, this would mean we could place a fourth subject on the dose when the result of the first subject has come in and a fifth subject as soon as the result of the second subject has come in.\nWhen the cap on the number of subjects without final results on an uncleared dose has been reached (“OE cap 1”) new potential subjects will be turned away, unless backfilling is enabled. Enabling backfilling allows these subjects to instead be included, allocating them to a lower dose that has already been cleared. Whilst such an allocation may not contribute as much to identifying the MTD as allocating to the current dose would, it can still contribute by:\n\nIncreasing the information on the next lower dose can inform the estimate of toxicity on the current dose through the Bayesian logistic model.\nProviding additional information on a dose that it may be necessary to de-escalate too if the current dose turns out to be too toxic.\n\nIt can also contribute information on other endpoints (such as efficacy). Once backfilling has been enabled, it is also possible to enable frontfilling. For more information on backfilling and frontfilling, see this section.\n\nAssume at a given point in time we want to allocate a subject to a specific dose, denoted by “candidate dose”. FACTS allows 3 different caps to be specified on how many subjects who have not yet got their final results (i.e. are not yet complete) can be allocated to this candidate dose:\n\n\n\n\n\n\nFigure 9\n\n\n\n\nMaximum subjects without final results if dose is uncleared: As described early in this section, we encounter this cap during escalation when the candidate dose is not yet cleared. This cap takes into account subjects not yet complete on the candidate dose and any higher dose (“OE cap 1”).\nMax subjects without final results if dose is cleared and below MTD: We encounter this cap when the candidate dose is cleared and below the estimated MTD (which can happen if the estimated MTD is beyond the range of available doses, when backfilling, or when allocating during the efficacy phase of a toxicity plus efficacy trial). This cap takes into account subject not yet complete on the candidate dose and any lower doses (“OE cap 2”).\nMax subjects without final results if dose is cleared and at MTD: We encounter this cap when the candidate dose is cleared and the current model estimated MTD (which can happen when after clearing a dose we decide not to escalate, or after de-escalating) (“OE cap 3”).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-concepts-bf",
    "href": "documentation/v71/userguides/crm.html#sec-concepts-bf",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Backfilling and Frontfilling",
    "text": "Backfilling and Frontfilling\nAs described in the preceding section, Backfilling is the allocation of subjects to a lower dose when, due to restrictions, it is not possible to allocate a subject who comes available to the current dose (Dehbi, O’Quigley, and Iasonos 2021). FACTS provides a number of options to configure how backfilling behaves. Backfilling can be enabled in the Study &gt; Study Info tab. The total sample size can be divided between the subjects allocated as part of conventional dose escalation and those allocated using backfill. When backfill is enabled, it is important to increase the total sample size and then limit the number that can be allocated using backfill, as subjects allocated using backfilling will not contribute to the escalation and the confirmation of the MTD and it’s usually important to retain sufficient sample size to achieve this aim.\n\n\n\n\n\n\nFigure 10\n\n\n\nWhen enabling backfilling, several options can be specified in the Design &gt; Backfill Allocation tab.\n\n\n\n\n\n\nFigure 11\n\n\n\n\nTwo maximum caps can be specified on the number of subjects that are assigned in the process of backfilling to a given dose:\n\nan overall cap on subjects per dose that cannot be exceeded by backfill, counting also subjects that were assigned to that dose through regular allocation\na cap on the number of subjects per dose that were allocated by backfill, counting only subjects that were assigned to that dose using backfilling.\n\nHow many dose levels below the current dose can be allocated to when backfilling. Backfilling will always be to the highest dose possible (which might be the current dose if frontfilling is enabled, otherwise it will be below the current dose). Allocation to the next highest dose might be limited either by an open enrolment cap if there are already subjects allocated to that dose who have not yet completed, or it might be limited by the backfill caps described above. If allocation to the dose below the current dose is not possible, backfilling will by default look at the dose below that (two levels below the current dose) and so on. Using this option can ensure no backfilling happens to doses that are too far below the current dose.\nThe lowest dose that can be allocated to when backfilling. This option is particularly useful when there is reason to believe doses below a certain level will not be effective.\nWhether frontfilling is allowed – frontfilling allows allocating more subjects to uncleared doses than the number required to clear that dose (see this section).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "href": "documentation/v71/userguides/crm.html#open-enrollment-backfilling-and-fine-grain-dosing",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Open Enrollment, Backfilling and Fine Grain Dosing",
    "text": "Open Enrollment, Backfilling and Fine Grain Dosing\nWhen using open enrolment and fine grain dosing, the interval defined on the Design &gt; Allocation Rule tab “Delta in dose strength considered as near +/-“, or “Max ratio of dose strengths considered as near” is crucial: it is used to define the range of doses where subjects allocated to any of them count towards clearing a dose.\n\nIf dose allocation rules are selected to apply to “Dose strength”, the interval is defined “Delta in dose strength considered as near +/-“. Thus, for example, if this is set to 2, then subjects complete on doses with strength in the range 4-8 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 8.\nIf dose allocation rules are selected to apply to “Ratio of strength”, the interval is defined “Max ratio of dose strengths considered as near“. Thus, for example if this is set to 1.5, then subjects complete on doses with strength in the range 4-9 will contribute to the dose of strength 6 being cleared. At the same time, clearing the dose of strength 6 will automatically clear the dose of strength 9. These ranges also apply when assessing “OE cap 1-3”, and how many subjects have been allocated overall, or via backfilling. In backfilling, FACTS checks each dose strength and the doses in its “near” interval range, at (if frontfilling) or below the current dose, until the first dose strength is found where backfilling can take place.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-studyinfo",
    "href": "documentation/v71/userguides/crm.html#sec-studyinfo",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Study Info",
    "text": "Study Info\nThe Study Info sub-tab provides parameters for specifying:\n\nWhether the trial has an Efficacy endpoint as well as a Toxicity one.\nWhether recruitment is in Cohorts or uses Open Enrolment,\nWhether the trial data is being analyzed as a single population (single group) or two groups (which could be 2 different patient types, or 2 different treatment types).\nThe option to specify that the trial should include an expansion cohort once the MTD has been identified.\nThe option (if using open enrolment) to specify the use of backfill.\n\nIncluding an efficacy endpoint – this allows the trial to include a binary efficacy outcome that is observed at the same time as the toxicity endpoint. Once the MTD has been sufficiently determined further cohorts are allocated to determine the MED (Minimum Effective Dose) as long as that is below the MTD, until the maximum sample size or MED stopping rules have been reached.\nCohort versus Open enrolment: cohort enrolment is the standard way of running a phase 1 trial, a cohort of subjects of pre-determined size are treated at the current dose and the trial pauses until all the subjects in the cohort are complete, then the dose for the next cohort is determined. A phase 1 trial using Open Enrolment recruits ‘all comers’ with a cap on the number of subjects who can be treated but have not yet had a final result. Subjects that arrive whilst this cap has been reached are deemed to be ‘lost’ to the study.\nIf Open Enrolment and Efficacy Endpoint options are being used together, then subjects who arrive who cannot be allocated to the MTD (because the cap number of subjects awaiting a final result has been reached), can be allocated to the MED (as long as that is below the MTD).\nIf the trial is analyzing 2 Groups then a joint statistical model is used with options to constrain the group 2 difference in the intercept term to be +ve or -ve, and options as to whether a common or separate estimates of the slope term are used.\nIf an expansion cohort is included, the this is a single cohort (or one per group, if 2 groups is being used) typically much larger than used during the dose escalation, that is assigned at the end of the study to the target dose. FACTS simulates the results that arise from this cohort and a final analysis.\nIf open enrolment is being used, the further option to use backfill becomes available. The parameters on this tab for backfill, are to specify the maximum number of subjects that can be allocated for escalation, and the maximum that can be allocated in backfill. These two maximums should not total less than the overall “Max subjects” that can be enrolled. If adding backfill to a trial, usually the previous “Max subjects” becomes the “Max study allocation for escalation”, and an additional sample is allowed for backfill and added to the overall Max subjects.\nIf the trial has two groups the backfill maximums are the sum of the subjects in the two groups.\nFor Cohort enrolment, the parameters are:\n\nMaximum Study Size, in cohorts: the maximum number of cohorts the trial can use, though designs can include conditions that cause them to stop earlier.\nIf the trial has two groups, the maximum number of cohorts of the second group.\nCohort size: The number of subjects in each cohort. The maximum sample size for the trial is simply: Max Trial Size (cohorts) * Cohort Size.\nCohort information delay: Normally left at zero, this allows for the trial to be simulated with cohorts recruited more quickly than if waiting for the last cohort to complete before treating the next cohort. This allows the trial to complete in shorter time, but is likely to lead to a larger incidence of toxicities. If the delay is set to 1, the second cohort will be recruited before the results for the first cohort are available, the third cohort will be recruited after the results of the first cohort are available but before the results for the second cohort are available, and so on.\nExecution rate: the time taken to recruit, treat and complete the observation of each cohort (in weeks). The value of this parameter does not affect the behavior of the simulations, but it allows a nominal “duration” of each simulation to be calculated. Unlike other FACTS simulations, this duration is not simulated stochastically, it is simply the number of cohorts * this duration. Its purpose is to give a figure to compare with open enrolment designs of the same trial.\n\n\n\n\n\n\n\nFigure 15: Study Info - Cohort Enrolment\n\n\n\n\n\n\n\n\n\nFigure 16\n\n\n\nIf rather than Cohorts, subjects are recruited using open enrolment, the parameters are:\n\nMax subjects: the maximum number of subjects who can be recruited into the study.\nTime unit – this is a text string that will change the “units” label for time on graphs. This allows data to be more easily entered when the natural time unit is not “weeks”, but “days” or “months”.\nMean recruitment rate: for simulation purposes this is the average number of subjects who will be expected to be available for recruitment during each time period. Recruitment is simulated using a standard Poisson process.\nTime until final result: the time for each subject for their final result to become available. This is simulated as a fixed length of time that is the same for all subjects.\nMaximum subjects without a final result: the cap on the number of subjects on the current dose or a backfill dose (if backfill is enabled) who can be recruited but not yet completed. This is also known as the ‘maximum queue length’. Subjects arriving and available for recruitment while the number of subjects on this dose who have been treated but have not yet completed is at this maximum, are dropped and assumed no longer available for recruitment. Once the current subjects complete the study has to await further new subjects to become available. There are three caps:\n\nMaximum subjects without final results if the dose is uncleared. This allows the design to be cautious when a new dose is used for the first time.\nMaximum subjects without final results if dose is cleared and below MTD. This allows a larger number of subjects without final results to be recruited at backfill doses or at the current escalation dose if backfill to the current escalation dose (“frontfilling”) is enabled, or at the MED in the efficacy phase of a trial that includes an efficacy endpoint.\nMaximum subjects without final results if dose is cleared and at MTD. This allows us to be more cautious if the model thinks all doses are toxic or if we are allocating at the model MTD and don’t want to expose too many subjects.\n\nBackfill – this can be enabled. Backfilling is the allocation of subjects to a dose below the current target dose, if the number of subjects allocated to the current target dose without final results has reached the maximum. Further parameters for backfill are set on the “Backfill” tab under the “Design” tab. On this tab, if backfill is enabled, two sub-maximums can optionally be specified:\n\nthe maximum number of subjects who can be allocated as part of usual allocation for escalation and MTD determination (and MED if efficacy is included in the trial),\nand the maximum number of subjects who can be allocated as part of “backfill”.\n\n\n\n\n\n\n\n\nFigure 17: Study Info - Open Enrolment\n\n\n\nGroups: a trial can be analyzed as a “single group” or as “two groups”. If analyzed as a single group, then all subjects are assumed to be the same and treated the same (except for the difference in the dose strength). If analyzed as two groups this allows either:\n\nThe subjects can be simulated as coming from two similar but distinct groups such as: adults and children, first line or recurrent, having some concomitant treatment or not. The separation into the two groups is based on some property of the subject.\nOr the subjects can be simulated as having been allocated (possibly randomized) to one of two versions of the treatment, with the same rang of dose strengths and differing in some other way such as dosing schedule, treatment duration or combination with an additional treatment. The separation of the subjects into the two groups is under the control of the protocol.\n\nIn either case the same analysis options are available (hence we use the generic term “groups” to describe this feature).\nIf enrolment is by cohort, the there are two separate “maximum study sizes” in cohorts – one for each group.\nThe Group 2 recruitment, while it overlaps in time with the Group 1 recruitment, is simulated as being in lock-step and the recruitment of the cohort in each group is concurrent and analyzed when both are complete. In the ‘cohorts.csv’ files that are output, the cohort numbers indicates which cohorts were concurrent. The options are:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the first group1 cohort has been allocated the specified dose (if cohorts can be accrued before the cohort before has completed, the group 2 is accrued too – it does not wait until the group 1 cohort completes unless the next group 1 also waits).\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 cohort is recruited in parallel with the next group 1 cohort after the specified number of subjects had been recruited into group 1.\n\nIf enrolment is open then there are options similar to the cohort enrolment to control when enrolment into group 2 starts:\n\nTo recruit group 1 first then recruit group 2 only when group 1 is complete.\nTo start recruiting both groups in parallel from the outset.\nTo start recruiting group 2 once group 1 has reached a specified dose. The first group 2 subject can be recruited after the first group1 subject has been allocated the specified dose.\nTo start recruiting group 2 once group 1 has recruited a specified number of subjects. The first group 2 subject can recruited after the specified number of subjects had been recruited into group 1.\n\nBut in addition the user specifies the maximum number of subjects in Group 2 and how the recruitment is to be simulated:\n\nWith the group membership a property of the subject – along with a mean recruitment rate for group 2.\nWith subjects randomized between the two groups (the randomization is fixed at 1:1).\n\nThe user specifies the three “Maximum subjects without final results …” for the second group.\nIf backfill is enabled, the backfill totals apply to the total of the subjects on both groups.\n\n\n\n\n\n\nFigure 18: Study info - 2 group options with open enrolment\n\n\n\nEnable Final Expansion Cohort: if this is enabled a final cohort of specified size will be allocated the dose selected as MTD at the end of the N-CRM phase of the study:\n\nIf the study includes a control arm, the number of subjects in this expansion cohort to be allocated to control is also specified.\nIf the study has two groups, two separate expansion cohorts will be allocated, their sizes are set separately.\nIf the study includes observing efficacy then the target dose can be changed from MTD to MED or OSD.\n\n\n\n\n\n\n\nFigure 19\n\n\n\nSimulating an additional efficacy outcome is simply specified by checking the “include efficacy” checkbox.\n\n\n\n\n\n\nFigure 20: Study tab with “Include efficacy” checked\n\n\n\nSimulating an efficacy endpoint can be combined with all the other features (two groups, open enrolment, backfilling) already discussed, as well as with ordinal toxicity and fine grain dosing that are described below.\nCurrently there are two significant limitations to the simulation of an efficacy endpoint:\n\nThe endpoint is assumed to be dichotomous.\nThe endpoint is assumed to be observable at the same time as toxicity.\n\nWe hope to lift these restrictions in a later version of FACTS.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity",
    "href": "documentation/v71/userguides/crm.html#toxicity",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Toxicity",
    "text": "Toxicity\nThe Toxicity sub-tab provides parameters for specifying:\n\nWhether toxicity should be simulated as dichotomous or ordinal. If simulated as Ordinal, toxicity can be simulated as a 1-3 scale or 1-4 scale. In both cases ‘1’ is “no-toxicity”, 2 is “mild toxicity” and 3 is the toxicity of interest (from the point of view of defining the MTD, target toxicity band and Overdose Control. If a four category scale is selected, 4 is “sever toxicity” or death. Choosing whether to model the ordinal response is a separate option and is present on the Design &gt; Toxicity Response tab.\nType of target: this allows the user to specify whether the dose selection targets “the dose who’s estimated toxicity rate is closest to a specified target rate” or “the dose with the highest posterior probability of having a toxicity rate in a target band”. The former is the target rule used in the original CRM papers ((O’Quigley, Shen, and Gamst 1999), (deMoor et al. 1996), and the latter rule was introduced in (Neuenschwander, Branson, and Gsponer 2008).\nToxicity target (only displayed if the type of target is “a single dose”): this allows the target toxicity rate to be specified and whether the target dose is the one nearest, the one nearest but with a lower rate or the one nearest but with a higher rate.\nTarget: this panel allows the target toxicity bands to be specified along with overdose control limits. The panel is displayed and enabled even if the target type is “a single dose” to allow overdose control limits to be specified.\nType of Target: controls the selection of the dose for the next cohort – this can be to target a single dose (to replicate the original CRM behavior, see this section) or to target the dose with the highest probability that its toxicity rate lies in a target band.\n\n\n\n\n\n\n\nFigure 21: Toxicity tab targeting a toxicity band\n\n\n\n\nTargeting a Toxicity Interval\nTargeting a toxicity band or interval is an innovation introduced with the N-CRM design, unlike other CRM designs that select the dose that is expected to have a toxicity response closest to the desired tolerated limit, the N-CRM selects the dose that has the highest posterior probability of having a toxicity rate in a target toxicity band. This has the advantage of a) having a clearer probability statement and b) having in addition probability statements about the probability of under and overdosing (the toxicity rate being below or above the target toxicity band).\n\nThe uncertainty in the estimate of toxicity at each dose is expressed by calculating the posterior distribution of the estimate of the rate of toxicity at each dose and calculating the proportion of that distribution that falls in to each of 4 bands of toxicity: ‘Under-dosing’ (toxicity so low that it is likely that a higher dose could be used), ‘Target’ toxicity (we want to select doses whose toxicity rate is most likely to be in this band), ‘Excess’ toxicity (toxicity higher than desired) and ‘Unacceptable’ toxicity.\n\nUnder-dosing: this band always starts at 0.0; the user specifies the upper bound.\nTarget band: this band always starts at the upper-bound of the under-dosing band; the user specifies the upper bound.\nExcess toxicity: this band always starts at the upper-bound of the target band; the user specifies the upper bound.\nUnacceptable toxicity: this band always starts at the upper-bound of the target band, with an upper bound of 1.0. The graph shows the width of the different bands using a simple, fixed, example posterior probability distribution of a toxicity rate.\n\nIntervals are relative to control: if a control arm is included in the study, then toxicity bands can be defined as the difference in toxicity rate relative to control. Negative differences (a lower toxicity rate than control) are always treated as under-dosing. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\nLimit max excess/unacceptable toxicity: the ‘overdose control’ in terms of a maximum allowed posterior probability that a dose’s toxicity rate lies in either the ‘Excess’ or the ‘Unacceptable’ toxicity bands. Any dose with a posterior probability of having a toxicity rate in either of these two bands that is higher than the specified limit, cannot be selected for allocation to the next cohort, nor selected as MTD at the end of the study (this gives rise to slightly different results compared to those in (Neuenschwander, Branson, and Gsponer 2008) where the overdose control was not applied to final selection).\n\nIt is possible to have the overdose control limit vary with the number of cohorts allocated. In particular this can be used to reduce the overdose limit as the number of cohorts (and the amount of information) grows. For example for a particular prior and final level of overdose control, it may be that initial escalation is excessively constrained, one way to allow early escalation in this setting is to use these parameters to allow an higher initial overdose control limit and gradually reduce it over time to the final desired limit. Tuning the parameters will require some iteration and simulation. A varying limit is specified by the specifying amount to change the limit by per cohort and the final limit. The amount to change by is always entered as a number in the range (0,1), whether this is an increment or decrement depends on whether the target limit is greater or less than the initial limit. Leaving the change in limit at its default of 0 means the limit does not vary.\nLimit max unacceptable toxicity: as for the previous parameter, but here the overdose control is only in terms of the posterior probability that a dose’s toxicity rate lies in the ‘Unacceptable’ toxicity band.\nAs or the limit on excess/unacceptable toxicity it is possible to have the overdose control limit vary with the number of cohorts, see the description above.\n\n\n\nTargeting a single dose\nIt is possible to use the N-CRM design engine with a conventional CRM allocation strategy - to “allocate to the nearest / highest dose below the maximum tolerated toxicity”; this allows conventional CRM design to be simulated with some of the additional features of N-CRM:\n\nOverdose control\nEstimate both parameters of the 2 parameter logistic\nThe “Recommender” to analyze a specific data set.\n\nThe target is calculated by:\n\nIn the MCMC sampling loop finding the dose that meets the target criteria, a doses probability of being the target is then the proportion of times that dose meets the target criteria across the MCMC sampling.\nRather than selecting the dose with the highest probability, the dose at the 50% quantile is used. The cumulative probability of being the target is calculated over the doses in ascending dose strength, and the dose when the cumulative probability passes 50% is selected. This addresses some problems that can arise when very little data is available: that the dose with the highest probability is at one end of the dose range, but that probability is not that high, or that doses are not evenly spaced and a dose close to both its immediate neighbors may never have greater probability than both of them.\n\nSetting the Type of Target option to Target a single dose, modifies the tab thus:\n\n\n\n\n\n\nFigure 22: N-CRM, targeting a single dose, not a toxicity band\n\n\n\nWhen targeting a single dose FACTS allows the user to specify:\n\nThe target toxicity rate\nWhether to allocate to the dose with the mean estimate of its toxicity rate nearest the target, highest dose with a mean estimate of its toxicity rate below the target or lowest dose with a mean estimate of its toxicity rate above the target.\nAn option to use the toxicity rate relative to control, rather than the default of the absolute toxicity rate. To enable this option it is necessary to first go to the Study &gt; Treatment Arms tab and select Include Control.\n\nThe definition of the boundaries of the toxicity bands is still included in order to allow the specification of overdose control limits. These are calculated and applied in exactly the same way as when targeting a toxicity interval.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#efficacy",
    "href": "documentation/v71/userguides/crm.html#efficacy",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Efficacy",
    "text": "Efficacy\nIf include efficacy has been checked on the Study Info tab, a simple additional input page is included:\n\n\n\n\n\n\nFigure 23: The Efficacy tab\n\n\n\nIf simulating and modelling an efficacy endpoint is included there are two items to be specified on this tab:\n\nWhether a subject experiencing a toxicity can also count towards efficacy or not. If unchecked patients outcomes are simply sampled separately and a patient can both have a toxicity and an efficacy response. If checked, a patient’s toxicity outcome is sampled first, and only if there is no toxicity is an efficacy outcome sampled.\nThe efficacy target – this consists of:\n\nThe target efficacy rate required for the Minimum Efficacy Dose (MED).\nWhether the target dose is the nearest dose to the MED rate, the lowest dose above the MED rate or the highest dose below the MED rate.\nIf a control arm has been included, whether the target rate is absolute or relative to the observed rate on the control arm.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#treatment-arms",
    "href": "documentation/v71/userguides/crm.html#treatment-arms",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nOn this tab the number of treatment arms (doses) available to the study is specified. The user can either define a set of specific doses that can be used or a continuous dose range with some granularity.\nSelecting Explicit Doses allows the user to specify the specific doses that can be used on the trial:\n\nA single new dose or multiple doses can be added either by clicking “Add” or “Generate”. Initially each dose is defined by a simple integer name and level. The dose levels and dose names can then be edited on by clicking on them and entering the desired value. The dose level can also be set later on in the Design &gt; Toxicity Response tab.\nThere is also the option to include a control arm. Including a control arm allows the toxicity rate to be relative to the control arm.\n\n\n\n\n\n\n\nFigure 24: The Treatment Arms tab specifying explicit doses\n\n\n\n\nFinely Spaced Doses\nSelecting Finely Spaced Doses allows the user to specify the dose range that can be used on the trial:\n\nThe minimum and maximum dose strength to be used\nThe ‘granularity’ of the actual dose used, either as a fixed delta (Fixed spacing) or a dose ratio (the ratio specified must be greater than 1) (Ratio spacing).\nThe number of ‘bins’ or ‘doses for which to report’ – this is because FACTS will still produce summary statistics in columns, many with a “column per dose” – it is possible to use more doses than it is practical to report on (and a limit in MS Windows of 32K pixels for the width of a table means that the GUI can only display simulation results for a maximum of ~40 doses). However this limitation is only for reporting summary statistics; the dose strengths modeled and allocated in the simulations are unaffected.\n\nSelecting ‘Finely Spaced Doses’ will also affect how some of the other parameters are specified in the FACTS GUI.\n\n\n\n\n\n\nFigure 25: The Treatment Arms tab, specifying a finely spaced dose range",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-variants",
    "href": "documentation/v71/userguides/crm.html#sec-variants",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Variants",
    "text": "Variants\nOn this tab the user can specify that a number of design variants should be created. Currently the only design feature that can be changed is the sample size (maximum number of cohorts).\nIf “multiple variants” is checked then the user can specify that simulations setups should be created for each simulation scenario with versions of the design with different maximum numbers of cohorts.\nThe user enters the number of variants they wish to create. Then in the resulting table, enter different “Max Cohorts” for each variant.\nThese will then appear on the simulations tab.\nIf open enrolment is being used, then the enrolment cap is specified by the number of subjects.\nIf there are two groups then separate caps are specified for each group.\n\n\n\n\n\n\nFigure 26: The Variants tab, specifying 5 variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#explicitly-defined",
    "href": "documentation/v71/userguides/crm.html#explicitly-defined",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\nToxicity profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 28. The user enters the toxicity rate to simulate at each dose into the Probability of Toxicity column of the table, and the graphical representation of these toxicity values updates accordingly.\nThis form of toxicity profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter toxicity rates for all of them. When using “finely spaced” doses the toxicity rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 27: Explicitly defined toxicity - binary endpoint\n\n\n\nIf the design is using ordinal toxicity, the toxicity response rates can be specified either:\n\nOn the “Toxicity” tab by specifying the category 3 or greater toxicity rate at each dose and then two offsets – one for the category 2 or greater rate and one for the category 4 rate.\nOn the “Ordinal Toxicity” tab by separately specifying the toxicity rates for each category of toxicity at each dose.\n\nSpecifying offsets: to ensure that the specified category 3 rate plus the category 2 offset doesn’t sum to more than 1, or the category 3 rate plus the -ve category 4 offset sum to less than 0, the offsets are applied to the logit of the category 3 toxicity rate.\nThus for the category 2+ rate:\n\\[\nln(\\frac{p_{2+}}{1-p_{2+}}) = ln(\\frac{p_{3}}{1-p_{3}} + \\Delta_2)\n\\]\nwhere:\n\n\\(p_{2+}\\) is the probability of observing a category 2 or greater toxicity at a dose\n\\(p_3\\) is the probability of observing a category 3 or greater toxicity at a dose\n\\(\\Delta_2\\) is the difference in the log odds between the two probabilities\n\nThe offset is defined at the lowest dose and highest dose and then varied linearly with dose strength at the intermediate doses. The plot of the curve can either use Pr(Tox) or Log-odds(Tox) as the y-axis and dose strength or log(dose strength) as the x-axis. A graph is displayed of the toxicity rates that have been entered, and the category 2+ and category 4 toxicity rates if applicable. This graph, as with all graphs in the application, may be copied to the clipboard or to a file using the “right-click” menu.\n\n\n\n\n\n\nFigure 28: Virtual Subject Response – Explicitly-Defined – ordinal endpoint\n\n\n\n\nExplicitly defined – Ordinal Toxicity\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 29: Virtual Subject Response – Explicitly-Defined: Ordinal Toxicity tab\n\n\n\n\n\nExplicitly defined toxicity – when simulating 2 groups\nIf simulating toxicity as a binary outcome, when simulating 2 groups, the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group.\n\n\n\n\n\n\nFigure 30: Explicitly defined toxicity - 2 groups\n\n\n\nIf simulating 2 groups and ordinal toxicity, then on the explicitly defined tab once again the tab displays two Pr(Tox) columns, one for each group, and the user enters the toxicity rate to be simulated at each dose for each group. As for a single group the Category 2 toxicity and Category 4 (if using) toxicity rates are defined by defining log odds offsets at the lowest and highest dose. Specification is limited to a single set of offsets that are applied to both groups.\n\n\n\n\n\n\nFigure 31: Explicitly defined toxicity - 2 groups and ordinal offsets\n\n\n\nAs in the single group case in addition to the Category 3 toxicity rates that are editable, columns showing the Pr(Tox 2+) and Pr(Tox 4) are shown, but these are not editable and derived from the Pr(Tox) rates and the offsets that have been specified. The ordinal toxicity rates are only shown for group 1.\n\n\nExplicitly defined – Ordinal Toxicity with 2 groups\nIf simulating ordinal toxicity an alternative option is to explicitly specify the rate at each toxicity level at each dose. This can be done under the Virtual Subject Response &gt; Explicitly Defined &gt; Ordinal Toxicity tab.\n\n\n\n\n\n\nFigure 32: Explicitly Defined, Ordinal Toxicity with 2 Groups\n\n\n\n\n\nEfficacy response profiles\nEntering efficacy response profiles is very similar to entering toxicity profiles. FACTS will construct scenarios to simulate of every combination of toxicity and efficacy response profiles.\n\n\nExplicitly Defined – Efficacy\nEfficacy profiles may be added, deleted, and renamed just like toxicity profiles. The user enters the efficacy rate to simulate at each dose into the Probability of Efficacy column of the table, and the graphical representation of these efficacy values updates accordingly.\nAs with toxicity this form of efficacy profile is not available if the dose range is “finely spaced” – typically this means there are too many doses for it to be practicable to enter efficacy rates for all of them. When using “finely spaced” doses the efficacy rates to be simulated have to be specified using parametric models (see the next section).\n\n\n\n\n\n\nFigure 33: Efficacy virtual subject response, explicitly defined\n\n\n\n\n\nExplicitly Defined – Efficacy with two groups\nIf the design included 2 groups, when explicitly defining an efficacy response profile, there is simply a second column of efficacy response rates to enter:\n\n\n\n\n\n\nFigure 34: Explicity defined efficacy response profile with 2 groups",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-parametric",
    "href": "documentation/v71/userguides/crm.html#sec-parametric",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Parametric",
    "text": "Parametric\nToxicity scenarios may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen. The user selects the model to use to determine the toxicity rate to simulate at each dose, and specifies the values of the model’s parameters. The graphical representation of these toxicity values updates accordingly.\nThe graph may be copied using the context menu functionality described in the previous section.\nFour models are available:\n\nLogistic: the probability of toxicity at dose x is given by: \\(P_x=\\frac{1}{1+e^{-s(x-x_{50})}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with linear effective doses \\(\\hat{x}=x-x_{ref}\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*(x_{ref}-x_{50})\\)\nEmax: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{x}{x+x_{50}}\\) with user specified parameter \\(x_{50}\\).\nLog Logistic: the probability of toxicity at dose \\(x\\) is given by: \\(P_x=\\frac{1}{1+e^{-s(ln(x)-ln(x_{50}))}}\\) with user specified parameters, \\(s\\) (slope) and \\(x_{50}\\) (the ED50 dose). Note this can be fitted exactly by the two parameter logistic with log effective doses \\(\\hat{x}=ln(\\frac{x}{x_{ref}})\\) when \\(\\beta=s\\) and \\(\\alpha=\\beta*ln(\\frac{x_{ref}}{x_{50}})\\)\nPiecewise linear: with the probability of toxicity specified at a series of knots, with the probability linearly interpolated between knots.\n\n\n\n\n\n\n\nFigure 35: Virtual Subject Response - Parametric Toxicity tab\n\n\n\nIf Ordinal toxicity is being simulated then the category 2 and greater toxicity rates and category 4 toxicity rates are specified using the logit offset methods as on the Explicitly-Defined &gt; Toxicity tab.\n\n\n\n\n\n\nFigure 36: Virtual Subject Response - Parametric Toxicity tab with Ordinal toxicity\n\n\n\nIf Ordinal Toxicity and 2 groups are being simulated then both the Cat 2+ and Cat 4 toxicities and the Group 2 toxicities are defined using the logit offset methods.\n\n\n\n\n\n\nFigure 37: Parametric definition of ordinal toxicity response with 2 groups\n\n\n\n\nParametric efficacy response\nParametric efficacy response profiles function exactly like toxicity profiles, with the same parametric models to choose from and if 2 groups are present the response of the second group is again defined by 2 log-odds offsets, one at the lowest dose and one at the highest.\n\n\n\n\n\n\nFigure 38: Parametric efficacy response profile",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#external",
    "href": "documentation/v71/userguides/crm.html#external",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "External",
    "text": "External\nSubject response data may be simulated from a PK-PD model in place of, or in addition to, choosing a response model from FACTS. The importing of subject response data (which must be in the form specified in the System Requirements Document) may be done from the External Files sub-tab depicted below (Figure 39).\nThe supplied data should be in the following format: an ascii file with data in comma separated value format with the following columns:\n\nPatient id\nDose index (1, 2, 3,… if a Control is to be included it should be index 0) this is not the user settable dose name or dose level\nToxicity (0,1)\nEfficacy (0. 1) even if efficacy not being simulated this value must be present\nGroup (*1, 2) only required if groups are being simulated\n\nThe GUI requires that the file name has a “.dat” suffix.\nSimulated subjects will be drawn from this supplied list, with replacement, to provide the simulated response values.\nTo import an external file, the user must first add a scenario to the table. After adding a scenario, the user must click “Browse” to locate the externally simulated data via a standard file browser dialog.\n\n\n\n\n\n\nFigure 39: External Data",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#x-hats",
    "href": "documentation/v71/userguides/crm.html#x-hats",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "X-Hats",
    "text": "X-Hats\nOn this tab the user specifies the reference dose \\(d^*\\) for use in calculating the adjusted dose value (the “x-hat” values). The default value to use is Median dose is reference, this uses the median of the dose range for the reference dose, minimizing the correlation in the sampled values of \\(\\alpha\\) and \\(\\beta\\). Note though that when allocation is restricted to explicit doses it is also recommended that the value of the reference dose is not the same as an actual dose that can be used (at this dose \\(\\hat{x}\\) will be 0 and the data on this dose can have undue weight on the estimate of \\(\\alpha\\)).\nDifferent reference doses can easily be used however – between the two doses thought most likely to be MTD, just below the lowest dose, just above the highest dose. The bi-variate Normal prior for \\((\\alpha, ln(\\beta))\\) will need to be recalibrated to take the change into account.\nX-Hats are log(dose strength) allows the user to select between:\n\nlinear effective dose \\(\\hat{x}_j = d_j - d^*\\)\n\\(log(\\hat{x}_j) = ln(\\frac{d_j}{d^*})\\)\n\nIf you have entered linear dose strengths for the doses (1, 2, 3, 4, … or 100, 150, 200, 250, …) then use the linear effective dose. If however the dose strengths that have been entered are non-linear (12.5, 25, 50, 100, …) but expected to be roughly linear in effect, then use the log of the dose ratio.\n\n\n\n\n\n\nFigure 40: Specifying the dose transformation - the “x-hats”.\n\n\n\n\nThe Pro’s & Con’s of using the median dose as the reference dose\nThe reason the median dose is recommended as the reference is that this minimizes the correlation in the fit of \\(\\alpha\\) and \\(ln(\\beta)\\), the parameters of the BLRM, and it maximises the flexibility of the fit of the model over the dose range.\nHowever care needs to be taken that the prior on \\(\\alpha\\) is not more restrictive than that on \\(ln(\\beta)\\) in order to avoid a phenomena observed when preparing tutorials: observing “no toxicities” below the reference dose resulted in a model with increased probability of toxicity above the reference dose compared to observing a toxicity below the reference dose. For a given value of \\(\\alpha\\), higher values of \\(ln(\\beta)\\) correspond to lower toxicity below the reference dose – as the \\(\\hat{x}\\)̂ values are -ve below the reference dose. The fitted curve thus “pivots” about the value of \\(\\alpha\\) at the reference dose.\nThere are two solutions to this:\n\nmove the reference dose, which involves a choice between two options\n\nmoving it to the first dose or below (normally allowing a relatively constrained prior around a low value for \\(\\alpha\\)),\nor to the highest dose or above (with a relatively uninformative prior).\n\nWe have seen both solutions perform well against the chosen scenarios – but the choice needs checking and refining with a full range of scenarios that represent the full uncertainty in the true response.\nor modify the priors on \\(\\alpha\\) and \\(ln(\\beta)\\) making the prior on \\(\\alpha\\) less informative (in particular increase the probability of low values) and make the prior on \\(ln(\\beta)\\) more informative (in particular lower the probability of high values less). Because the prior distribution on \\(\\beta\\), is on \\(ln(\\beta)\\), it is easy to make large values of \\(\\beta\\) more probable than intended.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-response",
    "href": "documentation/v71/userguides/crm.html#toxicity-response",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Toxicity Response",
    "text": "Toxicity Response\nThe parameters that can be specified on this page are:\n\nThe parameters of the bivariate Normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\). Specifying the mean and standard deviation of \\(\\alpha\\) \\((\\mu_{\\alpha}, \\sigma_{\\alpha})\\), and \\(ln(\\beta)\\) \\((\\mu_{ln(\\beta)},\\sigma_{ln(\\beta)})\\) and the correlation coefficient \\(\\rho\\).\n\nIf ordinal toxicity is being simulated, it is possible to model the ordinal toxicity, specifying the mean and standard deviation of \\(\\alpha_2\\) and \\(\\alpha_4\\). These priors are separate from the \\(\\alpha_3\\) and \\(ln(\\beta)\\) prior, there is no correlation term in the prior. There is the constraint in the model that \\(\\alpha_2 &gt; \\alpha_3 &gt; \\alpha_4\\).\nUse fixed Alpha: the value of Alpha can be fixed to allow the N-CRM model to behave like the traditional CRM models. [Where \\(\\alpha\\) was set to 3 and the reference dose is set above the top of the available dose range]\n\n\nRather than entering the priors directly, they can be derived based on indirect prior information or beliefs, see ‘Deriving the Prior’ below.\n\nThe Minimum and Maximum rates that the model is to be fitted too. The model fits the range \\((0,1)\\), asymptotically approaching each limit as the adjusted dose value tends to \\(-\\infty\\) or \\(+\\infty\\). By specifying an alternative minimum and maximum, inside the range \\((0,1)\\), the user can have the model scaled to fit data to fit event rates where the asymptotic rates are not \\(0\\) or \\(1\\). For instance if the event being observed has a non-zero background rate (probability of being observed in placebo treated subjects), then the model may fit better if the minimum is set to the lower limit of this expected rate. Similarly if, even at the most toxic dose the event being observed is only expected to effect a proportion of subjects, the model may fit better if the maximum is set to the upper limit of this expected rate.\nIf a control arm is present, the user can specify to have this modelled separately, and if so the user specifies the parameters for a prior Beta distribution – in terms of numbers of prior observations on control of subjects with and without a toxicity.\nGroup 2 priors: if a second ‘Group’ is being simulated – whether this is a subset of subjects, or a modified treatment that subjects can be randomized to, then the BLRM is jointly fitted to the responses for both groups, with group 2 having offsets \\(a\\) and \\(b\\) from the first group’s \\(\\alpha\\) and \\(\\beta\\). The priors for \\(a\\) and \\(b\\) can be full bivariate Normal or can use constraints such as \\(b = 1\\), or \\(a &gt; 0\\) or \\(a &lt; 0\\).\n\n\n\n\n\n\n\nFigure 41\n\n\n\n\nDeriving the prior\nThe priors of \\(\\alpha\\) and \\(ln(\\beta)\\), can be specified directly or derived in one of four ways. When entered explicitly, the user specifies the parameters of the prior bivariate-normal distribution for \\(\\alpha\\) and \\(ln(\\beta)\\): the means, standard deviations and the correlation term \\(\\rho\\).\nAlternatively, the user may click the ‘derive prior’ button and select from:\n\nQuantiles at the lowest and highest dose: (based on the “uninformative prior” given in the paper (Neuenschwander, Branson, and Gsponer 2008), for details see this section) - the user specifies the probability of an unacceptable toxicity at the lowest dose, and the probability of under-dosing at the highest dose (0.1 for both is the default, and 0.05 for both is the value used in the paper). Optionally the probability that toxicity is less than the mid-point of the target toxicity band at the median dose can be specified. (Prior to FACTS 6.5 this third data point was not optional and constrained to be at the reference dose, but this had problems if the reference dose was not the media dose – it might also be the lowest dose for example).\nNote this method does not work so well if the reference dose is outside the dose range.\n\n\n\n\n\n\nFigure 42\n\n\n\nScenarios: the model is fitted to each of the toxicity response scenarios (MLE), the parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\n\n\n\n\n\nFigure 43\n\n\n\nSpecific quantiles: The user selects which doses and toxicity rates to provide an expectation – a prior probability that the toxicity rate on the dose will be the specified rate or less. At least 3 such expectations using at least 2 different doses strengths must be supplied. If a large number of specific quantiles are specified (e.g. reproducing the all quantiles method) the large number of different beta distributions sampled from, with the monotonicity constraint applied, results in losing too much variability. So this should only be used quantiles specified at 2-4 doses.\n\n\n\n\n\n\n\nFigure 44\n\n\n\n\nAll quantiles: the user specifies the prior expected toxicity rate at the 2.5%, 50% and 97% quantiles for each dose. (Only available when using explicitly defined doses, not a continuous dose range). Note that using Create Prior with this option will require the facts file to be saved and for there to be at least one virtual subject response profile.\n\n\n\n\n\n\n\nFigure 45\n\n\n\nIn all cases once prior values have been derived they are displayed along with a graph of 100 sampled curves from the prior. The user can accept the values, change derivation method, or cancel the derivation.\nThe plot of the samples can either be viewed as Pr(Tox) or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\nDerivation of the Prior from Quantiles\nDerivation of the parameters of the bivariate Normal prior for \\(\\alpha\\) and \\(ln(\\beta)\\)) in the Quantiles at lowest and highest dose, Specific quantiles and All quantiles cases:\n\nMinimally informative unimodal Beta distributions are fitted for each of the doses where a prior expectation of a toxicity has been specified. For doses where no prior expectation has been specified, the median expected toxicity rate are derived by assuming that the median expected toxicity is linear in log dose on the logit scale, and again a minimally informative unimodal Beta distribution is fitted with the same median.\nPreviously and following (Neuenschwander, Branson, and Gsponer 2008), the parameters of the bivariate Normal distribution were found using a stochastic fit to the prior expectations of toxicity, minimizing the error in the prior toxicity rates at the 2.5%, 50% and 97.5% quantiles. This is still used in the All quantiles and Legacy prior cases. However experience with this method with the standard priors (previously called “uninformative”) showed that it yielded priors with too little uncertainty in the \\(ln(\\beta)\\)) and too high a value for the correlation parameter for many cases and certainly for the prior to be called “uninformative”.\nConsequently, in the Quantiles at lowest and highest dose and Specific quantiles cases, the prior is now derived by sampling from the minimally informative unimodal Beta distributions, and fitting the model to each set of sampled toxicity rates. The parameters of the bivariate normal are then calculated from resulting set of pairs of values for \\(\\alpha\\) and \\(ln(\\beta)\\).\n\nIf a control arm has been included, it may be included in the model, or modelled separately using a beta-binomial model, the user specifies the prior values for the Beta distribution.\n\n\nDerivation of the prior from the scenarios\nIn this screenshot, priors have been derived from the scenarios:\n\n\n\n\n\n\nFigure 46: Design - Toxicity Response sub-tab (after prior derivation)\n\n\n\nClearly in this example the scenarios have a very high correlation between the toxicity at the reference dose (\\(\\alpha\\)) and the log gradient (\\(ln(\\beta)\\)) giving a high value for the correlation in the prior (\\(\\rho\\)). As there are very few scenarios, and they didn’t include extreme cases, the SDs of the priors of parameters will be underestimated.\nSo In this instance we might round the prior means to 2 decimal places, double the SD of \\(\\alpha\\), slightly increase the SD of \\(ln(\\beta)\\) and reduce the correlation to 0.5.\nHowever, it is much better to use Drive from Scenarios after entering a large number of varied and credible scenarios. Indeed is such a collection of scenarios exists, deriving the prior from the scenarios is the simplest approach and often very effective. Only if the performance in the simulations in some scenarios does the prior need re-visiting (usually to slightly increase the \\(ln(\\beta)\\) SD and/or reduce the correlation).\nWe strongly recommend checking the performance of the prior across a wide range of scenarios, and of entering the reported derived values of the fitted prior as an explicit prior and then manually modifying them in the light of the model’s performance on the various scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-legacy-prior-methods",
    "href": "documentation/v71/userguides/crm.html#sec-legacy-prior-methods",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Legacy prior methods",
    "text": "Legacy prior methods\nIn old versions N-CRM (pre-FACTS 4.0) , the design could be left at the stage where the method of deriving the prior had been specified but the derivation postponed to the simulation stage. We now require that the derivation be performed first, this\n\nEnables the actual prior that results from the derivation to be inspected\nMakes simulation and recommendation faster, as the derivation is not repeated every time the design engine starts.\n\nWhen opening a FACTS N-CRM file created using a pre-FACTS 4.0 version of FACTS, to continue to use the original prior, select “Derive prior”, and the derive prior window will display the old prior and allow an explicit prior to be derived from it using the now deprecated methods in the older versions of FACTS N-CRM.\n\n\n\n\n\n\nFigure 47: Derivation from a legacy prior",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-efficacy-response",
    "href": "documentation/v71/userguides/crm.html#sec-efficacy-response",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Efficacy Response",
    "text": "Efficacy Response\nThe Efficacy Response tab is displayed if an efficacy endpoint is included in the trial.\nThe Efficacy Response is specified separately from the Toxicity Response. The Toxicity and Efficacy models are completely separate except for the x-hat values for the transformed dose strengths, where a common set of values is used for both models. Note that the use of a common set of x-hats for both endpoints is a difference from FACTS bCRM, that means it may not be possible to exactly replicate a bCRM design in FACTS N-CRM, however we think that the additional features and options in FACTS N-CRM will make it possible to create an overall superior design in FACTS N-CRM.\nThe features available for specifying the Efficacy Response model are the same as the Toxicity Response model (see above) – with the exception that the Toxicity Response includes an option for modeling ordinal toxicity, there is no corresponding ordinal efficacy option.\n\n\n\n\n\n\nFigure 48\n\n\n\nThe same options for deriving the prior are available as for the Toxicity Response Model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prior-pseudo-subjects",
    "href": "documentation/v71/userguides/crm.html#sec-prior-pseudo-subjects",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Prior Pseudo-subjects",
    "text": "Prior Pseudo-subjects\nThis option allows “prior data” or “pseudo data” to be specified that will be included in every analysis. This is equivalent to the model being fitted with the parameter prior to this pseudo data and the resulting posterior being the new prior, but it is easier and quicker to include the pseudo subject with the real data and do one analysis.\nThe user selects which dose levels at which to include the data and specifies the number of pseudo/prior subjects/observations and the number of toxicities. These are allowed to be fractional, and observations can be at dose levels not being tested in the trial. In each analysis the observed data is augmented with this specified data and the parameters of the toxicity response estimated.\nIf there is an efficacy endpoint as well as toxicity endpoint, pseudo subject data is specified separately for the two endpoints (not surprisingly!). If the data is to be analyzed as “2 groups” pseudo subject data is also specified separately for the two groups.\nThe effect of this data on the prior can be visualized by the “Update Plot” function that estimates the parameters of the toxicity response and plots the curves of 100 samples drawn from the posterior estimates of the parameters of the model.\nThe plot of the samples can either be viewed as Pr(Tox) vs Dose Strength or Log-odds(Tox) vs relative dose (“x-hat”).\n\n\n\n\n\n\nFigure 49: Specifying prior pseudo subjects\n\n\n\n\n\n\n\n\n\n\nExample of effect of pseudo subjects on prior\n\n\n\n\n\nPrior Only\n\n\n\nPrior plus 0.5/0 subjects toxicities on dose 1 and 1/0.5 on dose 8.\n\n\n\nPrior plus 3/0 subjects / toxicities on dose 1 and 3/1.5 on dose 8.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation-rules",
    "href": "documentation/v71/userguides/crm.html#allocation-rules",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Allocation Rules",
    "text": "Allocation Rules\nThe Allocation Rule sub-tab is depicted below (Figure 50); it allows the user to select and set the parameters for the allocation rules.\nThere is an option to mimic the original Neuenschwander paper and rely on just the toxicity model and the overdose control to guide allocation – i.e. allocate to as close to the estimate of the MTD as we can limited only by the overdose control limits. This is “Use only overdose control”. Normally however we think the clinical will want to impose additional allocation rules.\nThe next dose allocated is a combination of the current target dose, and 2 possible maximum allocatable doses:\n\nThe target dose is the dose with the highest probability of being the target: the dose with the greatest estimated probability that its toxicity rate lies in the target toxicity band, or the dose nearest or highest below the MTD depending on the options selected on the Study &gt; Toxicity tab.\nThe highest dose that meets the overdose criteria (if any): the highest dose that does not have a posterior probability that its toxicity rate lies in the excess & unacceptable toxicity bands or unacceptable toxicity bands above the threshold specified on the Study &gt; Toxicity tab.\nThe current cleared dose and how far above that dose can be allocated to as defined by the specified allocation rules (if any).\n\nThe next dose to be allocated to is the lowest of these 3 doses.\nIt is possible to specify a “run-in” phase before this dose escalation phase applies. A run-in phase has a fixed sequence of doses and cohort sizes (typically smaller than the cohort size used in the escalation phase) and lasts up to the first observed toxicity or the end of the sequence of doses.\n\n\n\n\n\n\nFigure 50\n\n\n\nIf the user selects to use an “Initial run-in” then there are 3 run-in types that can be selected from:\n\n“Simple run” (the only type available before FACTS 6.3): cohorts are of a single size and the dose sequence up to the “Run-in cannot go beyond” dose (if specifed) is either:\n\nat every dose starting at the specified “initial dose” if explicit doses have been defined on the the Study &gt; Treatment Arms tab,\nat the dose increment intervals defined in the allocation rules if finely spaced doses have been defined on the Study &gt; Treatment Arm tab.\n\n“Custom run-in” where the user specifies which doses are selected (leaving the “number of subjects” at a dose at 0 mans it is not selected) and at each dose how many subjects are allocated in the cohort at that dose.\nSmall cohort pre-escalation, cohorts are of a single size and the dose sequence follows the dose increment intervals defined in the allocation rules up to the “Run-in cannot go beyond” dose (if specifed).\n\n“Simple run-in” and “Small cohort pre-escalation” the user specifies the “small cohort size” and there is an option to specify a top dose that the “Run-in cannot go beyond”.\nFor all run-in schemes there are options:\n\nEnd run-in on 1st category 2 toxicity: The default for the simulation is to stop the run-in when the first full toxicity is observed, there is an option to instead stop when a lower grade toxicity is observed – a category 2 toxicity. If this option is selected, the simluation of virtual subjects is extended to include the simulation of category 2 tocxicity as well as category 3.\nInclude run-in subjects in overall maximum\nWhen run-in ends expand last cohort to full size\n\nIf this is not set then N-CRM model is applied and the next cohort is allocated as close to the target as possibly, restricted by the overdose restrictions and not allocating any higher than the dose reached in the run-in.\nIf this option is set then last allocated small cohort is treated as the start of a full cohort and the remaining subjects are allocated at the same dose. The N-CRM model is then applied and the next cohort allocated according to the overdose restrictions and allocation rules. [Note it is possible that after observing no toxicities in the run-in or in the expanded cohort, that the overdose control will force a de-escalation in dose, depending on the priors for the model parameters and the overdose control limits]\n\n\nIf simulating 2 groups, and a run-in is specified, group 2 will only use a run-in if on the Study tab the option “Recruit Group 1 and Group 2 together” has been selected. Otherwise only group 1 will use the specified run-in, once group 2 starts it starts with the full cohort (or if open enrollment is being used, the full number of subjects to clear the dose). If using a run-in and recruiting groups 1 and 2 together then both use the same run-in rules, and both will stop on the first toxicity regardless of which group the toxicity occurs in.\nIf used, the Allocation rules have the effect of setting a Highest Allocatable Dose that specifies the highest dose level that can be allocated to by the allocation rules.\n\nAt the start the Highest Allocatable Dose is the user specified Starting dose level.\nA dose is not ‘cleared’ until ‘The minimum cohorts on/near a dose before cleared’ have been allocated to it. If this is set to greater than one, then the specified number of cohorts must have been allocated to the current dose before it is ‘cleared’. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nIf using Open enrolment, this parameter is ’The minimum subjects on a dose before cleared’ and refers to the number of complete subjects.\nThe user can specify that Dose not cleared if proportion toxic on dose &gt; and a maximum level of toxicity that can be tolerated for the current dose to be ‘cleatred’. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe user specifies whether the dose increment rules are to be specified in terms of the number of dose levels that can be incremented or in terms of the ratio by which the dose strength can be increased. If using “dose strength” it is up to the user to ensure that the specified ratio is sufficient to allow all doses to be reached. For example if in the dose range there is a dose of strength 100 and the next dose is of strength 200, then if any dose increment rule is specified that doesn’t allow the dose strength to at least double (using “ratio of dose strength” of less than 2) then the rule will prevent escalation from the 100 dose to the 200 dose.\nThe maximum amount the dose can be incremented can be specified in 3 ways:\n\nSingle value: For all doses, once a dose has been cleared the next ‘highest allocatable dose’ is the dose above the cleared dose by the specified increment – either a number of dose levels or by a proportion of the dose strength.\nBy total number of toxicities: With this rule the maximum dose increase allowed depends on the total number of toxicities that have been observed in the trial so far. The user specifies the maximum increment (in terms of the number of dose levels or the maximum proportional increase in dose strength) when no toxicities, one toxicity, or more than one toxicity has been observed.\n\n\nFigure 9-5 Maximum dose increment varying by number of toxicities observed\n\n\n\n\n\n\nFigure 51: Maximum dose increment varying by number of toxicities observed\n\n\n\n\nBy dose levels: the maximal permitted increment is defined in terms of the number of dose levels or the proportional increase in dose strength that the dose can be increased, in up to 3 bands of dose strength. The user defines the upper and lower doses of the middle increment range, the lower range is then from the lowest up to this band and the upper band is from the top of the middle band to the highest dose. The user then specifies the maximum number of dose levels or the maximum proportional dose strength that can be incremented if the current dose is within each of these bands.\n\n\n\n\n\n\n\nFigure 52: Maximum dose increment varying by dose level\n\n\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently allocated dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\nThe “Fastest Possible Dose Escalation” graph shows the dose allocation permitted by the rules if no toxicities are observed. Or the graph can be changed to show the allocation if a toxicity occurs at a specific dose during the run-in.\nIf there are 2 groups and recruitment to the second group is delayed (either until the first group is complete, has reached a specific dose, or tested a specified number of subjects). Then starting dose for the second dose can be specified as\n\n\n\n\n\n\nFigure 53: Design - Allocation Rule Initial Dose Second Group\n\n\n\n\nAllocation rules when using a fine grained dose range\nWhen a fine grained dose range is being used, the specification of the allocation rules change to accommodate the fact that the trial is no longer stepping up a few pre-defined dose levels. As with the explicit doses, the allocation rules work with the notion of a Currently Permitted Maximum Dose (CPMD).\n\nAt the start the highest allocatable dose is the user specified Starting dose strength.\nA Run-in phase can be specified, this will always begin at the start dose and allocate ‘small cohorts’ following the maximum increment rules (this is different from when there are explicit doses – with explicit doses, the simple run-in simply allocates successive small cohorts to successive doses, ignoring the maximum permitted increment) for a simple run-in or small cohort pre-escalation, or the specified allocation pattern for a custom run-in, until the first toxicity is observed.\nThe degree of increment and specification of what counts as a ‘close’ dose, can be done either in Dose Strength or Ratio of dose strength.\nA dose is not cleared until ’The minimum cohorts on a dose before incrementing’ have been allocated to it. Typically this parameter would be set &gt; 1 when small cohort sizes are being used (such as 1 or 2 subjects per cohort) and/or when cohorts are assigned before a previous cohort’s results are available.\nThe user can specify that Dose not cleared if ppn toxic on/near dose &gt; and a maximum level of toxicity that can be tolerated before the dose is cleared. This is of course somewhat of a ‘belt and braces’ approach – it being likely that the model would not want to increment if the proportion of toxicity is high, and indeed if an undesired behavior is observed in simulations, the behavior of the model and the effect of the prior should be investigated.\nThe Max ratio of dose strengths considered as near or Delta in dose strength considered as near defines a margin such that when evaluating whether a dose that has been allocated to counts as “cleared”, then cohorts allocated within that margin all count as being “on” the dose to allow incrementing.\nThe Maximum increment selected by option allows the user to specify varying maximum increments either dependent on the current cleared dose (Dose strength), or on the Total number of toxicities that have been observed, as follows:\n\nSingle value: the amount by which the highest allocatable dose can be above the current cleared doses is constant throughout the trial. (This simple rule can work well when combined with overdose control. The two more complex rules are essentially trying to achieve the same thing as overdose control but are more simplistic and it may be confusing as to whether the allocation rule or overdose control is preventing escalation at any given moment).\nTotal number of toxicities: the amount by which the highest allocatable dose is above the current cleared dose is specified separately for whether zero, one or two or more toxicities have been observed in the entire study.\nDose strength: The user specifies:\n\nthe increment at low doses;\nthe increment at medium doses, along with the upper and lower dose strengths that define the bounds of what constitutes a ‘medium dose’; and\nthe increment at high doses.\n\n\nIf toxicities are observed that causes the N-CRM to allocate a dose below the current cleared dose, then there are two options for what happens to the current cleared dose:\n\nit is left unchanged and further increments are applied relative to the Highest cleared dose.\nit is reduced to the dose level selected by N-CRM, and further increments are applied relative to the Most recently cleared dose.\n\nIf a Control arm has been included, the user must specify the number of subjects per cohort to be allocated to Control. (The Control arm will never be the MTD and thus whole cohorts will never be assigned to it, so it must have a fixed number of subjects assigned from each cohort).\n\n\n\n\n\n\n\nFigure 54: Allocation rules with a finely spaced doses",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-backfill-allocation",
    "href": "documentation/v71/userguides/crm.html#sec-backfill-allocation",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Backfill Allocation",
    "text": "Backfill Allocation\nIf the trial is using open enrolment then “Backfill” allocation is an option. Backfilling is enabled on the Study &gt; Study Info tab, where the maximum number of subjects that can be allocated when backfilling is specified.\nBackfilling is the allocation of subjects to a dose below the current dose when the maximum number of subjects on the current dose without final results has been reached.\nThe Backfill Allocation tab allows detailed control of when backfilling can be used. The user specifies:\n\nThe maximum overall number of subjects that can be on a dose for backfilling to be allowed to that dose.\nThe maximum number of subjects that can be allocated to a dose by backfill.\nThe maximum number of dose levels below the current dose that can be used for backfilling (the highest that can be backfilled to will be used)\nThe lowest dose strength that can be backfilled to.\nWhether or not the current escalation dose is a candidate for backfilling as long as the maximum number of subjects in their DLT period (set in the Study &gt; Study Info tab) is not exceeded (also known as “frontfilling”).\nIf frontfilling is enabled, whether these subjects should count towards the backfill allocation cap or the regular study allocation cap (specified in the “Study/Study Info” tab).\n\n\n\n\n\n\n\nFigure 55: Backfill Allocation",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-stopping-criteria",
    "href": "documentation/v71/userguides/crm.html#sec-stopping-criteria",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Stopping Criteria",
    "text": "Stopping Criteria\nThe simulation will always stop when the maximum number of cohorts or subjects has been allocated.\nThe user may also specify that the study may stop early in terms of the amount of information that has been gathered about the target dose. In the N-CRM, the target dose can either be the Maximum Tolerated Dose or the “the allowed dose that has the highest posterior probability of having a toxicity rate in the Target Toxicity Band”; we use the label MTD (Maximum Target toxicity Dose) for this target dose too, for brevity and familiarity from previous CRM methods.\nNote: if overdose limits have been set (see this section), then doses with posterior probabilities of having a toxicity rate in the Excess Toxicity or Unacceptable Toxicity bands that are greater than the specified thresholds, are disallowed for both dose allocation and selection as MTD.\nTo enable early stopping, the user must select “Rules for stopping trial early” there are then 2 rules which if selected are always “AND”ed together, and a further block of rules, the result of which (if specified) are “AND”ed with the first two rules. If more than one rule is selected within the block these may be either logically “AND”ed together or “OR”ed together to give the result of the rules in the block.\nThe two first standalone rules are:\n\nIf a “Required number of cohorts/subjects near MTD” rule is set then the trial will only stop early if at least this number of cohorts or subject has been allocated to the MTD dose or nearby doses. In the box above the “Count as MYD doses differing from MTD by less than or equal” allows how far away a dose can be and for cohorts/subjects on those doses to count towards this total. This can be set to 0 to count only cohorts/subjects on the MTD. It is provided for when fine grain dosing or a large number of explicit doses have been specified. If specified this rule this must always be met along with any other rules set for the trial to stop early.\nIf a “Minimum number of cohorts/subjects accrued” has been set then this specifies a lower limit on the sample size before early stopping is allowed. It is provided to allow a higher number of subjects on the MTD if one of the first doses appears to be the MTD. If specified this rule this must always be met along with any other rules set for the trial to stop early.\n\n\n\n\n\n\n\nFigure 56: Design - Stopping Criteria sub tab\n\n\n\nThe available stopping rules are:\n\nRequired number of cohorts/subjects on/near MTD: Once the specified number of cohorts has been allocated to the dose currently determined to be the MTD, the trial may stop.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\nThe required number of cohorts/subjects can be “near” (rather than “on”) the MTD if a “Count as MTD” interval is defined. This interval is either defined by an interval of “dose strength” using the dose strengths defined on the Study &gt; Treatment Arms tab, or by an interval of “dose strength ratios”. Which is used will correspond to the Selection for “Dose allocation rules apply to” on the Allocation Rules tab. A difference of 0, or factor of 1 can be used to only count cohorts/subjects actually on the selected MTD dose.\nIf open enrolment is being used, there is an option to only pause accrual not stop it when the stopping rules have been met, in case the final results of any subjects that were not complete at the time the stopping rules were met cause the rules to be no longer met (for example by having results that change the dose estimated to be the MTD). If this option is selected then the stopping rules are re-assessed when all the current subjects are complete and the trial resumed if the rules are no longer met. Note this option is not required for cohort enrolment as stopping rules are only evaluated between cohorts.\n\nMinimum cohorts/subjects accrued: this rule ensures that a minimum overall number of cohorts have been tested before the trial is allowed to stop. It makes no sense to use this rule on its own (it would effectively just lower the overall study cap) it should only be used in conjunction with other stopping rules.\n\nThis limit is defined in subjects if open enrollment rather than cohort enrolment is being used.\n\nThe Additional Stopping Rules:\n\nRange/Ratio of dose strengths within the credible interval is less than or equal to: This is only enabled if on the Study &gt; Study Info tab, the Type of Target has been set to “Target a single dose”. In this case the credible interval of the dose range that might contain the MTD is calculated, and this option allows the user to specify the width of the credible interval in “dose strength” to consider. How this works is a little counter intuitive: in each MCMC sample the engine determines which dose is the MTD and each doses’ Pr(MTD) is the proportion of the samples it was the MTD. The engine then calculates the minimum range of dose strengths required so that the sum of Pr(MTD) of the doses in the range exceeds 1 – Alpha. (So if a single dose has Pr(MTD) &gt; 1-Alpha, the CI has zero width). If the xhats are log(dose strength) then the minimum range of log(dose strength) is found and exp() of this range returned.\n\nThe stopping rule is met if the width of the CI returned is less than that specified by the user.\nIf fine grain dosing is being used then the width of the target credible interval is defined as a dose strength ratio rather than a number of doses.\n\nStop if adding another DLT free cohort does not alter the MTD: this rule is evaluated by analyzing the existing data supplemented by an additional cohort of a specified size where all the responses are no-toxicity; if this results in no change in selection of MTD then this stopping rule is met. If the study is using Open Enrolment, the user additionally specifies the size of the ‘virtual’ cohort to use.\nProbability of dose being in the target band greater than: in order to stop, the MTD’s posterior probability that its toxicity rate lies in the Target Toxicity band must be at or above the required threshold.\nMaximum Cohorts/Subjects near MTD: this option is supplied for use if the additional stopping rules are being OR’d together. (When they are AND’d together it simply functions the same way as the “Required Cohorts on MTD and stopping will not occur until the higher of the two targets is met).\nJoin condition: if more than one additional stopping rule is selected, whether only any one of them needs to be met for the trial to be able to stop (Join condition = “OR”), of if all of the selected rules need to be met for the trial to be able to stop (Join condition = “AND”).\n\n\nThe study will also be stopped if there are no allowed doses by the overdose rules. However this can occur early in the study if a toxicity is observed in the first or second cohort. It is likely that in practice the clinical team would override the design stopping the study. Whilst it is difficult to fully represent the team’s decision making, a simple rule is included that is intended to approximate it:\n\nMinimum toxicities required before stopping: This allows a requirement to be specified to observe a ‘minimum number of toxicities’ before the trial stops. If no doses are allowed by the overdosing rules, cohorts are assigned to the lowest dose until the minimum number of toxicities are observed, the stopping rules are met, or doses become allowable again after the model is updated after seeing no toxicities.\n\n\n\n\n\n\n\nFigure 57: Stopping Criteria tab with open enrolment and finely spaced doses\n\n\n\nIf an additional efficacy endpoint is used, the MTD stopping criteria refer to when the trial jumps from assigning subjects with the aim of finding the MTD (“MTD phase”) to assigning subjects with the aim of finding the MED (“MED phase”). In the MED phase, it is possible that because of new data being observed, the MTD stopping rules are no longer met and the trial switches back to the MTD phase. A new stopping rule for the MTD phase is added, “Maximum subjects used to determine the MTD”. After this number of subjects has been enrolled, the trial switches from MTD to MED phase and there is no going back.\nIn the MED phase, there are several rules for stopping early (i.e. before the maximum sample size of the trial):\n\nMaximum cohorts/subjects on MED. This behaves analogous to Required number of cohorts/subjects on/near MTD in the MTD stopping rules, with the exception of not using a concept of “near” doses.\nNumber of doses within the credible interval is less than or equal to with the sub-option Alpha for width of credible interval. This behaves analogous to Range/Ratio of dose strengths within the credible interval is less than or equal to in the MTD stopping rules.\nProbability of dose being MED greater than. This behaves analogous to Probability of dose being in the target band greater than in the MTD stopping rules.\n\nA special case arises when the MED estimate is larger than the MTD estimate. If that is the case, subjects are allocated to MTD or the highest cleared dose (whatever is smaller) even in the MED phase. The option “If MED &gt; MTD: Continue until subjects near MTD reach” specifies how many subjects should be assigned to MTD in the MED phase before stopping the trial (and therefore giving up hope that the MED is a safe dose).\n\n\n\n\n\n\nFigure 58: Stopping Criteria tab with open enrolment and both a toxicity and efficacy endpoint",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-simulation-with-variants",
    "href": "documentation/v71/userguides/crm.html#sec-simulation-with-variants",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Simulation with Variants",
    "text": "Simulation with Variants\nThe only difference that specifying design variants (see this section) introduces is to create additional scenarios – one for each Virtual Subject Response (VSR) profile for each variant. For example if there were 5 VSR profiles and then 4 variants (different numbers of maximum cohorts) specified then would now be 20 scenarios in total. The scenario names have “_Var1”, “_Var2”, … appended to them. Once simulations have been run and the .facts file has been saved and re-opened the scenarios are listed in alphabetical order.\n\n\n\n\n\n\nFigure 60: Simulation tab showing variants",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-to-run-simulations",
    "href": "documentation/v71/userguides/crm.html#sec-to-run-simulations",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "To run simulations",
    "text": "To run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user may not modify any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-how-many-simulations-to-run",
    "href": "documentation/v71/userguides/crm.html#sec-how-many-simulations-to-run",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%). This degree of accuracy usually unnecessary for dose escalation designs.\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-simulation-results",
    "href": "documentation/v71/userguides/crm.html#sec-simulation-results",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Simulation results",
    "text": "Simulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nOpen Results Folder: Opens the windows “File Explorer” tool in the results folder of the currently selected scenario. This makes it very easy to locate and open results files.\nCopy to Clipboard: will copy the values displayed in the summary to the clipboard as “CSV” data, enabling it to be pasted straight into a spreadsheet.\nAll: A window containing all the summary results columns\nHighlights: a separate window with the results shown on the main tab\nAllocation, Observed: summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity: summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc: summary results of the posterior probabilities of the properties of interest\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\n\nExplore Results: offers three options:\n\n“Show Per Scenario Highlighted Scenario Graphs” that shows the graph of the simulation results for a specific scenario (see Section 12 below for a description of these graphs)\n“Show Across Scenario Graphs” that displays a trellis plot of summary graphs for each variant and each scenario (see Section 13 below for a description of these graphs).\n“Compare Scenarios in AIRSHIP” to open the simulation results in R with the AIRSHIP R-shiny graphing app. See the AIRSHIP User Guide for details.\n\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nDesign Report: it uses an R script and R libraries to generate a MS Word document describing the design. See the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.\n\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options corresponding to the above options that can be reached from the buttons, in what is sometimes a more ergonomic manner.\n\nMCMC Settings\n\n\n\n\n\n\nFigure 61\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nThe Number of samples per imputation value only applies to analyses using imputed data from a longitudinal model and is irrelevant for N-CRM, hence it is greyed out.\nIf the Number of MCMC files to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output, allowing the user to check convergence.\nThe MCMC output thinning parameter can be used to reduce the amount of data output to the MCMC file. It does not reduce the amount of MCMC samples used within the model fitting.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/crm.html#sec-facts-grid-simulation-settings",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-detailed-simulation-results",
    "href": "documentation/v71/userguides/crm.html#sec-detailed-simulation-results",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 62) will reveal more detailed results about the simulation.\n\n\n\n\n\n\nFigure 62: Detailed Simulation Results for a particular scenario\n\n\n\nRight-clicking on a row displays a context menu from which the user can view other columns (the default are the “highlihgts” columns) and also view the corresponding cohort results:\n\n\n\n\n\n\nFigure 63: Cohort Results for a particular simulation\n\n\n\nRight clicking on the cohort results, displays a context menu from which the user can view other columns (the default are the “highlights” columns).",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-sim-aggregation",
    "href": "documentation/v71/userguides/crm.html#sec-sim-aggregation",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 64\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose, plus two extra rows for ‘below lowest’ and ‘above highest’.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nIf there is no value for the below lowest or above highest rows, they will be left blank.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. cohortsNNN.csv files are aggregated into a single agg_[pivot_]cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-design-report",
    "href": "documentation/v71/userguides/crm.html#sec-design-report",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Design Report",
    "text": "Design Report\nThis button becomes enabled once there are simulation results, it uses an R script and R libraries to generate a MS Word document describing the design.\nSee the FACTS Design Report User Guide for details of what R packages need installing, how FACTS needs configuring to use the correct R instance, how the generate_report() function is run, and where the resulting report can be found.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "href": "documentation/v71/userguides/crm.html#sec-results-when-2-groups-have-been-simulated",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Results when 2 groups have been simulated",
    "text": "Results when 2 groups have been simulated\nWhen 2 groups have been simulated, the results displayed on the Simulation tab and all the directly viewable summary tables under the “Show other columns” button are from Group 1.\nTo see the results from Group 2 you need to first display the Group 2 highlights – either by selecting “Group 2” on the “Show other columns” menu or after right clocking on a row of results.\nOnce the Group 2 “highlights” results are being displayed, the other sets of summary results can be viewed by right clicking on a row of results in the Group 2 “hightlights” window.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-highlights",
    "href": "documentation/v71/userguides/crm.html#sec-highlights",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Highlights",
    "text": "Highlights\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nSelect\n1\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nSettings\n1\nDisplays an icon showing the status of the simulation results.\n\n\nStatus\n1\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nNum Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the “MTD” at the end of the study. In CRM this is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on “excess” or “unacceptable” toxicity have been specified, then doses with posterior probabilities above these limits are excluded from being chosen as MTD. (Note this is different from the calculation of MTD used in Neuenschwander, Branson, and Gsponer (2008) where doses with posterior probabilities above these limits were not excluded from being selected as MTD.)\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nIncluded if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the “MED” at the end of the study. This is the dose with the highest posterior probability of being the dose that is the ‘highest dose below’ / ‘nearest’ / ‘lowest dose above’ (as specified on the Study &gt; Effiacy tab) to the target efficacy rate.\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration in weeks of the simulated trials.\n\n\nSD Duration\n1\nThe standard deviation over the simulations of the duration in weeks of the simulated trials.\n\n\nMean Lost\n1\nIf the trial uses open enrollment, this is the number of subjects that could not be allocated a dose because the number of subjects treated but not yet complete had reached the specified “Maximum subjects without final result” limit. Otherwise the value is 0.\n\n\nSD Lost\n1\nThe standard deviation over the simulations of the number of subjects lost.\n\n\nPpn(All Tox)\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nPpn(early Success)\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nPpn(Cap)\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nPpn MTD Under\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Underdosing” toxicity range.\n\n\nPpn MTD Target\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Target” toxicity range.\n\n\nPpn MTD Excess\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Excess” toxicity range.\n\n\nPpn MTD Unacc\n1\nThe proportion of the simulations where true toxicity rate of the selected MTD dose lay in the “Unacceptable” toxicity range.\n\n\nPpn Correct Under\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Underdosing” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Target\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Target” range. This will be 0 if none of the doses in that scenario had a true toxicity that fell in that range.\n\n\nPpn Correct Excess+Unacc\n1\nThe proportion of the doses across all the simulations that were correctly identified as being in the “Excess” or “Unacceptable” ranges. This will be 0 if none of the doses in that scenario had a true toxicity that fell in those ranges.\n\n\nPpn MED Under\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or less. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over))\n\n\nPpn MED Over\n1\nIncluded if efficacy is being simulated. This is the proportion of the times the selected Minimum Efficacious Dose’s true efficacy rate was the target rate or more. (Note if the scenario contains a dose whose simulated true efficacy rate is exactly the minimum target rate selections of this dose will count as both MED Under AND Med Over)",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-allocations-observed",
    "href": "documentation/v71/userguides/crm.html#sec-allocations-observed",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Allocations, Observed",
    "text": "Allocations, Observed\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMean Subj.\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Toxic\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn. Eff\n1\nIncluded if efficacy is being simulated. This is the average proportion of the subjects recruited that experienced efficacy in this scenario.\n\n\nSD Ppn. Toxic\n1\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nSD Ppn Eff\n1\nIncluded if efficacy is being simulated. This is the standard deviation of the proportion of efficacy across the simulations\n\n\nTrue Ppn. Toxicity\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Ppn. Eff\n1\nIncluded if efficacy is being simulated. This is the average true probability of efficacy over the simulations.\n\n\nNum Phase 1\n1\nIncluded if efficacy is being simulated. This is the mean (over the simulations) of the number of subjects recruited in the first phase (up to the MTD stopping criteria being met) in this scenario.\n\n\nSubj. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subjects: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nCat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 2 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if an Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nCat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the mean (over the simulations) of the number of category 2 toxicities observed at each dose.\n\n\nSD Cat 4 Tox per Dose: &lt;dose&gt;\nOne per dose\nOnly included if a 4 point Ordinal toxicity outcome is being simulated. This is the standard deviation of the number of category 2 toxicities observed at each dose across the simulations.\n\n\n80% Num. Subj.\n1\nThe 80th percentile of the distribution of the number of subjects recruited in the simulations",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-fitted-toxicity",
    "href": "documentation/v71/userguides/crm.html#sec-fitted-toxicity",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nSD Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model.\n\n\nMean Alpha Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nSD Mean Alpha Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model.\n\n\nAlpha[234] Tox\n2-3\nIf Ordinal toxicity is being simulated, instead of Mean Alpha and SD Mean Alpha columns, there are pairs of columns Mean Alpha2, SD Mean Alpha2, … for Alpha2 & Alpha3 if a 3 point ordinal scale is being used and Alpha2, Alpha3 & Alpha4 if a 4 point ordinal scale is being used. These are the means (over the simulations) of the mean and standard deviation of the various Alpha coefficients in the BLRM model when fitting to the ordinal outcome.\n\n\nSD Alpha[234] Tox\n2-3\nsee row above\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Toxicity: &lt;Dose&gt;\nOne per dose\nThis is the true simulated toxicity rate for each dose in the scenario\n\n\nMean Fit Tox Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose.\n\n\nMean Fit Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the toxicity rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prmtd-etc",
    "href": "documentation/v71/userguides/crm.html#sec-prmtd-etc",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Pr(MTD) Etc.",
    "text": "Pr(MTD) Etc.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis is the name of the scenario simulated. If only toxicity has been simulated this is simply the name of the Toxicity response profile to be simulated, if both efficacy and toxicity have been simulated, it is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study. The dose chosen as MTD is the minimum of: 1) the dose with the highest posterior probability of having a toxicity rate in the ‘Acceptable’ band and 2) the highest cleared dose. If limits on excess or unacceptable toxicity have been specified, then doses with posterior probabilities above the specified limit, of having a toxicity rate in those bands are excluded from being chosen as MTD.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum [Tox] Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MTD was rule was met at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 1” (see below).\n\n\nNum [Tox] Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MTD is less than the specified number. [This stopping rule only evaluated if targeting an MTD rather than a toxicity band] If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 2” (see below).\n\n\nNum [Tox] Stop Rule 3\n1\nNumber of times the Pr(MTD) – that the probability that the dose was MTD or dose’s toxicity rate lies in the target toxicity rate band - met the stopping rule threshold at the end of a simulation. If efficacy is being simulated the column heading is “Num Tox Stop Rule”, and is the number of times this rule was met at the end of the “target the MTD” phase of the trial. The number of times this rule is met at the end of the MED phase is listed in the Pr(MED) results under the heading “Num Eff Stop Rule 3” (see below).\n\n\nNum Stop Rule 4\n1\nNumber of times that observing another cohort with no toxicities would not change the selected MTD stopping rule was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nNum Stop Rule 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation. This stopping rule does not have a “end of MED phase” counterpart hence does not acquire a the word “Tox” in the heading if efficacy is being simulated.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose&gt;\nOne per dose\nAs MTD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nOSD+ Selection: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. As OSD selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nOSD+ Selection: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where a dose above the tested range of doses was selected as the optimum selected dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. For each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(Under): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPost CE MTD+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the maximum tolerated dose.\n\n\nPost CE MTD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was the maximum tolerated dose allowing for a probability that the MTD is at a dose below or above the range of tested doses.\n\n\nPost CE MTD+: plus\n1\nThe posterior probability, after the results of the Cohort Expansion, that a dose above the tested range of doses was the maximum tolerated dose.\n\n\nPost CE OSD+: minus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose was selected as the optimum selected dose allowing for a probability that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nOnly included if efficacy is being simulated. The proportion of simulations where, after the results of the Cohort Expansion, a dose above the tested range of doses was selected as the optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-fitted-efficacy",
    "href": "documentation/v71/userguides/crm.html#sec-fitted-efficacy",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Fitted Efficacy",
    "text": "Fitted Efficacy\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nBeta Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nSD Beta Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response rate.\n\n\nAlpha Eff\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nSD Alpha Eff\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response rate.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy response model for each dose.\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates of the efficacy rate across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Efficacy: &lt;Dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nMean Fit Eff Lower: &lt;Dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.\n\n\nMean Fit Eff Upper: &lt;Dose&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sec-prmed-etc",
    "href": "documentation/v71/userguides/crm.html#sec-prmed-etc",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Pr(MED) Etc.",
    "text": "Pr(MED) Etc.\nThis summary table of results is only available if efficacy has been simulated.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario, which is formed from the concatenation of the toxicity response profile name and the efficacy response profile name.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study. The dose chosen as MED is the dose with the highest posterior probability of having a efficacy rate nearest the target rate / is the highest dose with a rate below the target rate / is the lowest dose with a rate above the target rate – as specified on the Study &gt; Efficacy tab.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose chosen as OSD is the dose with the highest posterior probability of being either the Minimum Effective Dose, or if that dose is above the MTD, of being the MTD.\n\n\nNum Eff Stop Rule 1\n1\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Eff CI\n1\nThe mean (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nSd Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval defined for efficacy stopping rule 2.\n\n\nPr(MED) &lt;dose&gt;\n\nThe mean (over the simulations) of the posterior probability that each dose is the MED.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose&gt;\nOne per dose\nAs MED Selection, but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the ‘Best’ dose at the end of the study. The dose chosen as ‘Best’ is the dose with the highest posterior probability that subjects treated with that dose experience efficacy and do not experience toxicity.\n\n\nPr(MED+): minus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose below the tested range of doses was the MED.\n\n\nPr(MED+): &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities that each dose was the MED. As Pr(MED) but with two extra columns for the case of the target being below the lowest (non-control) dose and above the highest dose.\n\n\nPr(MED+): plus\n1\nThis is the mean (over the simulations) of the posterior probabilities that a dose above the tested dose range is the MED.\n\n\nPost CE MED+: minus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose below the tested range of doses was the minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations, after the results of the Cohort Expansion, that each dose was selected as the minimum efficacious dose allowing for the possibility that the MED is at a dose below or above the range of tested doses.\n\n\nPost CE MED+: plus\n1\nThe proportion of simulations, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after the results of the Cohort Expansion, that each dose was selected as the optimum selected dose allowing for the possibility that the OSD is at a dose below or above the range of tested doses.\n\n\nPost CE OSD+: plus\n1\nThe proportion of simulations where, after the results of the Cohort Expansion, that a dose above the tested range of doses was selected as the optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation-box-and-whisker-plot",
    "href": "documentation/v71/userguides/crm.html#allocation-box-and-whisker-plot",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Allocation Box and Whisker Plot",
    "text": "Allocation Box and Whisker Plot\n\n\n\n\n\n\nFigure 68: Allocation box and whisker plot\n\n\n\nThis graph displays a box and whisker plot of the number of subjects allocated to each dose over the simulations. These plots show:\n\nThe mean allocation over all simulations at each dose plotted as a solid line.\n\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\n\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#response-and-subject-allocation",
    "href": "documentation/v71/userguides/crm.html#response-and-subject-allocation",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Response and Subject Allocation",
    "text": "Response and Subject Allocation\n\n\n\n\n\n\nFigure 69: Response and subject allocation graph\n\n\n\nThis plot shows the mean subject allocation to each dose as a blue bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation and the mean fitted toxicity separately.\nIf efficacy is being simulated, then lines for the mean fitted efficacy and true efficacy are also shown.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#response-and-mtd-distribution",
    "href": "documentation/v71/userguides/crm.html#response-and-mtd-distribution",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Response and MTD Distribution",
    "text": "Response and MTD Distribution\n\n\n\n\n\n\nFigure 70: MTD distribution and response graph\n\n\n\nThis plot shows the proportion of times each dose has been selected as the MTD as a brown bar, along with lines showing the mean estimated toxicity and the simulated ‘true’ toxicity. The ‘error’ bars on the mean estimated toxicity are the 95% interval of the mean estimates across the simulations.\nThere is an options to display the “MTD+” distribution. This is the proportion of times each dose has been selected as the MTD when a dose below the lowest dose and a dose above the highest dose is included.\nIf 2 groups are simulated, there are two versions of the graph showing the distributions of allocation for each group separately.\nIf efficacy is being simulated then there are versions of the graph showing the histograms showing the distribution of selection of MTD, MED, OSD, and TE targets. The plot of the MTD shows the true and mean fitted toxicity, the plot of the MED shows the true and mean fitted efficacy and the plots of the OSD and TE show both the true and mean fitted toxicity and efficacy.\n\n\n\n\n\n\nFigure 71: Response and TE Distribution for a group",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities",
    "href": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Toxicity Interval Probabilities",
    "text": "Toxicity Interval Probabilities\n\n\n\n\n\n\nFigure 72: Toxicity interval probabilities graph\n\n\n\nThis plot shows the posterior probability for each dose that it’s toxicity rate lies in each of the four toxicity intervals as a stacked bar chart.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#observed-toxicity-and-allocation",
    "href": "documentation/v71/userguides/crm.html#observed-toxicity-and-allocation",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Observed Toxicity and Allocation",
    "text": "Observed Toxicity and Allocation\n\n\n\n\n\n\nFigure 73: Observed Toxicities and Allocation Histogram\n\n\n\nThis graph shows the mean allocation to each dose and the mean number of toxicities observed at each dose. The total height of the bar shows the total allocation, and the red section of the bar shows the proportion that experienced toxicity.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sample-size-mtd-histogram",
    "href": "documentation/v71/userguides/crm.html#sample-size-mtd-histogram",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Sample Size MTD Histogram",
    "text": "Sample Size MTD Histogram\n\n\n\n\n\n\nFigure 74: Sample Size MTD Histogram\n\n\n\nThis graph shows the number of times different sample sizes (number of subjects tested) were observed across the simulations. Each bar is shown as a stacked plot with each color indicating the proportion of times a particular dose was selected as the MTD in the simulations that ended with a particular sample size.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#alloc-history-summary",
    "href": "documentation/v71/userguides/crm.html#alloc-history-summary",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Alloc History Summary",
    "text": "Alloc History Summary\n\n\n\n\n\n\nFigure 75: Allocation History Summary Graph\n\n\n\nThis graph overlays all the allocation histories of those simulations for which “cohorts” files have been output. The lines show the “route” the dose escalation followed and the circles show the dose selected as MTD at the end of the simulation. The lines are heavier and the circles darker the more simulations followed the same route or made the same selection.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nThis graph allows a quick appraisal of how well the design and priors allow the dose escalation to reach and then stop in the target band.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#per-sim-alloc-and-tox-history",
    "href": "documentation/v71/userguides/crm.html#per-sim-alloc-and-tox-history",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Per Sim Alloc and Tox History",
    "text": "Per Sim Alloc and Tox History\n\n\n\n\n\n\nFigure 76: Allocation and toxicity history plot\n\n\n\nThis graph shows the allocation and toxicity history for an individual simulation. Each dot represents a subject, the dot is colored red if toxicity was observed, grey otherwise. The subjects’ outcomes are displayed left to right in the order in which they were dosed, and at a height corresponding to the dose they were given.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed. A history can only be displayed for trials for which a ‘cohorts’ file has been written out.\nThere is an option to show “the truth”, which if selected (the default) colors the background according to the simulated toxicity rate of the dose strength on the y-axis. Thus the background has bands of color for “under-dosing”, “target”, “excess” and “unacceptable” toxicity.\nOn the left hand side, a black triangle next to the y-axis indicates the highest cleared dose. Green/red triangles show the Model MTD/MED+ and finally the selected MTD/MED.\nIf the trial uses open enrolment this graph is slightly changed.\n\n\n\n\n\n\nFigure 77: Open Enrolment Allocation and Toxicity History graph\n\n\n\nIf the trial uses open enrolment then this graph has an “Interim” picker that shows the data available at a specific interim. Subjects whose outcome has not been observed at this interim are shown as grey squares. Subjects who were not included in the trial because there were already the maximum number of subjects treated but who had not attained their final toxicity / non-toxicity status are shown as yellow crosses at the level below the lowest dose. As the interim displayed is increased subjects symbol will change as their endpoint data becomes available.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#cohort-responses",
    "href": "documentation/v71/userguides/crm.html#cohort-responses",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Cohort Responses",
    "text": "Cohort Responses\n\n\n\n\n\n\nFigure 78: Cohort response plot\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the fitted dose-toxicity model for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIt the trial uses open enrolment then subjects whose outcome has not been observed at the time of the interim being displayed are shown as a light grey part of the “allocated subjects” bar.\nIf trial simulates 2 groups then there are two graphs one for each group.\nIf the trial simulates efficacy, then the bar shows both toxicities and efficacies using half width bars of different colors.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED.\n\n\n\n\n\n\nFigure 79: Cohort Responses for one of two groups showing efficacy and toxicity",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#cohort-band-probabilities",
    "href": "documentation/v71/userguides/crm.html#cohort-band-probabilities",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Cohort Band Probabilities",
    "text": "Cohort Band Probabilities\n\n\n\n\n\n\nFigure 80: Cohort band probabilities graph\n\n\n\nThis graph shows the dose allocation and resulting toxicities along with the posterior probabilities that the toxicity rate lies in each of the toxicity bands for each dose, for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nGreen/Red triangles indicate the Model MTD/MED and finally the selected MTD/MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#mtd-change-on-expansion",
    "href": "documentation/v71/userguides/crm.html#mtd-change-on-expansion",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "MTD Change on Expansion",
    "text": "MTD Change on Expansion\n\n\n\n\n\n\nFigure 81: MTD change on expansion\n\n\n\nThis graph plots the dose selected as MTD before the results of the expansion cohort are available, against the dose selected as MTD after the expansion cohort results are available. Each simulation result is plotted as a ‘dot’, and the graph is divided up into a grid and dots located in the correct cell in the grid and then arranged within the cell depending on how many results fall into the cell. This allows the relative number of results in each cell to be appraised quickly by eye.\nThis graph allows the trial designer to see the degree of risk that the determination of the MTD will change as a result of the expansion cohort and whether the risk is predominantly that the determination will increase or decrease.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#selected-mtd-graph",
    "href": "documentation/v71/userguides/crm.html#selected-mtd-graph",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Selected MTD graph",
    "text": "Selected MTD graph\nThe Selected MTD “Across Scenarios” graph shows for each scenario and each variant a histogram of the proportion of times each dose was selected as the MTD at the end of the simulations in that scenario. The bars are colored to reflect the toxicity band that the “true” toxicity rate of the dose falls into in that scenario.\n\n\n\n\n\n\nFigure 83",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities-graph",
    "href": "documentation/v71/userguides/crm.html#toxicity-interval-probabilities-graph",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Toxicity Interval Probabilities graph",
    "text": "Toxicity Interval Probabilities graph\nThe Toxicity Interval Probabilities “Across Scenarios” graph shows for each scenario and each variant a stacked bar chart of the posterior probability that the toxicity rate at each dose falls into one of the user defined 4 toxicity bands.\n\n\n\n\n\n\nFigure 84",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#fitted-toxicity-1",
    "href": "documentation/v71/userguides/crm.html#fitted-toxicity-1",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Fitted Toxicity",
    "text": "Fitted Toxicity\nThe Fitted Toxicity “Across Scenarios” graph shows for each scenario and each variant, the mean fitted toxicity and the 95-percentile spread of the fitted toxicities across the simulations.\n\n\n\n\n\n\nFigure 85",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#allocation",
    "href": "documentation/v71/userguides/crm.html#allocation",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Allocation",
    "text": "Allocation\nThe Allocation “Across Scenarios” graph shows for each scenario and each variant, a box plot of the spread of the number of subjects allocated to each dose across the simulations. As N-CRM may only be allocating a small number of cohorts the number of subjects allocated to each dose is often not a smooth distribution, but somewhat discontinuous.\n\n\n\n\n\n\nFigure 86",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#sample-size",
    "href": "documentation/v71/userguides/crm.html#sample-size",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Sample Size",
    "text": "Sample Size\nThe Sample Size “Across Scenarios” graph is the only “Across Scenario” graph that is not a trellis plot. It is a single graph with a line plotted per scenario of the mean sample size at each maximum sample size.\n\n\n\n\n\n\nFigure 87",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#entering-the-data-directly",
    "href": "documentation/v71/userguides/crm.html#entering-the-data-directly",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Entering the data directly",
    "text": "Entering the data directly\nThe data can be entered directly in the data grid – each row containing the date for a single subject.\nThe format of the grid is the same as though viewing the “subject.csv” file in a spreadsheet.\nFor historical reasons the data file format contains 5 columns that are now unused but have been retained. Thus the columns are:\n\nFirst column is the subject ID. This column can be left blank, FACTS does not used the value, it is there to allow the data to be cross-referenced to an external data source. If not required there is no harm simply entering ‘1’ on each row.\nThe next 5 columns are unused and can be left blank. Do not enter text containing comma’s in these fields, these will be read as column separators if the data is saved and read back in.\nCohort number, this should be an integer indicating which cohort the subject belonged to (and hence the order in which they entered the trial). This data is sometimes used when determining what doses the allocation rules permit to be used. The FACTS GUI now checks to ensure that this value has been entered.\nDose Strength, if explicit doses are being used this value must match the dose strength of one of the doses defined on the ‘Treatment Arms’ tab – as the dose escalation rules are defined in terms of “number of doses”. If doses have been defined using ‘finely spaced doses’ then this column can contain any value as dose escalation rules defined using “dose strength”. The value entered will be used as the strength of the dose the subject was administered.\nApart from the requirement that the dose strength corresponds to one of the planned doses if the design uses explicit doses, there is no need for the data entered to represent a dose escalation permitted by the design. The team can have been more or less cautious, and there can be more or less data, than originally planned.\nToxicity – if a binary endpoint is being used this must be 0 (not-toxicity) or 1 (toxicity), if Ordinal toxicity is being used then this must be 1, 2, 3 or 4; where 1 is now no toxicity, 2 mild toxicity, 3 toxicity, and 4 severe toxicity.\nEfficacy – this must be 0 (no efficacy) or 1 (efficacy), even if efficacy is not being modelled in the design. If it is not being modelled then whether the value is 0 or 1 is immaterial.\n\n\n\n\n\n\n\nFigure 91\n\n\n\nIf the data entered and then ‘Run Analysis’ clicked, the data is saved to a file called ‘subject.csv’ and the analysis results saved to a folder called ‘Analysis’ within the “_results” folder of the design.\n\n\n\n\n\n\nFigure 92\n\n\n\nThe ‘Save As’ button can be used to save the file with a different name, but it will still be saved within the _Results folder.\nA specific results folder is also created, called ‘Analysis_’.\n\n\n\n\n\n\nFigure 93\n\n\n\n\n\n\n\n\n\nFigure 94",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#loading-data-from-a-file",
    "href": "documentation/v71/userguides/crm.html#loading-data-from-a-file",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Loading Data From a File",
    "text": "Loading Data From a File\nAs well as entering the data via FACTS its possible to load the data from a ‘subject.csv’ file. These can be cerated within FACTS or outside of FACTS and once created can be edited FACTS or outside of FACTS.\nWe have tried to make it particularly easy to enter, modify and analyze data in N-CRM because this is a useful way to explore the properties of the design in addition to simulation.\n\nThe subject.csv file format\nThe file is a simple ascii file with the different data items separated by commas. Any line starting with ‘#’’ is ignored, here we use that to include a header row. The data may also include spaces to aid readability (but not, currently, tabs). Each line defines the dose given to a single subject and that subject’s response.\nFor historic reasons the CRM subject.csv file format includes fields for patient identification data, however the FACTS design engine does not use this data, but does require that the columns contain data of the expected format. This is the first 6 columns: patient ID, Patient Initials, Year, Month, Day, Time & Cohort. The simplest thing to do is enter simple default values as shown below.\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Toxicity, Efficacy\n1, , , , , , 1, 12.5, 0, 0\nIn FACTS only the last four columns are ever significant: Cohort, Dose, Toxicity and Efficacy. “Dose” needs to contain the dose level (not the dose index)) of the dose given to the subject, For example if the dose levels were specified as 12.5, 25, 50, 100, 150, 200 and 250 so this column should contain one of these values.\nIf the toxicity endpoint is dichotomous then “Toxicity” needs to contain either a ‘1’ (to indicate toxicity observed) or a ‘0’ (for no toxicity). If its ordinal, then it needs to contain a ‘1’, ‘2’, ‘3’ or ‘4’ corresponding to the observed level of toxicity for that subject, where ‘1’ is now “no toxicity’ and ‘3’ is the ‘target toxicity’, ‘2’ is ‘mild toxicity’ and ‘4’ is ‘severe toxicity’.\nA value in the final “Efficacy” column is also required whether or not efficacy is being modelled in the design. The column should contain either a ‘1’ (to indicate efficacy observed) or a ‘0’ (for no efficacy).\nThus if the first cohort had been allocated the lowest dose ’12.5’ and no subjects experienced toxicity (‘0’), the subjects.csv file looks like this:\n#Patient ID, Patient Initials, Year, Month, Day, Time, Cohort, Dose, Tox, Efficacy\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0\n1, , , , , , 1, 12.5, 0, 0",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "href": "documentation/v71/userguides/crm.html#data-file-management-on-the-analysis-tab",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Data file management on the Analysis tab",
    "text": "Data file management on the Analysis tab\nFrom FACTS 6.2 onwards FACTS now supports multiple subject data files and analysis folders.\nButtons that allow the subject data file to be changed:\n\n’Select File to Create New Analysis: launches a file browser that allows the user to select a new “.csv” file from any location. The selected file is copied to the “_Results” folder (retaining its current name) and made the current subject data file.\n‘Rename Current Analysis’ allows the name of the current subject data file to be changed.\n‘Select Difference Analysis’ allows a different subject data file that is in the “_Results” folder to be made the current subject data file.\n‘Delete Analysis’ allows any of the subject data files that are in the “_Results” folder to be deleted.\n\nThe name of the current subject data file and the name of the corresponding analysis folder are shown below the subject data file buttons.\nThere are five buttons that allow the currently loaded subject data file to be modified:\n\n‘Delete Row’ deletes the currently selected row in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Delete All’ clears all the data in the data grid. The data in the current subject file is not updated unless the current data is saved.\n‘Reload data’ replaces the data in the data grid with the data that is still in the current subject data file.\n‘Save As’ saves the current data in the data grid to a new subject data file in the “_Results” folder, and makes that the current subject data file.\n“Save” saves the current data in the data grid to the current subject data file.\n\nRunning an analysis performs a ‘Save’ before running the analysis.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#running-an-analysis",
    "href": "documentation/v71/userguides/crm.html#running-an-analysis",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Running an Analysis",
    "text": "Running an Analysis\nOnce data has been loaded or entered, the user can click the ‘Run Analysis’ button.\nOnce the analysis has run, FACTS displays the recommendation, and a graph showing the data the fitted toxicity.\n\n\n\n\n\n\nFigure 95: Analysis tab - analysis results\n\n\n\nThe available analysis parameters are\n\nMCMC Burn-in: how many of the initial MCMC samples are discarded before accumulating samples to estimate the posterior distributions of the values of interest.\nNumber of samples: the number of MCMC samples to take in-order to estimate the posterior distributions of the values of interest.\nRandom Seed: the seed to be used initialize the random number sequence, with the same design data and random seed FACTS will return the same results.\nEdit command parameters. This allows the command line string to the design engine to modified, this is an advanced option. The command line options are described in the “FACTS DE User Guide for Trial Execution”.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#analysis-graphs",
    "href": "documentation/v71/userguides/crm.html#analysis-graphs",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Analysis Graphs",
    "text": "Analysis Graphs\nThree graphs are available, the first shows the subject allocation, observed toxicities and resulting fitted curve:\n\n\n\n\n\n\nFigure 96\n\n\n\nThe second shows the posterior probabilities for each dose that its toxicity rate falls in each of the 4 toxicity bands:\n\n\n\n\n\n\nFigure 97\n\n\n\nThe third simply shows the observed data:\n\n\n\n\n\n\nFigure 98\n\n\n\nA fourth graph is available if “Generate MCMC file” is checked before running the analysis, we can now view MCMC trace plots of the fitted parameters such as Alpha and Beta:\n\n\n\n\n\n\nFigure 99",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-summary",
    "href": "documentation/v71/userguides/crm.html#contents-of-summary",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Contents of summary.csv",
    "text": "Contents of summary.csv\nThe columns in summary.csv are common across all the FACST Dose Finding design engines.\nSome columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate summary files. The first summary file “summary.csv” contains the results for the first group, the second summary file “summary2.csv” contains the result for the second group.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber of Sims\n1\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nMean num subjects\n1\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario (including any single patient run-in, but excluding any expansion cohort..\n\n\nPpn Tox\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nPpn Eff\n1 – Efficacy only\nThis is the average proportion of the subjects recruited that experienced a efficacy in this scenario.\n\n\nSD Ppn Tox\n1\nThis is the standard deviation of the proportion of toxicity across the simulations.\n\n\nSD Ppn Eff\n1 – Efficacy only\nThis is the standard deviation of the proportion of efficacy across the simulations.\n\n\nTrue Mean Tox\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile that we are simulating from.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile that we are simulating from.\n\n\nMean Beta Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\ns.d.Beta Tox\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the toxicity logistic model.\n\n\nMean Alpha 3 Tox\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\ns.d.Alpha 3 Tox3\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha 3 parameter of the toxicity logistic model.\n\n\nMean Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\ns.d.Beta Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the efficacy logistic model.\n\n\nMean Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\ns.d.Alpha 3 Eff\n1 – Efficacy only\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the efficacy logistic model.\n\n\nMTD Selection &lt;dose index&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study, as defined by the Selected MTD simulation results column. Index starts at 0 if a control arm is included.\n\n\nMED Selection &lt;dose index&gt;\nOne per dose – Efficacy only\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study, as defined by the Selected MED simulation results column. Index starts at 0 if a control arm is included.\n\n\nOSD Selection &lt;dose index&gt;\nOne per dose\nUnused [it contains values that are copies of the MTD selection. OSD = Optimum Safe Dose, it differs from the MTD only if there is an efficacy endpoint to take into account too]\n\n\nMean num Ph1\n1 – Efficacy only\nThis is the mean (over the simulations) of the number of subjects dosed during the first phase of the trial that targets the MTD.\n\n\nMean Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Fitted Efficacy &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Efficacy &lt;dose index&gt;\nOne per dose -\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nMean Subj per dose&lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subj per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nMean Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nMean Eff per dose &lt;dose index&gt;\nOne per dose -Efficacy only\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\nMean Known per dose &lt;dose index&gt;\nOne per dose\nThe number of known patient outcomes for a dose. In N-CRM there is no simulation of drop-outs so this will always be the same as “Mean subj per dose”\n\n\nSD Known per dose &lt;dose index&gt;\nOne per dose\nThis is the standard deviation of the number of known patient outcomes for each dose across the simulations. In N-CRM there is no simulation of drop-outs so this will always be the same as “SD subj per dose”\n\n\nNum subj 80%ile\n1\nThis is the eightieth percentile across the simulations of the number of subjects recruited into the trial.\n\n\nTox Stopping 1\n1\nThe number of times that toxicity stopping rule 1 was true at the end of the simulation. This is the rule that the trial should stop when the specified minimum number of cohorts has been allocated to the dose currently selected as the MTD.\n\n\nTox Stopping 2\n1\nThe number of times that toxicity stopping rule 2 was true at the end of the simulation. This is the rule that the trial should stop when no more than the specified number of doses within the credible interval for the target toxicity.\n\n\nTox Stopping 3\n1\nThe number of times that toxicity stopping rule 3 was true at the end of the simulation. This is the rule that the trial should stop when a dose achieves the minimum posterior probability of having a toxicity rate within the target band/of being MTD.\n\n\nTox Stopping 4\n1\nThe number of times that toxicity stopping rule 4 was true at the end of the simulation. This is the rule that the trial should stop if testing another cohort and observing no toxicities does not change the dose selected as the MTD.\n\n\nTox stopping 5\n1\nNumber of times that the maximum cohorts /subjects on MTD was met at the end of a simulation.\n\n\nEff Stopping 1\n1 – Efficacy only\nNumber of times the minimum cohorts / subjects on MED was rule was met at the end of a simulation.\n\n\nEff Stopping 2\n1 – Efficacy only\nNumber of times the range of dose strengths within the credible interval of the estimate of the dose strength of the MED is less than the specified number.\n\n\nEff Stopping 3\n1 – Efficacy only\nNumber of times that the probability that the dose was MED the stopping rule threshold was met at the end of a simulation.\n\n\nMean Tox CI\n1 – MTD target only\nThe mean number of doses in the MTD CI\n\n\nSD Tox CI\n1 – MTD target only\nThe SD of the number of doses in the MTD CI\n\n\nMean Eff CI\n1 – Efficacy only\nThe mean number of doses in the MED CI\n\n\nSD Eff CI\n1 – Efficacy only\nThe SD of the number of doses in the MED CI\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe mean probability of the dose being MTD\n\n\nPr(MED)\nOne per dose – Efficacy only\nThe mean probability of the dose being the MED\n\n\nMTD+ &lt;1\n1\nThe number of times the MTD was deemed to be less than the lowest dose\n\n\nMTD+ &lt;dose index&gt;\nOne per dose\nThe number of times each dose was selected as the MTD\n\n\nMTD+ &gt;&lt;D&gt;\n1\nThe number of times the MTD was deemed to be above the highest dose\n\n\nMED+ &lt;1\n1 – Efficacy only\nThe number of times the MED was deemed to be less than the lowest dose\n\n\nMED+ &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of times each dose was selected as the MED\n\n\nMED+ &gt;&lt;D&gt;\n1 – Efficacy only\nThe number of times the MED was deemed to be above the highest dose\n\n\nOSD+ &lt;1\n1\nThe same as MTD+ &lt;1, can be ignored\n\n\nOSD+ &lt;dose index&gt;\nOne per dose\nThe same as MTD+ &lt;dose index&gt;, can be ignored\n\n\nOSD+ &gt;&lt;D&gt;\n1\nThe same as MTD+ &gt; &lt;D&gt;, can be ignored\n\n\nDE Version\n1\nThis is the version of the N-CRM design engine that simulated these trial results.\n\n\nGUI Version\n1\nThis is the version of the FACTS GUI that was used to specify the parameters for the trials to be simulated.\n\n\nProject\n1\nThe name of the project or design\n\n\nScenario\n1\nThe name of the scenario within the project or design\n\n\nDate/Time\n1\nThe date and time the simulations were started\n\n\nBest &lt;dose index&gt;\nOne per dose – Efficacy only\nThe probability that the dose is ‘the best’ – that is it has the highest probability of efficacy without toxicity.\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe mean probability that the MTD is below the lowest dose\n\n\nPr(MTD+) &lt;D+1&gt;\n1 – MTD target only\nThe mean probability that the MTD is above the highest dose\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe mean probability that the MED is below the lowest dose\n\n\nPr(MED+) &lt;D+1&gt;\n1 –Efficacy only\nThe mean probability that the MED is above the highest dose\n\n\nPr(Under) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Under-dosing’ toxicity band for each dose.\n\n\nPr(Target) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Acceptable’ toxicity band for each dose.\n\n\nPr(Excess) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Excess’ toxicity band for each dose.\n\n\nPr(Unacc) &lt;dose index&gt;\nOne per dose\nThis is the mean (over the simulations) of the posterior probabilities of having a toxicity in the ‘Unacceptable’ toxicity band for each dose.\n\n\nPrior Mean ln(Beta) Tox\n1\nThe value used for the mean value for the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior s.d.(Beta) Tox\n1\nThe value used for the standard deviation of the prior distribution of log Beta in the toxicity model for all simulations\n\n\nPrior Mean Alpha Tox\n1\nThe value used for mean value for the prior distribution of Alpha in the toxicity model for all the simulations\n\n\nPrior s.d.Alpha Tox\n1\nThe value used for the standard deviation of the prior distribution of Alpha in the toxicity model for all simulations\n\n\nPrior Mean Rho Tox\n1\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the toxicity model for all simulations\n\n\nPrior Mean ln(Beta) Eff\n1 – Efficacy only\nThe value used for the mean value for the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior s.d.(Beta) Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of log Beta in the efficacy model for all simulations\n\n\nPrior Mean Alpha Eff\n1 – Efficacy only\nThe value used for mean value for the prior distribution of Alpha in the efficacy model for all the simulations\n\n\nPrior s.d.Alpha Eff\n1 – Efficacy only\nThe value used for the standard deviation of the prior distribution of Alpha in the efficacy model for all simulations\n\n\nPrior Mean Rho Eff\n1 – Efficacy only\nThe value used for the correlation between Alpha and log Beta in the bivariate Normal prior distribution for Alpha and log Beta in the efficacy model for all simulations\n\n\nMean Duration\n1\nThe mean (over the simulations) of the duration of the trial.\n\n\ns.d.Duration\n1\nThe SD (over the simulations) of the duration of the trial.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe actual toxicity rate being simulated for each dose\n\n\nMean Lost\n1 – Open Enrolment only\nThe mean (over the simulations) of the number of subjects who were available for treatment but could not be included in the trial because the number of treated subjects for whom the final result is not available equals the maximum queue length. This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\ns.d.Lost\n1 – Open Enrolment only\nThe SD (over the simulations) of the number of subjects ‘lost’ (see above). This only applies to Open Enrolment designs otherwise the column is omitted.\n\n\nPostCE MTD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE MTD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as MTD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;1\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘below the first dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &lt;Dose&gt;\nOne per dose – Cohort expansion only\nThe number of times after the Cohort Expansion the dose was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nPostCE OSD+ &gt; &lt;D&gt;\n1 – Cohort expansion only\nThe number of times after the Cohort Expansion the dose ‘above the top dose’ was selected as OSD+. (Omitted if no Expansion Cohort is defined)\n\n\nAll Tox Stop\n1\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nEarly Success\n1\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nCap Stop\n1\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.\n\n\nMean Fitted Tox Lower &lt;D&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the estimates of the toxicity rate across the simulations, at each dose.\n\n\nMean Fitted Tox Upper &lt;D&gt;\nOne per dose\nThis is the upper bound of the 95-percentile of the estimates of the toxicity rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "href": "documentation/v71/userguides/crm.html#contents-of-simulations.csv-and-cohortsnnn",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Contents of simulations.csv and cohortsNNN.csv",
    "text": "Contents of simulations.csv and cohortsNNN.csv\nMost of the columns are common to the two file types, but the first few are different.\nAs with the summary.csv file, some columns are only output if certain options are being used:\nEfficacy Only These columns are output only if an efficacy endpoint is simulated as well as the toxicity endpoint.\nOpen Enrolment Only These columns are output only if open enrolment is being simulated rather than cohort enrolment.\nCohort Expansion Only These columns are output only of an expansion cohort is being simulated.\nMTD only These columns are output only if the target dose for the toxicity endpoint is a single MTD, rather than the dose most likely to have a toxicity rate in the target toxicity band.\nIf 2 groups are being simulated, then there are two separate simulation.csv and cohorts.csv files. The first simulations file “simulations.csv” contains the results for the first group, the second file “simulations2.csv” contains the result for the second group.\nFor the cohorts files if two groups are being simulated, the files names ‘cohortsNNN.csv’ contain the results for the first group, and the files ‘named cohorts2_NNN.csv’ contain the results for the second group\n\nsimulations.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nNumber\n1\nThe number of the simulation.\n\n\nRandom Number Seed\n1\nThe random number seed used to simulate the scenario.\n\n\nNo.Subjects\n1\nThe number of subjects recruited in this trial in the small cohort run-in and in the cohorts used to locate the MTD, but not those in the expansion cohort.\n\n\nPpn Tox\n1\nThis is the proportion of the subjects recruited that experienced a toxicity in this trial\n\n\nPpn Eff\n1 – Efficacy only\nThis is the proportion of the subjects recruited that had an efficacious outcome in this trial\n\n\nTrue Mean Tox\n1\nThis is the average probability of toxicity for the subjects in the trial, given the doses they were treated with and the toxicity rate being simulated for each of those doses.\n\n\nTrue Mean Eff\n1 – Efficacy only\nThis is the average probability of efficacy for the subjects in the trial, given the doses they were treated with and the efficacy rate being simulated for each of those doses.\n\n\nSeeds\n2\nThe two 32 bit numbers that make up the random number seed at the end of the simulation. To exactly re-simulate a specific simulation (e.g. in order to generate an mcmc file or cohorts file for that simulation) enter these values from the line above the simulation to be re-simulated.\n\n\n\n\n\ncohortsNNN.csv initial columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe number of the cohort in the simulation.\n\n\nAlloc Dose\n1\nThe index of the dose assigned to that cohort. If using open enrolment, -2 means the subject was ‘lost’, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nNumToxic\n1\nThe number of subjects in that cohort that experienced a toxicity\n\n\n\n\n\nCommon simulations.csv and cohortsNNN.csv columns\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nmean Beta Tox\n1\nThe mean fitted value for the toxicity model Beta parameter\n\n\ns.d.Beta Tox\n1\nThe standard deviation of the toxicity model fitted Beta parameter\n\n\nMean Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\ns.d.Alpha 2 Tox\n1 – Only if Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 2 toxicity or higher\n\n\nMean Alpha 3 Tox\n1\nThe mean fitted value for the toxicity model Alpha parameter for category 3 (or the only category) toxicity or higher\n\n\ns.d.Alpha 3 Tox\n1\nThe standard deviation of the toxicity model Alpha parameter for category 3 toxicity (or the only category) or higher\n\n\nMean Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe mean fitted value for the toxicity model Alpha parameter for category 4 toxicity\n\n\ns.d.Alpha 4 Tox\n1 – Only if 4 category Ordinal Toxicity\nThe standard deviation of the toxicity model Alpha parameter for category 4 toxicity\n\n\nMean Beta Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Beta parameter\n\n\ns.d.Beta Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model fitted Beta parameter\n\n\nMean Alpha Eff\n1 – Efficacy Only\nThe mean fitted value for the efficacy model Alpha parameter\n\n\ns.d.Alpha Eff\n1 – Efficacy Only\nThe standard deviation of the efficacy model Alpha parameter\n\n\nModel MTD\n1\nThe dose index of the model MTD\n\n\nModel MED\n1 – Efficacy only\nThe dose index of the model MED\n\n\nModel OSD\n1 [simulations only]\nThe index of the dose selected as the Optimal Safe Dose (OSD), in N-CRM this will always be the same as the MTD as there is no efficacy to take into consideration.\n\n\nHighest Cleared Dose\n1\nThe highest cleared dose\n\n\nSelected MTD\n1\nThe dose index of the selected MTD (minimum of the Highest Cleared Dose and the model MTD+)\n\n\nSelected MED\n1\nThe dose index of the selected MED (minimum of the Highest Cleared Dose and the model MED+)\n\n\nSelected OSD\n1 [simulations only]\nThe dose index of the selected OSD (minimum of the Highest Cleared Dose and the model OSD+)\n\n\nNo. Ph1\n1 – Efficacy only\nThe number of subjects enrolled during the first, MTD locating, phase, before switching to the MED locating phase\n\n\nToxicity &lt;dose index&gt;\nOne per dose\nThe mean of the final posterior estimate of the toxicity rate at each dose.\n\n\nEfficacy\nOne per dose – Efficacy only\nThe mean of the final posterior estimate of the efficacy rate at each dose.\n\n\nNo. Subj &lt;dose index&gt;\nOne per dose\nThe number of subjects that have been allocated to each dose.\n\n\nTox per dose &lt;dose index&gt;\nOne per dose\nThe number of toxicities that have observed at each dose\n\n\nEff per dose &lt;dose index&gt;\nOne per dose – Efficacy only\nThe number of efficacies that have observed at each dose\n\n\nKnown per dose &lt;dose index&gt;\nOne per dose\nThe number of subjects for whom final results are available at each dose.\n\n\nTox CI\n1 – MTD target only\nOnly used if targeting a single dose. It’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nEff CI\n1 – Efficacy only\nIt’s the CI used in the stopping rule “range of dose strength within the credible interval”, where the credible interval has a default alpha of 0.05.\n\n\nFlags\n1\nA flag value comprising a number of (possible) flag values ’OR’d together to show the current allocation or stopping decisions. See the Flag Values table below for details\n\n\nPr(MTD) &lt;dose index&gt;\nOne per dose – MTD target only\nThe posterior probability for each dose that that dose is the MTD.\n\n\nPr(MED) &lt;dose index&gt;\nOne per dose – Efficacy only\nThe posterior probability for each dose that that dose is the MED.\n\n\n1st full size\n1 [simulations only]\nThe number of the first full sized cohort, this will be ‘1’ unless the small cohort run-in is enabled. If the small cohort run-in is enabled, then this is the index of the first full size cohort.\n\n\nCohort size\n1 [cohorts only]\nThe number of patients in the cohort.\n\n\nMTD+\n1\nThe dose index of the model MTD+, this is the model MTD using the dose range extended by one dose either end. Dose 0 will be selected if all doses are too toxic and dose D+1 will be selected if no doses are toxic enough.\n\n\nMED+\n1 – Efficacy only\nThe dose index of the model MED+, this is the model MED using the dose range extended by one dose either end. Dose 0 will be selected if all doses are efficacious enough and dose D+1 will be selected if no doses are efficacious enough.\n\n\nOSD+\n1 [simulations only]\nThe dose index of the model OSD+, will be the same as the model MED+ unless this is above the model MTD+, in which case it is same as the model MTD+. If there is no efficacy to take into consideration this is the same as the model MTD+..\n\n\nPr(MTD+) 0\n1 – MTD target only\nThe probability that the model MTD+ is at a dose below the lowest tested doses.\n\n\nPr(MTD+) D+1\n1 – MTD target only\nThe probability that the model MTD+ is at a dose above the highest tested doses.\n\n\nPr(Good) &lt;dose index&gt;\n1 – Efficacy only\nThe posterior probability for each dose of observing efficacy without observing toxicity.\n\n\nBest\n1 – Efficacy only\nThe index of the dose with the highest Pr(Good)\n\n\nPr(MED+) 0\n1 – Efficacy only\nThe probability that the model MED+ is at a dose below the lowest tested doses.\n\n\nPr(MED+) D+1\n1 – Efficacy only\nThe probability that the model MED+ is at a dose above the highest tested doses.\n\n\nPr(Under) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Under-dosing’ toxicity band.\n\n\nPr(Target) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Acceptable’ toxicity band.\n\n\nPr(Excess) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Excessive’ toxicity band.\n\n\nPr(Unacc) Tox &lt;dose index&gt;\nOne per dose\nThe posterior probability for each dose that its toxicity rate is in the ‘Unacceptable’ toxicity band.\n\n\nDuration\n1 (simulations only)\nDuration of the trial in weeks.\n\n\nTrue Toxicity &lt;Dose&gt;\nOne per dose\nThe toxicity rate being simulated for that dose in the scenario.\n\n\nTrue Efficacy &lt;Dose&gt;\nOne per dose – Efficacy only\nThe efficacy rate being simulated for that dose in the scenario.\n\n\nRec Time\n1 - Cohorts only\nIf the trial is using open enrolment this column record the time the subject was available to be dosed.\n\n\nNum Lost\n1 – Open enrolment only (simulations only)\nIf the trial is using open enrolment this is the total number of subjects ‘lost’ during the simulation, that is they couldn’t be allocated a dose because there were already the maximum number of subjects treated but not complete.\n\n\nPostCE MTD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MTD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model MTD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE MED+\n1 – Expansion cohort & Efficacy only (simulations only)\nIn the case of a trial using an expansion cohort, and simulating efficacy, this is the dose that is the model MED+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPostCE OSD+\n1 – Expansion cohort only (simulations only)\nIn the case of a trial using an expansion cohort, this is the dose that is the model OSD+ at the end of the dose escalation phase, after the expansion cohort.\n\n\nPr(Tox) Lower &lt;Dose&gt;\nOne per dose\nThe lower bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Tox) Upper &lt;Dose&gt;\nOne per dose\nThe upper bound of the 95% credible interval of the fitted toxicity at each dose.\n\n\nPr(Eff) Lower &lt;Dose&gt;\nOne per dose – Efficacy only\nThe lower bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\nPr(Eff) Upper &lt;Dose&gt;\nOne per dose – Efficacy only\nThe upper bound of the 95% credible interval of the fitted efficacy at each dose.\n\n\n\nThe ‘Flags’ column is a number comprised of a number of binary flags that are used to indicate the ‘state’ of the simulator at different analyses. These flags are:\n\n\n\n\n\n\n\n\nFlag Values\n\n\n\n\n\n\nBits used\nValue\nMeaning\n\n\n0x001F\n0x0001\nAllocating to MTD\n\n\n0x001F\n0x0002\nAllocating to MED\n\n\n0x001F\n0x0003\nAllocating to initial dose\n\n\n0x001F\n0x0004\nAllocating new single patient cohort\n\n\n0x001F\n0x0005\nAllocating 1st dose of 2nd sample to dose below MTD from 1st sample\n\n\n0x001F\n0x0006\nAllocating 1st dose of 2nd sample to MTD from 1st sample\n\n\n0x001F\n0x0007\nAllocating expansion cohort\n\n\n0x001F\n0x0008\nAllocating to MTD because can’t allocate to MED because its above MTD\n\n\n0x001F\n0x0009\nExpanding at the current dose\n\n\n0x001F\n0x000A\nExpanding at the dose below\n\n\n0x001F\n0x000D\nAllocating as a backfill\n\n\n0x001F\n0x000E\nAllocating as a frontfill\n\n\n0x001F\n0x000F\nAllocating to max/min for fixed probability\n\n\n0x001F\n0x0011\nStopping for early futility\n\n\n0x001F\n0x0012\nStopping because MED is found\n\n\n0x001F\n0x0013\nStopping because all doses are toxic\n\n\n0x001F\n0x0014\nStopping because MTD is found\n\n\n0x001F\n0x0015\nStopping because MTD is found but there is no MED\n\n\n0x0020\n0x0020\nReached max subjects on MTD\n\n\n0x0030\n0x0030\nReached min required subjects on MTD\n\n\n0x0040\n0x0040\nToxicity confidence interval test passed\n\n\n0x0080\n0x0080\nPr(MTD) test passed\n\n\n0x0100\n0x0100\nReached max subjects on MED\n\n\n0x0200\n0x0200\nEfficacy confidence interval test passed\n\n\n0x0400\n0x0400\nPr(MED) test passed\n\n\n0x1000\n0x1000\nNo MED so using maximum instead\n\n\n0x2000\n0x2000\nUsing maximum permitted dose in place of MTD\n\n\n0x4000\n0x4000\nUnable to escalate due to part filled dose\n\n\n0x8000\n0x8000\nUnable to allocate during open enrolment because maximum permitted subjects without final result reached\n\n\n0x10000\n0x10000\nReached max subjects on MTD\n\n\n0x20000\n0x20000\nUnable to allocate during open enrolment because reached max subjects on MTD, but not all subjects on MTD have final results, so trial is ‘paused’ – resuming if the final results move the MTD so the trial continues, or resuming to allocate an expansion cohort.\n\n\n0x40000\n0x40000\nMax subjects reached and no other stopping rules met\n\n\n0x80000\n0x80000\nMaximum cohorts used to determine MTD has been met when there is also an efficacy endpoint, switching to searching for the MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#contents-of-mcmcnnnnn",
    "href": "documentation/v71/userguides/crm.html#contents-of-mcmcnnnnn",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Contents of MCMCNNNNN.csv",
    "text": "Contents of MCMCNNNNN.csv\nThe MCMC file, if requested for output by the user, contains all the MCMC samples for the fitted parameters in the design. There is one row per sample (excluding the burnin) and the samples from all the analyses (i.e from every cohort – or if using open enrollment, every subject) in the simulation are included. The first two columns are the cohort’s index and the sample (within analysis) index. The remaining columns are the parameters whose sample values are being reported, the number and constituents of these columns are highly variable depending on design of the statistical analysis.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta\n1\nThe estimate of the slope of the logistic regression\n\n\nAlpha &lt;O&gt;\nO\nThe estimate of intercept of the logistic regression – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the second group.\n\n\na\n1\nIf a second group is included this is the estimate of the offset for the intercept of the cat-3 toxicity model.\n\n\n\n\nMCMC File if Efficacy is included\nIf an efficacy endpoint is included in the design, then all the model parameters columns described above are included suffixed with “Tox” and then duplicate, suffixed with “Eff”.\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nCohort\n1\nThe index of the analysis (interim) in the simulation\n\n\nSample\n1\nThe index of the sample within the analysis\n\n\nBeta Tox\n1\nThe estimate of the slope of the logistic regression of the toxicity model\n\n\nAlpha &lt;O&gt; Tox\nO\nThe estimate of intercept of the logistic regression of the toxicity model – if the endpoint is not Ordinal, this is just Alpha, if the endpoint is Ordinal there will be columns Alpha 2, Alpha 3 and (if using four category toxicity), Alpha 4\n\n\nb Tox\n1\nIf a second group is included this is the estimate of ‘b’ the slope offset for the toxicity model for the second group.\n\n\na Tox\n1\nIf a second group is included this is the estimate of the offset of the intercept for the cat-3 toxicity model.\n\n\nBeta Eff\n1\nThe estimate of the slope of the logistic regression of the efficacy model\n\n\nAlpha Eff\n1\nThe estimate of intercept of the logistic regression of the efficacy model.\n\n\nB Eff\n1\nIf a second group is included this is the estimate of ‘B’ the slope offset for the efficacy model for the second group.\n\n\nA Eff\n1\nIf a second group is included this is the estimate of the offset of the intercept for the efficacy model.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/crm.html#exporting-the-results",
    "href": "documentation/v71/userguides/crm.html#exporting-the-results",
    "title": "FACTS Model-Based Dose Escalation",
    "section": "Exporting the Results",
    "text": "Exporting the Results\nUsing the menu item File -&gt; Export Project, the .facts file and all the results files can be saved as a single zip file.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Model-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html",
    "href": "documentation/v71/userguides/de.html",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "",
    "text": "This document describes how to install and use the Rule-Based Dose Escalation (DE) Fixed and Adaptive Clinical Trial Simulator (FACTS) software (from now on referred to as Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software). It is intended for all end users of the system.\n\n\n\nThis document covers the Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software by describing the user interface. It covers the 3+3 and mTPI. It does not cover the CRM design engine which has its own User Guide.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS, 5 or later if changed, installed on Windows 7 and Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.\n\n\n\nThis document is a guide to the version 7.1 release of Dose Escalation FACTS.\nThere have been no changes to these elements of FACTS since FACTS 6.1.\n\n\n\nPlease cite FACTS wherever applicable using this citation.\n\n\n\nAn overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#purpose-of-this-document",
    "href": "documentation/v71/userguides/de.html#purpose-of-this-document",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "",
    "text": "This document describes how to install and use the Rule-Based Dose Escalation (DE) Fixed and Adaptive Clinical Trial Simulator (FACTS) software (from now on referred to as Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software). It is intended for all end users of the system.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#scope-of-this-document",
    "href": "documentation/v71/userguides/de.html#scope-of-this-document",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "",
    "text": "This document covers the Dose Escalation Fixed and Adaptive Clinical Trial Simulator Software by describing the user interface. It covers the 3+3 and mTPI. It does not cover the CRM design engine which has its own User Guide.\nThis document does not address the internal workings of the design engines or algorithms, which are addressed in the associated Design Engine Specification. It also does not address the use of FACTS Core Designs or Enrichment Designs, which are covered in other User Guides.\nThe screenshots provided are specific to a particular installation and may not reflect the exact layout of the information seen by any particular user. They were taken from FACTS, 5 or later if changed, installed on Windows 7 and Windows 10. Different versions of Windows or the use of different Windows themes will introduce some differences in appearance. The contents of each tab, however, will be consistent with the software.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#context-of-this-issue",
    "href": "documentation/v71/userguides/de.html#context-of-this-issue",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "",
    "text": "This document is a guide to the version 7.1 release of Dose Escalation FACTS.\nThere have been no changes to these elements of FACTS since FACTS 6.1.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#citing-facts",
    "href": "documentation/v71/userguides/de.html#citing-facts",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "",
    "text": "Please cite FACTS wherever applicable using this citation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#definition-of-terms",
    "href": "documentation/v71/userguides/de.html#definition-of-terms",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "",
    "text": "An overview of the acronyms and abbreviations used in this document can be found here.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-7.1-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-7.1-changes-to-dose-escalation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "FACTS 7.1 Changes to Dose Escalation",
    "text": "FACTS 7.1 Changes to Dose Escalation\nThe CRM (Toxicity), CRM (Ordinal), CRM (Efficacy), bCRM are now considered deprecated. Existing designs will still work as expected, but users starting a new design are encouraged to switch over to the new CRM engine if possible.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-7.0-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-7.0-changes-to-dose-escalation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "FACTS 7.0 Changes to Dose Escalation",
    "text": "FACTS 7.0 Changes to Dose Escalation\nIn FACTS 7.0 the only change was on the Analysis tab, when entering subject data manually, the FACTS GUI now ensures that a cohort number is entered for every subject.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.5-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.5-changes-to-dose-escalation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "FACTS 6.5 Changes to Dose Escalation",
    "text": "FACTS 6.5 Changes to Dose Escalation\nIn FACTS 6.5 there were no changes to the Dose Escalation simulators.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.4-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.4-changes-to-dose-escalation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "FACTS 6.4 Changes to Dose Escalation",
    "text": "FACTS 6.4 Changes to Dose Escalation\nIn FACTS 6.4 there were no changes to the Dose Escalation simulators.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.3-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.3-changes-to-dose-escalation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "FACTS 6.3 Changes to Dose Escalation",
    "text": "FACTS 6.3 Changes to Dose Escalation\nIn FACTS 6.3 there were no changes to Dose Escalation except in N-CRM which is described in its own User Guide.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-6.1-changes-to-dose-escalation",
    "href": "documentation/v71/userguides/de.html#facts-6.1-changes-to-dose-escalation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "FACTS 6.1 Changes to Dose Escalation",
    "text": "FACTS 6.1 Changes to Dose Escalation\nIn FACTS 6.1:\n\nTwo new types of design have been added:\n\nThe mTPI design - described in this document.\nThe 2D-CRM for phase 1 dose escalation trials of combinations of multiple doses of 2 drugs described in the FACTS DE 2D-CRM User Guide.\n\nA ‘design variant’ facility has been added to the N-CRM engine that allows the user to easily simulate and evaluate an N-CRM design at different sample sizes. See the updated FACTS DE N-CRM User Guide for details.\nNote that from FACTS 6.0 onwards the default MCMC sampling length for Dose Escalation designs was increased from 2500 to 25000. This was because it was found that with the shorter sampling length the results were too inconsistent. It has however significantly slowed the speed with which FACTS DE designs are run.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#introduction-1",
    "href": "documentation/v71/userguides/de.html#introduction-1",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Introduction",
    "text": "Introduction\nTo open the application, select FACTS Dose Escalation from your Start menu. At different points in the FACTS GUI, the user is required to make decisions about how to model or simulate various aspects of a clinical trial. To simplify data entry, FACTS shows the user only the information that is relevant to the current decisions.\nFACTS has a tabbed design to allow entry of different categories of information about the design.\nThe File menu allows the user to create a new design, open an existing design, save a design, or copy a design to a new name (Save as). The saved design includes all parameters entered as well as simulation results, if they have been produced.\nThe Settings menu provides the ability to configure how simulation jobs should be submitted to the Grid. Selecting whether to execute simulations locally or on the Grid is done from the Simulation tab.\nThe Help menu allows the user to learn more about the FACTS software and verify the application’s version number.\nFinally, note that in all tabs of the application, red exclamation points\n\n\n\n\n\n\nFigure 2\n\n\n\nindicate errors in data entry from the user that must be corrected. Moving the cursor over the exclamation point causes a pop-up help text indicating what the error is, helping the user remedy the error.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#selecting-a-design-model",
    "href": "documentation/v71/userguides/de.html#selecting-a-design-model",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Selecting a Design Model",
    "text": "Selecting a Design Model\nOnce the FACTS application has been opened, the user may select to model a clinical trial using any of the following design engines from the introduction screen or the File &gt; New &gt; Dose Escalation menu:\n\n\n\n\n\n\nFigure 3\n\n\n\nOnce a design has been selected, the associated form of the GUI will be displayed, the contents of the tabs and sub-tabs of FACTS displaying the information relevant to the selected model. Additionally, the model type selected will appear in the title bar of FACTS to verify the user’s choice of design.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#study-info",
    "href": "documentation/v71/userguides/de.html#study-info",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Study Info",
    "text": "Study Info\nThe Study Info sub-tab provides basic study parameters such as Trial Size and Cohort options, as relevant to the type of design. Depending on the choice of design engine, the Study info tab may also allow the user to enter specification for a two-sample population, Toxicity and/or Efficacy targets, response category options, or Joint Efficacy and Toxicity options.\n\nRecruitment, like N-CRM the mTPI design includes the option to use Open Enrollment instead of recruitment by cohorts. A phase 1 trial using open enrolment recruits ‘all comers’ with a cap on the number of subjects who can be treated but have not yet had a final result. Subjects that arrive whilst this cap has been reached are deemed to be ‘lost’ to the study. If selected the user specifies the following additional parameters.\n\nMaximum study size (subjects): the maximum number of subjects who can be recruited into the study.\nMean recruitment ate: for simulation purposes this is the average number of subjects who will be expected to be available for recruitment during each time period. Recruitment is simulated using a standard Poisson process.\nTime until final result: the time for each subject’s final result to become available. This is simulated as a fixed length of time that is the same for all subjects.\nMaximum subjects without a final result: the cap on the number of subjects who can be recruited but not yet completed. This is also known as the ‘maximum queue length’. Subjects arriving and available for recruitment while the number of subjects treated but not yet completed is a the cap are dropped and assumed not available for recruitment once the current subjects complete but the study has to await further new subjects to become available.\n\nStudy Size – (unless an mTPI design using open enrollment) for all designs this is specified in terms of the maximum number of cohorts.\n\nIf the design is CRM(Toxicity) or CRM(Efficacy) and the option to include a “Two sample population” analysis is included then the Study Size applies to the first sample and with an additional parameter the user specifies the maximum size of the second sample, also in cohorts.\n\nCohort - (unless an mTPI design using open enrollment and if not a “3+3” design where cohorts are always of size 3 and must always complete before the next cohort is recruited) then the user can specify:\n\nCohort size: The number of subjects in each cohort. The maximum sample size for the trial is simply: Max Trial Size (cohorts) * Cohort Size.\nCohort information delay: Normally left at zero, this allows for the trial to be simulated with cohorts recruited more quickly than if waiting for the last cohort to complete before treating the next cohort. This allows the trial to complete in shorter time, but is likely to lead to a larger incidence of toxicities. If the delay is set to 1, the second cohort will be recruited before the results for the first cohort are available, the third cohort will be recruited after the results of the first cohort are available but before the results for the second cohort are available, and so on.\n\nJoint Efficacy and Toxicity – (bCRM only) – there is a flag that the user can set to indicate that a subject cannot experience both toxicity and efficacy (toxicity censors or prevents efficacy).\n\n\n\nSingle subject run-in – (CRM(Toxixity), bCRM, CRM(Ordinal) only) – this allows the user to specify that the trial is to start with a single subject run-in. With a single subject run-in, subjects are allocated in cohorts of one, incrementing by one dose strength each cohort until a toxicity is observed. The user has the option to specify whether, on observing the first toxicity, the first full cohort is then allocated at that dose, or at the dose below.\nTarget – (mTPI only) – somewhat similar to the N-CRM, in mTPI the target is defined as toxicity bands rather than single toxicity rate. The user specifies the lower and upper bounds of the target toxicity band as well as a target toxicity rate within that band.\nThe parameters to specify the Toxicity target are displayed for CRM(Toxicity), bCRM and CRM(Ordinal) designs. These allow the user to specify the maximum tolerated toxicity target and choose which dose should be selected based one of the following criteria:\n\nnearest dose to target\nnearest dose above the target (except for ordinal CRM)\nnearest dose below the target\nIf a control arm is included with the Treatment arms then there is the option to specify that the target toxicity rate is relative to control, not absolute.\n\nSimilarly the parameters to specify the Efficacy target are displayed for CRM(Efficacy) and bCRM. These allow the user to specify the maximum tolerated toxicity target and choose which dose should be selected based one of the following criteria:\n\nnearest dose to target\nnearest dose above the target (except for ordinal CRM)\nnearest dose below the target\nIf a control arm is included with the Treatment arms then there is the option to specify that the target efficacy rate is relative to control, not absolute.\n\nCohort Expansion – (all designs except CRM(Efficacy) The user can specify that after the dose escalation phase of the trial has completed, the simulation is to include an ‘expansion cohort’ allocated at the final value of the target dose, and how large that expansion cohort will be.\n\nIf a Control arm has been included, then there can be a specific allocation to Control as well (these are taken from the overall cohort size, they are not additional to it).\nIn bCRM the user selects whether the expansion cohort is allocated to the MTD, MED or OSD.\n\n\nFigure 4 displays the Study Info sub-tab for a bCRM design. The functionality and look of the tab is similar for other CRM designs and mTPI.\n\n\n\n\n\n\nFigure 4: Study Info (bCRM)",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#treatment-arms",
    "href": "documentation/v71/userguides/de.html#treatment-arms",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Treatment Arms",
    "text": "Treatment Arms\nThe Treatment Arms sub-tab (Figure 5) provides an interface for specifying the various dose levels, and (except in “3+3”) a Control treatment arm.\nIf a control arm is included:\n\nTarget rates can be specified to be relative to control (otherwise they are absolute).\nA specific number of subjects are specified to be allocated to control in each cohort\nThe response on the Control arm can be included in the dose response model, or modeled separately (using a simple beta-binomial model) in which case monotonicity is not enforced.\n\nThe user may add doses either explicitly or by auto-generation, as depicted below. The user may also edit the Dose Names within the table by double clicking on any existing dose name, however the index cannot be edited.\nNote that the CRM designs use transformed dose levels, and unlike the Dose Finding designs do not have a relative dose level specified.\n\n\n\n\n\n\nFigure 5: Treatment Arms",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#explicitly-defined",
    "href": "documentation/v71/userguides/de.html#explicitly-defined",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Explicitly Defined",
    "text": "Explicitly Defined\n\nToxicity [3+3, CRM(Toxicity), mTPI and bCRM]\nThe Toxicity sub-tab provides an interface for specifying one or more Toxicity profiles.\nToxicity profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 6. Toxicity values are entered directly into the Probability of Toxicity column of the table, and the graphical representation of these toxicity values updates accordingly. The graph may be modified by plotting the log of the dose strength as the x-axis, and by plotting the logit or the probability of toxicity as the y-axis.\nThis graph – as with all graphs in the application – may be easily copied using the ‘Copy Graph’ option in the context menu accessed by right-clicking on the graph. The user is given the option to copy the graph to their clipboard (for easy pasting into other applications, such as Microsoft Word or PowerPoint), or to save the graph as an image file.\n\n\n\n\n\n\nFigure 6: Toxicity Virtual Subject Response (bCRM)\n\n\n\nWhen using a bCRM design engine, if the Toxicity and Efficacy profiles are specified separately, then the results are simulated without correlation. To simulate correlation in the results it is necessary to specify joint profiles.\n\n\nToxicity with 2 Samples [CRM(Toxicity)]\nIf utilizing a CRM (Toxicity) design, the user has the option to model two sample populations (this option may be selected on the Study Info tab, as described here). If the user elects to enable the modeling of two samples, then the Toxicity sub-tab allows the user to input probabilities of Toxicity for each sample, as displayed in Figure 7 below.\n\n\n\n\n\n\nFigure 7: Toxicity Virtual Subject Response with 2 samples (CRM (Toxicity))\n\n\n\n\n\nOrdinal Toxicity [CRM(Ordinal)]\nFor CRM (Ordinal) designs, the user must specify the probability of toxicity at or above each category. Ordinal designs can use either three or four categories, with a category three toxicity corresponding to a toxic response in a CRM (Toxicity) design. Toxicity data must be entered for each category greater than 1 and must be monotonically decreasing with category (Figure 8).\n\n\n\n\n\n\nFigure 8: Toxicity Virtual Subject Response (CRM (Ordinal))\n\n\n\n\n\nEfficacy [CRM (Efficacy) and bCRM]\nSimilar to the Toxicity sub-tab, the Efficacy sub-tab provides an interface for specifying one or more Efficacy profiles.\nEfficacy profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen.\nEfficacy values are entered directly into the Probability of Efficacy column of the table, and the graphical representation of these efficacy values updates accordingly.\nIf utilizing a bCRM design, in which both Toxicity and Efficacy profiles are utilized, the user must ensure that profile names are unique. FACTS does not allow the user to enter Toxicity profiles and Efficacy profiles which are named identically.\nFinally, if utilizing a CRM (Efficacy) design, the user has the option to model two sample populations (this option may be selected on the Study Info tab, as described here). If the user elects to enable the modeling of two samples, then the Efficacy sub-tab allows the user to input probabilities of Efficacy for each sample, just as in the case of CRM (Toxicity) illustrated in Figure 9.\n\n\n\n\n\n\nFigure 9: Efficacy Virtual Subject Response (CRM Efficacy)\n\n\n\n\n\nJoint Efficacy / Toxicity [bCRM]\nThe Joint Efficacy / Toxicity sub-tab (Figure 10) allows the user to specify the probability of efficacy, toxicity, and success at each dose.\nJoint profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in (Figure 10). The user must ensure that any profile name entered on this tab is unique from all other profile names in the application, whether on this tab, or on the Toxicity sub-tab or the Efficacy sub-tab.\nProbabilities of toxicity, efficacy, and success are entered directly into the table, and the graphical representation of these probabilities updates accordingly. Success is the probability of observing efficacy without toxicity, thus the rate for success is naturally bounded:\n\nThe probability of success at a dose cannot exceed the probability of efficacy at that dose.\nThe probability of success at a dose cannot exceed (1 – the probability of toxicity) at that dose.\nThe probability of success cannot be less than the probability of efficacy minus the probability of toxicity at that dose.\nIf the user enters probabilities violating these limit, then FACTS mark them as invalid and will refuse to simulate the scenarios including the profile. FACTS reports to the user, on the Simulation tab, that the following error has been found: “True toxicity and Efficacy curve is not in range [0,1]” and will ask the user to resolve the error before running simulations.\n\nJoint Efficacy / Toxicity profiles may also be generated from models, rather than explicitly. This way of specifying Joint Efficacy / Toxicity profiles is described in this section.\n\n\n\n\n\n\nFigure 10: Joint Efficacy",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#external",
    "href": "documentation/v71/userguides/de.html#external",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "External",
    "text": "External\nSubject response data may be simulated from a PK-PD model in place of or in addition to choosing a response model from FACTS. The importing of subject response data (which must be in the form specified in the System Requirements Document) may be done from the External Files sub-tab depicted below (Figure 11).\nTo import an external file, the user must first add a profile to the table. After adding a profile, the user must click “Browse” to locate the externally simulated data. The user will then be prompted to locate the external file on their computer with a dialog box.\n\n\n\n\n\n\nFigure 11: External Data",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#parametric-bcrm-sec-parametric-bcrm",
    "href": "documentation/v71/userguides/de.html#parametric-bcrm-sec-parametric-bcrm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Parametric [bCRM] (#sec-parametric-bcrm)",
    "text": "Parametric [bCRM] (#sec-parametric-bcrm)\nThe Parametric sub-tab allows the user to specify the Joint Efficacy / Toxicity profiles as modeled by Cox or Gumbel functions. This allowance is in contrast to the explicit definition of Joint Efficacy / Toxicity profiles, as may be done in the Joint Efficacy / Toxicity sub-tab of the Explicitly Defined Response tab (see this section).\nThe interface of the Parametric tab is displayed below (Figure 12). As with other Virtual Response sub-tabs, profiles may be added, deleted, and renamed using the table and corresponding buttons on the left hand side of the screen in Figure 12.\n\n\n\n\n\n\nFigure 12: bCRM Parametric Joint Toxicity & Efficacy VSR - Cox Model\n\n\n\n\n\n\n\n\n\nFigure 13: bCRM Parametric Joint Toxicity & Efficacy VSR - Gumbel Model",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-and-efficacy-response-tabs-crm-efficacy-crm-toxicity-and-bcrm",
    "href": "documentation/v71/userguides/de.html#toxicity-and-efficacy-response-tabs-crm-efficacy-crm-toxicity-and-bcrm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Toxicity and Efficacy Response tabs [CRM (Efficacy), CRM (Toxicity) and bCRM]",
    "text": "Toxicity and Efficacy Response tabs [CRM (Efficacy), CRM (Toxicity) and bCRM]\nOn the Toxicity and Efficacy Response sub-tabs, the user may choose toxicity and efficacy models from among a list available for this release. The supported models for modeling toxicity and efficacy response in this release are logistic, tanh, and power for one sample studies, and tanh (x-b) for two sample studies.\nThe user has the option to set the Minimum and Maximum parameter values, which define the toxicity/efficacy asymptotes. These values are used to rescale the probabilities and calculated the scaled dose values, X^ (X-hat). These default to 0 and 1, but if the endpoint being observed is expected to have a background rate (that would be observed even if placebo were administered) or a natural maximum rate (that would not be exceeded whatever the strength of dose used), then the model fitting will be improved\nThe Toxicity Response tab is depicted below in Figure 14; the Efficacy Response tab has a similar appearance.\n\n\n\n\n\n\nFigure 14: Toxicity Response (bCRM)\n\n\n\nThe user selects the model type:\n\nLogisitic, in which case the fixed value for the Alpha parameter is specified (usually this is set to 3 – giving a toxicity rate of 0.953 at a dose with a transformed dose strength of 0. Effective dose strengths should typically be in the range (-8, -1)\nTanh in which case the effective dose strengths should typically be in the range (-2, 1)\nPower in which case the effective dose strengths should typically be in the range (0,1)\n\nThe user specified the prior distribution of the estimated model parameter (Beta) either as an Exponential (1) distribution or a uniform distribution (0,U) where U is specified by the user.\nThe effective dose strengths of the doses are specified in the “Model Priors” table. They can be specified in one of two ways:\n\nBy defining a prior probability of toxicity for each dose. The corresponding transformed dose strength is then calculated as the value that would yield that toxicity rate from the model with Beta=1. The corresponding transformed dose strength is shown in the right hand column. Care needs to be taken with this approach that the resulting transformed dose strengths are well spaced out – this can be checked on the graph that shows the transformed dose strengths, and the corresponding toxicity rate on the graph of the Beta=1 model.\nBe specifying the effective dose strength explicitly. Care needs to be taken with this approach, depending on the model this may yield nonsensical toxicity rates, but once the correct range for the transformed doses is understood, it easier to ensure a good spacing of the transformed dose strengths.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-response-tab-crmordinal",
    "href": "documentation/v71/userguides/de.html#toxicity-response-tab-crmordinal",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Toxicity Response tab [CRM(Ordinal)]",
    "text": "Toxicity Response tab [CRM(Ordinal)]\n\n\n\n\n\n\nFigure 15: CRM(Ordinal) Toxicity Response tab",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#design-tab-mtpi",
    "href": "documentation/v71/userguides/de.html#design-tab-mtpi",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Design tab [mTPI]",
    "text": "Design tab [mTPI]\nThe mTPI method has two parameters that can be specified\n\nThe first is an option to prevent re-escalating to a dose (the “DU” or Do not Use category in the diagram) if the posterior probability that the toxicity rate is above the target toxicity plus the delta: pT+εU (on the Study &gt; Study Info tab this is the “Upper Bound” parameter) exceeds a specified threshold (0.95 is the default).\nThe second is an option that allows an early stopping rule to be specified that if the dose to be allocated next already has the maximum number of subjects on it then it is declared the MTD. Setting this value shrinks the size of the table displayed.\n\nGiven the target toxicity rate and the boundaries specified on the Study &gt; Study Info tab, and the parameters specified here the table of mTPI dosing decisions is displayed. This shows the dosing decision given the number of subject treated at the current dose and the number of toxicities observed:\n\nE Escalate\nS Stay\nD De-escalate\nDU De-escalate and do not revisit\n\n\n\n\n\n\n\nFigure 16: mTPI Design tab",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#allocation-rule",
    "href": "documentation/v71/userguides/de.html#allocation-rule",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Allocation Rule",
    "text": "Allocation Rule\nThe Allocation Rule sub-tab is depicted below (Figure 17); it allows the user to set basic allocation parameters, including Maximum Dose Increment, Minimum Cohorts on Dose prior to Increment, and Maximum number of Cohorts used to determine MTD (bCRM only).\nThe dose range may be split with a greater maximum increment allowed in the upper part of the range than in the lower.\nWhen specifying the maximum dose increment, the user has two options of how to apply that increment; relative to the current dose or relative to the highest dose with a specified number of cohorts. In other words, the largest possible value for the next allocation at any point in the trial is either the current dose level plus the Maximum Dose Increment, or the highest dose that has been allocated to Max Subjects Before Incrementing plus the Maximum Dose Increment.\n\n\n\n\n\n\nFigure 17: Design & Allocation Rule tab (CRM Toxicity)\n\n\n\nAlso on this tab, the user may specify sampling rule parameters, namely the initial dose level for sampling. If using a “3+3” design, the sampling rule parameters will be the only parameters on this screen (and hence the tab will be called “Sampling Rule” in place of “Allocation Rule”), since allocation does not apply to the “3+3” design engine.\n\n\n\n\n\n\nFigure 18: Design & Allocation Rule tab (bCRM)\n\n\n\nIf utilizing a CRM (Toxicity) design, and if the option to model two sample populations has been enabled (this option may be selected on the Study Info tab, as described in this section), the user also has the option to set the initial dose for the second population.\nAdditionally, a single subject run-in can also be specified on the Allocation Rule tab. A single subject run-in starts at the specified starting dose and allocates a single subject to each dose until either a toxicity is observed or the maximum dose is reached. This option is available for CRM (Toxicity), CRM (Ordinal), and bCRM only. Note that subjects in the single subject run-in are not included in the “max trial size”, so with a single subject run-in, it is possible to end a simulation with more subjects that were specified in the max trial size.\nIf a Control treatment arm has been included, the user must also specify the number of subjects per cohort that should be allocated to Control. The Control dose is treated differently than the active doses, and cannot ever be found as the MTD or MED. Thus, it will never be assigned a cohort based on the fitted model, and must have subjects assigned per cohort.\nFinally, if utilizing a CRM (Efficacy) design, the user also has the option on this tab to allocate extra subjects to the min or max dose, as shown in Figure 19. “Probability gamma” is used to ensure that the numbers of subjects allocated to the maximum and minimum stay close to their specified values. Setting gamma (γ) to 0 turns this off, so the actual number allocated may drift away from the number expected. 2 is a good value to enable this correction.\nThe formula used is:\n\\(p = p_T^{\\gamma (p_O - p_T)}\\)\nwhere pT is the target probability as entered above, pO is the observed probability measured from the subjects allocated so far and p is corrected the probability that will be used in allocation. To prevent wild allocations, p is subject to the restrictions, p&lt;0.5 and pT/4 &lt; p &lt; 4pT.\n\n\n\n\n\n\nFigure 19: Allocation Rule (CRM (Efficacy))\n\n\n\nWhen a Control treatment arm is included, it is treated as the minimum dose, and gives the user another option of how to allocate to it. In this case, the user can specify a probability of allocating whole cohorts to the Control, or a number of subjects per cohort allocated to Control. The two options cannot be combined.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#stopping-criteria-crm-efficacy-crm-toxicity-crm-ordinal-n-crm-and-bcrm-only",
    "href": "documentation/v71/userguides/de.html#stopping-criteria-crm-efficacy-crm-toxicity-crm-ordinal-n-crm-and-bcrm-only",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Stopping Criteria (CRM (Efficacy), CRM (Toxicity), CRM (Ordinal), N-CRM and bCRM only)",
    "text": "Stopping Criteria (CRM (Efficacy), CRM (Toxicity), CRM (Ordinal), N-CRM and bCRM only)\nFor any design, including “3+3”, study simulation will always stop when the maximum number of subjects has been achieved.\nThe study simulation may also stop early when the following conditions that have been enabled by the user are met:\n\nMTD finding (CRM (Toxicity), N-CRM and bCRM only)\n\nthe user-specified minimum number of subjects are allocated to the MTD\nAND the following optional rules either ANDed or ORed together.\n\nThe number of dose levels in the specified confidence interval meets the specified the threshold\nThere is a dose with a probability of being the MTD that is greater than the specified threshold.\nThe minimum number of cohorts have been accrued\n\n\nMED finding (CRM (Efficacy) and bCRM only)\n\nthe user-specified maximum number of subjects are allocated to the MED\nthe number of dose levels in the confidence interval meets the specified number\nthere is a dose with a probability of being MED that is greater than the specified threshold\nThe minimum number of cohorts have been accrued\n\nmTPI when the user specified number of subjects are already allocated to the next dose to be allocated – this is specified on the mTPI Design tab.\n\nWhen studying two samples, stopping rules will be evaluated and applied separately for each sample.\nThese parameters are set on the Stopping Criteria sub-tab as depicted below in Figure 20 Stopping tab CRM Toxicity.\n\n\n\n\n\n\nFigure 20: Stopping tab CRM Toxicity\n\n\n\nFor bCRM the stopping rules become the rules for stopping the MTD phase and starting the MED phase. There are then similar rules that can be used for stopping the MED phase early.\n\n\n\n\n\n\nFigure 21: Stopping Criteria (bCRM)\n\n\n\nFinally, if utilizing a CRM (Efficacy) design, the user may specify an additional set of parameters for stopping for futility, as displayed at the bottom of Figure 22. These allow the trial to be stopped early for futility. If the user enable these rules then the user can specify:\n\nThat a minimum number of cohorts must have been allocated to the minimum and maximum doses.\nThat the trial will stop unless there is a minimum difference in the mean response between the minimum and maximum doses.\n\n\n\n\n\n\n\nFigure 22: Stopping Criteria (CRM (Efficacy))",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#to-run-simulations",
    "href": "documentation/v71/userguides/de.html#to-run-simulations",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "To run simulations",
    "text": "To run simulations\nClick in the check box in each of the rows corresponding the to the scenarios to be run. FACTS displays a row for each possible combination of the ‘profiles’ that have been specified: - baseline response, dose response, longitudinal response, accrual rate and dropout rate. Or simply click on “Select All”.\nThen click on the “Simulate” button.\nDuring simulation, the user may not modify any parameters on any other tab of the application. This safeguard ensures that the simulation results reflect the parameters specified in the user interface.\nWhen simulations are started, FACTS saves all the study parameters, and when the simulations are complete all the simulation results are saved in results files in a “_results” folder in the same directory as the “.facts” file. Within the “_results” folder there will be a sub-folder that holds the results for each scenario.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#how-many-simulations-to-run",
    "href": "documentation/v71/userguides/de.html#how-many-simulations-to-run",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "How many simulations to run?",
    "text": "How many simulations to run?\n\nAfter first entering a design it is worth running just a small number of simulations such as 10 to check that the scenarios and design have been entered correctly. If all 10 simulations of a ‘null’ scenario are successful, or all 10 simulations of what was intended to be an effective drug scenario are futile, it is likely there has been a mistake or misunderstanding in the specification of the scenarios or the final evaluation or early stopping criteria.\nOnce the design and scenarios look broadly correct, it is usually worth quickly collecting rough estimates of the operating characteristics using around 100 simulations for each scenario. 100 simulations is enough to spot designs having very poor operating characteristics such as very high type-1 error, very poor power, a strong tendency to stop early for the wrong reason, or poor probability of selecting the correct target. 100 simulations is also usually sufficient to spot problems with the data analysis such as poor model fits and significant bias in the posterior estimates.\nTypically 1,000 simulations of each scenario of interest is required to get estimates of the operating characteristics precise enough to compare designs and tune the parameters. (Very roughly rates of 5% (such as type-1 error) can be estimated to about +/-1.5% and rates of around 80% (such as power) estimated +/- 2.5%)\nFinally around 10,000 simulations of the scenarios of interest is required to give confidence in operating characteristics of a design and possibly to select between the final shortlisted designs (Approximately rates of 5% can be estimated to about +/-0.5% and rates of around 80% estimated +/- 1%).\n\nThere may be many operating characteristics need to be compared over a number of scenarios, such as expected sample size, type-1 error, power, probability of selecting a good dose as the target and quality of estimation of the dose-response.\nHowever frequently these will be compared over a range of scenarios, it may not be necessary to run very large number of simulations for each scenario if a design shows a consistent advantage on the key operating characteristics over the majority of the scenarios.\n\nSimulation results\nIn the main screen the summary results that are usually of principal interest are displayed, the results summarised by scenario.\n\nShow other columns: Allows the user to open additional windows on the simulation results, the windows available are:\n\nAll: A window containing all the summary results columns\nHighlights: (all) a separate window with the results shown on the main tab\nAllocation, Observed: (all) summary results of the number of subjects allocated, the number allocated to each dose, the number of toxicities observed and the number of toxicities observed per dose\nFitted toxicity/efficacy/response: (all CRM) summary results of the estimates of the model parameters and the dose-toxicity.\nPr(MTD) etc / Pr(MED) etc: (all CRM) summary results of the posterior probabilities of the properties of interest. In bCRM these values are organised in three groupings called “Probabilities”, “Toxicity” and “Efficacy”.\nSimulation Results: a window displaying the individual simulation results for each simulation of the currently selected scenario\nTwo Sample results: (CRM Toxicity)\n\nView Graph: opens the FACTS built in graph utility displaying the results for the currently selected scenario. See Section 11 below for a description of the graphs.\nAggregate: opens a control that allows the user create aggregated results files from all the scenarios. The user can select which scenarios to include, and whether the results should be pivoted by dose. The resulting files are stored in the simulation results folder.\nOpen in R: If aggregated files have been created then clicking this button on the simulations tab will open control that allows the user to select which aggregations to load into data frames as R is opened. Otherwise it will offer to open R using the files in the currently selected scenario.\nRight Click Menu: right clicking the mouse on a row of results in the simulation tab brings up a ‘local’ menu of options:\n\nOpen results folder: Opens a file browser in the results folder of the scenario, allowing swift access to any of the results files.\nSimulation results: Opens a window displaying the individual simulation results for each simulation of the currently selected scenario\nOpen in R: opens a control that will launch R, first loading the selected files in the results folder as data frames.\nView Graphs: launches the graph viewer to view the results of the currently selected scenario.\n\n\n\n\nMCMC Settings\n\n\n\n\n\n\nFigure 24\n\n\n\nThe first two values specify two standard MCMC parameters –\n\nThe length of burn-in is the number of the initial iterations whose results are discarded, to allow the sampled values to settle down.\nThe number of samples is the number of subsequent iterations whose results are recorded in order to give posterior estimates of the values of interest (in this case α and β).\n\nIf the Number of MCMC samples to output value is set to N, where N &gt; 0, then all the sampled values at each interim in the first N simulations are output, allowing the user to check convergence.\nTo obtain the MCMC sampled values for a particular simulation, keep the same Random number seed and use Start at simulation to specify the simulation in question and run just 1 simulation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#facts-grid-simulation-settings",
    "href": "documentation/v71/userguides/de.html#facts-grid-simulation-settings",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "FACTS Grid Simulation Settings",
    "text": "FACTS Grid Simulation Settings\nIf you have access to a computational grid, you may choose to have your simulations run on the grid instead of running them locally. This frees your computer from the computationally intensive task of simulating so you can continue other work or even shutdown your PC or laptop. In order to run simulations on the grid, it must first be configured, this is normally done via a configuration file supplied with the FACTS installation by the IT group responsible for the FACTS installation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#detailed-simulation-results",
    "href": "documentation/v71/userguides/de.html#detailed-simulation-results",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Detailed Simulation Results",
    "text": "Detailed Simulation Results\nAfter simulation has completed and simulation results have been loaded, the user may examine detailed results for any scenario with simulation data in the table by double-clicking on the row. A separate window (as in Figure 25) will reveal more detailed results about the simulation.\n\n\n\n\n\n\nFigure 25: Detailed Simulation Results\n\n\n\nRight-clicking on a row displays a context menu from which the user can also view the corresponding cohort results:\n\n\n\n\n\n\nFigure 26: bCRM Parametric Joint Toxicity & Efficacy VSR - Gumbel Model",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#aggregation",
    "href": "documentation/v71/userguides/de.html#aggregation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation combines the csv output from multiple scenarios into fewer csv files. The Aggregate… button displays a dialog which allows the user to select what to aggregate.\n\n\n\n\n\n\nFigure 27\n\n\n\nThe default location for the aggregated files is the results directory for the study, but this can be changed.\nAggregation may be performed with or without pivoting on group, or both.\n\nUnpivoted files will have one row for each row in the original files.\nIn pivoted files each original row will be split into one row per dose, plus two extra rows for ‘below lowest’ and ‘above highest’.\n\nWhere there is a group of columns for each dose, they will be turned into a single column with each value on a new row.\nIf there is no value for the below lowest or above highest rows, they will be left blank.\nValues in columns that are independent of group will be repeated on each row.\n\n\nThe default is to aggregate all scenarios, but any combination may be selected.\nPressing “Aggregate” generates the aggregated files.\nEach type of csv file is aggregated into a separate csv file whose name begins agg_ or agg_pivot_, so agg_summary.csv will contain the rows from each of the summary.csv files, unpivoted. cohortsNNN.csv files are aggregated into a single agg_[pivot_]cohorts.csv file. Doses.csv is identical for all scenarios and so is not aggregated.\nEach aggregated file begins with the following extra columns, followed by the columns from the original csv file:\n\n\n\n\n\n\n\nColumn Name\nComments\n\n\n\n\nScenario ID\nIndex of the scenario\n\n\nToxicity Profile\nA series of columns containing the names of the various profiles used to construct the scenario. Columns that are never used are omitted (e.g External Subjects Profile if there are no external scenarios)\n\n\nExternal Subjects Profile\n\n\n\nAgg Timestamp\nDate and time when aggregation was performed\n\n\nSim\nSimulation number. Only present in the cohorts file.\n\n\nDose\nOnly present if pivoted",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#highlights-all",
    "href": "documentation/v71/userguides/de.html#highlights-all",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Highlights (All)",
    "text": "Highlights (All)\nThese are the columns displayed on the simulations tab after simulations are completed, the can also be displayed in the separate “Highlights” results window.\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nEngines\nDescription\n\n\n\n\nSelect\n1\nAll\nNot an output column, this column contains check box to allow the user to select which scenario to simulate. The ‘Select All’ button causes them all to be checked.\n\n\nStatus\n1\nAll\nThis column reports on the current status of simulations: Completed, Running, No Results, Out of date, Error. It is updated automatically.\n\n\nScenario\n1\nAll\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nNum Sims\n1\nAll\nThe number of simulations that were run to produce the displayed results.\n\n\nRandom Number Seed\n1\nAll\nBase random number seed used to perform the simulations.\n\n\nMean Subj.\n1\nAll\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPPn. Tox\n1\nAll except CRM Efficacy\nThis is the average proportion of the subjects recruited that experienced a toxicity in the simulations of this scenario.\n\n\nSD Ppn. Tox\n1\nAll except CRM Efficacy\nThis is the standard deviation of the proportion of toxicity across the simulations of this scenario.\n\n\nPpn Eff\n1\nCRM Efficacy & bCRM\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nCRM Efficacy & bCRM\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Tox\n1\nAll except CRM Efficacy\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nTrue Ppn Eff\n1\nCRM Efficacy & bCRM\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile of the scenario\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nFor each dose, this is proportion of the simulations where it was selected as the MTD (Maximum Tolerated Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MTD (closest dose to having the target toxicity rate – or “nearest below” or “nearest above” as selected by the user on the Study Info tab.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nCRM Efficacy & bCRM\nFor each dose, this is proportion of the simulations where it was selected as the MED (Minimum Efficacious Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MED (“closest” dose to having the target efficacy rate – or “nearest below” or “nearest above” as selected by the user on the Study Info tab.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nbCRM only\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. The dose chosen as the OSD will be the MED if that is below the MTD, otherwise it will be the MTD.\n\n\nPpn(All Tox)\n1\nAll\nThe proportion of the simulations that stopped because all doses were too toxic.\n\n\nPpn(early Success)\n1\nAll\nThe proportion of the simulations that stopped because sufficient stopping rules were met before the maximum number of cohorts had been treated.\n\n\nPpn(Cap)\n1\nAll\nThe proportion of the simulations that stopped because the maximum number of cohorts had been treated.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#allocations-observed-all",
    "href": "documentation/v71/userguides/de.html#allocations-observed-all",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Allocations, Observed (All)",
    "text": "Allocations, Observed (All)\n\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nEngines\nDescription\n\n\n\n\nScenario\n1\nAll\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Subj.\n1\nAll\nThis is the mean (over the simulations) of the number of subjects recruited in this scenario.\n\n\nPpn. Tox\n1\nAll except CRM Efficacy\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nSD PPn. Tox\n1\nAll except CRM efficacy\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nPpn, Eff\n1\nCRM Efficacy and bCRM only\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nCRM Efficacy and bCRM only\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Tox\n1\nAll except CRM Efficacy\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nTrue Ppn Eff\n1\nCRM Efficacy and bCRM only\nThis is the average true probability of efficacy over the simulations. The ‘true probability of efficacy’ in a simulated trial is the average of the probabilities of efficacy all the subjects are exposed to, given the efficacy response profile of the scenario\n\n\nNum Phase 1\n1\nbCRM Only\nThe mean (over the simulations) of the number of subjects allocated during phase 1 (the MTD finding part) of the trial.\n\n\nSubj. Per Dose: &lt;dose&gt;\nOne per dose\nAll\nThis is the mean (over the simulations) of the number of subjects assigned to each dose.\n\n\nSD Subjects: &lt;dose&gt;\nOne per dose\nAll\nThis is the standard deviation of the number of subjects assigned to each dose across the simulations.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox. Per Dose: &lt;dose&gt;\nOne per dose\nAll except CRM Efficacy\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nEff. Per Dose: &lt;dose&gt;\nOne per dose\nCRM Efficacy and bCRM only\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff. Per Dose: &lt;dose&gt;\nOne per dose\nCRM Efficacy and bCRM only\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\n80% Num Subj\n1\nAll\nThis is the 80th centile of the overall number of subjects recruited in each simulation.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#fitted-toxicity-crm-toxicity-crm-ordinal-bcrm",
    "href": "documentation/v71/userguides/de.html#fitted-toxicity-crm-toxicity-crm-ordinal-bcrm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Fitted Toxicity (CRM Toxicity, CRM Ordinal, bCRM)",
    "text": "Fitted Toxicity (CRM Toxicity, CRM Ordinal, bCRM)\n\n\n\nColumn Title\nNumber of columns\nEngines\n\n\n\n\nScenario\n1\nAll\n\n\nMean Beta 1\n1\nAll\n\n\nSD Beta 1\n1\nAll\n\n\nMean Beta 2\n1\nbCRM only\n\n\nSD Beat 2\n1\nbCRM only\n\n\nPsi\n1\nbCRM only\n\n\nSD Psi\n1\nbCRM only\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nAll\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nTrue Toxicity: &lt;Dose&gt;\nOne per dose\nAll\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nMean Fit Tox Lower: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fit Tox Upper: &lt;dose&gt;\nOne per dose\nAll\n\n\nMean Fit Eff Lower: &lt;dose&gt;\nOne per dose\nbCRM only\n\n\nMean Fit Eff Upper: &lt;dose&gt;\nOnse per dose\nbCRM only",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#fitted-efficacy-crm-efficacy",
    "href": "documentation/v71/userguides/de.html#fitted-efficacy-crm-efficacy",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Fitted Efficacy (CRM Efficacy)",
    "text": "Fitted Efficacy (CRM Efficacy)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMean Beta 1\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response\n\n\nSD Beta 1\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy model for each dose\n\n\nSD Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nMean Fit Eff Lower: &lt;dose&gt;\nOne per dose\nThis is the lower bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.\n\n\nMean Fit Eff Upper: &lt;dose&gt;\nOnse per dose\nThis is the upper bound of the 95-percentile of the mean estimate of the efficacy rate across the simulations at each dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#prmtd-etc.-crm-toxicity-crm-ordinal-probabilities-etc.-bcrm",
    "href": "documentation/v71/userguides/de.html#prmtd-etc.-crm-toxicity-crm-ordinal-probabilities-etc.-bcrm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Pr(MTD) etc. (CRM Toxicity, CRM Ordinal, “Probabilities etc.” bCRM)",
    "text": "Pr(MTD) etc. (CRM Toxicity, CRM Ordinal, “Probabilities etc.” bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD at the end of the study. The dose chosen will be the dose with the highest posterior probability of having a toxicity rate that is closest to / nearest below / nearest above the target toxicity rate.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nFor each dose, this is proportion of the simulations where it was selected as the OSD at the end of the study. The dose selected will be the MED if that is below the MTD, otherwise it will be the MTD.\n\n\nNum Stop Rule 1\n1\nNumber of times the minimum subjects on MTD was met.\n\n\nNum Stop Rule 2\n1\nNumber of times the number of doses in the credible interval of the estimate of the MTD was met.\n\n\nNum Stop Rule 3\n1\nNumber of times a dose met the required threshold for Pr(MTD).\n\n\nNum Eff Stop Rule 1\n1 (bCRM only)\nNumber of times the minimum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1 (bCRM only)\nNumber of times the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1 (bCRM only)\nNumber of times a dose met the required threshold for Pr(MED).\n\n\nMean Tox CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MTD.\n\n\nSD Tox CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MTD.\n\n\nMean Eff CI\n1 (bCRM only)\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1 (bCRM only)\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MTD): &lt;dose&gt;\nOne per dose\nThe mean (over the simulations) of posterior probability that a dose is the dose nearest / closest below / closest above the target toxicity rate.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose+&gt;\n1\nAs MTD Selection, but allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nMED+ Selection: minus\n1 (bCRM only)\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1 (bCRM only)\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nOSD+ Selection: minus\n1 (bCRM only)\nThe proportion of simulations where a dose below the tested range of doses is the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose&gt;\nOne per dose (bCRM only)\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1 (bCRM only)\nThe proportion of simulations where a dose above the tested range of doses is the optimum selected dose.\n\n\nPpn. Best: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being both at or above the MED and at or below the MTD.\n\n\nPr(MTD+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MTD.\n\n\nPr(MTD+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MTD.\n\n\nPost CE MTD+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as maximum tolerated dose.\n\n\nPost CE MTD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the maximum tolerated dose, allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nPost CE MTD+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as maximum tolerated dose.\n\n\nPost CE MED+: minus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of simulations where, after cohort expansion, the dose was selected as the minimum efficacious dose, allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nPost CE MED+: plus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose (bCRM only)\nThe proportion of simulations where, after cohort expansion, the dose was selected as the optimum selected dose, allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nPost CE OSD+: plus\n1 (bCRM only)\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#prmed-etc.-crm-efficacy",
    "href": "documentation/v71/userguides/de.html#prmed-etc.-crm-efficacy",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Pr(MED) Etc. (CRM Efficacy)",
    "text": "Pr(MED) Etc. (CRM Efficacy)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED at the end of the study. The dose chosen will be the dose with the highest posterior probability of having an efficacy rate in closest to / nearest below o/ nearest above the target efficacy rate..\n\n\nNum Eff Stop Rule 1\n1\nNumber of times the maximum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times a dose met the required threshold for Pr(MED).\n\n\nMean Eff CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MED): &lt;dose&gt;\n\nThe mean (over the simulations) of posterior probability that a dose is MED.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose+&gt;\nOne per dose\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nPpn Early Futility\n1\nThe proportion of simulations where the trials stopped because the early futility rule was met.\n\n\nPr(MED+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MED.\n\n\nPr(MED+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MED.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-bcrm",
    "href": "documentation/v71/userguides/de.html#toxicity-bcrm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Toxicity (bCRM)",
    "text": "Toxicity (bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nPpn Tox\n1\nThis is the average proportion of the subjects recruited that experienced a toxicity in this scenario.\n\n\nSD Ppn Tox\n1\nThis is the standard deviation of the proportion of toxicity across the simulations,\n\n\nTrue Ppn Tox\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nMean Beta 1\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the toxicity response.\n\n\nSD Beta 1\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Beta parameter of the logistic model of the toxicity response.\n\n\nPsi\n1\nThe mean estimate over the simulations of the overall probability of observing both a toxicity and efficacy response in a subject.\n\n\nSD Psi\n1\nThe standard deviation of the estimate of Psi over the simulations.\n\n\nMTD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MTD (Maximum Tolerated Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MTD (closest dose to having the target toxicity rate – or nearest below or above as selected by the user.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. This will be the MED if it is below the MTD, otherwise it will be the MTD.\n\n\nMean Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of toxicity from the fitted toxicity response model for each dose.\n\n\nSD Fitted Toxicity: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nTox. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of toxicities observed at each dose.\n\n\nSD Tox Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of toxicities observed at each dose across the simulations.\n\n\nNum Stop Rule 1\n1\nNumber of times simulations stopped because maximum subjects on MTD was met.\n\n\nNum Stop Rule 2\n1\nNumber of times simulations stopped because the number of doses in the credible interval was met.\n\n\nNum Stop Rule 3\n1\nNumber of times simulations stopped because Pr(MTD) was met.\n\n\nMean Tox CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MTD.\n\n\nSD Tox CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MTD.\n\n\nPr(MTD): &lt;dose&gt;\nOne per dose\nThe posterior probability that a dose is the dose nearest / closest below / closest above the target toxicity rate.\n\n\nMTD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was selected as the maximum tolerated dose.\n\n\nMTD+ Selection: &lt;dose+&gt;\n1\nAs MTD Selection, but allowing for the possibility that the MTD is below or above the dose range being tested.\n\n\nMTD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was selected as the maximum tolerated dose.\n\n\nOSD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose+&gt;\nOne per dose\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was the optimum selected dose.\n\n\nPpn Best: &lt;dose&gt;\nOne per dose\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being at or above the MED and at or below the MTD.\n\n\nPr(MTD+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MTD.\n\n\nPr(MTD+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MTD.\n\n\nTrue Toxicity: &lt;dose&gt;\nOne per dose\nThis is the true simulated toxicity rate for each dose in the scenario",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#efficacy-bcrm",
    "href": "documentation/v71/userguides/de.html#efficacy-bcrm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Efficacy (bCRM)",
    "text": "Efficacy (bCRM)\n\n\n\n\n\n\n\n\nColumn Title\nNumber of columns\nDescription\n\n\n\n\nScenario\n1\nThis gives the name of the scenario. In the N-CRM this is simply the name of the Toxicity response profile to be simulated (in other Design Engines the scenario may be a combination of a number of profiles – e.g. one for the toxicity response and one for the efficacy response).\n\n\nPpn Eff\n1\nThis is the average proportion of the subjects recruited that experienced an efficacy response in the simulations of this scenario.\n\n\nSD Ppn Eff\n1\nThis is the standard deviation of the proportion of efficacy across the simulations of this scenario.\n\n\nTrue Ppn Eff\n1\nThis is the average true probability of toxicity over the simulations. The ‘true probability of toxicity’ in a simulated trial is the average of the probabilities of toxicity all the subjects are exposed to, given the toxicity response profile of the scenario.\n\n\nMean Beta 2\n1\nThis is the mean (over the simulations) of the mean of the posterior distribution of the estimate of the Beta parameter of the logistic model of the efficacy response.\n\n\nSD Beta 2\n1\nThis is the mean (over the simulations) of the standard deviation of the posterior distribution of the estimate of the Alpha parameter of the logistic model of the efficacy response.\n\n\nPsi\n1\nThe mean estimate over the simulations of the overall probability of observing both a toxicity and efficacy response in a subject.\n\n\nSD Psi\n1\nThe standard deviation of the estimate of Psi over the simulations.\n\n\nMED Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the MED (Minimum Efficacious Dose) at the end of the study. The dose chosen will be the dose with the highest posterior probability being the MED (closest dose to having the target toxicity rate – or nearest below or above as selected by the user.\n\n\nOSD Selection: &lt;dose&gt;\nOne per dose\nFor each dose, this is proportion of the simulations where it was selected as the OSD (Optimum Selected Dose) at the end of the study. This will be the MED if it is below the MTD, otherwise it will be the MTD.\n\n\nMean Fitted Efficacy: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the mean estimate of efficacy from the fitted efficacy model for each dose\n\n\nSD Fitted Effiacy: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the mean estimates across the simulations for each dose. Note it is not based on the standard deviation estimated in the simulations, it is the SD observed across the simulation results.\n\n\nEff. Per Dose: &lt;dose&gt;\nOne per dose\nThis is the mean (over the simulations) of the number of efficacies observed at each dose.\n\n\nSD Eff Per Dose: &lt;dose&gt;\nOne per dose\nThis is the standard deviation of the number of efficacies observed at each dose across the simulations.\n\n\nNum Eff Stop Rule 1\n1\nNumber of times simulations stopped because the maximum subjects on MED was met.\n\n\nNum Eff Stop Rule 2\n1\nNumber of times simulations stopped because the number of doses in the credible interval of the estimate of the MED was met.\n\n\nNum Eff Stop Rule 3\n1\nNumber of times simulations stopped because a dose met the required threshold for Pr(MED).\n\n\nMean Eff CI\n1\nThe mean (over the simulations) number of doses in the credible interval of the estimate of the MED.\n\n\nSD Eff CI\n1\nThe standard deviation (over the simulations) of the number of doses in the credible interval of the estimate of the MED.\n\n\nPr(MED): &lt;dose&gt;\nOne per dose\nThe mean (over the simulations) of the posterior probability that a dose is the dose nearest / closest below / closest above the target efficacy rate.\n\n\nMED+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nMED+ Selection: &lt;dose+&gt;\nOne per dose\nAs MED Selection, but allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nMED+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses is most likely to be the minimum efficacious dose.\n\n\nOSD+ Selection: minus\n1\nThe proportion of simulations where a dose below the tested range of doses was the optimum selected dose.\n\n\nOSD+ Selection: &lt;dose+&gt;\nOne per dose\nAs OSD Selection, but allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nOSD+ Selection: plus\n1\nThe proportion of simulations where a dose above the tested range of doses was the optimum selected dose.\n\n\nPpn Best: &lt;dose&gt;\nOne per dose\nThe proportion of times the dose was the dose with the highest probability of being “Good”. Where Pr(Good) is the probability of a dose being both at or above the MED and at or below the MTD.\n\n\nPr(MED+) Below\n1\nThe mean (over the simulations) of the posterior probability that a dose below the dose range being tested is the MED.\n\n\nPr(MED+) Above\n1\nThe mean (over the simulations) of the posterior probability that a dose above the dose range being tested is the MED.\n\n\nTrue Efficacy: &lt;dose&gt;\nOne per dose\nThis is the true simulated efficacy rate for each dose in the scenario\n\n\nPost CE MED+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE MED+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the minimum efficacious dose, allowing for the possibility that the MED is below or above the dose range being tested.\n\n\nPost CE MED+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as minimum efficacious dose.\n\n\nPost CE OSD+: minus\n1\nThe proportion of simulations where after the cohort expansion a dose below the tested range of doses was selected as the optimum selected dose.\n\n\nPost CE OSD+: &lt;dose&gt;\nOne per dose\nThe proportion of simulations where, after cohort expansion, the dose was selected as the optimum selected dose, allowing for the possibility that the OSD is below or above the dose range being tested.\n\n\nPost CE OSD+: plus\n1\nThe proportion of simulations where after the cohort expansion a dose above the tested range of doses was selected as optimum selected dose.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#allocation-box-and-whisker-plot-all",
    "href": "documentation/v71/userguides/de.html#allocation-box-and-whisker-plot-all",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Allocation Box and Whisker plot (All)",
    "text": "Allocation Box and Whisker plot (All)\n\n\n\n\n\n\nFigure 29: Allocation Box and Whisker plot\n\n\n\nThis graph displays a box and whisker plot of the number of subjects allocated to each dose over the simulations. These plots show:\n\nThe mean allocation over all simulations at each dose plotted as a solid line.\nThe median value is plotted as a dashed line.\nThe 25-75th quantile range is plotted as the “box” portion of each point.\nThe “whiskers” extend to the largest and smallest values within 1 ½ times the interquartile range from either end of the box.\nPoints outside the whisker range are considered outliers, and are plotted as small blue dots. Note that it may be difficult to see all of these symbols if they are plotted at the same value.\n\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#resp-and-subject-alloc-all-crm",
    "href": "documentation/v71/userguides/de.html#resp-and-subject-alloc-all-crm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Resp and Subject Alloc (all CRM)",
    "text": "Resp and Subject Alloc (all CRM)\n\n\n\n\n\n\n\nCRM Toxicity, CRM Ordinal\nCRM Efficacy\n\n\n\n\n\n\n\n\nbCRM\n\n\n\n\n\n\n\n\nThese graphs show the mean subject allocation to each dose as a blue bar, along with, as appropriate lines showing the mean estimated toxicity/efficacy and the simulated ‘true’ toxicity/efficacy. The ‘error’ bars on the mean estimated toxicity/efficacy are the 95% interval of the mean estimates across the simulations.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#distribution-of-mtd-med-osd-and-te",
    "href": "documentation/v71/userguides/de.html#distribution-of-mtd-med-osd-and-te",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Distribution of MTD, MED, OSD and TE",
    "text": "Distribution of MTD, MED, OSD and TE\nMTD: Maximum Tolerated Dose\nMED: Minimum Effective Dose\nOSD: Optimum Selected Dose – if the MED is below the MTD then the OSD is the MED, otherwise it is the MTD.\nTE: Tolerated and Effective – the probability the dose is both below the MTD and above the MTD. In the results file this is under the column heading “Pr(Good)”.\n\n\n\n\n\n\n\n3+3 version\nCRM Toxicity, CRM Ordinal, bCRM - MTD\n\n\n\n\n\n\n\n\nCRM Efficacy, bCRM - MED\nbCRM – OSD and TE\n\n\n\n\n\n\n\nThese graphs show the proportion of times each dose has been selected as a particular target dose as a brown bar, along with lines showing the mean estimated toxicity/efficacy and the simulated ‘true’ toxicity/efficacy. The ‘error’ bars on the mean estimated toxicity/efficacy are the 95% interval of the mean estimates across the simulations.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#toxicity-and-allocation-obs-tox-and-alloc-obs-efficacy-and-allocation",
    "href": "documentation/v71/userguides/de.html#toxicity-and-allocation-obs-tox-and-alloc-obs-efficacy-and-allocation",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "“Toxicity and Allocation” / “Obs Tox and Alloc” / “Obs Efficacy and Allocation”",
    "text": "“Toxicity and Allocation” / “Obs Tox and Alloc” / “Obs Efficacy and Allocation”\n\n\n\n\n\n\n\n3+3 version, CRM Toxicity, CRM Ordinal, bCRM\nCRM Efficacy, bCRM\n\n\n\n\n\n\n\n\n\nThese graph show the mean allocation to each dose and the mean number of toxicities/efficacies observed at each dose.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#mtd-credible-interval",
    "href": "documentation/v71/userguides/de.html#mtd-credible-interval",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "MTD Credible Interval",
    "text": "MTD Credible Interval\n\n\n\n\n\n\nFigure 30\n\n\n\nCurrently only available for CRM Toxicity, this histogram shows the distribution of the final number of doses in the MTD credible interval.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#sample-size-mtd-histogram-all-crm",
    "href": "documentation/v71/userguides/de.html#sample-size-mtd-histogram-all-crm",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Sample Size MTD Histogram (All CRM)",
    "text": "Sample Size MTD Histogram (All CRM)\n\n\n\n\n\n\nFigure 31\n\n\n\nThis graph plots the distribution of the final sample sizes a a stacked bar chart with different shades of color indicating the proportion of simulations with that sample size had selected the different doses as MTD.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#simulation-allocation-history",
    "href": "documentation/v71/userguides/de.html#simulation-allocation-history",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Simulation Allocation History",
    "text": "Simulation Allocation History\n\n\n\n\n\n\n\n\n\n\n\n\n\n3+3 version, CRM Toxicity, bCRM\nCRM Ordinal\n\n\n\n\n\n\nCRm Efficacy, bCRM\n\n\n\n\nThis graph shows the allocation and toxicity/efficacy history for an individual simulation. Each dot represents a subject, the dot is colored red if toxicity was observed, light pink for a mild toxicity, blue for efficacy and grey otherwise. The subjects’ outcomes are displayed left to right in the order in which they were dosed, and at a height corresponding to the dose they were given.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed. A history can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#cohort-responses-sample-all-crm-except-efficacy",
    "href": "documentation/v71/userguides/de.html#cohort-responses-sample-all-crm-except-efficacy",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "Cohort Responses Sample (All CRM except Efficacy)",
    "text": "Cohort Responses Sample (All CRM except Efficacy)\n\n\n\n\n\n\n\nCRM Toxicity\nCRM Ordinal\n\n\n\n\n\n\n\n\nbCRM\n\n\n\n\n\n\n\n\nThis graph shows the dose allocation and resulting toxicities/efficacies along with the fitted dose-toxicity/efficacy model for a single simulated trial after the results have been gathered for a specific cohort of a specific trial.\nThe graph includes a drop down selector box allowing the user to select which simulated trial is displayed, and a further drop down selector box allowing the user to select at which cohort to display the state of the trial.\nThese details can only be displayed for trials for which a ‘cohorts’ file has been written out.\nIn CRM(Toxicity) if two samples (two populations) are used then there are separate plots for each sample.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/userguides/de.html#mtd-change-on-expansion",
    "href": "documentation/v71/userguides/de.html#mtd-change-on-expansion",
    "title": "FACTS Rule-Based Dose Escalation",
    "section": "MTD Change on Expansion",
    "text": "MTD Change on Expansion\n\n\n\n\n\n\nFigure 32: MTD change on expansion graph\n\n\n\nThis graph plots the dose selected as MTD before the results of the expansion cohort are available, against the dose selected as MTD after the expansion cohort results are available. Each simulation result is plotted as a ‘dot’, and the graph is divided up into a grid and dots located in the correct cell in the grid and then arranged within the cell depending on how many results fall into the cell. This allows the relative number of results in each cell to be appraised quickly by eye.\nThis graph allows the trial designer to see the degree of risk that the determination of the MTD will change as a result of the expansion cohort and whether the risk is predominantly that the determination will increase or decrease.\nThere are similar graphs showing the change in the estimate of the MED and OSD after the expansion cohort.",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "User Guides",
      "FACTS Rule-Based Dose Escalation"
    ]
  },
  {
    "objectID": "documentation/v71/examples/CRM/example1.html",
    "href": "documentation/v71/examples/CRM/example1.html",
    "title": "Example 1",
    "section": "",
    "text": "First example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "CRM",
      "Example 1"
    ]
  },
  {
    "objectID": "documentation/v71/examples/Staged/example1.html",
    "href": "documentation/v71/examples/Staged/example1.html",
    "title": "Example 1",
    "section": "",
    "text": "First example",
    "crumbs": [
      "Documentation",
      "Version 7.1",
      "Examples",
      "Staged",
      "Example 1"
    ]
  },
  {
    "objectID": "documentation/index.html",
    "href": "documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Welcome to the Documentation section — your go-to repository of in-depth user guides and practical examples that illuminate every aspect of FACTS. Whether you’re just getting started or delving into advanced functionalities, these resources are designed to help you navigate our software with confidence and ease.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/index.html#what-youll-find-here",
    "href": "documentation/index.html#what-youll-find-here",
    "title": "Documentation",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nUser Guides: Access detailed manuals covering the full range of our engines. From foundations to advanced features, learn how to set up your desired trial design, learn about best practices and obtain focused understanding of the subject matter, and troubleshoot common issues.\n\nCore Designs\nCRM\n\nExamples: Explore a range of constructed and real-world studies that demonstrate FACTS’s capabilities in action and discover strategies to optimize performance.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation/index.html#how-to-get-the-most-out-of-this-section",
    "href": "documentation/index.html#how-to-get-the-most-out-of-this-section",
    "title": "Documentation",
    "section": "How to Get the Most Out of This Section",
    "text": "How to Get the Most Out of This Section\nThis section contains resources for the current and past versions of FACTS, which you can choose in the sidebar. If necessary, begin by reviewing the installation guides. Later, you may consult engine-specific user manuals for deeper insights, and explore our curated examples to bring theory into practice. With these resources at your fingertips, you’ll gain a richer, more informed understanding of FACTS and maximize your potential for success.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "introduction/citation.html",
    "href": "introduction/citation.html",
    "title": "Citing FACTS and the FACTS Knowledge Hub",
    "section": "",
    "text": "FACTS\nPlease cite FACTS when you are writing papers or preparing slides for which you have used FACTS. Here are the most common citation formats:\n\nAPAMLAChicagoHarvardVancouverBibtex\n\n\nFACTS Development Team. (2024). FACTS: Fixed and Adaptive Clinical Trial Simulator (Version 7.1) [Computer software]. Berry Consultants LLC. https://www.berryconsultants.com/software/facts/\n\n\nFACTS Development Team. FACTS: Fixed and Adaptive Clinical Trial Simulator. Version 7.1, Berry Consultants LLC, 2024, https://www.berryconsultants.com/software/facts/.\n\n\nFACTS Development Team. 2024. FACTS: Fixed and Adaptive Clinical Trial Simulator (Version 7.1). Computer software. Austin, TX: Berry Consultants LLC. https://www.berryconsultants.com/software/facts/.\n\n\nFACTS Development Team, 2024. FACTS: Fixed and Adaptive Clinical Trial Simulator (Version 7.1) [Computer software]. Berry Consultants LLC, Austin, TX. Available at: &lt;https://www.berryconsultants.com/software/facts/&gt; [Accessed 31 Jan. 2025].\n\n\nFACTS Development Team. FACTS: Fixed and Adaptive Clinical Trial Simulator (Version 7.1) [Computer software]. Austin (TX): Berry Consultants LLC; 2024. Available from: https://www.berryconsultants.com/software/facts/.\n\n\n@techreport{FACTS71,\n  author = {{FACTS Development Team}},\n  title = {{FACTS}: Fixed and Adaptive Clinical Trial Simulator},\n  year  = {2024},\n  month = {12},\n  number = {Version 7.1},\n  type         = {Computer Software},\n  institution = {Berry Consultants LLC},\n  address = {Austin, TX},\n  note   = {https://www.berryconsultants.com/software/facts/}\n}\n\n\n\n\n\nFACTS Knowledge Hub\nPlease cite the FACTS Knowledge Hub when you are writing papers or preparing slides for which you have used the FACTS Knowledge Hub. Here are the most common citation formats:\n\nAPAMLAChicagoHarvardVancouverBibtex\n\n\nBerry Consultants. (2025). FACTS Knowledge Hub. http://docs.berryconsultants.com.\n\n\nBerry Consultants. \"FACTS Knowledge Hub.\" 2025, http://docs.berryconsultants.com.\n\n\nBerry Consultants. \"FACTS Knowledge Hub.\" 2025. http://docs.berryconsultants.com.\n\n\nBerry Consultants, 2025. FACTS Knowledge Hub. Available at: http://docs.berryconsultants.com. \n\n\nBerry Consultants. FACTS Knowledge Hub [Internet]. 2025. Available from: http://docs.berryconsultants.com.\n\n\n@misc{factshub,\n  author       = {Consultants, Berry},\n  title        = {FACTS Knowledge Hub},\n  howpublished = {\\url{http://docs.berryconsultants.com}},\n  year         = {2025}\n}",
    "crumbs": [
      "Introduction",
      "Citing FACTS and the FACTS Knowledge Hub"
    ]
  },
  {
    "objectID": "introduction/index.html",
    "href": "introduction/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the Introduction section — your starting point for learning how to use FACTS. Here, you’ll find everything you need to understand the fundamentals of our software - from step-by-step tutorials and guided walkthroughs to relevant webinars and other community resources.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#what-youll-find-here",
    "href": "introduction/index.html#what-youll-find-here",
    "title": "Introduction",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nGetting Started: Begin with the fundamentals. Understand the initial setup steps, such as downloading and installing FACTS.\nWebinars: Explore in-depth demonstrations led by our experts. Gain clarity on complex features, see best practices in action, and deepen your FACTS knowledge.\nTutorials: Learn how FACTS can create value for you at the design stage of a clinical trial or when conducting research. Navigate to the dropdown menu on the left to see tutorials for different types of clinical trials, from early to late stage.\nAcademic Materials: See how other teams and users are leveraging FACTS to achieve their goals. A broad collection of papers using FACTS, slides from presentations and other materials.\n\nIf you have any questions, don’t hesitate to contact us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html",
    "href": "introduction/gettingStarted/factsMenus.html",
    "title": "The Standard FACTS Menus",
    "section": "",
    "text": "FACTS has a File menu with commands similar to those found in most Windows applications, but also some unique to FACTS. The commands can be found in Table 1.\n\n\n\nTable 1: List of commands in the CRM file menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nNew\nAllows the user to select a trial design type and start a new design; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nOpen\nAllows the user to select an existing FACTS file and open it; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nSave\nSaves the current changes in the design to the currently open FACTS file.\n\n\nSave As\nAllows the current version of the design to be saved to a new FATCS file; this file is then the “currently open” FACTS file going forward.\n\n\nExport Project\nCreates a “Zip” archive containing the current FACTS file and all the simulation results, so these can be easily archived, emailed, saved, etc.\n\n\nRecent Projects\nContains entries for the last ten FACTS files opened: selecting one re-opens it.\n\n\nRecent Folders\nContains entries for the folders that contain the last ten FACTS files opened: selecting one opens a file browser window on that folder allowing a ‘.facts’ file in that folder to be selected (or allowing the user to navigate to some different folder and select a ‘.facts’ file there).\n\n\nExamples\nContains example projects for the different design engines: selecting one opens it.\n\n\nClose\nCloses the current FACTS file, returning the user to the FACTS introduction screen.\n\n\nExit\nCloses FACTS altogether.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html#the-file-menu",
    "href": "introduction/gettingStarted/factsMenus.html#the-file-menu",
    "title": "The Standard FACTS Menus",
    "section": "",
    "text": "FACTS has a File menu with commands similar to those found in most Windows applications, but also some unique to FACTS. The commands can be found in Table 1.\n\n\n\nTable 1: List of commands in the CRM file menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nNew\nAllows the user to select a trial design type and start a new design; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nOpen\nAllows the user to select an existing FACTS file and open it; if there are unsaved changes in the current design the user is prompted to save them first.\n\n\nSave\nSaves the current changes in the design to the currently open FACTS file.\n\n\nSave As\nAllows the current version of the design to be saved to a new FATCS file; this file is then the “currently open” FACTS file going forward.\n\n\nExport Project\nCreates a “Zip” archive containing the current FACTS file and all the simulation results, so these can be easily archived, emailed, saved, etc.\n\n\nRecent Projects\nContains entries for the last ten FACTS files opened: selecting one re-opens it.\n\n\nRecent Folders\nContains entries for the folders that contain the last ten FACTS files opened: selecting one opens a file browser window on that folder allowing a ‘.facts’ file in that folder to be selected (or allowing the user to navigate to some different folder and select a ‘.facts’ file there).\n\n\nExamples\nContains example projects for the different design engines: selecting one opens it.\n\n\nClose\nCloses the current FACTS file, returning the user to the FACTS introduction screen.\n\n\nExit\nCloses FACTS altogether.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html#facts-settings-menu",
    "href": "introduction/gettingStarted/factsMenus.html#facts-settings-menu",
    "title": "The Standard FACTS Menus",
    "section": "FACTS Settings Menu",
    "text": "FACTS Settings Menu\nThe “Settings” command menu allows the user to do 2 things:\n\nSet various FACTS options to local settings – see below for details.\nReset the options based on the stored configuration file. This file, “config.xml”, will initially be installed during the FACTS installation process and is stored in the Windows “Program Files” folder, in the sub-folder where FACTS get installed.\nChange the stored configuration file. This command allows you to select a new configuration file and have FACTS copy it to the sub-folder within the Windows “Program Files” folder, where FACTS get installed so it becomes the new stored configuration file. This allows IT support to easily disseminate configuration changes.\nEnter a new or changed license key.\n\n\nSet Options\nThe FACTS Options dialog allows the user to:\n\nSet and Test the connection parameters to access a compute grid for running simulations.\nConfigure the version and location of R or R Studio that can be launched from within FACTS\nSelect how gamma distributions are parameterized.\n\n\nGrid Configuration\nA grid compute facility for running simulations will only be available if your local IT services have set one up. If they have done so, they\n\nMay have already set the appropriate parameters In the FACTS configuration file included with the FACTS installation files.\nInform you of the parameters to be set manually via this dialog\nSend a new configuration file that can be installed using the “Load Options” menu command.\n\nIf modifying the grid options manually, select the “Options” menu command and enter the values on the “Grid Configuration” tab of the displayed dialog window.\n\n\n\n\n\n\nFigure 1: Webservice Configuration\n\n\n\nFirst select the type of interface to the grid to be used, this is either:\n\nVia a network shared drive (with a “sweeper script” running on a client machine to transfer jobs to the grid management system and return results from it).\nVia a web service system using a webserver and database to communicate to a grid management system. The IT group supporting the grid should be able to tell you which interface they have implemented, if any. If access to the grid is via a Network Share it is necessary to specify:\nThe location of the network share folder, usually in the form \\&lt;server name&gt;\\&lt;folder name\\&gt;.\nWhether the grid client is running Windows or Linux (so end-of-line characters can be corrected)\nThe listener delay – this is the interval between “looks” when FACTS is waiting for simulation results to be complete\n\nOnce specified it is possible to use the “Test” button to check that the Network Shared folder is accessible and writeable.\nIf access to the grid is via a web service:\n\nThe location of the web service endpoint.\n\nClicking on “Test Configuration” and will cause FACTS will attempt to connect to the FACTS grid controller. The control will show which components of the connection are working.\nSee the FACTS Installation Guide and FACTS Simple Grid Interface Guide for more details of setting up a grid.\n\n\nR Configuration\nIn FACTS on the Simulation tab there are two controls that launch R – “Open in R” and “ Design Report” (in FACTS 6.2.0 the latter only available for FACTS Core designs).\nTo enable these to work the user must specify where the R or RStudio executable is installed and (if there is more than one version of R installed) which version of R to use.\n\n\n\n\n\n\nFigure 2: The R Configuration Dialog\n\n\n\nThe dialog allows the user to Add, Edit, Test and Remove links to versions of R.\n\n\n\n\n\n\nFigure 3: Adding a link to R\n\n\n\nClicking on “Add” opens a normal Windows directory browser window, the user must navigate to the location of an R installation (for example “C:\\Program Files\\R\\R-2.15.2\\bin”, select the file R.exe, and click “Open”. This adds a new entry on the R configuration dialog.\nClicking on “Edit” operates similarly to “Add” above, except the selected location replaces that currently selected entry on the R configuration dialog rather than adding a new one.\nClicking on “Test” checks whether the currently selected entry on the R configuration dialog is available, if it is not an error dialog is displayed:\n\n\n\n\n\n\nFigure 4: Example of R Configuration error\n\n\n\nClicking on “Remove” removes the currently selected location on the R configuration dialog.\nThe version of R to use by default is selected by clicking on the ‘Active’ check box of the version to use.\n\n\nGamma Distribution Parameters\nIn FACTS a number of parameters require inverse gamma distributions to be specified as priors for the parameter value. There are two different parameterization of the inverse gamma provided so that the user can select the form they find the most intuitive.\n\n\n\n\n\n\nFigure 5: The parameterisation of Inverse Gamma Distributions\n\n\n\nThe first form uses parameters that are the mean of the distribution and the equivalent weight in terms of the equivalent number of observations. The second form uses an ‘Alpha’ and ‘Beta’ parameterization that some statisticians are familiar with and will find natural to use.\n\n\n\nEnter a license key\nIf a new license key is required, this command can be used to enter one. There are two ways of entering a new license:\n\n\n\n\n\n\nFigure 6: Enter FACTS License Key\n\n\n\nThe key can be entered directly, along with the associated Organization name, or by selecting a supplied license file.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/factsMenus.html#the-help-menu",
    "href": "introduction/gettingStarted/factsMenus.html#the-help-menu",
    "title": "The Standard FACTS Menus",
    "section": "The Help Menu",
    "text": "The Help Menu\nFACTS has a Help menu with commands to assist you with the use of FACTS, providing links to users guides, tutorial and training videos. The commands are:\n\n\n\nTable 2: List of commands in the CRM help menu\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\n\n\n\n\nUser Guides\nProvides access to documents such as this one, with (mainly) one user guide to each design type within FACTS. Exceptions to this simple structure are:  1. Core Design User Guide: A guide to the options under the ’Design” tab for FACTS Core for all endpoints.  2. Staged Design User Guide: As the staged design allows the design of one FACTS Core stage followed by a second, most of the interface is common to the basic FACTS Core. This guide describes the differences and additional aspects for all endpoints.  3. Dose Escalation User Guide: This covers all the Dose Escalation engines except for N-CRM and 2D-CRM that have their own. It thus covers the 3+3, mTPI, CRM(Toxicity), CRM(Ordinal), CRM(Efficacy) and bCRM engines.\n\n\nTutorials\nProvides access to all the tutorial documents, which describes detailed examples of use of all the engines in FACTS and many of their options. The examples under the File &gt; Examples menu option largely correspond to the different tutorials described here.\n\n\nDesign Specifications\nThese are technical documents that describe the mathematical models implemented in FACTS in detail.\n\n\nExecution Guides\nThe FACTS GUI can be run in command line mode so simulations can be run/re-run from scripts. With the simulation command line flag, and passed a directory rather than a file, FACTS will run simulations for every “.facts” file in that directory – and recurse into any sub-directories and simulate any “.facts” files there too. A full guide to command line mode can be found here. The FACTS simulation engines are also available in “command line executable” form. There are guides here that document their command line parameters and how to use them to analyse a data set – e.g. to perform an interim analysis whilst executing a trial designed with FACTS.\n\n\nFACTS file XML Specs\nThese guides describe the parameters in the “.facts” files, which are text files in XML format. For expert users understanding this format allows them to use scripts to generate versions of an initial “.facts” file with slight variations in the parameters such as stopping thresholds or priors. Modification of “.facts” files outside of FACTS needs to be done with care, errors may render the file unusable by FACTS.\n\n\nVideos\nProvides access to links to the introductory, training and webinar videos that Berry Consultants has recorded and makes available over the internet to FACTS users.\n\n\nView log…\nIf an error has occurred in FACTS, often the FACTS log file can shed light on what is going wrong. The log file is hidden away in some unfashionable and hard to locate Windows folder; this command option provides easy access to it. Allowing you to email facts support with a description of what occurred, attaching a copy of this log file having saved it somewhere convenient such as your desktop.\n\n\nSupport\nLaunch a simple editor for sending an email to our support account: facts@berryconsultants.net\n\n\nAbout\nDisplays a simple “about box” that includes the detailed version number of FACTS.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "The Standard FACTS Menus"
    ]
  },
  {
    "objectID": "introduction/gettingStarted/startingFacts.html",
    "href": "introduction/gettingStarted/startingFacts.html",
    "title": "Starting FACTS",
    "section": "",
    "text": "Starting FACTS\nFACTS is usually first started using the FACTS icon installed on the Windows desktop or from the Windows Start &gt; Programs menu.\nWhen the application opens, you are presented with the main introduction screen of FACTS (Figure 1). From here you can start designing an your trial, by double clicking on the design in the list of available designs, or by selecting the design option from the File &gt; New menu.\nFACTS will also be associated the parameter files it writes out with a ‘.facts’ file extension. Clicking on any one of these files will automatically start FACTS with that file opened.\nNote: Depending on your license, some design options may not be available.\n\n\n\n\n\n\nFigure 1: FACTS introduction screen\n\n\n\nFrom this screen a new design can be started by selecting the design type in the list and double clicking it, or clicking on the ‘create new’ button at the bottom of the list.\nIn the ‘Recent Work’ panel, the use can select from the list of most recently opened FACTS files, or from the list of directories where those files were located, opening the folder and selecting a FACTS file from there.\nOnce a FACTS design has been created, the appropriate FACTS design module can be launched by double clicking on the “.facts” file in Windows Explorer.\nOnce a specific type of FACTS design has been selected or an existing FACTS file opened, the FACTS GUI displays the tabs and parameters appropriate to that type of trial design.",
    "crumbs": [
      "Introduction",
      "Getting Started",
      "Starting FACTS"
    ]
  },
  {
    "objectID": "introduction/references.html",
    "href": "introduction/references.html",
    "title": "Academic Materials",
    "section": "",
    "text": "Explanation.\n\n\n\nExplanation.",
    "crumbs": [
      "Introduction",
      "Academic Materials"
    ]
  },
  {
    "objectID": "introduction/references.html#selected-papers",
    "href": "introduction/references.html#selected-papers",
    "title": "Academic Materials",
    "section": "",
    "text": "Explanation.\n\n\n\nExplanation.",
    "crumbs": [
      "Introduction",
      "Academic Materials"
    ]
  },
  {
    "objectID": "introduction/references.html#selected-slides",
    "href": "introduction/references.html#selected-slides",
    "title": "Academic Materials",
    "section": "Selected Slides",
    "text": "Selected Slides\n\nSlides 1\nExplanation.",
    "crumbs": [
      "Introduction",
      "Academic Materials"
    ]
  },
  {
    "objectID": "introduction/references.html#all-papers-citing-facts",
    "href": "introduction/references.html#all-papers-citing-facts",
    "title": "Academic Materials",
    "section": "All Papers citing FACTS",
    "text": "All Papers citing FACTS\nFor a comprehensive overview of all academic papers citing FACTS, click here (you will be redirected to Google Scholar).",
    "crumbs": [
      "Introduction",
      "Academic Materials"
    ]
  },
  {
    "objectID": "versions/v6/facts625.html",
    "href": "versions/v6/facts625.html",
    "title": "FACTS 6.2.5 Release Notes",
    "section": "",
    "text": "Berry Consultants would like to announce a new maintenance release, FACTS 6.2.5. FACTS 6.2.5 contains the following improvements to the FACTS 6.2.4 version:\nThis release addresses three rare situations in FACTS 6.2.4. If any of your designs replicate these exact circumstances you are recommended to upgrade to FACTS 6.2.5:\n\nIn FACTS Staged Design with a Time-to-Event end point and a predictor, if using, in stage 1, a predictive probability of success in stage 2, the imputation from the predictor was not being performed correctly.\nIn FACTS Staged Design, if all recruitment is completed in the first stage, so that only follow up remains in the second stage, if the second stage contains interims by time, these interims were not simulated.\nIn FACTS Core, if a Dunnett’s adjusted p-value QOI was defined and there was an additional p-value QOI defined after it, the results reported for the Dunnett’s adjusted QOI were corrupted.\n\nThe remaining, minor enhancement is in the FACTS 6.2.5 GUI:\n\nIn FACTS Core TTE, if QOIs using a Predictor endpoint were defined over and above the default ones, the GUI could delete these on re-opening the “.facts” file. Should this have happened to you, you would have seen FACTS display a warning message that it was deleting these QOIs. The GUI has been fixed so that this deletion no longer occurs. There have been no changes to Dose Escalation or Enrichment Designs. There have been no updates to the documentation or examples.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.2.5 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts630.html",
    "href": "versions/v6/facts630.html",
    "title": "FACTS 6.3.0 Release Notes",
    "section": "",
    "text": "FACTS 6.3.0 is now available for official release. This version contains significant changes to FACTS N-CRM Open Enrollment to make it more efficient, and adds to FACTS Core and FACTS Staged Designs (Continuous, Dichotomous and Multiple Endpoint) options to model arms that differ in strength along 2 dimensions (for example, but not limited to: dose strength and dosing frequency). Please contact us regarding any questions.\nIn detail the new features in FACTS 6.3.0 are:\n\nIn FACTS Dose Escalation N-CRM the open enrollment option has been improved:\n\nThe limit of the “maximum number of subjects without final results” is now applied per dose not overall. This means that after escalating to a higher dose, allocation is not held up waiting for later subjects on the lower dose to complete. Accrual is faster, fewer subjects are lost. If you are thinking of doing an open enrollment N-CRM design, we strongly recommend you update to FACTS 6.3.0.\nThe user supplies two limits, one used while allocating to an “uncleared” dose, the other used when allocating to a “cleared” dose (and hence allocating to the MTD).\nIf recruiting 2 groups, different maximums can be specified for the second group.\nThere is now an option so that the simulation of Open enrollment only “pauses” when the early stopping criteria are met, allowing enrollment to be re-started if the final follow up data move MTD to a dose where the stopping criteria are not met.\n\nIn FACTS Dose Escalation N-CRM the open enrollment option has a new feature, the option to use “backfill”. Enabling “backfill” allows a subject who would otherwise be lost (because the “maximum number of subjects without final results” is currently met) to be allocated to a lower dose. There are parameters to control the backfill:\n\nseparate trial maximums can be specified for the subjects allocated in escalation or to the MTD, or in backfill.\nlimits on how many subjects can be on a dose for it to be open for backfill.\nlimits on how high the dose must be before it is open for backfill.\nlimits on how close a dose must be to the current dose for it to be open for backfill.\n\nIn FACTS Dose Escalation N-CRM there are now more “run-in” options:\n\nsimple run-in (as in FACTS 6.2.0)\ncustom run-in – where the user precisely specifies the sequence of doses to be tested and the number of subjects to test at each dose.\nsmall cohort pre-escalation – this follows the full escalation rules, including overdose control but with a smaller cohort size – and the same number of cohorts required to clear doses. Like all run-ins, it ends when a toxicity is observed.\n\nIn FACTS Dose Escalation N-CRM the calculation of the likelihood when analyzing an Ordinal Toxicity endpoint has been improved. This means however that a design using Ordinal Toxicity created under FACTS 6.2.0 is likely to behave noticeably differently under FACTS 6.3.0. If the design is well advanced, or in use, you are advised to stay with using FACTS 6.2.0 for that design. If you are just starting out designing an Ordinal Toxicity endpoint N-CRM we recommend upgrading to FACTS 6.3.0.\nFACTS Core and FACTS Staged Designs features a new 2D treatment arm option and associated 2D response models. The 2D options are available for the Continuous, Dichotomous and Multiple Endpoints. The 2D treatment arm option allows:\n\nArms to be defined as a combination of 2 “factors” e.g. dose strength and dosing frequency, or dose strengths of two different agents.\nThe combinations can be analyzed independently, mapped onto a 1D ordering and analyzed with any of the standard 1D dose models, or with one of the three new 2D response models: a 2D NDLM, a 2D continuous factorial model, or a 2D discrete factorial model.\nTarget Quantities of Interest can be defined to be confined to those combinations in a particular row or column (e.g. the calculate the Pr(max) of the once a day doses).\n\nIn FACTS Enrichment Designs the implementation of fitting of the Hierarchical model (options for treatment arms across groups and control arms across group) have been improved. They should converge somewhat faster and at the FACTS default MCMC sample length (2500), will typically be more accurate than before.\n\nThis release addresses some situations in FACTS 6.2.0 that could cause errors. If any of your designs replicate these circumstances, you are recommended to upgrade to FACTS 6.3.0:\n\nIn FACTS 6.2.0 Dose Escalation 3+3, the simulations don’t properly implement the re-escalation rules after de-escalation. This was introduced when we made the significant extensions to N-CRM in FACTS 6.2.0.\nIn FACTS 6.2.0 Dose Escalation N-CRM many “pseudo-patients” parameters are not interpreted correctly.\nIn FACTS 6.2.0 Enrichment Designs with a Continuous endpoint, when using the Linear Regression Longitudinal Model, it fitted incorrectly when informative priors were used.\nIn FACTS 6 Core with a Continuous endpoint and simulating baseline, calculating a p-value QOI, with BOCF for missing data, the BOCF value for missing subjects was being set incorrectly (only a problem if baseline values are very difference from 0).\n\nThe following minor issues in the FACTS GUI were also fixed:\n\nIn FACTS Dose Escalation with N-CRM when specifying an open enrollment design, maximum subjects on MTD for “clearing” a dose and for stopping are meant to be entered in “subjects” but the GUI interpreted the input as “cohorts’ using whatever was the last cohort size in that “.facts” file.\nWhen using the “Ppn Correct Arm” in FACTS Core by marking arms as “should succeed” in the VSR profiles, if variants were not enabled, the variant target QOI arm selection criteria would incorrectly re-set to “Pr(Max)” when re-opening the file.\nWhen using a large external data file, running simulations with lots of packets could cause “out-of-memory” issues. Finally, some enhancements and fixes in the Design Report in FACTS Core have been implemented.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.3.0 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts641.html",
    "href": "versions/v6/facts641.html",
    "title": "FACTS 6.4.1 Release Notes",
    "section": "",
    "text": "1 Introduction\nBerry Consultants would like to announce a new maintenance release, FACTS 6.4.1. FACTS 6.4.1 contains the following improvements to the FACTS 6.4.0 version. Please contact us regarding any questions.\n\n\n2 FACTS (Staged) Core Improvements\n\nIn Time-to-Event designs, the sigmoidal, 3-parameter logistic and hierarchical logistic dose response models have been improved to better handle their respective likelihood evaluation. Namely, when the dose response is non-monotone, or the doses are widely separated.\nIn Time-to-Event designs, the prior for the sigmoidal model’s a2 parameter is now properly applied. As a result, estimates for the sigmoidal model’s a1 and a2 parameter have now been corrected.\nIn Time-to-Event designs, the option to model control separately in TTE predictor models is now applied correctly.\nIn Dichotomous designs, selecting the “Log-odds” parametrization of Posterior Probability QOIs will no longer be rejected as invalid if the Delta values for comparison are outside of [-1, 1].\nIn Multiple Endpoint designs with a dichotomous endpoint, Posterior Probability QOIs with the “Log-odds” parametrization will now be computed correctly.\nA very rare bug has been fixed that occurred when an adaptive design was converted back to a fixed design. The simulator would check the now irrelevant details of the interims and crash.\n\n\n\n3 FACTS Dose Escalation Improvements\n\nIn CRM(Efficacy) designs, FACTS files created with FACTS 6.1.0 or older versions will have their “Model control separately” setting correctly migrated over in FACTS 6.4.1 and later versions.\nIn N-CRM designs, the number of beta distribution samples in the specific quantiles prior derivation algorithm has been increased from 1,000 to 10,000.\nIn Dose Escalation designs, Windows and Linux simulation result differences have been resolved.\nIn 2D-CRM dose values of 0 are now allowed with some restrictions:\n\nany combination where the transformed dose strengths of both drugs are very low (or 0) must be excluded from the study and not have any prior toxicities specified as to have occurred on that combination. The model cannot fit toxicity on such combinations.\nif there is a combination where the transformed dose strength of both drugs are 0, the response model must be re-scaled (using the “Asymptotes” option) so the lower bound is not asymptotically 0, but some value slightly above that (such as 0.0001).\n\nIn 2D-CRM the prior graph on the Response Model tab can now show the sampled priors for the individual drugs without the lowest dose being plotted (when a dose 0 or very low dose is included this can compress the plot for all the other doses). The x-axis has also been re-labelled to make it clear the doses are being plotted at the log of their transformed dose values.\nIn N-CRM if using Open Enrolment and Backfill, the “Max Study Allocation for Escalation” was not being respected, this is fixed in FACTS 6.4.1.\n\n\n\n4 FACTS Enrichment Design Improvements\n\nFACTS will no longer error when running multiple scenarios when using external data files.\n\n\n\n5 Framework Improvements\n\nSimulation of FACTS files stored on a shared drive will be handled more robustly in the case of intermittent connectivity to the shared drive.\nRenaming of FACTS analyses on the Analysis tab will now correctly handle the situation when the analysis name has been unchanged.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.4.1 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts600.html",
    "href": "versions/v6/facts600.html",
    "title": "FACTS 6.0.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.0.0\nBerry Consultants is delighted to announce that FACTS 6.0.0 is ready for release!\nBuilding on FACTS 5, FACTS 6.0.0 adds a new simulation type: “FACTS Staged Design”.\n\nFACTS “Staged Design” is a simulator that runs a “FACTS Core” simulation followed by a second “FACTS Core” simulation that can take decisions based on the result of the first simulation and include data from the first simulation. This allows, for example, the simulation of a Phase II trial followed by a Phase III trial, whether as separate trials or as a seamless Phase II/III.\nFACTS Enrichment Designs includes the flexibility over the timing of interims and the ability to set different decision thresholds at different interims.\n\nFACTS 6.0.0 is fully backwards compatible with FACTS 5 – it can load and run all your FACTS 5 designs – and then add new FACTS 6.0.0 features to them. In particular you can load a FACTS Core design into FACTS Staged Design as the starting point for the design of the first stage. You can have FACTS 5 and FACTS 6.0.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Staged Design:\n\nThe simulation of one treatment selection stage followed by another.\nThe stages can be connected on a scale from completely seamless to completely independent.\nFACTS Staged Design can be used to simulate:\n\na Phase II and the consequential Phase III trials, or a seamless Phase II/III trial\na Phase IIA and the consequential Phase IIB trials, or a seamless Phase IIA/B trial\na Phase II trial with a treatment arm selection and expansion stage\n\nThe simulations include:\n\nDifferent options for specifying the interval between the stages\nDifferent options for which data from the first stage can be included in the second stage: all of it, none of it, all the data on the arms retained in the second stage, all the data on the study drug arms in the first stage pooled on the one study drug arm retained in the second stage and just subjects from the first stage who did not complete in that stage.\nRules for selecting which treatment arms are kept in the second stage or are dropped after the first stage, including rules on specific arms (such as “retain the top dose if …”), rules on specific target arms (such as “retain the Minimum Efficacious Dose which has a Hazard Ratio of X or less compared to the Control Arm”) rules across all arms (such as “retain the 2 treatment arms with the highest probability of having a response greater than control, as long as their probability of toxicity is less than …”) and rules applied to groups of treatment arms (such as “retain the two arms that are once a day treatments rather than the two that are twice a day treatments if …”).\nDifferent analysis models, allocation rules, interims and decision criteria for each stage.\n\nThe ability to take decision in Stage 1 based on the predictive probability of the outcome of stage 2.\nThe full simulation output of both stages.\nGraphs of the Stage 1, Stage 2, Dose Selection and Overall results.\n\nFACTS Enrichment Designs:\n\nAs in FACTS Core, the scheduling of interims can now be specified by the number of subjects who have completed or have completed up to a particular visit.\nThe decision criteria thresholds can be specified separately for different interims.\n\nFACTS Core:\n\nThe option to specify a deterministic accrual and/or deterministic allocation sequence, for example allowing custom dose escalation trials with cohort accrual, while allowing the full functionality of the Core engine\n\nFACTS Dose Escalation:\n\nIs unchanged.\n\n\n\n\n3 Downloading FACTS 6.0.0\nThe FACTS 6.0.0 release is available for download from the Berry Consultants FACTS 6 ftp repository at http://berry1.berryconsultants.com/facts_6/.\nLogin using:\n\nUsername: facts_6\nPassword: DynIrgyur4\n\nThere are, as usual, 4 files to download: Documents.zip, Examples.zip, Setup.msi, setup.exe. Obfuscated versions of setup.exe and Setup.msi have been included for those of you accessing through firewalls that prevent the download of .exe and .msi files. Having downloaded setup.e_e and Setup.m_i simply rename the file suffix before using.\n\n\n4 Installing FACTS 6.0.0\nAs with previous version of FACTS, FACTS 6.0.0 installs on a PC. Simply place the downloaded files on the PC / laptop it is to be installed on and run the file “setup.exe”. You will need a new FACTS license key for this new version. It will be sent to you separately.\nFor any support questions or issues, please contact us.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.0.0 Release Notes"
    ]
  },
  {
    "objectID": "versions/v6/facts610.html",
    "href": "versions/v6/facts610.html",
    "title": "FACTS 6.1.0 Release Notes",
    "section": "",
    "text": "1 Introducing FACTS 6.1.0\nBerry Consultants is delighted to announce that FACTS 6.1.0 is ready for release! Building on FACTS 6.0.0, FACTS 6.1.0 adds two new Dose Escalation simulation types: “FACTS 2D-CRM” and “FACTS mTPI”:\n\nFACTS “2D-CRM” is a simulator that runs simulations of dose escalation trials testing combinations of doses from 2 drugs. The implementation follows that of the 2D-CRM prototype that was available earlier this year.\n\n\n\n\n\n\n\nFACTS mTPI is an implementation of Yuan Ji’s “Modified toxicity probability interval method for dose-finding trials”.\n\n\n\n\n\n\nFACTS 6.1.0 also adds a major piece of simulation functionality across (almost) all FACTS engines: ‘Design Variants’, these allow you to have within one “.facts” file, multiple designs with different maximum sample sizes. This makes it much easier to estimate the required sample size for a design. The feature includes the ability to mark specific treatment arms or groups as ‘correct choices’, and FACTS now summarizes not only the proportions of successful and unsuccessful trials, but also proportions of successful trials that also made correct choices.\n\n\n\n\n\nFACTS 6.1.0 is fully backwards compatible with FACTS 6.0.0 and 5 – it can load and run all your FACTS 6.0.0 and FACTS 5 designs – and then use new FACTS 6.1.0 features with those designs. You can have FACTS 6.1.0 and FACTS 6.0.0 installed on the same machine, so it’s easy to have a transition period as you move to the new version.\n\n\n2 Key New Features\n\nFACTS Dose Escalation:\n\nDesign Variants in N-CRM.\n2D-CRM\nmTPI\n\nFACTS Enrichment Designs:\n\nDesign Variants\nThe ability to extend hierarchical modeling with clustered model.\n\nFACTS Core:\n\nDesign Variants\nBetter control over which frequentist calculations are performed.\nThe ability to use p-value QOIs for early success/futility decision making.\n\nFACTS Staged Design:\n\nDesign Variants\nThere is now an ‘Analysis’ tab in Staged Design.\n\n\n\n\n3 Downloading FACTS 6.1.0\nThe FACTS 6.1.0 release is available for download from the Berry Consultants FACTS 6 ftp repository at http://berry1.berryconsultants.com/facts_6/.\nLogin using:\n\nUsername: facts_6\nPassword: DynIrgyur4\n\nThere are, as usual, 4 files to download: Documents.zip, Examples.zip, Setup.msi, setup.exe. Obfuscated versions of setup.exe and Setup.msi have been included for those of you accessing through firewalls that prevent the download of .exe and .msi files. Having downloaded setup.e_e and Setup.m_i simply rename the file suffix before using.\n\n\n4 Installing FACTS 6.1.0\nAs with previous version of FACTS, FACTS 6.1.0 installs on a PC. Simply place the downloaded files on the PC / laptop it is to be installed on and run the file “setup.exe”. You will need a new FACTS license key for this new version. It will be sent to you separately.\nFor any support questions or issues, please contact us.",
    "crumbs": [
      "Versions",
      "FACTS 6 Release Notes",
      "FACTS 6.1.0 Release Notes"
    ]
  },
  {
    "objectID": "versions/v7/facts710.html",
    "href": "versions/v7/facts710.html",
    "title": "FACTS 7.1.0 Release Notes",
    "section": "",
    "text": "Introduction\nFACTS 7.1.0 is now available for download via App Center. FACTS users can now:\n\nPerform concurrent control analyses using posterior probabilities, predictive probabilities and p-values in Platform Trial designs.\nMake interim and final decisions based on conditional power as well as Bayesian predictive probabilities.\nExport the data associated with the in-built graphs FACTS provides into a CSV file.\nExplore, via integration with AIRSHIP, simulation results graphically in a much more generic, versatile way; namely, by allowing the user to view the impact of simulation input dimensions through dynamic filtering of the simulation results.\nCreate designs which use an independent Beta Binomial dose response model for analyzing (simulated) data for dichotomous endpoints in Core and Staged designs.\n\nCreate designs where frequentist (p-value) calculations are performed using the Fisher exact test rather than a normal approximation for dichotomous endpoints.\nProvide three separate patient queue lengths for Continual Reassessment Methods (CRM) designs, based on dose clearance and the current model estimate of the MTD. In addition, the ability to backfill to the current escalation dose (“frontfill”) is now available.\nView explanations of some of the most important inputs in Core Continuous/Dichotomous designs through informative tooltips.\nSet different random number seeds when simulating multiple design scenarios.\nCreate designs where decision QOIs can be used in multiple stopping criteria simultaneously.\n\n\n\nFACTS Core and Staged Improvements\n\nIn Core and Staged designs making use of dichotomous endpoints, FACTS users can specify a beta binomial model to independently model the response rate on each arm. With this model, the user specifies a Beta distribution prior on the response rate.\nIn Core and Staged designs which perform frequentist analyses on dichotomous endpoints, users can now specify whether p-value calculations (including Current Trial Predictive Probability QOIs) should be performed using a normal approximation as currently, or using a Fisher exact test. The latter is of particular use when sample sizes are small.\nIn Staged designs, the reported Early Success Time will now correctly only be reported for simulations which have stopped for early success. Previously, simulations which have graduated early to stage 2 in stage 1 were being reported as having an Early Success Time.\nWhen running analyses in FACTS Core and Staged designs, the user can now specify the number of MCMC samples to generate per imputation of missing data modelled by a longitudinal model.\nIn Core and Staged designs with a dichotomous endpoint, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Core and Staged designs, a new Operating Characteristics graph displaying the cumulative proportion of simulations having stopped at a given duration, sample size or number of events (in Time-to-Event) has been added.\nIn Core designs, two new summary columns “Ppn Overall Success” and “Ppn Overall Futility” have been added, which simply sum up the existing columns “Ppn Early Success” and “Ppn Late Success”, and “Ppn Early Futility” and “Ppn Late Futility”, respectively. This allows users to quickly view the estimated type I error and power of the design.\nIn Core and Staged designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Staged designs, the Stage 2 “Explore” Final Success/Futility graphs will now have the option to include/exclude simulations which have stopped in Stage 1.\nIn Core and Staged designs, the criteria for selecting a dose at the end of the trial have moved from the “Variants” tab to the Success/Futility tab. These criteria will be used when reporting the proportion of times the correct/incorrect arm was selected at the end of the trial (as reported in the “Ppn Correct/Incorrect Arm” columns, which are now reported in the summary file).\nIn Core and Staged Multiple Endpoint designs, designs not using a control arm will no longer crash when adding a new endpoint, and no longer crash when adding a new treatment to a design which uses Virtual Subject Response external data files.\nIn Core and Staged designs, the “Legacy Second Order NDLM” dose response model and the “Legacy Adaptation” allocation option have been removed. Older designs making use of these features will be migrated over to the “Second Order NDLM” dose response model and the “Fixed Allocation” allocation option, respectively, when loaded in FACTS 7.1. A warning prompt will appear for such designs.\nUsers can now choose a decision QOI in multiple stopping criteria simultaneously. Previously, after a stopping rule was created based on a certain decision QOI, any additional stopping rule could not use this decision QOI.\nIn Core and Staged Designs, a new class of QOI as a sub-category of Predictive Probabilities was introduced: Conditional Power. Contrary to the existing Bayesian Predictive Probabilities, Conditional Power assumes the observed treatment effect estimates to be the truth and then calculates the probability of being successful either: 1) at a later final analysis, 2) after the currently enrolled subjects are followed up or 3) in a future trial.\nFACTS will now correctly calculate the Bayesian Predictive Probabilities of success at the final analysis when a continuous endpoint and change from baseline as the final endpoint are used.\nIn Core and Staged Time-to-Event designs, minimum information required to trigger an interim will now be available when interims are based on predictor events.\nIn Core and Staged Time-to-Event designs, the complete information columns report at interims with the weeks files will now display the correct information when interims are based on predictor events.\nIn Core and Staged Time-to-Event designs with a Time-to-Event predictor, when interim information events and the event max cap differ (e.g. max cap on final events but interims based on predictor events), simulations will correctly stop at the specified max event cap.\nIn Core and Staged Time-to-Event designs with a Time-to-Event predictor, current trial predictive probability QOIs are now handled correctly when max event caps refer to predictor events.\nIn Staged Time-to-Event designs with a Time-to-Event predictor, current trial predictive probability QOIs are now handled correctly when interim information events and the event max cap differ (e.g. max cap on final events but interims based on predictor events).\nIn Staged Time-to-Event designs, the censoring and event time of subjects in Stage 2 which have not observed their event in Stage 1 will now be handled correctly for both their final and predictor endpoints.\nIn Core and Staged design, (Continuous, Dichotomous, and TTE) when a trial stops for early success or early futility and the option to continue follow-up after that decision is not selected, the final analysis model is no longer run. The data at the final analysis would be identical to the data at the interim analysis that the stopping threshold was hit at, so the model output for the final is now identical to the interim.\nTime Course Hierarchical longitudinal model has improved performance with informative priors on the variance components.\nIn Staged designs, FACTS will now correctly handle the Stage 1, Stage 2 and overall sample size caps (maximum number of subject caps and maximum number of event caps) specified in the Variants tab.\nIn Core and Staged Time-to-Event designs, the Cox Proportional Hazards current trial predictive probability calculation has been corrected.\nIn Core and Staged designs, when using predictive probabilities that predict success at trial maximum, but the trial is stopped at an interim, the predictive probability is now calculated based on the originally specified number of subjects at trial maximum rather that assigning the predictive probability a value of 0 or 1 depending on the p-value of the interim data.\nIn Core and Staged designs, when using predictive probabilities, visit values are now correctly imputed when there is only baseline visit data available.\nIn Core and Staged designs, when using a dichotomous endpoint and predictive probabilities of treatment versus control, FACTS now performs a Farrington and Manning Test when there is a superiority or non-inferiority margin.\nIn Staged design, the arm selection logic has been updated to properly account for arm-dropping in stage 1 when selecting a single arm from each group.\n\n\n\nFACTS Enrichment Design Improvements\n\nIn Enrichment Dichotomous designs, users can now perform frequentist calculations using a normal approximation as currently, or using a Fisher exact test. The latter is of particular use when sample sizes are small.\nIn Enrichment Time-to-Event designs, the GUI will now correctly calculate the mean frequentist estimated treatment effect and its standard error.\nWhen running analyses in Enrichment designs, the user can now specify the number of MCMC samples to generate per imputation of missing data modelled by a longitudinal model.\nIn Enrichment Dichotomous designs, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Enrichment designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration, sample size or number of events (in Time-to-Event) has been added.\nIn Enrichment designs, two new summary columns “Ppn Overall Success” and “Ppn Overall Futility” have been added, which simply sum up the existing columns “Ppn Early Success” and “Ppn Late Success”, and “Ppn Early Futility” and “Ppn Late Futility”, respectively. This allows users to quickly view the estimated type I error and power of the design.\nIn Enrichment designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Enrichment designs, the frequentist test reported in the frequentist simulations file will now make it clear whether the performed test is a one-sided or two-sided test. In the associated frequentist summary file, the proportion of frequentist results for each test type that are significant will now correctly take into account the correct alpha level depending on whether the test is one-sided or two-sided.\n\n\n\nFACTS Platform Trial Improvements\n\nBREAKING CHANGE: in Platform trial designs, the numerical value representing the outcomes for late futility and early futility have been swapped in FACTS 7.1.0. Any older designs with futility criteria will need to be re-simulated. Decision numeric values in FACTS Platform Trials and FACTS Core now match.\nIn Platform Trial designs, users can now specify whether Posterior Probability QOIs, Predictive Probability QOIs or p-value QOIs should be calculated based on the entire control population in the trial (as previously) or based on the given treatment’s concurrent control population. These concurrent control QOIs can be used as treatment stopping criteria like any other QOI.\nA time window allowing a treatment’s concurrent controls to additionally include control patients a certain number of weeks from the treatment entering the trial can also be specified.\nPr(Max) Target Dose QOIs can now be calculated based on all arms in the trial (as previously), only active arms or only randomizing arms.\nIn Platform Trial Dichotomous designs, the 95% confidence interval of the raw treatment estimates will now be reported correctly in relevant graphs.\nIn Platform Trial designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration or sample size has been added.\nIn Platform trial designs, users can now specify different variants of the design by modifying both the maximum number of participants per treatment as well as the maximum number of concurrent treatments. These variants will display as separate scenarios on the Simulations tab.\nIn Platform trial designs, the number of participants at the end of each simulated trial who have had outcomes (“completers”) will now be summarized. The interim subject information (reported in the weeks file “Complete Info” columns) will now also be computed at the final analysis, reported in the simulations file and summarized.\nIn Platform trial designs, the “Per Sim: Arm and Participant Arrival” graph will now correctly display the accrual period for the control arm to end when the last treatment’s accrual period ends.\nUsers can now choose a decision QOI in multiple stopping criteria simultaneously. Previously, after a stopping rule was created based on a certain decision QOI, any additional stopping rule could not use this decision QOI.\nFACTS will now correctly calculate the Bayesian Predictive Probabilities of success at the final analysis when a continuous endpoint and change from baseline as the final endpoint are used.\n\n\n\nFACTS Dose Escalation Improvements\n\nThe Dose Escalation design types CRM(Toxicity), CRM(Ordinal), bCRM and CRM(Efficacy) have all been deprecated. Users can still create new designs or open existing designs with these design types. We recommend using the more general and versatile “Continual Reassessment Methods (CRM)” design type (formerly known as “N-CRM”) for any 1D model based CRM designs.\nIn CRM with open enrollment, users can now specify whether they want to allow backfilling to the highest dose (“frontfilling”) and conditions under which to do so. Users can choose whether to count these patients towards the backfill subject cap or regular allocation cap.\nIn CRM with open enrollment, previously two queues (maximum number of subjects on uncleared doses and maximum number of subjects on cleared doses) determined the allocation behavior. In order to give users more flexibility and control, there are now three queue concepts (maximum number of subjects on uncleared doses, maximum number of subjects on cleared doses at MTD and maximum number of subjects below MTD). These queues are now used in the same way in the MTD and MED phase of the trial. The concept of max cleared dose and the new queues are now harmonized.\nIn CRM, the stopping rule checker design was updated. In case of regular dosing, a concept of near doses like that of fine spaced dosing was introduced and both concepts aligned for both stopping in the MTD and MED phase. The stopping rule checker now also correctly checks the hierarchies and join conditions.\nIn CRM with open enrollment, backfill will now correctly evaluate all cleared doses for eligibility.\nIn CRM with open enrollment and fine spaced dosing, near doses are now correctly evaluated in regular allocation, backfill and for the purposes of stopping.\nIn CRM designs, FACTS will now correctly apply study size constraints and queue size constraints when the underlying toxicity model considers all doses to be toxic. This also includes preventing an expansion cohort from being allocated in this situation.\nIn Dose Escalation designs, a new Operating Characteristics graph displaying the cumulative proportion of simulation having stopped at a given duration or sample size has been added.\nIn 2D-CRM designs, the dose response model’s eta parameter can now be specified in log-normal space.\nIn 2D-CRM designs with custom run-in, FACTS now proceeds to the chosen escalation scheme more promptly, without first allocating more than one full-size cohort unless they are specified in the run-in.\nThe “Per Sim Allocation History Grouped” plot will now be displayed correctly if, previously, the “Per Sim Allocation History Group 1” plot had been set to space interims equally.\nIn CRM with an initial run-in, escalation will now be performed correctly and maximum cleared dose tracked correctly in all circumstances.\nIn CRM with small-cohort pre-escalation, we now only switch to regular escalation based on observed toxicities, not MTD estimate or overdose control rules.\nIn CRM, introduce the notion of “Selected MTD”, “Selected OSD” and “Selected MED” at the end of trial as a function of the respective model estimates and the max cleared dose.\nIn CRM with two groups that enroll consecutively, there are now several options regarding what dose level to start escalation at in group 2.\nIn CRM with fine grained dosing, the concept of near doses is now correctly applied in the escalation phase, when calculating the max cleared dose and adjusted for the new queue concepts.\nIn CRM, cohort expansion will now enroll the correct number of patients even when accrual is very fast.\nIn CRM, when using a cohort expansion or a two group design, the trial state on which both of these concepts depends is now the final state of the previous trial, not the state of the previous trial when its stopping criteria were met.\nIn CRM, when using two groups and expansion cohorts in both groups, the group 2 expansion cohort now correctly uses its own cap regarding maximum number of subjects. In the same setting, a rare circumstance led to the allocation in the group 1 cohort expansion to continue beyond the max cap – this is now resolved.\nIn CRM, stopping based on the max cap of subjects for escalation now does not lead to an overrun in patients in rare circumstances.\nIn CRM with both an MTD and an MED phase (toxicity and efficacy endpoints), the transition between the phases is now handled correctly.\nIn CRM, improved labels and default values in the GUI, such as improved values for the overdose control and open enrollment queue lengths and clearer labels in the Allocation tab.\nSeveral GUI stability improvements in the CRM engine.\nIn CRM, there are several improvements to the “Per Sim Allocation History”, “Alloc and Tox History”, “Cohort Band Probabilities” and “Cohort Response” graphs, including showing the cohort expansions subjects separately, showing the max cleared dose at any point in time and showing the selected MTD/MED at the end of the trial.\n\n\n\nGeneral Improvements\n\nFACTS has now been integrated with AIRSHIP, which allows simulation results to be explored graphically in a much more generic, versatile way. Once simulation have completed, results can be explored with AIRSHIP by clicking on “Explore Results…” &gt; “Compare Scenarios in AIRSHIP”. Note that use of AIRSHIP requires at least two scenarios to have been simulated and their results aggregated.\nThe process of making the FACTS inputs more intuitive has started as of FACTS 7.1.0. In FACTS Core Continuous/Dichotomous designs, tooltips will appear against many of the inputs (when hovering over the relevant input) with explanatory text about their use and impact on the design. Tooltips can be disabled by going to Settings &gt; Options &gt; Tooltip Configuration.\nThe data associated with all graphs displayed in FACTS can now be exported into a CSV format. In addition, hovering over these graphs will display the associated data point value as a tooltip.\nWhen simulating multiple scenarios, each simulated scenario can now be simulated with a different random number seed.\nWhen running simulations for a directory of FACTS files in FACTS Command Line or FLFLL, a different base seed can be set for each design within the directory.\nThe order of scenarios to simulate as displayed on the Simulation tab has been set to be consistently displayed in alphabetical order.\nWhen aggregating simulation results for a design using variants, the relevant variant number will now correctly be displayed in the aggregated files.\nSeveral bug fixes and improvements have been made to all design reports.\nThe associated engine executable now provides a new -o output flag argument specifying the location where to output all files generated by the engine [Enterprise licensees only].\n\nPlease contact us regarding any questions.",
    "crumbs": [
      "Versions",
      "FACTS 7 Release Notes",
      "FACTS 7.1.0 Release Notes"
    ]
  },
  {
    "objectID": "versions/roadmap.html",
    "href": "versions/roadmap.html",
    "title": "FACTS Roadmap",
    "section": "",
    "text": "We are excited to announce that we are currently working on:\n\nFACTS Core Ordinal Endpoint\nFACTS Dose Escalation: i3+3, mTPI-2 and BOIN\nNew Response Adaptive Allocation options\nGreater flexibility in adaptive decisions (e.g. being able to combine different QOIs in an AND/OR logic, but also many more)\nQuick Start options to enter standard design types (such as widley used group sequential designs)\n\nIf you have any questions, suggestions or requests, please contact us.",
    "crumbs": [
      "Versions",
      "FACTS Roadmap"
    ]
  },
  {
    "objectID": "versions/roadmap.html#section",
    "href": "versions/roadmap.html#section",
    "title": "FACTS Roadmap",
    "section": "",
    "text": "We are excited to announce that we are currently working on:\n\nFACTS Core Ordinal Endpoint\nFACTS Dose Escalation: i3+3, mTPI-2 and BOIN\nNew Response Adaptive Allocation options\nGreater flexibility in adaptive decisions (e.g. being able to combine different QOIs in an AND/OR logic, but also many more)\nQuick Start options to enter standard design types (such as widley used group sequential designs)\n\nIf you have any questions, suggestions or requests, please contact us.",
    "crumbs": [
      "Versions",
      "FACTS Roadmap"
    ]
  },
  {
    "objectID": "get/index.html",
    "href": "get/index.html",
    "title": "Get FACTS",
    "section": "",
    "text": "FACTS is a powerful software tool that is trusted by half of the top 20 pharma companies to help them design efficient and successful clinical trials. We are proud that also numerous academic, government and regulatory institutions trust FACTS.\n\n\nIndustry\nWe offer a 3-months free FACTS Evaluation License to showcase the power and features of our FACTS simulation tool. Please contact us to get a free demo, or learn more about this offer and our regular licenses.\n\n\nAcademia / Charities / Regulatory Bodies / Government\nTo academic and other non-profit research institutions and regulatory bodies, we will generally offer a free FACTS license under certain conditions. Please contact us to see if your organization qualifies.",
    "crumbs": [
      "Get FACTS"
    ]
  },
  {
    "objectID": "blog/posts/2024-10-14.html",
    "href": "blog/posts/2024-10-14.html",
    "title": "Post1",
    "section": "",
    "text": "TEXT TEXT\nTEXT"
  },
  {
    "objectID": "blog/posts/2024-10-13.html",
    "href": "blog/posts/2024-10-13.html",
    "title": "Post2",
    "section": "",
    "text": "TEXT TEXT\nTEXT"
  },
  {
    "objectID": "concepts/bayes/primer.html",
    "href": "concepts/bayes/primer.html",
    "title": "Primer",
    "section": "",
    "text": "Bayesian statistics is a paradigm in statistical inference that emphasizes the use of probability to quantify uncertainty in parameters of interest. Unlike frequentist methods that consider parameters as fixed values and probabilities as long-run frequencies of repeated experiments, Bayesian methods treat parameters as random variables and update beliefs about these parameters through observed data.\nIn the context of biostatistics and clinical trials, Bayesian approaches can be particularly powerful due to their flexibility in incorporating prior information, handling complex hierarchical structures, and adapting trial designs in real time.\n\n\n\nBayes’ Theorem\n\n\\[\n   p(\\theta \\mid x) \\;=\\; \\frac{p(x \\mid \\theta) \\, p(\\theta)}{p(x)},\n\\]\n\n\\(p(\\theta \\mid x)\\) is the posterior distribution of the parameter(s) \\(\\theta\\) after observing data \\(x\\).\n\\(p(x \\mid \\theta)\\) is the likelihood, describing how probable the observed data \\(x\\) are under a specific value of \\(\\theta\\).\n\\(p(\\theta)\\) is the prior distribution, reflecting one’s beliefs about \\(\\theta\\) before seeing data.\n\\(p(x)\\) is the marginal likelihood or evidence, often treated as a normalizing constant.\n\n\nPrior Distributions\n\nA prior distribution encodes existing knowledge or expert belief about a parameter before observing new data.\nIn clinical trials, priors can come from historical data, expert opinion, or pilot studies.\n\nPosterior Distributions\n\nAfter data collection, the prior is updated into the posterior, balancing what we believed before with the evidence from the data.\nSummaries of the posterior (e.g., mean, median, credible intervals) provide the final inference on \\(\\theta\\).\n\nPredictive Distributions\n\nBayesian methods naturally extend to predictive distributions, which forecast future observations based on current data and knowledge.\nIn clinical trials, predictive distributions can guide adaptive decision-making, such as adding or dropping arms in a multi-arm study or changing randomization probabilities.\n\nModel Checking and Diagnostics\n\nBayesian methods typically involve complex models that may need diagnostic checks (e.g., posterior predictive checks) to evaluate model fit and reasonableness of priors.\n\n\n\n\n\n\nAdaptive Designs\n\nMany modern clinical trials are adaptive, allowing modifications to the trial as data accumulate.\nBayesian adaptive designs can enable real-time updating of randomization probabilities, early stopping for futility or efficacy, and seamless phase transitions.\n\nBorrowing Information\n\nIn rare diseases or small populations, data may be limited.\nHierarchical Bayesian models allow borrowing of information across subgroups (e.g., biomarkers, disease subtypes), leveraging prior trial data or real-world evidence.\n\nFlexibility\n\nBayesian methods handle complex data structures (e.g., survival data with covariates, longitudinal measurements, high-dimensional biomarker data) within a coherent probabilistic framework.\n\nTransparency in Uncertainty\n\nPosterior distributions directly quantify uncertainty about parameters, often providing intuitive credible intervals (e.g., a 95% credible interval has a 95% probability of containing the true parameter).\n\n\n\n\n\nBayesian statistics offers a flexible and coherent framework for inference, particularly suited to biostatistics and clinical trials where:\n\nAdaptive decisions are made mid-trial,\nBorrowing of information is highly valuable (e.g., rare diseases, historical controls),\n\nComplex modeling scenarios frequently arise (e.g., hierarchical structures, multiple endpoints).\n\nBy updating beliefs as new data accumulate, Bayesian methods naturally align with the iterative learning process that defines scientific discovery and the realities of clinical research.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/bayes/primer.html#key-concepts-in-bayesian-statistics",
    "href": "concepts/bayes/primer.html#key-concepts-in-bayesian-statistics",
    "title": "Primer",
    "section": "",
    "text": "Bayes’ Theorem\n\n\\[\n   p(\\theta \\mid x) \\;=\\; \\frac{p(x \\mid \\theta) \\, p(\\theta)}{p(x)},\n\\]\n\n\\(p(\\theta \\mid x)\\) is the posterior distribution of the parameter(s) \\(\\theta\\) after observing data \\(x\\).\n\\(p(x \\mid \\theta)\\) is the likelihood, describing how probable the observed data \\(x\\) are under a specific value of \\(\\theta\\).\n\\(p(\\theta)\\) is the prior distribution, reflecting one’s beliefs about \\(\\theta\\) before seeing data.\n\\(p(x)\\) is the marginal likelihood or evidence, often treated as a normalizing constant.\n\n\nPrior Distributions\n\nA prior distribution encodes existing knowledge or expert belief about a parameter before observing new data.\nIn clinical trials, priors can come from historical data, expert opinion, or pilot studies.\n\nPosterior Distributions\n\nAfter data collection, the prior is updated into the posterior, balancing what we believed before with the evidence from the data.\nSummaries of the posterior (e.g., mean, median, credible intervals) provide the final inference on \\(\\theta\\).\n\nPredictive Distributions\n\nBayesian methods naturally extend to predictive distributions, which forecast future observations based on current data and knowledge.\nIn clinical trials, predictive distributions can guide adaptive decision-making, such as adding or dropping arms in a multi-arm study or changing randomization probabilities.\n\nModel Checking and Diagnostics\n\nBayesian methods typically involve complex models that may need diagnostic checks (e.g., posterior predictive checks) to evaluate model fit and reasonableness of priors.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/bayes/primer.html#why-bayesian-methods-are-useful-in-biostatistics-and-clinical-trials",
    "href": "concepts/bayes/primer.html#why-bayesian-methods-are-useful-in-biostatistics-and-clinical-trials",
    "title": "Primer",
    "section": "",
    "text": "Adaptive Designs\n\nMany modern clinical trials are adaptive, allowing modifications to the trial as data accumulate.\nBayesian adaptive designs can enable real-time updating of randomization probabilities, early stopping for futility or efficacy, and seamless phase transitions.\n\nBorrowing Information\n\nIn rare diseases or small populations, data may be limited.\nHierarchical Bayesian models allow borrowing of information across subgroups (e.g., biomarkers, disease subtypes), leveraging prior trial data or real-world evidence.\n\nFlexibility\n\nBayesian methods handle complex data structures (e.g., survival data with covariates, longitudinal measurements, high-dimensional biomarker data) within a coherent probabilistic framework.\n\nTransparency in Uncertainty\n\nPosterior distributions directly quantify uncertainty about parameters, often providing intuitive credible intervals (e.g., a 95% credible interval has a 95% probability of containing the true parameter).",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/bayes/primer.html#summary",
    "href": "concepts/bayes/primer.html#summary",
    "title": "Primer",
    "section": "",
    "text": "Bayesian statistics offers a flexible and coherent framework for inference, particularly suited to biostatistics and clinical trials where:\n\nAdaptive decisions are made mid-trial,\nBorrowing of information is highly valuable (e.g., rare diseases, historical controls),\n\nComplex modeling scenarios frequently arise (e.g., hierarchical structures, multiple endpoints).\n\nBy updating beliefs as new data accumulate, Bayesian methods naturally align with the iterative learning process that defines scientific discovery and the realities of clinical research.",
    "crumbs": [
      "Concepts",
      "The Bayesian Approach",
      "Primer"
    ]
  },
  {
    "objectID": "concepts/index.html",
    "href": "concepts/index.html",
    "title": "Concepts",
    "section": "",
    "text": "Welcome to the Concepts section — your resource for building a strong intellectual foundation on clinical biostatistics and clinical trial simulation. Here, we break down the core principles you’ll need to understand clinical trials and Bayesian statistics and provide pro tips to get the most out of FACTS.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#what-youll-find-here",
    "href": "concepts/index.html#what-youll-find-here",
    "title": "Concepts",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nBasics of Clinical Trials: Learn the essentials, terminology, and best practices that define the clinical trial landscape. From study design and data collection to regulatory considerations, we’ll walk you through the elements that matter most.\nStatistical & Mathematical Fundamentals: Uncover the foundational concepts in statistics and mathematics that empower you to make data-driven decisions. Understand key analyses, Bayesian approaches, and appreciate why certain metrics and models are essential to reliable results.\nTips & Tricks for Using FACTS: Gain insider know-how that helps you work smarter and faster.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/index.html#how-to-make-the-most-of-this-section",
    "href": "concepts/index.html#how-to-make-the-most-of-this-section",
    "title": "Concepts",
    "section": "How to Make the Most of This Section",
    "text": "How to Make the Most of This Section\nUse the Concepts section as a springboard to build your expertise, whether you’re new to clinical trials or looking to refine your analytical skills. By solidifying your grasp of fundamentals and strengthening your analytical toolkit, you’ll be better equipped to understand results and ask the right questions. Check back often as we continue expanding these resources to support your growth.",
    "crumbs": [
      "Concepts"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/index.html",
    "href": "concepts/adaptiveDesigns/index.html",
    "title": "Adaptive Trials",
    "section": "",
    "text": "To understand adaptive trials it may be easiest to look at a fixed trial. Traditionally a fixed sample size is specified with fixed allocation and fixed entry criteria. The data is not analyzed until trial completion, which is typically years after the trial began. This 75-year-old design approach creates very restrictive design types – and really forces the design team to guess at the doses, range, patient population, duration and frequency of treatment, etc, because a fixed design requires linear effort spent for each treatment arm.\nAn adaptive trial is like driving with your eyes open! It allows the pre-specification of flexible components to the major aspects of the trial, like the treatment arms used (dose, frequency, duration, combinations, etc), the allocation to the different treatment arms, the patient population used, and the sample size. An adaptive design can learn from the accruing data what the most therapeutic doses or arms are, allowing the design to hone in on the best arms. This allows the design to start with a wider range of doses – say 8 instead of 3 – with using a smaller number of patients. The result is a smart design, using resources (including time) much more efficiently, at the same time increasing the scientific precision. Well constructed adaptive designs can be better for all involved – better learning, more efficient, better treatment for subject in a trial, better information for regulators and the medical community.\nThe one drawback is that adaptive designs are more work to construct. It involves clinical trial simulation to make sure the design works well, meets regulatory scrutiny and is efficient. Creating an adaptive design means getting all parties involved in the process – having statisticians work with clinicians, marketing, regulatory experts, execution teams, drug supply, etc, to construct a highly efficient design.\nAdaptive Designs are not restricted to phase I, but rather all stages of development of drugs and devices. The following list of phases/stages is where we have constructed adaptive designs:\nPhase I: Sample size, Dose escalation, Combination of arms, Seamless phase I-II.\nPhase II/Pilot: Sample size, Dose allocation, Introduce/Drop arms, Histology investigation, Prediction of Phase III, Seamless Phase II-III.\nPhase III/Confirmatory: Sample size, Multiple arms, Accrual Interim Analysis, Futility Analyses, Timing of Conclusions.\nPhase IV: Sample size, Timing of Conclusions, Indications",
    "crumbs": [
      "Concepts",
      "Adaptive Trials"
    ]
  },
  {
    "objectID": "concepts/adaptiveDesigns/index.html#what-is-an-adaptive-trial",
    "href": "concepts/adaptiveDesigns/index.html#what-is-an-adaptive-trial",
    "title": "Adaptive Trials",
    "section": "",
    "text": "To understand adaptive trials it may be easiest to look at a fixed trial. Traditionally a fixed sample size is specified with fixed allocation and fixed entry criteria. The data is not analyzed until trial completion, which is typically years after the trial began. This 75-year-old design approach creates very restrictive design types – and really forces the design team to guess at the doses, range, patient population, duration and frequency of treatment, etc, because a fixed design requires linear effort spent for each treatment arm.\nAn adaptive trial is like driving with your eyes open! It allows the pre-specification of flexible components to the major aspects of the trial, like the treatment arms used (dose, frequency, duration, combinations, etc), the allocation to the different treatment arms, the patient population used, and the sample size. An adaptive design can learn from the accruing data what the most therapeutic doses or arms are, allowing the design to hone in on the best arms. This allows the design to start with a wider range of doses – say 8 instead of 3 – with using a smaller number of patients. The result is a smart design, using resources (including time) much more efficiently, at the same time increasing the scientific precision. Well constructed adaptive designs can be better for all involved – better learning, more efficient, better treatment for subject in a trial, better information for regulators and the medical community.\nThe one drawback is that adaptive designs are more work to construct. It involves clinical trial simulation to make sure the design works well, meets regulatory scrutiny and is efficient. Creating an adaptive design means getting all parties involved in the process – having statisticians work with clinicians, marketing, regulatory experts, execution teams, drug supply, etc, to construct a highly efficient design.\nAdaptive Designs are not restricted to phase I, but rather all stages of development of drugs and devices. The following list of phases/stages is where we have constructed adaptive designs:\nPhase I: Sample size, Dose escalation, Combination of arms, Seamless phase I-II.\nPhase II/Pilot: Sample size, Dose allocation, Introduce/Drop arms, Histology investigation, Prediction of Phase III, Seamless Phase II-III.\nPhase III/Confirmatory: Sample size, Multiple arms, Accrual Interim Analysis, Futility Analyses, Timing of Conclusions.\nPhase IV: Sample size, Timing of Conclusions, Indications",
    "crumbs": [
      "Concepts",
      "Adaptive Trials"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html",
    "href": "concepts/facts/LinearRegressionLMPriors.html",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "",
    "text": "Jump to widget\n\n\n\nClick to jump straight to prior specification application.",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html#prior-specification-for-the-linear-regression-multiple-imputation-model",
    "href": "concepts/facts/LinearRegressionLMPriors.html#prior-specification-for-the-linear-regression-multiple-imputation-model",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "Prior Specification for the Linear Regression Multiple Imputation Model",
    "text": "Prior Specification for the Linear Regression Multiple Imputation Model\nThe default setting of “Same priors across all model instances and visits,” implies that each parameter \\(\\alpha\\), \\(\\beta\\), and \\(\\lambda\\) have the same prior for all visits \\(t\\). Estimation of the posterior distribution for these parameters is still done independently for each model instance.\n\nSame prior for all visits and model instances\nThe one prior across all model instance are formulated as: \\[\\alpha_t \\sim \\text{N}\\left(\\alpha_\\mu, \\alpha_{\\sigma}^2\\right)\\] \\[\\beta_t \\sim \\text{N}\\left(\\beta_\\mu, \\beta_{\\sigma}^2\\right)\\] \\[\\lambda_t^2 \\sim \\text{IG}\\left(\\frac{\\lambda_n}{2}, \\frac{\\lambda_\\mu^2 \\lambda_n}{2}\\right)\\]\n\n\nSame prior for all model instances, different prior per visit\nSince each visit will likely have a different estimated intercept and slope needed to accurately impute the final endpoint, the above prior formulation may not be desirable if specifying priors that are not extremely diffuse – especially on the \\(\\beta\\) parameters. Instead, selecting “Specify priors per visit across all model instances,” will share the prior specification across all instances of the model, but allows for different priors to be put on the parameters associated with each visit. The user inputted prior parameters are now subscripted with \\(t\\) to denote the visit they correspond to. These priors apply to all model instances:\n\\[\\alpha_t \\sim \\text{N}\\left(\\alpha_{\\mu_t}, \\alpha_{\\sigma_t}^2\\right)\\] \\[\\beta_t \\sim \\text{N}\\left(\\beta_{\\mu_t}, \\beta_{\\sigma_t}^2\\right)\\] \\[\\lambda_t^2 \\sim \\text{IG}\\left(\\frac{\\lambda_{n_t}}{2}, \\frac{\\lambda_{\\mu_t}^2 \\lambda_{n_t}}{2}\\right)\\]\n\n\nDifferent prior for all model instances and visits\nIt is also possible to specify priors “Per model instance and visit,” in which every visit has separate priors, and those differing priors vary across model instances. This is the most flexible prior specification method. The user inputted prior parameters are now subscripted by both t for visit and i for model instance. \\[\\alpha_{ti} \\sim \\text{N}\\left(\\alpha_{\\mu_{ti}}, \\alpha_{\\sigma_{ti}}^2\\right)\\] \\[\\beta_{ti} \\sim \\text{N}\\left(\\beta_{\\mu_{ti}}, \\beta_{\\sigma_{ti}}^2\\right)\\] \\[\\lambda_{ti}^2 \\sim \\text{IG}\\left(\\frac{\\lambda_{n_{ti}}}{2}, \\frac{\\lambda_{\\mu_{ti}}^2 \\lambda_{n_{ti}}}{2}\\right)\\]",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  },
  {
    "objectID": "concepts/facts/LinearRegressionLMPriors.html#sec-priorSpecWidget",
    "href": "concepts/facts/LinearRegressionLMPriors.html#sec-priorSpecWidget",
    "title": "Linear Regression Multiple Imputation Model",
    "section": "Prior Specification Widget",
    "text": "Prior Specification Widget\n\nInterpretation of parameters\n\n\\(\\alpha_{t}\\)\n\nThe expected response on the final endpoint when the early visit \\(t\\) has a response of 0\n\n\\(\\beta_{t}\\)\n\nIf \\(\\alpha=0\\), then \\(\\beta\\) is how many times larger the final endpoint response is than the early endpoint at visit \\(t\\). If \\(\\beta=0\\), then no matter what the early visit response is, the expectation for the final visit is \\(\\alpha\\). If \\(\\beta=1\\), then for any early response the expectation of the final response is the \\(\\text{early response} + \\alpha\\). A \\(\\beta \\lt 1\\) generally implies that the final endpoint is expected to regress towards 0 (when \\(\\alpha=0\\)), and a \\(\\beta \\gt 1\\) implies that the final response is expected to keep growing relative to the early visit response.\n\n\\(\\lambda_{t}\\)\n\nThe standard deviation around the expectation of the final visit response. This dictates how close the imputed final endpoint responses are to the mean response for a subject given \\(\\alpha\\) and \\(\\beta\\). Lower \\(\\lambda_t\\) implies higher correlation between the early visit response and final visit response.\n\n\n```lvgfuiklyrwly #| standalone: true #| viewerHeight: 1000\nlibrary(shiny) library(DT) library(ggplot2) library(htmltools)\nalignCenter &lt;- function(el) { htmltools::tagAppendAttributes(el, style=“margin-left:auto;margin-right:auto;” ) }\nsketch = htmltools::withTags(table( class = ‘display’, thead( tr( th(rowspan = 2, ’‘), th(rowspan = 2, style = “border-right: solid 1px;”,’Observed Visit Data’), th(colspan = 2, class=” dt-center”, style = “border-right: solid 1px;”,‘B1 priors’), th(colspan = 2, class=” dt-center”, style = “border-right: solid 1px;”,‘B2 priors’), th(colspan = 2, class=” dt-center”, style = “border-right: solid 1px;”,‘BB priors’) ), tr( th(“mean”), th(style = “border-right: solid 1px;”, “SD”), th(“mean”), th(style = “border-right: solid 1px;”, “SD”), th(“center”), th(style = “border-right: solid 1px;”, “weight”), ) ) ))\nui &lt;- fluidPage( tags\\(head(\n    # Note the wrapping of the string in HTML()\n    tags\\)style(HTML(” .my_col_class { align-content: center; }“) ) ),\ntitlePanel(h1(“Linear Regression LM Priors”, align = “center”)), alignCenter(sliderInput(“numVisits”, “Number of visits:”, min = 2, max = 20, value = 5, step = 1)), DTOutput(“dataInputTable”), h5(“Double click on a cell to edit.”, align = “center”), br(), titlePanel(h2(“Plot a subject’s prior predictive”, align = “center”)), fluidRow( #column(5, offset = 1, uiOutput(“slider”)), column(5, offset = 1, uiOutput(“slider”)), column(6, fluidRow( column(6, offset = 2, checkboxInput(“fixAlpha”, “Fix alpha at its mean?”, value = FALSE, width = “100%”)), column(6, offset = 2, checkboxInput(“fixBeta”, “Fix beta at its mean?”, value = FALSE, width = “100%”)), column(6, offset = 2, checkboxInput(“removePredictive”, “Remove endpoint prior predictive?”, value = FALSE, width = “100%”)) )) ), fluidRow( column(6, plotOutput(“visitToFinalPlot”)), column(6, plotOutput(“priorPredictive”)) )\n)\ngetLowerMedianUpper = function(earlyVisitVal, alpha = c(0,1), beta = c(0,1), lambda = c(1,1)) { distMeanFinal = c(alpha[1] + beta[1]earlyVisitVal, sqrt(alpha[2]^2 + beta[2]^2earlyVisitVal^2))\ndeviates = rnorm(10000) deviates = (deviates - mean(deviates))/(sd(deviates))\nif(any(is.na(lambda))) { samps = (deviatesdistMeanFinal[2] + distMeanFinal[1]) } else { samps = (deviatesdistMeanFinal[2] + distMeanFinal[1]) + rnorm(10000, 0, sd = sqrt(1/rgamma(10000, lambda[2]/2, lambda[1]^2*lambda[2]/2))) }\ndistValueFinal = c(mean(samps), sd(samps))\nreturn(list(“meanFinal” = data.frame(lower = distMeanFinal[1] + qnorm(.025)distMeanFinal[2], median = distMeanFinal[1], upper = distMeanFinal[1] + qnorm(0.975)distMeanFinal[2]), “predictionFinal” = data.frame(lower = quantile(samps,.025), median = median(samps), upper = quantile(samps,.975)))) }\nserver &lt;- function(input, output, session) {\ndf = data.frame(VisitResponse = c(2,5,3,7,11), alphaPriorMean = 0, alphaPriorSD = 2, betaPriorMean = 1, betaPriorSD = 2, lambdaPriorCenter = 5, lambdaPriorWeight = 3) df[5,] = c(5, NA, NA, NA, NA, NA, NA) row.names(df) = paste(“Visit”, 1:5)\n## Render DF to actually change output\\(dataInputTable = renderDT(datatable(df,\n                                             options = list(\n                                               pageLength = 20,\n                                               dom = \"t\",\n                                               autoWidth = TRUE,\n                                               columnDefs = list(list(className = 'dt-center', orderable = FALSE, width = '75px', targets = 0:7),\n                                                                 list(width = \"150px\", targets = 0:1))\n                                             ),\n                                             container = sketch,\n                                             rownames = TRUE,\n                                             # escape = FALSE,\n                                             selection = 'none',\n                                             editable = list(target = \"cell\")\n  ) |&gt; formatStyle(c(1,3,5,7), `border-right` = \"solid 1px\") |&gt;\n    formatRound(1, digits = 4) |&gt; formatRound(2:7, digits = 2) |&gt;\n    formatStyle(0,\n                target = \"row\",\n                backgroundColor = styleEqual(paste(\"Visit\",input\\)lastVisitWithData), “lightblue”, ‘white’)) )\n## Update from Conditional proxy = dataTableProxy(‘dataInputTable’)\nobserveEvent(input\\(dataInputTable_cell_edit, {\n    info = input\\)dataInputTable_cell_edit i = info\\(row\n    j = info\\)col v = info$value\nif(i &lt; nrow(df) | j == 1) {\n  df &lt;&lt;- editData(df, info)\n} else {\n  df[i,j] &lt;&lt;- NA\n}\nreplaceData(proxy, df) \n})\nobserve({ nv = input$numVisits if(nv &gt; nrow(df)) { tempd = df for(i in 1:(nv-nrow(df))) { tempd = rbind(tempd, setNames(data.frame(c(tempd[nrow(tempd),])), names(tempd))) rownames(tempd)[nrow(tempd)] = paste(“Visit”, nrow(tempd)) tempd[nrow(tempd)-1,-1] = tempd[nrow(tempd)-2,-1] } df &lt;&lt;- tempd } else if(nv &lt; nrow(df)) { df &lt;&lt;- df[1:nv,] df[nv,-1] &lt;&lt;- NA } replaceData(proxy, df) })\nsliderParams &lt;- reactiveValues(max = 5, value = 3) output\\(slider &lt;- renderUI({\n    sliderInput(\"lastVisitWithData\", \"Last complete visit:\", min = 1, max = sliderParams\\)max, value = sliderParams\\(value, step = 1)\n  })\n  observeEvent(input\\)numVisits, { sliderParams\\(max = input\\)numVisits if(!is.null(input\\(lastVisitWithData)) {\n      sliderParams\\)value &lt;- min(input\\(lastVisitWithData, input\\)numVisits) } else { sliderParams$value = 3 } })\noutput\\(priorPredictive = renderPlot({\n    req(input\\)lastVisitWithData) input$dataInputTable_cell_edit\nlvIndex = input$lastVisitWithData\nfinalVisitIndex = input$numVisits\ntempDF = df\ndataToPlot = getLowerMedianUpper(tempDF[lvIndex,1],\n                                 alpha = c(tempDF[lvIndex,2], ifelse(input$fixAlpha, 0, tempDF[lvIndex,3])),\n                                 beta = c(tempDF[lvIndex,4], ifelse(input$fixBeta, 0, tempDF[lvIndex,5])),\n                                 lambda = c(tempDF[lvIndex,6], tempDF[lvIndex,7]))\n\ntempDF$RowVisitIndex = 1:nrow(tempDF)\ntempDF$visitKnown = \"included\"\ntempDF$visitKnown[tempDF$RowVisitIndex &gt; lvIndex] = \"excluded\"\n\n# tempDF = rbind(setNames(data.frame(c(tempDF[1,])), names(tempDF)), tempDF)\n# tempDF[1,1] = 0\n# tempDF$RowVisitIndex[1] = 0\n# rownames(tempDF)[1] = \"Baseline\"\n\np1 = ggplot() + \n  geom_point(dat = tempDF, aes(x = RowVisitIndex, y = VisitResponse, color = visitKnown), size = 3) + \n  scale_color_manual(breaks = c(\"included\", \"excluded\"), values = c(\"black\", \"gray70\"), guide = \"none\") +\n  coord_cartesian(xlim = c(0, finalVisitIndex)) + \n  scale_x_continuous(breaks = 0:finalVisitIndex, labels = c(\"Baseline\", 1:finalVisitIndex)) +\n  xlab(\"Visit\") + ylab(\"Response\") + ggtitle(\"Predicting Final Endpoint of a Subject\") +\n  theme_bw() + \n  theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"left\", legend.position = \"bottom\", legend.direction = \"vertical\")\n\nif(lvIndex &lt; finalVisitIndex) {\n  if(!input$removePredictive) {\n    p1 = p1 + \n      geom_segment(data = dataToPlot[[2]], aes(x = finalVisitIndex, y = lower, yend = upper), color = \"darkgreen\", linewidth = 2.5) +\n      annotate(geom = \"point\", x = finalVisitIndex, y = dataToPlot[[2]]$median, color = \"darkgreen\", size = 3, shape = 18) +\n      geom_ribbon(data = NULL, aes(x = c(lvIndex, finalVisitIndex),\n                                   ymin = c(tempDF$VisitResponse[lvIndex], dataToPlot[[2]]$lower),\n                                   ymax = c(tempDF$VisitResponse[lvIndex], dataToPlot[[2]]$upper),\n                                   fill = \"preds\"),  color = NA, alpha = .2)\n  }\n  p1 = p1 +\n    geom_segment(data = dataToPlot[[1]], aes(x = finalVisitIndex, y = lower, yend = upper), color = \"darkblue\", linewidth = 1.5) +\n    annotate(geom = \"point\", x = finalVisitIndex, y = dataToPlot[[1]]$median, color = \"darkblue\", size = 3, shape = 18) +\n    annotate(geom = \"segment\", x = lvIndex, xend = finalVisitIndex, y = tempDF$VisitResponse[lvIndex], yend = dataToPlot[[1]]$median, linetype = \"dashed\", color = \"darkblue\")+\n    geom_ribbon(data = NULL, aes(x = c(lvIndex, finalVisitIndex),\n                                 ymin = c(tempDF$VisitResponse[lvIndex], dataToPlot[[1]]$lower),\n                                 ymax = c(tempDF$VisitResponse[lvIndex], dataToPlot[[1]]$upper)),\n                fill = \"darkblue\",  color = NA, alpha = .4)\n} else {\n  p1 = p1 + annotate(geom=\"text\", label = \"Final Visit Value Known\",\n                     alpha = .5, size = 10, x = (finalVisitIndex)/2, y = Inf, vjust = 1.3)\n}\n\np1 = p1 + scale_fill_manual(NULL, breaks = c(\"preds\"), limits = c(\"preds\"), values = c(\"darkgreen\"), labels = c(\"Prior predictive distribution for final endpoint of subject.\"))\n#  guides(fill = guide_legend(override.aes = list(limits = c(\"darkgreen\", \"darkblue\"), labels = c(\"Prior distribution for mean of imputed value.\", \"Prior predictive distribution for final endpoint of subject.\")))) \n\np1\n})\noutput\\(visitToFinalPlot = renderPlot({\n    input\\)dataInputTable_cell_edit req(input$lastVisitWithData)\nlvIndex = input$lastVisitWithData\nfinalVisitIndex = input$numVisits\n\ntempDF = df\n\nmin_s = ifelse(min(tempDF$VisitResponse, na.rm = TRUE) &lt; 0, (min(tempDF$VisitResponse, na.rm = TRUE)+1)*1.1, 0)\nmax_s = ifelse(max(tempDF$VisitResponse, na.rm = TRUE) &gt; 0, (max(tempDF$VisitResponse, na.rm = TRUE)+1)*1.1, 0)\n\ns = seq(min_s-5, max_s+5, length.out = 101)\n\nmeanDist_mean = tempDF$alphaPriorMean[lvIndex] + tempDF$betaPriorMean[lvIndex]*s\nif(!input$fixAlpha & !input$fixBeta) {\n  meanDist_sd = sqrt(tempDF$alphaPriorSD[lvIndex]^2 + tempDF$betaPriorSD[lvIndex]^2*s^2)\n} else if(input$fixAlpha & !input$fixBeta) {\n  meanDist_sd = sqrt(tempDF$betaPriorSD[lvIndex]^2*s^2)\n} else if(!input$fixAlpha & input$fixBeta) {\n  meanDist_sd = rep(sqrt(tempDF$alphaPriorSD[lvIndex]^2), length(s))\n} else {\n  meanDist_sd = rep(0, length(meanDist_mean))\n}\n\nplotDF = data.frame(earlyVis = s,\n                    lower = meanDist_mean + qnorm(0.025)*meanDist_sd,\n                    median= meanDist_mean,\n                    upper = meanDist_mean + qnorm(0.975)*meanDist_sd)\n\nif(lvIndex &lt; finalVisitIndex) {\n  if(!input$removePredictive) {\n    numSamps = 2500\n    \n    deviates = rnorm(numSamps)\n    deviates = (deviates - mean(deviates))/(sd(deviates))\n    normigsamps = rnorm(numSamps, 0, sd = sqrt(1/rgamma(numSamps, tempDF$lambdaPriorWeight[lvIndex]/2, tempDF$lambdaPriorCenter[lvIndex]^2*tempDF$lambdaPriorWeight[lvIndex]/2)))\n    \n    distVals = matrix(NA, ncol = 2, nrow = length(s))\n    for(i in 1:length(s)) {\n      distVals[i,] = quantile((deviates*meanDist_sd[i] + meanDist_mean[i] + normigsamps), c(0.025, 0.975))\n    }\n    \n    plotDF$lowerPred = distVals[,1]\n    plotDF$upperPred = distVals[,2]\n  }\n  \n  p2 = ggplot(data = plotDF) + geom_abline(aes(slope = tempDF$betaPriorMean[lvIndex], intercept = tempDF$alphaPriorMean[lvIndex]), color = \"darkblue\", linewidth = 1.5) + \n    geom_ribbon(aes(x = s, ymin = lower, ymax = upper, fill = \"means\"), color = NA, alpha = 0.4)\n  \n  if(!input$removePredictive) {\n    p2 = p2 + geom_ribbon(aes(x = s, ymin = lowerPred, ymax = upperPred), fill = \"darkgreen\", color = NA, alpha = 0.2) \n  }\n  p2 = p2 +\n    coord_cartesian(xlim = c(min_s, max_s)) +\n    xlab(\"Early Visit Known Value\") + ylab(\"Expectation of Final Visit Response\") + ggtitle(\"Expectation of Final Visit Given Early Visit\") +\n    theme_bw() + \n    theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"right\", legend.position = \"bottom\", legend.direction = \"vertical\")\n} else {\n  p2 = ggplot(data = NULL) + geom_abline(aes(fill = \"means\"), slope = 1, intercept = 0, color = \"darkblue\", linewidth = 1.5) + \n    coord_cartesian(xlim = c(min_s, max_s), ylim = c(min_s, max_s)) +\n    annotate(geom=\"text\", label = \"Final Visit Value Known\",\n             alpha = .5, size = 10, x = (max_s + min_s)/2, y = Inf, vjust = 1.3) +\n    xlab(\"Early Visit Known Value\") + ylab(\"Expectation of Final Visit Response\") + ggtitle(\"Expectation of Final Visit Given Early Visit\") +\n    theme_bw() + \n    theme(plot.title = element_text(size = 20), text = element_text(size = 16), legend.justification = \"right\", legend.position = \"bottom\", legend.direction = \"vertical\")\n}\np2 = p2 + scale_fill_manual(NULL, breaks = c(\"means\"), limits = c(\"means\"), values = c(\"darkblue\"), labels = c(\"Prior distribution for mean of imputed value.\"))  \np2\n}) }\nshinyApp(ui = ui, server = server)```",
    "crumbs": [
      "Concepts",
      "FACTS Tips and Tricks",
      "Linear Regression Multiple Imputation Model"
    ]
  }
]