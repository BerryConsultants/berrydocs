---
title: "FACTS Core Designs"
subtitle: "All Endpoints: Quantities of Interest and Design options."
format:
  html:
    toc: true
    toc-depth: 4
    toc-title: "Table of Contents"
    number-sections: true
    number-depth: 4
---
<img src="coreUGattachments/CoreUserGuide/media/image1.png"
style="width:6.93268in;height:1.84409in" alt="facts_splash.png" />

#  Introduction

## Purpose of this document

This document describes how to use the ‘Quantities of Interest’ and ‘Design’ options that are common across the FACTS Core design engines. It is intended for all end users of the system.

## Scope of this document

This document covers the design options that are common across the four FACTS Core Design Engines: Continuous, Dichotomous, Time-to-Event, and Multiple Endpoint. Design elements that are unique to a particular engine (primarily data simulation and simulation output) are covered in the endpoint specific Core Engine User Guide.

This document does not address the use of FACTS Enrichment Designs, Dose Escalation, or Platform Trials, which have separate User Guides.

The screenshots provided are specific to a particular installation and [may not reflect the exact layout](## "Screenshots from earlier versions of FACTS 6 are retained only where they are unchanged in FACTS 7.1") of the information seen by any particular user. They were taken from FACTS V7 & V6 installed on Windows 10 or 11. If FACTS is installed on different versions of Windows, or with different Windows ‘themes’ there will be some differences in appearance introduced by Windows. The contents of each tab, however, will still be consistent with the screenshots in this document.

## Context of this Issue

This is the version of the user guide for inclusion with the FACTS 7.1 release.

## Citing FACTS

If writing in \LaTex and using Bibtex, if you wish to cite FACTS (thank you!), we suggest the following:

```{verbatim}
@techreport{FACTS71,
  author = {{FACTS Development Team}},
  title = {{FACTS}: Fixed and Adaptive Clinical Trial Simulator},
  year  = {2024},
  month = {03},
  number = {Version 7.1},
  type         = {Computer Software},
  institution = {Berry Consultants LLC},
  address = {Austin, TX},
  note   = {<https://www.berryconsultants.com/software/facts/>}
}
```
This will result in a reference that, for example in the APA style, will
look like the following:

```{verbatim}
FACTS Development Team (2024). FACTS: Fixed and adaptive clinical trial
simulator. Computer Software Version 7.1, Berry Consultants LLC, Austin, TX. <https://www.berryconsultants.com/software/facts/>.
```

# FACTS Core Overview

For an overview of the form entry and layout, see the User Guide for the
Core Engine of the particular endpoint that you are using for your
trial.

FACTS Core is for simulating trials where there are a number of related
treatments being tested against a common control arm. Commonly, these
different treatments will be different doses of the same drug and hence
dose-response modelling is justifiable. If the treatments do not differ
by dose but in some other way such as dosing frequency or treatment
combination, then analysis can be by pairwise comparison with control,
this is referred to as the ‘No model’ option.

FACTS provides numerous options for the statistical analysis, some of
them straight forward, some of them quite novel, though all have been
used in actual trials:

-   The endpoint can be continuous, dichotomous or time to event.

-   As well as a control arm, the study arms can be compared with an
    active comparator.

-   When simulations are executed, they can be executed for a wide range of
    scenarios. A scenario can be composed of a combination of one of each
    type of profile – dose response, longitudinal, accrual and dropout.

-   With either a dichotomous or continuous endpoint, longitudinal
    models can used to impute the patient’s likely final outcome from
    early interim measures. This can be used when final endpoint data is
    missing due to subject drop-out or at interims for subjects who’ve
    not reached their final endpoint yet.

-   Estimation of the response on the control arm can be augmented using
    a hierarchical model to borrow from data from previous studies (This
    is known as Bayesian Augmented Control, BAC).

-   Interim analyses can be specified at fixed intervals by time, the number of
    subjects recruited or the number of events observed.

-   At interim analyses, options include:

    -   Choosing to stop the whole study for success or futility.

    -   Dropping treatment arms

    -   Adapting the randomization proportions to favor allocating to
        the doses that are most likely to be the desired target – which
        can be the study arm with the maximum response, the EDx, or
        minimum efficacious dose.

# Simulating Virtual Subjects

## Subject Responses

The methods used to simulate subject responses vary by endpoint type.
For each endpoint, the endpoint specific user guides provide information
about simulating subject responses.

For simulating dichotomous responses see: [FACTS Core Dichotomous User
Guide](## "Add correct link here.")

For simulating continuous responses see: [FACTS Core Continuous User
Guide](## "Add correct link here.")

For simulating time to event responses see: [FACTS Core Time-to-Event
User Guide](## "Add correct link here.")

For simulating multiple endpoint responses see the continuous or
dichotomous user guide, depending on the type of endpoints used in the
multiple endpoint study.

## Accrual

The Accrual sub-tab provides an interface for specifying accrual profiles. Accrual profiles define the mean recruitment rate week by week during the course of the trial. Virtual subjects are simulated from a Poisson process in which the expected number of subjects per week is allowed to change week by week.

Accrual profiles are shown as a list on the left of the screen, as depicted below. These accrual profiles may be renamed by double-clicking on them and typing a new profile name. After creating a profile, the user must create at least one recruitment region. Early in the trial design process, detailed simulation of the expected accrual pattern is typically not necessary and a single region with a simple mean accrual rate is sufficient.

To model the expected accrual rates more precisely over the course of the trial, the user may specify multiple regions for each accrual profile and
separately parameterize them. Regions are added via the table in the
center of the screen (Figure 7‑1). Within this table, the user may
modify:

-   the peak mean weekly recruitment rate,

-   the start date (in weeks from the start of the trial) for this
    recruitment region,

-   whether the region will have a ramp up phase and if so when the ramp
    up will be complete (in weeks from the start of the trial).

-   Whether the region will have a ramp down, and if so when the ramp
    down start and when the ramp down will complete (in weeks from the
    start of the trial).

Ramp up/ramp down define simple linear increase/decreases in mean
recruitment rate from the start to the end of the ramp. Note that
simulation of accrual is probabilistic, but ramp downs are defined in
terms of time, so even if ramp downs are planned so that at the average
accrual rate they will occur as the trial reaches cap, there is a risk
in simulations when accrual has been slower than average, that ramp
downs occur before the full sample size is reached. It is advisable to
have at least one region that doesn’t ramp down to prevent simulations
being unable to complete.

A graph of the recruitment rate of the highlighted region is shown as
well. As the recruitment parameters are changed, the graph will update
to show the time at which full accrual is reached. An accrual profile
that does not reach full accrual is invalid and cannot be used to run
simulations.

<img src="coreUGattachments/CoreUserGuide/media/image2.png"
style="width:5.44318in;height:4.13315in"
alt="A screenshot of a social media post Description automatically generated" />

In the screenshot above you can see the two step ramp up in accrual from
two regions – each starting at different offsets into the trial.

Note that the accrual profile graph is only the mean expectation; actual
accrual is simulated using exponential distributions for the intervals
between subjects, derived from the mean accrual profile specified here.
Thus some simulated trials will recruit more quickly than this and some
more slowly.

There are commands to import and export region details from/to simple
external XML files. When importing, the regions defined in the external
file are **added** to the regions already defined, they don’t replace
them.

This is an example of a very simple region file defining just one
region:

```{verbatim}
<?xml version="1.0" encoding="utf-8"?>
<regions>
<region>
<name>Region 1</name>
<rate>5</rate>
<start>0</start>
<ramp-up />
<ramp-down />
</region>
</regions>
```
### Deterministic Accrual

If “Deterministic” accrual has been specified on the Study &gt; Study
Info tab, then on the Accrual tab, rather that specifying an accrual
profile from which subject recruitment times are simulated, the user
loads a file of specific accrual dates for every subject.

<img src="coreUGattachments/CoreUserGuide/media/image3.png"
style="width:5.00179in;height:3.85875in" />

The user specifies a “.dat” file to load that contains the [subject
accrual dates in weeks](## "This value is in weeks from FACTS 7.0 onwards, previously it was in days.") from the start of the trial.

The required file format is a text file with comma separate values. One
row per subject, with 3 fields on each row:

1.  the subject ID, (an integer)

2.  the ID of the region where the subject was recruited (an integer)

3.  and the subjects randomization date (in weeks from the start of the
    trial – this is a ‘real’ number allowing fractions of a week to be
    specified)

The file must contain sufficient entries to allow the maximum number of
subjects specified on the Study > Study Info tab to be recruited.

After successfully loading a file, the FACTS GUI shows a plot of the
resulting weekly accrual rate

<img src="coreUGattachments/CoreUserGuide/media/image4.png"
style="width:5.00179in;height:3.85875in" />

## Drop-out Rates

For the continuous and dichotomous engines, and the multiple endpoint
engine, the default dropout scenario is that no subjects drop out of the
study before observing their final endpoint data. If dropouts are
expected, the user can specify either the “Dropouts per Dose,” or
“Dropouts per Dose per Visit.”

If “Dropouts per Dose” is selected, then each subject has a probability
of not having an observable final endpoint value equal to the dropout
rate of the dose that subject is randomized to. If each subject has
multiple visits and “Dropouts per Dose” is selected, then the
conditional probability of dropping out before each visit given that the
subject had not dropped out up to the visit before rates are all equal.
In other words, if the total dropout rate is $\pi_D$, the
probability of dropping out between visits $i$ and $i+1$ given that
the subject had not dropped out at visit $i$ is
$1 - \left( 1 - \pi_{D} \right)^{\frac{1}{V}}$ where $V$ is the total
number of visits.

If “Dropouts per Dose per Visit” is selected, then each subject has a
user specified probability of dropping out before a visit $v$ that is
specified as the conditional probability of dropping out before visit
$v$ given that that they had not dropped out by visit $v-1$. This
leads to a total dropout rate $\pi_D$ for a participant that
is equal to:

$$\pi_{D} = 1 - \prod_{v = 0}^{V}{(1 - \pi_{v})}$$

# Quantities of Interest (QOI)

The quantities of interest tab allows the user to specify the Bayesian
posterior probabilities and frequentist p-values to be calculated and
reported in the simulation results and available for use in early
stopping decisions, trial adaptations and final evaluation.

There are 3 classes of QOI:

1.  A probability calculated independently **for each dose**, there are
    3 types of comparison:

    1.  The posterior probability that the estimate of the response of
        the subjects on that dose is better/worse than an absolute value
        or the estimate of response of the subjects on a reference dose,
        such as Control.

    2.  The predictive probability/conditional power of success (achieving frequentist statistical significance) in the current trial or a future trial comparing the estimate of response on the dose against that on the Control arm.

    3.  P-values for each dose comparing the estimate of response on
        that dose against that on the Control arm.

2.  Probabilities calculated across the doses for which dose is most
    likely to satisfy a specified **target dose** criteria. The target dose criteria can be to determine the dose with the maximum effect (QOI is called [Pr(Max)]{.fake-code-block}), the minimum dose that achieves some known minimum effect (called [Pr(MED ...)]{.fake-code-block}), or that it is the minimum dose that achieves some percentago of the effect estimated for the most effective dose (called [Pr(EDq ...)]{.fake-code-block} where $q$ is the effect percentage).

3.  A **decision quantity** – which is the value of either of the other types of QOIs at a specific dose level. The method of choosing the dose level that should be used for decision QOI is specified at the time of creating the decision QOI. As an example: a QOI may be created that calculates the Probability that each dose is superior to the control arm - [Pr(PBO)]{.fake-code-block}. Additionally, a target QOI can be created that calculates the probability that each dose is the ED90 - [Pr(EDq relative to Control: Quantile=0.9)]{.fake-code-block}. Then, a decision quantity could be created that is a scalar value representing the probability that the dose that is the most likely to be the ED90 is superior to control - [Pr($\theta_d>\theta_{Control}$); d=Greatest Pr(EDq relative to Control: Quantile=0.9)]{.fake-code-block}. It is also easy to create a decision QOI that is simply the largest (or smallest) value of the dose specific QOI. An example of this would be the p-value of the dose with the smallest p-value.

Trial decisions at interim analyses or the final analysis are made based on decision quantities, but adaptations like adaptive allocation can be based on posterior probabilities, predictive probabilities, and target probabilities.

Note that to creating a QOI for early stopping or final evaluation
decisions will involve using 2 or 3 QOIs:

1.  The probability to be tested e.g. the probability of being better
    than the Control by a clinically significant difference.

2.  The target dose criteria for selecting the dose that is to be used
    in the test – e.g. the ED90. This step is not always necessary.

3.  The decision quantity that combines 1 & 2 – e.g. the probability
    that the ED90 is better than Control by a clinically significant
    difference.

There are a number of pre-defined, default QOIs which simplifies the
specification of the most commonly used decision quantities, and the
importation of past FACTS designs. These are:

**Default Posterior Probabilities**

-   The probability of being better than the control arm: [Pr($\theta_d > \theta_{Control}$)]{.fake-code-block}, previously referred to as [Pr($\theta_d – \theta_0$)]{.fake-code-block}, [Pr(Pbo)]{.fake-code-block} and [Prob. Beats Ctrl]{.fake-code-block} in earlier versions of FACTS.

-   The probability of being better than the control arm by a clinically significant difference [Pr($\theta_d - \theta_{Control} > \delta^*$)]{.fake-code-block}”, previously referred to as [Pr($\theta_d – \theta_0 >$ CSD)]{.fake-code-block}, [Pr(CSD)]{.fake-code-block} and [Prob. Beats CSD]{.fake-code-block} in earlier versions of FACTS. The value for the $\delta^*$ is the CSD, which is set in the “Standard Evaluation Variables” panel at the bottom of the QOI tab.

**Default Predictive Probabilities**

-   The probability of success in a future trial [Pr(Succ. Future Trial): N=$N_{Future}$, *Sup/Noninf*, $\alpha=\alpha^*$; $\delta=\delta^*$]{.fake-code-block}, previously referred to as [Pr(S Phase III)]{.fake-code-block} and [Prob. Stat Sig]{.fake-code-block} in earlier versions of FACTS. The parameters for the future trial can be modified from the default by clicking on the QOI’s row in the table.

**Default Target Doses**

-   The probability for each dose that it is the dose with the maximum response, [Pr(Max)]{.fake-code-block}, previously referred to as [$d_{max}$]{.fake-code-block} and [Ppn Max]{.fake-code-block} in earlier versions of FACTS. When used to select a dose in a decision quantity the label [p… $d$ = Greatest Pr(MAX)]{.fake-code-block} is used.

-   The probability for each dose that it is the minimum dose that is
    better than Control by the specified CSD, [Pr(MED relative to
    Control: Delta = $\delta^*$]{.fake-code-block}, previously referred to as [Ppn (MED)]{.fake-code-block} in
    earlier versions of FACTS. The CSD used for comparison is specified
    in the Evaluation Variables panel at the bottom of the QOI tab. When
    used to select a dose in a decision quantity the label [$d$=
    Greatest Pr(MED relative to *Control/Active Comparator*: Delta =
    $delta^*$)]{.fake-code-block} is used.

-   The minimum dose that gives a certain proportion of the maximum
    estimated response [Pr(EDq relative to Control: Quantile=$Q$)]{.fake-code-block},
    previously referred to as [$d_{EDx}]{.fake-code-block} and [Ppn(EDx)]{.fake-code-block} in
    earlier versions of FACTS. The Effective Dose quantile to use can be
    modified by the clicking on QOI’s row in the table. When used to
    select a dose in a decision quantity [… $d$=Greatest Pr(EDq relative
    to Control: Quantile=$Q$)]{.fake-code-block} is used.

<img src="coreUGattachments/CoreUserGuide/media/image5.png"
style="width:6.14271in;height:4.66432in"
alt="Graphical user interface, text, application Description automatically generated" />

In each panel for each type of quantity, existing quantities can be
deleted by clicking on the
<img src="coreUGattachments/CoreUserGuide/media/image6.png"
style="width:0.32292in;height:0.23958in" /> at the end of the
corresponding row. Each quantity’s definition can be displayed and
edited (only edited for the default QOIs) by clicking on the row
displaying the quantity’s definition. A new quantity can be defined by
clicking on the bottom row of the corresponding panel labelled “Add…”.

## Posterior Probabilities

These are Bayesian quantities to be calculated at each interim and at
the final analysis.

<img src="coreUGattachments/CoreUserGuide/media/image7.png"
style="width:3.59231in;height:2.61812in" />

A Posterior Probability is specified as:

-   Compare:

    -   Continuous: Means

    -   Dichotomous: Rates or Log-odds

    -   Time-to-Event: Hazard Ratio or Hazard Rates.

<!-- -->

-   Condition: “>” or “<” a comparison value.

-   Relative to an absolute value or relative to the response on a
    *specific* dose.

-   The comparison can include a delta, which is the absolute value to
    be compared against if the comparison is absolute, or a value that
    the difference relative to the comparison arm is compared to.

-   The QOI will be given a “name” derived from these details and a
    short or Alternative name that will be used in the output files to
    allow easier access from other software, e.g. from within R.

-   If the endpoint is TTE and the design includes a predictor endpoint,
    then the definition of the QOI includes the specification of which
    endpoint it applies to.

### Notes on setting Deltas

In the three endpoints delta’s are defined as:

Continuous
: A CSD (Clinically Significant Difference) in the estimates of the mean response.

Dichotomous
: A CSD in the estimate of the response rates if Rates is selected in the QOI, and of Odds Ratios

Time-to-Event
: A CSHRD (Clinically Significant Hazard Ratio Difference) in the estimate of the Hazard Ratio

A *standard* hypothesis test for demonstrating superiority to control
uses a delta of 0. Testing for superiority with a non-zero delta is
different, and the implications need to be [carefully understood](## "Simulating its use in FACTS is a good way to achieve that
understanding!"). Where the term CSD is used in this document, it should be taken to refer
to CSHRD as well unless it is specifically stated otherwise.

When setting a target treatment difference from control for the study to
beat, it is also common to require a degree of confidence that the
target has been beaten. To achieve posterior probabilities of >50%
that the target has been beaten, the estimated mean difference will have
to be **greater** than the target difference.

Thus, when setting a delta, we are setting up two hurdles the study drug
must beat, first the delta and then on top of that an additional margin
to give a >50% confidence that the margin has been beaten. Thus, it
is necessary to avoid setting the target delta too large. A common
mistake is to set the delta to the ‘expected difference’ (the value
that might have been used in a conventional sample size calculation). In
scenarios where the simulated response is equal to the ‘expected
difference’ and hence the CSD this will give probabilities of being
better than control by the delta of 50% on average, regardless of the
sample size.

It is inadvisable to require a posterior probability of 50% that the response is better than the Control by the delta margin as this turns the test into one that simply depends on whether the point estimate of the response is better.

It is inadvisable to require posterior probability of less than 50% for
success, as such criteria have the undesirable characteristic that they
can be met in circumstances where it can be seen that if further data
was gathered consistent with what has already been seen, it would lead
the threshold no longer being met! The posterior distribution would
shrink so that there was no longer sufficient of the tail above the CSD.

It is better therefore to use a delta that is less than the expected
difference that it is hoped to achieve, somewhat like having a
non-inferiority margin around the target. So, when the design is
simulated with a response at the expected difference the target will be
clearly exceeded. A useful default value to use for a delta is half the
‘expected difference’. This usually yields “comprehensible” probability
thresholds for both success and futility.

Using a delta has benefits however – simply being better than control
but for very small difference can be established simply by having large
sample sizes. Being better than control by a delta establishes some
confidence that there is some patient benefit. Use of a delta is also
useful if otherwise success thresholds “well out in the tail” (e.g. &gt;
0.99) are required such as the equivalent of an early look in a
conservative Group Sequential design. Though the use of a delta does
mean that there is a less direct equivalence to a p-value.

### P-value Delta’s

Separately from the CSD/CSHRD delta, with a continuous or dichotomous
endpoint (but not time-to-event), a frequentist
super-superiority/non-inferiority delta can be specified.

These use the same selection of super-superiority/non-inferiority as the
CSD

<img src="coreUGattachments/CoreUserGuide/media/image8.png"
style="width:4.06126in;height:1.06908in"
alt="Graphical user interface, text, application Description automatically generated" />

Currently, unlike the CSD delta that is only applied to the default
posterior probability QOI and MED target QOI, the p-value delta is
applied to **all** the p-value QOIs and it cannot be overridden.

The value of the p-value delta is constrained to be 0 or positive. The
‘direction’ of the delta depends then on the direction of the endpoint
(whether “higher/lower is better” or “a response is a positive/negative
outcome”.

<table>
<caption><p>Figure 8‑1: Accrual</p></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 37%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Higher is better / Response is positive</th>
<th>Lower is better / Response is negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Super-Superiority</td>
<td>Trt – Control &gt; delta</td>
<td>Trt – Control &lt; -delta</td>
</tr>
<tr class="even">
<td>Non-inferiority</td>
<td>Trt – Control &gt; -delta</td>
<td>Trt – Control &lt; delta</td>
</tr>
</tbody>
</table>

Figure 8‑1: Accrual

### P-value Comparisons with No Control Arm

If no control arm is included p-values, with a continuous or dichotomous
endpoint, the treatment response is compared to a fixed value. The fixed value is specified as "Frequentist *response/rate* to compare to for p-value QOIs:" in the Standard Evaluation Variables section at the bottom of the QOIs tab. It is not currently possible to compare different p-value QOIs to different fixed responses or rates.

## Predictive Probabilities

There are two types of predictive probabilities –

1.  Bayesian predictive probabilities, which are Bayesian predictions of
    frequentist outcomes and take into account the uncertainty around a
    set of parameters, and

2.  Conditional Power, which aims to calculate the frequentist
    probability of success assuming a set of parameters to be true.

The primary difference between the Bayesian predictive probabilities and
the conditional power calculations is that the Bayesian predictive
probabilities are calculated taking the variability around the estimated
treatment effect into account, while the conditional power calculations
assume that the observed test statistic of past data is the exact effect
that future data will be generated from.

For both Bayesian predictive probabilities and conditional power, we
differentiate two types, predicting the outcome in the current trial and
predicting the outcome in a future trial.

### Bayesian predictive probabilities

#### Current Trial Bayesian Predictive Probabilities

In the current trial, the outcome can be predicted under one of two
assumptions:

1.  That no new subjects are recruited, but all those who have been
    recruited are followed up until they are complete.

2.  That the trial continues recruiting using the current allocation
    ratios until the trial maximum sample size is reached, and all
    subjects recruited are followed up until complete.

Predictive probabilities currently can only predict outcomes based on
p-values. Note that since p-values are only calculated as comparisons
against a control arm, predictive probabilities of success in the
current trial are only available if the current trial includes a control
arm.

<img src="coreUGattachments/CoreUserGuide/media/image9.png"
style="width:2.64056in;height:2.61544in" />

The user specifies how missingness is handled in the final analysis, the
p-value test type – unadjusted, Bonferroni or [Dunnett's](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.") and the (one
sided) alpha level for the significance test. The QOI assumes that the
specified default p-value delta is used in the final test.

The predictive probability of the current trial at the maximum sample
size is only available:

-   If the allocation is either fixed, or fixed with arm dropping, but
    not adaptive allocation.

The predictive probability is calculated by simulating the remaining
subjects, assuming their allocated at the current probabilities of
allocation and simulating their final response based on the rate of
response or (Normal) distribution of responses observed so far for each
arm.

-   Ignoring the possibility of the trial stopping or dropping an arm at
    a future interim

-   Ignoring the possibility of future subject drop-outs.

There is one simulation of the remaining data per MCMC loop, potentially
significantly increasing the time to perform the overall simulation of
the trial.

If there is no data available on the arms involved in the Bayesian
predictive probability calculation, FACTS will still perform the
simulation of future subject outcomes based on posterior distribution.
The posterior distribution for the arms with no available data will be
the same as the user specified prior distribution.

#### Current Trial Bayesian Predictive Probabilities – Time-to-Event

Unlike the Continuous and Dichotomous endpoints, to predict the
probability of success at full enrollment with a Time-to-Event endpoint,
it is also necessary to simulate the accrual rate (and hence how many
events might be observed).

<img src="coreUGattachments/CoreUserGuide/media/image10.png"
style="width:2.94043in;height:2.92636in" />

For TTE, for a Predictive Probability of Success at Full Enrollment,
there are new parameters to determine how accrual is modeled. There are
3 models for accrual

-   Fixed Rate, the parameters for this are:

    -   The fixed (mean) accrual rate per week to simulate.

-   Estimated from the Last ‘W’ Weeks accrual data, using a Poisson
    distribution, with a Gamma prior. The parameters for this are

    -   The number of past weeks W to use the accrual data from.

    -   The prior mean for the model of the accrual rate

    -   The weight of the prior

-   Estimated from all the accrual data from the start of the trial,
    using a Poisson distribution, with a Gamma prior, the parameters for
    this are:

    -   The prior mean for the model of the accrual rate

    -   The weight of the prior

#### Future Trial Bayesian Predictive Probabilities

For predictive probabilities for a future trial, a predictive
probability of success in a subsequent phase 3 trial is estimated during
the current trial. The probability of success in phase 3 is calculated
for each study arm, based on the specification of the phase 3 trial
given here of:

-   whether the aim is to show superiority or non-inferiority,

-   the sample size per arm,

-   the required one-sided alpha,

-   and the super-superiority margin or non-inferiority margin (if any).

Given these criteria, FACTS calculates the predicted probability of
success of the subsequent trial for that endpoint assuming the estimate
of the response on that endpoint, integrated over the uncertainty in
those estimates. The conventional expected power of the specified future
trial is calculated for the treatment effect in each MCMC sample and
then averaged. This predicted probability of success can then be used in
the stopping criteria and final evaluation criteria for the current
trial.

This QOI is an extension of the “probability of success in phase 3” in
earlier versions of FACTS – the difference now is that multiple
different possible future trials can be specified in different QOIs and
used for different decisions.

<img src="coreUGattachments/CoreUserGuide/media/image11.png"
style="width:3.21594in;height:3.21594in" />

This predictive probability has the following parameters that must be
specified:

-   Whether the future trial will be for Superiority or Non-inferiority.

-   The size of the future trial in terms of the number of subjects on
    each arm.

-   The (one sided) alpha level that will be used to determine the
    significance of the trial.

-   The Super-superiority margin (if any) or the non-inferiority margin.
    The default p-value delta is **not** used for this QOI it is
    specified as part of the QOI and can be different from the default.

As with all QOIs, the future trial predictive probability QOI will be given an alternative shorter name that
can be used when accessing the output files from other software such as
R.

If there is no data available on the arms involved in the Bayesian
predictive probability calculation, FACTS will still calculate the
predictive probability of the future trial being a success based on the
estimated posterior distribution. The posterior distribution for the
arms with no available data will be the same as the user specified prior
distribution.

### Conditional Power

Conditional power is currently available for Continuous and Dichotomous
endpoints in Core and Staged Designs.

When creating a Conditional Power QOI, the first specification that must
be made is whether the conditional power should be calculated for the
current trial or for a hypothetical future trial. The two selections are
fundamentally different.

The Current Trial conditional power takes the information collected in
the trial so far, and assumes that for the remainder of the current
trial the data collected has an effect equal to the point estimate of
the already collected data. The previously collected trial data is then
combined with hypothetical future data and weighted based on the amount
of information that has been and will be observed by the end of the
trial. The result is the probability that the current trial will reject
the null hypothesis at a provided alpha level under the assumption that
the future patients are generated from a population having a treatment
effect exactly equal to the past data’s test statistic.

The Future Trial conditional power also uses the information accrued in
the trial so far to calculate the assumed treatment effect moving
forward, but assumes that a new study would be started that does not use
the data from the current trial in its analysis. The new study is
assumed to be a 1:1 randomized study if there is a control arm, and a
single arm study if no control, with a sample size per arm, objective,
and alpha level specified at the time of QOI creation.

If a conditional power is ever calculated for an arm with no final
response data or against a control arm with no final response data, the
conditional power will be called missing (-9999 in FACTS).

#### Current Trial Conditional Power

When creating a current trial conditional power, the missingness
strategy, multiplicity adjustment, final sample size, and alpha level
for the significance test must be specified.

<img src="coreUGattachments/CoreUserGuide/media/image12.png"
style="width:2.61858in;height:2.61544in" />

##### Handle missingness using:

Missingness handling for a continuous endpoint can be specified as:

-   **Ignore:** subjects that are known dropouts are not included in the
    current or final sample size, and are not used to estimate the
    treatment effect.

-   **Last Observation Carried Forward (LOCF):** subjects that are known
    dropouts have their last observed endpoint value assumed to be their
    final endpoint. Subjects with no early observed data are ignored.
    Subjects that have data to carry forward are considered complete,
    and are used to estimate the current treatment effect.

-   **Baseline Observation Carried Forward (BOCF):** subjects that are known
    dropouts have their baseline value assumed to be their final
    endpoint. They are included in the current information as complete
    subjects and are used to estimate the current treatment effect. BOCF
    is only available for the continuous endpoint and when simulating
    baseline for subjects.

-   **Failure:** subjects that are known dropouts are assumed to have a
    negative outcome at their final visit. If responses are good, the
    subject is a non-response, and if responses are bad, the subject is
    a responder. Failure is only available for dichotomous endpoints.

##### Test Type

The test type can either be Unadjusted or Bonferroni. An unadjusted test
will always use the specified alpha level in the significance test. The
Bonferroni adjustment will divide the specified alpha value by the
number of non-control arms enrolling in the study in the enrolment
period leading up to the current analysis time.

##### Sample Size:

The current trial conditional power can be calculated at two different
future time points.

1.  **Current Enrollment**: That no new subjects are recruited, but all
    those who have been recruited are followed up until they are
    complete.

2.  **Trial Maximum**: That the trial would continue recruiting subjects
    using the current allocation ratios until the trial maximum sample
    size is reached, and all subjects recruited are followed up until
    they have the opportunity to complete.

##### One-sided Alpha

The threshold that the current trial would have to be less than in order
to be considered a success. The success/futility values specified in the
design tab are not used in the conditional power calculation. This value
may be adjusted by the “Test type:” input.

##### Super-Superiority (Non-inferiority) margin for p-value:

This value cannot be changed on the QOI pop-up. It’s instead modified at
the bottom of the QOI page. The objective of the current trial cannot be
different for each individual current trial conditional power QOIs.

##### Additional Notes

Currently, conditional power will always assume that there is no
correction applied to combine the different p-values from the analysis
timepoints, i.e. no combination test is used.

The conditional power of the current trial at the maximum sample size is
only available if the allocation is either fixed, or fixed with arm
dropping, but not adaptive allocation or deterministic allocation.

Conditional power for the current trial is calculated

-   Ignoring the possibility of the trial stopping or dropping an arm at
    a future interim

-   Ignoring the possibility of future subject drop-outs.

#### Future Trial Conditional Power

Conditional power of a future trial is reduced to a power calculation
for a future trial, given the current frequentist estimates of treatment
effect and standard deviation and a set of assumptions for the future
trial, such as sample sizes, hypothesis to be tested, and significance
levels to be used.

The test type of the future trial can be set to Superiority or
Non-inferiority. If the superiority or non-inferiority margin is set to
0, then both types of tests simplify to traditional tests of superiority
with no margin. If a non-zero margin is set, then the future trial must
meet the success criteria indicated by the test type and the margin. The
margin cannot be negative, but a super superiority test with a margin of
-0.5 is equivalent to a non-inferiority test with a margin of 0.5.

The subjects per arm dictates how many subjects would be enrolled on
each arm in the future study. If there is a control arm in the current
trial, then the future trial is assumed to be randomized 1:1 between the
active arm and the control. If there is no control arm in the current
study, then the future trial is a single arm trial testing against the
performance goal specified in the Freq. Comparison Response box in the
QOI creation pop-up.

The One-sided Alpha is the significance level of the final analysis test
of the future trial. The future trial is assumed to have 1 active arm,
so there are no adjustments to the alpha level available.

The superiority margin or non-inferiority margin indicate the amount
that the active arm must be better/not worse than the control arm by. If
there is no control arm then this is the Freq Comparison response, which
is the value that the active arms must be significantly better than in
order to be declared a success. The frequentist margin for p-value QOIs
on the QOI tab is not used for Future conditional power calculations –
the future trial can have a different objective than the current trial.

<img src="coreUGattachments/CoreUserGuide/media/image13.png"
style="width:3.20399in;height:3.21594in" />

#### Technical Aspects of Conditional Power Calculations

The conditional power calculations in FACTS are all calculated similarly
to [Jennison and Turnbull](#jennisonturnbull "Jennison, C., & Turnbull, B.W. (1999). Group Sequential Methods with Applications to Clinical Trials (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780367805326").

For continuous endpoint conditional power calculations, the conditional
power of the future trial is calculated using a pooled standard
deviation from all arms. For dichotomous conditional power calculations,
each arm has its own standard deviation based on the MLE estimate of its
proportion. These decision result in the conditional power tests
matching how standard p-value QOIs are calculated for continuous and
dichotomous endpoints.

The following sections will provide formulae to calculate the
conditional power in the case where there is a control arm.
Simplifications for when the arm is being compared to a fixed value
rather than a control arm are trivial: information fractions are only a
function of the active arm standard deviation and the test statistic
does not have a second sample mean subtracted from the active arm.

The value of $\delta$, which can be a non-inferiority margin or a super
superiority margin, is always positive coming out of FACTS. To make the
math more concise we can define a couple of coefficients for the $\delta$
term that allow it to be used without re-writing formulas for different
hypothesis test setups. If high values of the endpoint are good, then $s_1 = 1$, and if low values of the endpoint are good, then
$s_1=−1$. If the specified $\delta$ is a non-inferiority margin,
then $s_2 = 1$, and if it’s a super superiority margin then
$s_2=-1$.

##### Continuous Conditional Power for the Current Trial

Let t be the interim index that the conditional power is being computed
at, and T be the time of the analysis that the conditional power is
being computed for. Then $Z_k$ is the test statistic of the
data collected up to the current interim analysis in the study, $I_k$ is the information level at the time of the interim
analysis, and $I_K$ is the information level at the end of
the study that the conditional power is being calculated for.

Let arm 1 be the control and arm 2 be the active arm, $\bar{x_{it}}$ be the sample mean of arm $i$ at time $t$, $\widehat{\sigma_{i}^{2}}$ be the sample variance of arm $i$ at time
$t$, $n_{it}$ be the number of subjects with complete known
final data on arm $i$ at interim analysis $t$, and $n_{iT}$ be
the number of subjects with complete known final data on arm $i$ at the
time that conditional power is being calculated for. The pooled variance
estimate is
$\widehat{\sigma^{2}} = \sum_{d = 1}^{D}\widehat{\frac{\sigma_{d}^{2}}{n_{dt}}}$
where D is the total number of arms in the study.

Then,

$$I_{t} = \left( \frac{\widehat{\sigma^{2}}}{n_{1t}} + \widehat{\frac{\sigma^{2}}{n_{2t}}} \right)^{-1}$$

$$I_{T} = \left( \frac{\widehat{\sigma^{2}}}{n_{1T}} + \widehat{\frac{\sigma^{2}}{n_{2T}}} \right)^{-1}$$

$$Z_{t} = \left( {\overline{x}}_{2t} - {\overline{x}}_{1t} + s_{1}s_{2}\delta \right)\sqrt{I_{t}}$$

where $\delta$ is the non-inferiority or super superiority margin.

Then for a one-sided alpha level of $\alpha$, let $Z_{1-\alpha}$ be
the critical value corresponding to $\alpha$.

If **high values of the endpoint are good,** the conditional power of
the **current** trial is:

$$CP_{T} = \Phi\left( \frac{Z_{t}\sqrt{I_{t}} - z_{1 - \alpha}\sqrt{I_{T}} + ({\overline{x}}_{2t} - {\overline{x}}_{1t} + s_{2}\delta)\left( I_{T} - I_{t} \right)}{\sqrt{I_{T} - I_{t}}} \right)$$

If **low values of the endpoint are good**, the conditional power of the
**current** trial is:

$$CP_{T} = \Phi\left( \frac{{- Z}_{t}\sqrt{I_{t}} - z_{1 - \alpha}\sqrt{I_{T}} - ({\overline{x}}_{2t} - {\overline{x}}_{1t} - s_{2}\delta)\left( I_{T} - I_{t} \right)}{\sqrt{I_{T} - I_{t}}} \right)$$

##### Calculation of Continuous Conditional Power for a Future Trial

Most of the future trial conditional power calculation is the same as
the above current trial conditional power. There are some
simplifications.

${\overline{x}}_{it}$ and $\widehat{\sigma_{i}^{2}}$ are the same as
in the current conditional power calculation. $I_t$, the
weight of the current trial Z-score, is set to 0. $I_T$ is
now the information at the end of the future trial, and is calculated
as:

$$I_{T} = \left( \frac{\widehat{\sigma^{2}}}{n_{T}} + \widehat{\frac{\sigma^{2}}{n_{T}}} \right)^{- 1}$$

where $n_T$ is the sample size per arm in the future trial
and again $\widehat{\sigma^{2}}$ is the pooled variance.

If **high values of the endpoint are good,** the conditional power of a
**future** trial is:

$$CP_{T} = \Phi\left( \frac{- z_{1 - \alpha}\sqrt{I_{T}} + ({\overline{x}}_{2t} - {\overline{x}}_{1t} + s_{2}\delta)\left( I_{T} \right)}{\sqrt{I_{T}}} \right)$$

If **low values of the endpoint are good**, the conditional power of a
**future** trial is:

$$CP_{T} = \Phi\left( \frac{- z_{1 - \alpha}\sqrt{I_{T}} - ({\overline{x}}_{2t} - {\overline{x}}_{1t} - s_{2}\delta)\left( I_{T} \right)}{\sqrt{I_{T}}} \right)$$

##### Calculation of Dichotomous Conditional Power for the Current Trial

The dichotomous conditional power calculations are similar in spirit to
the continuous conditional power calculations. One substantial
difference is in how the dichotomous conditional power tests handle a
non-inferiority or super superiority margin, $\delta$. The method of
calculating an appropriate test statistic for a frequentist hypothesis
test is not obvious when the null scenario has a non-zero $\delta$.

When there is no margin, the estimate for each treatment is simply based
on the observed response proportion $\widehat{p_{i}}$ for arm $i$, and
the test statistic for a comparison of the control arm, $c$, with dose
$d$ is the usual Wald test

$$Z_{d} = \frac{\widehat{p_{d}} - \widehat{p_{c}}}{\sqrt{\frac{\widehat{p_{d}}(1 - \widehat{p_{d}})}{n_{d}} + \frac{\widehat{p_{c}}(1 - \widehat{p_{c}})}{n_{c}}}}$$

When there is a margin, FACTS uses the Farrington-Manning Likelihood
Score test statistic to estimate quantities $\widetilde{p_{d}}$ and
$\widetilde{p_{c}}$ based on the MLEs of the arm proportions governed
by the constraint that
$\widetilde{p_{d}} - \widetilde{p_{c}} = - s_{1}s_{2}\delta$. These
constrained MLE estimates are used in the standard error of the test
statistic. The FM test statistic is then,

$$Z_{FM,d} = \frac{\widehat{p_{d}} - \widehat{p_{c}} + s_{1}s_{2}\delta}{\sqrt{\frac{\widetilde{p_{d}}(1 - \widetilde{p_{d}})}{n_{d}} + \frac{\widetilde{p_{c}}(1 - \widetilde{p_{c}})}{n_{c}}}}$$

See the [PASS documentation](#pass "NCSS, LLC. Assurance for Non-Inferiority Tests for the Difference Between Two Proportions. In PASS Sample Size Software Documentation (pp. 289-1-289–31).") or [SAS documentation](#sas "SAS Institute Inc. 2016. Base SAS® 9.4 Procedures Guide: Statistical Procedures, Sixth Edition. Cary, NC: SAS Institute Inc.") for a complete
description of the calculations that go into the FM test. In the SAS
documentation, note that the FM test is the same as the
Miettinen-Nurminen test without including the $\frac{n}{n - 1}$
variance correction. The FM test was used rather than the MN test
because as $\delta \rightarrow 0$, the FM test converges to the simple Wald test.

Once the test statistic has been resolved, the dichotomous conditional
power for the current trial is calculated as follows. For calculating
the conditional power for arm 2 compared to the control arm, called arm
1 without loss of generality, let $I_t$ be the current
information amount and $I_T$ be the amount of information
that the conditional power is being calculated for. Then,

$$I_{t} = \left( \frac{\widetilde{p_{1}}\left( 1 - \widetilde{p_{1}} \right)}{n_{1t}} + \frac{\widetilde{p_{2}}\left( 1 - \widetilde{p_{2}} \right)}{n_{2t}} \right)^{- 1}$$

$$I_{T} = \left( \frac{\widetilde{p_{1}}\left( 1 - \widetilde{p_{1}} \right)}{n_{1T}} + \frac{\widetilde{p_{2}}\left( 1 - \widetilde{p_{2}} \right)}{n_{2T}} \right)^{- 1}$$

$$Z_{t} = \left( \widehat{p_{2}} - \widehat{p_{1}} + s_{1}s_{2}\delta \right)*\sqrt{I_{t}}$$

where $\delta$ is the super superiority or non-inferiority margin, and
$n_{1t}$ and $n_{2t}$ are current number of
completers on the control and active arm, and $n_{1T}$ and
$n_{2T}$ are the number of completers that will be on the
control and active arm at time that the conditional power is being
calculated for. Additionally, if there is no non-inferiority or super
superiority margin $\delta$, then all $\widetilde{p_{*}}$ values are equal
to their corresponding $\widehat{p_{*}}$ values.

For a one-sided alpha level of $\alpha$, let $z_{1-\alpha}$ be the
critical value corresponding to $\alpha$.

If **high values of the endpoint are good,** the conditional power of
the **current** trial is:

$$CP_{T} = \Phi\left( \frac{Z_{t}\sqrt{I_{t}} - z_{1 - \alpha}\sqrt{I_{T}} + (\widehat{p_{2}} - \widehat{p_{1}} + s_{2}\delta)\left( I_{T} - I_{t} \right)}{\sqrt{I_{T} - I_{t}}} \right)$$

If **low values of the endpoint are good**, the conditional power of the
**current** trial is:

$$CP_{T} = \Phi\left( \frac{{- Z}_{t}\sqrt{I_{t}} - z_{1 - \alpha}\sqrt{I_{T}} - (\widehat{p_{2}} - \widehat{p_{1}} - s_{2}\delta)\left( I_{T} - I_{t} \right)}{\sqrt{I_{T} - I_{t}}} \right)$$

##### Calculation of Dichotomous Conditional Power for a Future Trial

Most of the calculation quantities for the future trial conditional
power are the same as the current trial conditional power. The only
difference is that for the future trial conditional power we discard the
influence of the current trial test statistic on the final analysis, so
$I_t=0$. Then the conditional power calculations become:

If **high values of the endpoint are good,** the conditional power of
a **future** trial is:

$$CP_{T} = \Phi\left( \frac{- z_{1 - \alpha}\sqrt{I_{T}} + (\widehat{p_{2}} - \widehat{p_{1}} + s_{2}\delta)\left( I_{T} \right)}{\sqrt{I_{T}}} \right)$$

If **low values of the endpoint are good**, the conditional power of a
**future** trial is:

$$CP_{T} = \Phi\left( \frac{- z_{1 - \alpha}\sqrt{I_{T}} - (\widehat{p_{2}} - \widehat{p_{1}} - s_{2}\delta)\left( I_{T} \right)}{\sqrt{I_{T}}} \right)$$

## P-values

A p-value QOI simply allows the user to specify which test type to be
used (unadjusted, Bonferroni, [Dunnett's](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121."), or Trend Test), and how missing
data is to be handled (ignored, LOCF, [BOCF](## "Only if continuous endpoint and baseline is being simulated."), and [missing is failure](## "Dichotomous endpoint only.")). If a control arm is present, p-values are comparisons
against the control arm. If there is no control arm present, p-value
QOIs are comparisons against a fixed response or fixed rate that is
specified at the bottom of the QOI tab (it cannot be set differently for
separate QOIs).

Note that for dichotomous endpoints p-value are calculated using the
Test of Proportions, this test statistic asymptotically approaches a
normal distribution, and at least [5 success and 5 failures](## "Sometimes this is said to be 10 and 10, or 15 and 15. Your rule of thumb can be based on your comfort level for allowing CLT to kick in.") should be
observed for this to be reasonable.

The p-values are calculated including the default frequentist delta that
has been specified at the bottom of the screen. The delta margin cannot be modified as part of the QOI, the value of the delta and nature of the test is the same for all the p-value QOIs.

In a TTE design with a predictor, the p-values are only calculated for
the final event endpoint, not the predictors.

<img src="coreUGattachments/CoreUserGuide/media/image14.png"
style="width:3.08054in;height:1.89707in"
alt="Graphical user interface, application Description automatically generated" />

### P-values when there is no control arm

If there is no control arm (not currently an option in Time-to-Event),
the p-value delta is instead interpreted as the objective response or
response rate to compare to. The option to set the trial type is also
disabled ("Superiority” is selected and the “non-inferiority option is
greyed out). For a non-inferiority trial comparing to an objective
response or response rate simply use the objective rate minus the
non-inferiority delta (if a higher response is better) or the objective
rate plus the non-inferiority delta (if a lower response is better).

It is currently only possible to have one objective rate to compare
against.

The same objective rate will be used for the target p-value test in the
predictive probabilities.

<img src="coreUGattachments/CoreUserGuide/media/image15.png"
style="width:5.07319in;height:3.85221in"
alt="Graphical user interface, text, application Description automatically generated" />

### Fisher-Exact Test

When specifying the QOIs for a dichotomous endpoint in a trial with a control arm, the bottom of the QOI tab allows the user to specify what statistical test to
use when calculating p-values. Options are “Normal approximation”
(default) and “Fisher exact test”. “Normal approximation” uses a t-test
and should yield similar results to a Chi-Square test without continuity
correction, while “Fisher exact test” uses an exact Fisher’s exact test
for 2x2 tables.

If “Fisher exact test” is chosen as the test type, all p-value QOIs will
use a Fisher’s exact test, except for future trial predictive
probabilities, which will still use a t-test.

If “Fisher exact test” is chosen as the test type, only “Bonferroni” and
“Unadjusted” are available as multiplicity corrections and any QOIs
previously created using different multiplicity corrections will be
deleted.

“Fisher exact test” is not available for non-inferiority comparisons.

<img src="coreUGattachments/CoreUserGuide/media/image16.png"
style="width:6.925in;height:5.07292in"
alt="A screenshot of a computer Description automatically generated" />

## Target Doses

The target dose QOIs are Bayesian posterior probabilities based on MCMC
sampling of how often each dose meets the specified target criteria.
There are 3 types of target specifiable

-   Max – the dose with the maximum response, this has no parameters to
    specify, so is available as a pre-defined default QOI.

-   MED – a Minimum Effective Dose, the lowest dose that has a response
    better than an absolute or relative target.

-   EDq – an effective dose, the dose that achieves a specified
    proportion (quantile $q$) of the maximum improvement in response
    relative to an absolute value or the performance of a specific
    treatment arm.

<img src="coreUGattachments/CoreUserGuide/media/image17.png"
style="width:3.00783in;height:2.07488in" />

## Decision Quantities

The QOIs described so far have defined values to be calculated across all
doses. For a Success/Futility decision to be made it is necessary to specify the treatment arm
whose QOI value is to be used in comparison to the success and/or futility criteria. This selection can be done by specifying a
specific arm, but normally it is specified by using a “Probability of
being Target” selection criteria. A simple and common example is “The
probability of the response being better than that of the Control of the
Treatment arm with the maximum response”:

<img src="coreUGattachments/CoreUserGuide/media/image18.png"
style="width:2.98863in;height:1.43143in" />

<img src="coreUGattachments/CoreUserGuide/media/image19.png"
style="width:3.71051in;height:1.77718in" />

A decision QOI consists of (1) a QOI that has been calculated for each
dose, and (2) a method of choosing a dose to use the QOI value of. Choosing the dose can be done either by using a target Dose QOI like [Pr(Max)]{.fake-code-block}, [Pr(EDq...)]{.fake-code-block}, etc, by choosing the dose with the highest or lowest value of a QOI, or by explicitly choosing a dose level in advance.

As an example using a target QOI, you can imagine evaluating a decision QOI that is specified to choose the probability of being better than Control by 2 units [Pr($\theta_d - \theta_0 > 2$)]{.fake-code-block} based on the arm with the highest ED90 [EDq relative to control; Quantile 0.9]{.fake-code-block}.

Instead of a Target Dose QOI, it is also possible to specify a specific
dose, or to use the terms “Max probability over all doses” and “Min
probability over all doses” which mean that the dose with the minimum or
maximum value of the per dose QOI is used. This enables:

-   Decisions QOIs testing specific doses, for example by defining a
    Posterior probability QOI that compares all doses against the lowest
    dose, and then a Decision QOI using that posterior probability with
    the highest dose selected as the target dose it is possible to based
    decisions on the posterior probability that the highest dose is
    better than the lowest dose.

-   A Decision QOI using “Max probability over all doses” allows a decision to be based on
    whether a probability or p-value is greater than a threshold at any
    dose, or less than a threshold at all doses.

-   A Decision QOI using “Min probability over all doses” allows a decision to be based on
    whether a probability or p-value is less than a threshold for any
    dose or greater than a threshold at all doses.

There is one special case: if the QOI chosen is the Trend Test p-value,
then because this is not actually a per-dose value, only one “Target
dose” can be selected: “Overall Significance”.

## Standard Evaluation Variables

These 2 parameters are used across some of the default QOIs and hence
specified outside the normal QOI dialog boxes.

-   The CSD value

-   and whether absolute or relative to the Control arm

these are used in both the default “Pr(CSD)” and the “MED relative to
Control” QOIs.

<img src="coreUGattachments/CoreUserGuide/media/image20.png"
style="width:6.925in;height:0.49375in" />

Note that the CSD value here is designed to be usually entered as a
positive value, as in earlier versions of FACTS. Its sign is
automatically adjusted if “lower score means subject improvement” or if
the trial is a non-inferiority trial.

These adjustments are **not** made for other user entered QOIs. The
directions of comparison both for the definition of the probability to
be calculated and the comparison of the resulting probability with a
threshold are under the user’s control, as are whether delta’s are negative or positive. This allows the user to define QOIs in whatever fashion is natural to them and their team.

### The direction of comparison for default QOIs

Note that as a result of being able to specify whether a higher or lower
response represents improvement, and whether the aim is superiority or
non-inferiority, the CSD or the NIM must always be a positive
value. FACTS will automatically determine which direction is appropriate
(e.g. if ***lower*** values are subject improvement, the engine will
realize a CSD will need to be ***subtracted*** from the control score
before comparing with the estimate of response on a treatment arm).

# Design Overview

The FACTS core engine allows for the design and simulation of fixed and
adaptive clinical trials, especially focused on, but not limited to,
Bayesian designs with multiple active arms. Trials designed in the core
engines are comprised of a number of elements:

1.  The dose response model: the user must specify how the doses are related to eachother in the primary analysis, though there is a simple ‘no model’ option that estimates
    the mean treatment effect of each arm independently. A fixed trial
    uses the dose response model for the final Bayesian analysis of the
    data; an adaptive trial uses the same model both for the final
    analysis and at the interim updates.

2.  The longitudinal (predictor) model: whether the trial is adaptive or
    fixed, the user may select to whether to use a longitudinal model
    (similarly, a predictor model in time to event). In a fixed trial
    the longitudinal model can be used to multiply impute final values for subjects that have dropped out. In an adaptive trial it is also used at the
    interim updates to multiply impute final values for subjects who have been
    recruited but do not yet have final values. In a fixed trial with no
    subject dropouts using a longitudinal model would have no effect on
    the outcome, analysis, or conduct of the trial.

3.  Allocation rules: in a fixed trial the user just specifies the
    proportion of subjects to be recruited to each arm, and the same can
    be done in an adaptive trial (i.e. an adaptive trial does not have
    to adapt the allocation), but an adaptive trial has a range of
    options that the user can use to adapt how subjects are allocated to
    the different treatment arms as the trial progresses.

4.  Early stopping rules: in an adaptive trial the user can select the
    criteria and specify the thresholds at which trial should be stopped
    at any interim where the conditions are satisfied. Early stopping is
    optional, and even in an adaptive design the user can opt to always
    recruit the maximum permitted number of subjects. In a fixed trial
    there are no interim analyses and hence no opportunity to stop early.

5.  Final evaluation criteria: the same Bayesian evaluation criteria are
    available whether the trial is fixed or adaptive. The user selects
    which criteria to use and what thresholds will constitute success or
    failure. The success and failure criteria do not have to be complements of eachother, and any analysis that doesn't completely satisfy either the success or futility criteria is called, "inconclusive."

6.  Frequentist analysis: frequentist p-values can be calculated
    comparing each dose to the control arm (or a fixed value if there is no control). P-values can be used as decision making quantities at interim updates or final analyses. p-value cannot benefit from the dose reponse models or longitudinal models, which are specific to the Bayesian model in FACTS.

## Evaluation of Bayesian Posterior Estimates

At every interim and final analysis there is a Bayesian model fit to the data observed up to that point in the trial. The Bayesian model contains a dose
response model and, often, a longitudinal model. In the absence of a
longitudinal model, the posterior is calculated as:

$$p(\omega|Y) \propto \prod_{i = 1}^{n}{p(y_{i}|\phi)p(\phi)}$$

where $\phi$ is the set of parameters of the selected response model,
$p(\phi)$ is the prior for those parameters, $y_i$ is the
final response for each subject and $n$ is the number of subjects with complete data.

With a longitudinal model, this becomes:

$$p(\omega|Y) \propto \prod_{i = 1}^{n}{p(y_{i}|\phi)p(\phi)\prod_{i = 1}^{n}{\prod_{j = 1}^{L}{p(y_{ij}|\psi)p(\psi)}}}$$

where $\psi$ is the set of parameters of the selected longitudinal model,
$p(\psi)$ is the prior for those parameters, $y_{ij}$ is the
response for each subject $i$ at each visit $j$ and $L$ is the number of visits.

The posterior is evaluated using [MCMC](## "Markov Chain Monte Carlo") with individual parameters updated
by Metropolis Hastings (or Gibbs sampling where possible), using only
the $y_i$ and $y_{ij}$ data available at the
time of the update.

# Dose Response

Dose response models in FACTS may be more accurately called final
endpoint models. They create and model a relationship across the doses
specified in the Treatment Arms tab. Often, but not always, the dose
strength, called “Effective Dose Strength” in the Study > Treatment
Arms tab of FACTS, is used in the dose response models to determine the
order of doses, and which doses are more related to others.

The dose response models can be simple, and model the doses largely
independently, as is done with the Independent Dose Model or the
Independent Beta Binomial Model (dichotomous only). They can have
logistic style models with interpretable parameters, like the
3-parameter logistic or the $E_{max}$ model (called Sigmoidal in
FACTS). The dose response model can also be a model that creates a
smooth, spline like, model over the doses using a normal dynamic linear
model (NDLM), a monotonic NDLM, or a 2nd order NDLM.

For all endpoints, we model the response at each dose, d, in terms of
$\theta_d$ on a continuous scale, allowing a consistent and rich
range of dose response models to be used for all endpoint types.
Transformations (see below) of the dichotomous and time-to-event
responses are used to achieve this.

## Continuous, Dichotomous, and Time-To-Event

With the exception of two dose response models specific to a dichotomous
endpoint, the same dose response modeling facilities are available for
all endpoints.

Let there be D total doses including the control arm if it exists. For
any endpoint, the estimate of dose response model is called
$\theta_d$ for a dose $d \in \{1, \ldots , D\}$.

::: {.panel-tabset}
## Continuous

In the continuous dose response models the individual response or change from baseline (if it is being used) $Y_i$ of the $i^{th}$ subject allocated to dose $d_i$ is modeled:
$$
Y_i \sim \text{N}(\theta_{d_i}, \sigma^2)
$$
The variance $\sigma^2$ has an inverse-gamma prior. For a description in how FACTS elicits parameterizations for the Inverse Gamma distribution, [see here](../../../../concepts/facts/InverseGammaDistribution.qmd "Widget for assessing how the center and weight parameters in FACTS impact the inverse gamma distribution.").

## Dichotomous

In the dichotomous case the final endpoint of the $i^{th}$ subject who has been allocated to dose $d_i$ is modeled:

$$ Y_i \sim \text{Bernoulli}(P_{d_i}) $$
where $P_{d_i}$ is the probability of response
for a subject on dose $d_i$. The probability
$P_d$ is modeled on the logit scale, so
$$P_{d} = \frac{e^{\theta_{d}}}{1 + e^{\theta_{d}}},$$ and
$\theta_d$ is the log-odds ratio:
$$\theta_{d} = ln\left( \frac{P_{d}}{1 - P_{d}} \right)$$,

## Time-To-Event

In the time-to-event case, the time to a subject’s response,
$Y_i$ is modeled as piece-wise exponentially distributed
with hazard rates, $\lambda_s$, for pieces $s \in \{1,\ldots,S\}$. So,

$$ Y_i \sim \text{PWExp}(\{\lambda_1,\ldots,\lambda_S\})$$

for a subject on the control arm, and

$$ Y_i \sim \text{PWExp}(\{\lambda_1 e^{\theta_d},\ldots,\lambda_S e^{\theta_d}\})$$

for non-control doses. Then, $\theta_d$ is the log-hazard
ratio
$\left( \ln\left( \frac{\lambda_{s}e^{\theta_{d}}}{\lambda_{s}} \right) = \ln\left( e^{\theta_{d}} \right) = \theta_{d} \right)$
averaged over the observation time segments. This formulation
implies a proportional treatment effect across the pieces of the
piece-wise exponential.
:::

Each dose response model is parameterized in terms of $\theta_d$,
but each endpoint models this parameter on a different scale. The
dichotomous dose response models are on the log-odds scale, and the
time-to-event endpoint models are on the log hazard ratio. When
specifying a prior distribution for a continuous endpoint dose response
model the expected data mean and variance determine which priors should
be considered non-informative. When estimating a probability in the
dichotomous case, using a prior with standard deviation above, say, 10
leads to a diffuse distribution on the log-odds scale, but results in a
prior distribution on the probability scale that is heavily peaked at 0
and 1. This can lead to undesirable model results and decisions being
made in small sample size situations, and numerical instability in
extreme cases. Similarly on the time-to-event scale, the prior put on
the log-hazard ratio $\theta_d$ is exponentiated before being
multiplied by the hazard rate, so diffuse priors on the log-hazard can
have unexpected modeled results. Again, time-to-event $\theta_d$
priors that have standard deviations less than about 10 are generally
acceptably diffuse for most situations while avoiding edge case
curiosities.

<img src="coreUGattachments/CoreUserGuide/media/image21.png"
style="width:2.07519in;height:2.68327in" />

## Descriptions of Dose Response Models

The Dose Response section of the Design tab allows the user to specify
how to analyze the relationship between dose/treatment arm and the final
response and hence estimate the values $\theta_d$. The
interpretation of $\theta_d$ depends on the nature of the
endpoint:

::: {.panel-tabset}
## Continuous

Where the response is continuous, $\theta_d$ is the estimate
of the mean response/change from baseline on treatment arm $d$, and
the common inter-subject variance of the response $\sigma^2$,
is also estimated.

The response on the control arm, $\theta_0$, is estimated either in the dose response model or modeled separately.

## Dichotomous
Where the response is dichotomous, $\theta_d$ is the estimate
of the log-odds of the probability of observing a response on the
treatment arm $d$.

The response on the control arm, $\theta_0$, is estimated either in the dose response model or modeled separately.

## Time-To-Event

Where the response is time-to-event, $\theta_d$ is the
estimate of the log hazard ratio compared to the control arm on the
treatment arm $d$.

When the primary endpoint is time-to-event, the response rate on the control arm, $\lambda$, is estimated and $\theta_d$ for $d\in \{1,2,\ldots,D\}$ is the log hazard of the response rate of each treatment arm compared to the control arm, so $\theta_0\equiv 0$.
:::

Some, but not all, of the dose response models use the effective dose
strength, $\nu_d$, to model the dose response
$\theta_d$. The effective dose strength is specified on the Study > Treatment Arms tab. It is always fixed at 0 for the control arm ($\nu_0\equiv 0$).

### Independent Dose Model

The “Independent Dose Model” providing a simple pair-wise comparison
between the study drug arms and the control arm and/or the active
comparator arm. The doses are modelled as independent and normally
distributed with a prior of:

$$\theta_d \sim \text{N}(\mu_d, \nu_d^2)$$

Where $\mu_d$ and $\nu_d^2$ are specified
in FACTS and can either be the same or vary across arms.

This model is useful:

-   When there is only one or two experimental arms

-   When the study drug arms don’t differ by dose but in other ways, so there is no ordered sequence of treatments – e.g. each arm is the study drug in combination with a different additional drug.

-   For simulating simple trial designs

-   For simulating a ‘conventional’ or ‘strawman’ design to compare more complex designs against

Otherwise for trials with multiple related arms that differ by dose, it
is usually more efficient to use a dose-response model.

### Independent Beta-Binomial Model (Dichotomous Only)

This is a “no model” option similar to the above, but only available for the Dichotomous endpoint. Unlike any other dose response model, this model uses the beta distribution to model the probability of response directly rather than fitting a model to the log-odds of the probability.

The final endpoint response $Y_i$ is modeled as:

$$Y_i \sim \text{Bernoulli}(P_d)$$
where $P_d$ is the probability that a patient is a response
at the final endpoint for subjects randomized to dose $d$. With posterior

$$P_d \sim \text{Beta}(\alpha_d + \text{responders}_d, \beta_d + \text{non_responders}_d)$$

Where $\alpha_d$, $\beta_d$ are the priors for the arm $d$,
$\text{responders}_d$ is the number of responders
on arm $d$ and $\text{non_responders}_d$ is
the number of non-responders on arm $d$.

This model has the advantages of an easier to understand prior, and
better estimation of $P_d$ when the number of responders and
non-responders is small (either is  < 5) compared to the log-odds
model. As it’s a independent dose model, it is well suited to analyzing
single arm, two arm or three arm trials, or trials where the
experimental arms are unrelated, and so there is no basis on which to
borrow information between them. With trials with multiple arms of the
same treatment at different doses it is usually more advantageous to fit
a dose response model.

###  Simple NDLM

The Normal Dynamic Linear Model (NDLM) estimates the final endpoint as a
smoothed curve across doses included in the model. Doses are tied
directly to their nearest neighbor, and the prior expectation for a dose
is that it is equal to its neighboring doses, tending to a constant dose
response across doses in the absence of data. The model is defined as
follows:

Let doses $d=d', \ldots, D$ be doses in the dose response model and
$\theta_d$ be the estimated dose response for dose $d$. The
initial dose $d'=1$ if there is no control or control is
included in the dose response model, and $d'=2$ if the
control arm is modeled separately.

The dose response of the first dose, $d'$, has a prior of:

$$\theta_{d'} \sim N\left(\mu_{d'},\tau^2_{d'}\right)$$

where $\mu_{d'}$ and $\tau_{d'}^2$ are
specified directly in FACTS. Subsequent dose response estimates
$\theta_{d'+1}, \ldots, \theta_D$ have priors
centered at the previous dose response with variances based on the
distance between the dose $d$ strength and the dose $d-1$ strength.
Specifically,

$$\theta_d \sim N\left(\theta_{d-1},\tau^2_{d-1}\right) \text{ for } d=d'+1, \ldots, D$$

where for dose strengths $\nu_d$ and $\nu_{d-1}$,
$\tau_{d-1}^2$ is defined as
$$\tau^2_{d-1}=\tau^2\left(\nu_d-\nu_{d-1}\right)$$

The prior distribution for the “drift” parameter, which controls the
amount of smoothing is:

$$\tau^{2}\sim IG\left( \frac{\tau_{n}}{2},\frac{\tau_{\mu}^{2}\tau_{n}}{2} \right)$$

where $\tau_\mu$ and $\tau_n$ are specified in the Dose
Response tab in FACTS under Model Parameters. [See here](../../../../concepts/facts/InverseGammaDistribution.qmd "Widget for assessing how the center and weight parameters in FACTS impact the inverse gamma distribution.") for help with specifying an inverse gamma distribution with center and weight.

In the continuous case the residual error around the estimated dose
response is

$$\sigma^{2}\sim IG\left( \frac{\sigma_{n}}{2},\frac{\sigma_{\mu}^{2}\sigma_{n}}{2} \right)$$

where $\sigma_\mu$ and $\sigma_n$ are specified on the Dose
Response tab in FACTS under Error Parameters.

The Simple NDLM is generally a good model to use when the shape of the
likely dose response is unknown, but we would like to borrow information
between a dose and its neighboring doses. No monotonicity or
pre-determined shape is enforced by the Simple NDLM. The variance in
dose response from dose to dose is updated based on the observed data,
but as the number of doses in a trial is typically small, a moderately
informative prior is often used.

In a ‘null’ scenario where the response on all the doses is the same as
control, the simple NDLM reduces type-1 error. The estimate of
$\tau^2$ tends towards zero and the estimate of the dose response
tends to a flat line. The smoothing of the estimates of the response
reduces the random fluctuations in the estimates from dose to dose and
is a very effective counter to the problem of type-1 error inflation due
to the multiplicity of doses being tested. A small prior for the mean
value of $\tau$ will accentuate this effect. The price to be paid for this
benefit is a tendency to underestimate large changes in response or
where there is a peak in the response. This over-smoothing can reduce
power. A prior for the mean of $\tau$ centered at large values will
minimize over-smoothing, and a very large prior would simply remove any
smoothing effect at all. A standard prior for the mean of $\tau$ would be
the expected average change in response from dose to dose, scaled to
take into account the differences in the specified effective dose
strengths.

Usually, the choice of prior for $\tau^2$ is tuned based on
simulation results, trading off reducing type-1 error in the null case
against loss of power and reduced estimate of maximum response in the
effective scenario with the largest jump in response or steepest peak.

**Aside**: When using the NDLM model or any of its alternatives
(2$^{nd}$ order or Monotonic NDLM), and including doses in a
design that receive no subject allocation (ie. Allocation ratio set to
0), some important characteristics of the model should be considered in
the interpretation of results. First, the NDLM model “borrows”
information from neighboring doses, and therefore including unallocated
intermediate doses will increase uncertainty in the fitted response.
Information must “pass through” these intermediate doses, and
uncertainty should be expected to increase as the number of intermediate
unallocated doses between two allocated doses increases. Secondly, the
NDLM model estimates the response at unallocated doses, but without
subject data at these doses the uncertainty in the estimate can be
considerably larger than doses with subject data. This high degree of
uncertainty can lead to a vestigial probability that an unallocated dose
is the target dose ([Pr(EDq)]{.fake-code-block}, [Pr(MED)]{.fake-code-block}, [Pr(Max)]{.fake-code-block}) even though
the mean response estimate and probabilities at neighboring doses with
subject data would not suggest this to be the case.

### Monotonic NDLM

The Monotonic NDLM endpoint model is similar to the NDLM in the sense
that it smooths dose response estimates using neighboring doses, but is
constrained to be either monotonically increasing or monotonically
decreasing across doses. The direction of monotonicity depends on the
value specified in the “Monotonic:” selection in the Model Parameters
section of the Monotonic NDLM Dose Response page.

The use of this model is generally limited to endpoints where a strong
presumption of increasing/decreasing response with dose is reasonable –
such as a biomarker or a toxicity endpoint.

Let doses $d = d', \ldots, D$ be doses in the dose response model and
$\theta_d$ be the estimated dose response for dose $d$. The
initial dose $d'=1$ if there is no control or control is
included in the dose response model, and $d'=2$ if the
control arm is modeled separately. The following model is the
monotonically positive NDLM:

$$\theta_{d'} \sim N\left(\mu_{d'},\tau^2_{d'}\right)$$

and

$$\theta_d \sim N^+\left(\theta_{d-1},\tau^2_{d-1}\right) \mbox{ for } d = d', \ldots, D,$$
where $\tau_{d-1}^2$ is defined as in the NDLM, and
$X \sim \text{N}^+(\mu, \sigma^2)$ refers to a positive
truncated normal distribution with density function:

$$f_{X}(x) = \frac{1 - \Phi\left( - \frac{\mu}{\sigma} \right)}{\sqrt{2\pi}\sigma}\exp\left\{ - \frac{1}{2\sigma^{2}}(x - \mu)^{2} \right\} \text{ for } x>0$$

The result of this dose-response model is that the curve is
monotonically increasing, in that $\theta_d>\theta_{d-1}$.

The monotonically decreasing NDLM is similar except:
$$\theta_d \sim N^-\left(\theta_{d-1},\tau^2_{d-1}\right) \mbox{ for } d = d', \ldots, D,$$

where $X \sim \text{N}^-(\mu, \sigma^2)$ refers to a negative
truncated normal distribution:

$$f_{X}(x) = \frac{\Phi\left( - \frac{\mu}{\sigma} \right)}{\sqrt{2\pi}\sigma}\exp\left\{ - \frac{1}{2\sigma^{2}}(x - \mu)^{2} \right\} \text{ for } x<0$$

The result of this dose-response model is that the curve is
monotonically decreasing, in that
$\theta_d<\theta_{d-1}$.

### Second Order NDLM

::: {.callout-tip}
## Note

The second order NDLM described in this section is the version utilized in FACTS version 4.0 and later. The model labelled “Second Order NDLM” in versions before 4.0 is was maintained as the model labelled “Legacy 2nd Order NDLM” until the release of FACTS 7.1, at which time it was removed.
:::

The second order NDLM serves as a “smoother” similar to the
NDLM, however it tends to smooth toward a linear fit with a slope (the
NDLM itself tends to shrink each dose’s fit toward its neighbors, while
the second order NDLM prefers any trend in the neighbors).

Let doses $d=d', \ldots, D$ be doses in the dose response model and $\theta_d$ be the estimated dose response for dose $d$. The
initial dose $d'=1$ if there is no control arm or control
is included in the dose response model, and $d'=2$ if the
control arm is modelled separately. The initial dose $d'$ is modeled:

$$\theta_{d'} \sim N\left(\mu_{0},\tau^2_{0}\right)$$

where $\mu_0$ and $\tau_0^2$ are specified
directly in FACTS.

In the case of a time-to-event endpoint, the initial dose $d'$ is the control arm, and has a $\theta_{d'}= 0$ by definition, so no prior distribution is needed.

The prior distribution for the dose response for the dose directly after
the initial dose is specified on the difference between the $d'$ and
$d'+1$ level doses:

\theta_{d'+1} - \theta_{d'} \sim N\left(\mu_{1},\tau^2_{1}\right)

Successive doses are then modeled based on differences in slope between
the dose and the two doses below them. Let:

$$\theta_{d} = \theta_{d - 1} + \Delta_{d}\zeta_{d} + \frac{\Delta_{d}}{\Delta_{d - 1}}\left( \theta_{d - 1} - \theta_{d - 2} \right)$$

for doses $d=d'+2,\ldots,D$, where
$\Delta_d=\nu_d-\nu_{d-1}$ and $\Delta_{d-1}=\nu_{d-1}-\nu_{d-2}$.
The priors for the dose response smoothing terms $\zeta_d$ are:

$$\zeta_d \sim \text{N}(0, \tau_2^2)$$
The smoothing is determined by the parameter $\tau_2$. Small
values of $\tau_2$ lead to more smoothing, while large values of
$\tau_2$ lead to little information sharing between doses (above
dose 1). Note that “small” and “large” is dependent on the scale of the
doses. The prior on the smoothing parameter is:

$$\tau_{2}^{2}\sim \text{IG}\left(\frac{\tau_{n}}{2},\frac{\tau_{\mu}^{2}\tau_{n}}{2} \right)$$

where $\tau_\mu$ is a central value for $\tau_2$, and
$\tau_n$ is the prior weight. [See here](../../../../concepts/facts/InverseGammaDistribution.qmd "Widget for assessing how the center and weight parameters in FACTS impact the inverse gamma distribution.") for help with specifying an inverse gamma distribution with center and weight.

Note that that in this formulation, $\tau_2^2$ can be
thought of as the deviation from linearity. The linear scaling of
deviations thus leads to a conditional variance that scales as the
square of the dose difference:

$$\text{Var}[\theta_d \mid \theta_{d-1}, \theta_{d-2}]=\tau_2^2\cdot (\nu_d-\nu_{d-1})^2$$

The second order NDLM, like the simple NDLM, is generally a good model
to use when the shape of the likely dose response is unknown, but we
would like to borrow information between a dose and its neighboring
doses. No monotonicity or pre-determined shape is enforced by the Second Order NDLM. The variance in dose response from dose to dose is updated
based on the observed data, but as the number of doses in a trial is
typically small, a moderately informative prior is often used.

In a ‘null’ scenario where the response on all the doses is the same as
control the Second Order NDLM, like the Simple NDLM, tends to reduce type-1
error. As the estimate of $\tau^2$ tends to zero the estimate
of the dose response tends to a line (with non-zero slope if
appropriate).

The second order NDLM has more power than the Simple NDLM when the dose
response is smooth, but will tend to shrink estimates to the control by more than the
Simple NDLM if there are marked peaks or steps in the response. Because
it borrows from two neighboring doses, it is better at estimating a response for a
dose where there is no data than the simple NDLM can, and can be used
with burn-in’s where not all doses have been allocated to, but doesn’t
cope well if there are consecutive doses where there is no data.

However, unlike the simple NDLMs where the variance structure is
preserved if doses are added to or dropped from the design, this is not
true for the 2<sup>nd</sup> order NLDM. Thus, if using the
2<sup>nd</sup> order NDLM and the doses that are available to the model
are changed, then the parameters for the prior for
$\tau_2^2$ may need to be re-visited.

The simple NDLM is likely to be preferred where there are fewer doses
(4-5), and the second order NDLM is often preferred when there are 7 or
more (it will depend on the expected smoothness of the response – the
smoother it is the more useful the second order NDLM will be).

As with the simple NDLM, the choice of prior for $\tau_2^2$ can be
tuned based on simulation results: balancing having a smaller prior that
limits type-1 error in the null case, against having a larger prior that
yields more power and better estimate of maximum response in the
effective scenarios with the large jumps in response or pronounced
peaks.

### 3-Parameter Logistic

The 3-parameter logistic dose response model estimates efficacy as a
smooth curve using an intuitive parameterization. The dose response for
a dose $d$ with effective dose strength $\nu_d$ is:

$$\theta_{d} = a_{1} + \frac{a_{2}v_{d}}{v_{d} + a_{3}}$$

Where the $a$ parameters have the following description:

$a_1$
: the estimated dose response for a dose of strength 0

$a_2$
: the estimated change in dose response for a dose with strength ∞ when compared to a dose of strength 0.

$a_3$
: the estimated ED50, the dose that has 50% of the dose response maximum ($a_2$)

The shape of the 3-parameter logistic curve can vary wildly based on the
parameter values, but always starts at $a_1$ at dose strength
0 and monotonically increases to $a_1+a_2$ as
the effective dose strength goes to infinity.

The following independent prior distributions are assumed:

$$a_1\sim \text{N}(\Lambda_1, \lambda_1^2)$$
$$a_2\sim \text{N}(\Lambda_2, \lambda_2^2)$$
$$a_3\sim \text{N}^+(\Lambda_3, \lambda_3^2)$$

In the continuous case, the prior distribution for the error term in the
3-Parameter Logistic is Inverse Gamma:

$$\sigma^{2}\sim IG\left( \frac{\sigma_{n}}{2},\frac{\sigma_{\mu}^{2}\sigma_{n}}{2} \right)$$

An advantage of the 3-Parameter Logistic model is that it is has a small
number of parameters that have defined, intuitive interpretations. Since
there are few parameters it’s generally easy to estimate the parameters
even on small amounts of data. Because of the limited flexibility of the
logistic curve, the 3-parameter logistic model should only be used when
there is evidence that it is appropriate. See the [Hierarchical Logistic](#hierlog)
and [Sigmoid](#sigmoid) (E<sub>max</sub>) models for dose response models with a
similar pattern, but slightly more flexibility in shape.

<a name="hierlog"/>

### Hierarchical Logistic

The [hierarchical logistic model](## "Scott Berry's favorite dose response model.") is an extension of the 3-parameter
logistic with the form:

$$\theta_{d} = a_{1} + \frac{a_{2}v_{d}}{v_{d} + a_{3}} + \zeta_{d}$$

where $\zeta_d$ is a random intercept term that modifies $a_1$
 differently for each dose under the constraint that all
$\zeta_d$ must sum to 0.

The additional term $\zeta_d$ is a per dose ‘off-curve’ effect.
It allows for a secondary non-parametric dose effect over and above the
logistic model. It has been used in circumstances in which there is
actually some falling away of effect at the highest dose. This is an
example mean fit from 100 simulations. The model fit and error bars are
in green, and the true dose responses are shown by the black line.

<img src="coreUGattachments/CoreUserGuide/media/image22.png"
style="width:2.97677in;height:2.6604in" />

$\zeta_d$ is modelled as:

$$\zeta_d \sim \text{N}(0, a_4^2)$$

conditioned that

$$\sum_{d}^{}\zeta_{d} = 0$$

And $a_4^2$ has an inverse gamma prior:

$$a_{4}^{2}\sim IG\left( \frac{\Lambda_{n}}{2},\frac{\Lambda_{\mu}^{2}\Lambda_{n}}{2} \right)$$

The following independent prior distributions are assumed:

$$a_1\sim \text{N}(\Lambda_1, \lambda_1^2)$$
$$a_2\sim \text{N}(\Lambda_2, \lambda_2^2)$$
$$a_3\sim \text{N}^+(\Lambda_3, \lambda_3^2)$$

A typical recommended value for the center of the prior distribution of
$\alpha_4$ is the largest expected likely deviation in true
response from the underlying logistic with a weight of 1. The effect of
this prior is then checked through simulation of a range of plausible
dose response profiles, with a range of designs to check the sensitivity
of the final estimate of response to this prior. Additionally, [see here](../../../../concepts/facts/InverseGammaDistribution.qmd "Widget for assessing how the center and weight parameters in FACTS impact the inverse gamma distribution.") for help with specifying an inverse gamma distribution with center and weight.

In this model, compared to the ‘plain’ 3-parameter logistic above, it is
important that the true ED50 lies within the expected dose range, and
that the prior for the ED50, $a_3$, has the majority of its
probability mass in the available dose range. For example, if the range
of the effective dose strengths is from 0 to $\nu_D$ then a
typical ‘weakly informative’ prior for $a_3$ would be:

$$a_{3}\sim N^{+}\left( \frac{\nu_{D}}{2},\left( \frac{\nu_{D}}{2} \right)^{2} \right)$$

Using a weaker prior, such as $\text{N}^+(\nu_D, \nu_D^2)$ leads to a more linear fit. With just this change to the prior for $a_3$ the average of the estimated of the mean response changes from the graph above to:

<img src="coreUGattachments/CoreUserGuide/media/image23.png"
style="width:2.9739in;height:3.15818in" />

<a name="sigmoid"/>

### Sigmoid Model

A sigmoid model (*E*<sub>max</sub> model) provides a coherent
parameterization of the dose response trend similar to the 3-parameter
logistic, but with one extra shape parameter, $a_4$.

The model formula is:

$$\theta_{d} = a_{1} + \frac{(a_{2} - a_{1})v_{d}^{a_{4}}}{{a_{3}}^{a_{4}} + v_{d}^{a_{4}}}$$

The interpretation of the four parameters is:

$a_1$
: the estimated dose response for a dose of strength 0

$a_2$
: the estimated dose response for a dose of strength $\infty$ (slight difference from Logistic models)

$a_3$
: the estimated ED50, the dose that has 50% of the dose response maximum attainable effect ($a_2-a_1$)

$a_4$
: controls the slope of the dose response model at the ED50. A larger value of $a_4$ corresponds to a steeper slope. A value of $a_4=1$ makes the Sigmoid model equivalent to a Three Parameter Logistic model with $a_2$ equal to $a_1 + a_2$ from the Sigmoid model. A value of $a_4$ approaching 0 corresponds to a dose response model that is nearly flat at $\frac{a_{1} + a_{2}}{2}$. By differentiation, it can be seen that the slope where the effective dose $\nu_d=a_3$ is $(a_{2} - a_{1})\frac{a_{4}}{4a_{3}}$.

The following independent prior distributions are assumed:

$$a_1\sim \text{N}(\Lambda_1, \lambda_1^2)$$
$$a_2\sim \text{N}(\Lambda_2, \lambda_2^2)$$
$$a_3\sim \text{N}^+(\Lambda_3, \lambda_3^2)$$
$$a_3\sim \text{N}^+(\Lambda_4, \lambda_4^2)$$

The advantage of this model is that it is considerably more flexible than
the logistic, but still more efficient to estimate than an NDLM. This
model (if applicable) is ideal if the target is an Effective Dose (ED)
such as an ED90. Estimating an ED requires estimating the response on
control, the maximum response, and the gradient which is requires a lot
of data to do well if using no model or a smoothing model such as one of
the NDLMs. With the 4-parameter Sigmoid model (or 3-parameter logistic –
but that has a more limited applicability due to being significantly
less flexible in shape), it’s usually not an issue to estimate the EDx
well.

The caveats to using this model are:

-   Whilst being much more flexible than the logistic, it still has
    model assumptions, in particular monotonicity, that must be met for
    this to be a reasonable model to be used.

-   The curve is only well estimated if the true ED50 lies within the
    doses tested.

-   Like the hierarchical logistic model above, the prior for
    $a_3$ should be weakly informative, with the majority of
    its probability mass in the available dose range. So similarly, if
    the range of the effective dose strengths is from 0 to
    $\nu_D$ then a typical ‘weakly informative’ prior for
    $a_3$ would be: $$a_3 \sim N^{+}\left( \frac{\nu_{D}}{2},\left( \frac{\nu_{D}}{2} \right)^{2} \right)$$

<img src="coreUGattachments/CoreUserGuide/media/image24.png"
style="width:3.384in;height:2.12987in" />

### U-Shaped Model

The U-shaped model when the allows for an initial increase (decrease) in
the dose response with dose, followed by a leveling out of the mean,
then a decrease (increase). Whether the U-Shaped model increases then
decreases or decreases then increases is specified by the user on the
Dose Response tab. An example curve for this model is given in the
figure below.

The dose-response curve can be characterized in four different regions
of doses. For the first region, which contains the lowest doses, $0<\nu_d<p_{min}$, the dose-response curve
is increasing (decreasing):

$$\theta_{d} = \theta_{0} + S \cdot \delta \cdot \left( \frac{\nu_{d}}{p_{\min}} \right)^{\alpha}$$

The next region is the plateau, where the dose-response curve is
constant. For $p_{min} < \nu_d < p_{min}+p_{width}$:
$$\theta_d=\theta_0 + S\cdot\delta$$
For the third region, the dose-response curve is decreasing
(increasing).
For $p_{min}+p_{width} < \nu_d < p_{min}+p_{width} + w_{width}$,

$$\theta_{d} = \theta_{0} + S \cdot \delta \cdot \left( 1 - \frac{\nu_{d} - \left( p_{\min} + p_{width} \right)}{w_{width}} \right)^{\beta}$$

For the final region, the dose-response curve is again constant, at the
same level as the zero-dose.
For $\nu_d > p_{min}+p_{width} + w_{width}$,
$$\theta_d = \theta_0$$

The parameters of the model are described below:

1.  $S$ is $1$ or $-1$, as determined by the Model is increasing/decreasing
    radio buttons. $S=1$ if Model is Increasing is selected, indicating
    that the model starts increasing at low doses.

2.  $\theta_0$ represents the zero-strength dose response. Its prior is: $$\theta_0 \sim \text{N}(\mu_0, \sigma_0^2)$$

3.  $\delta$ represents the maximal change in response from the zero-strength
    dose. It is restricted to be positive, and has a prior of: $$\delta \sim \text{N}^+(\mu_\delta, \sigma_\delta^2)$$

4.  $p_{min}$ represents the lowest dose at which the maximal
    change in response from the zero-strength is achieved. It is
    restricted to be positive, and has a prior of: $$p_{min} \sim \text{N}^+(\mu_{min}, \sigma_{min}^2)$$

5.  $p_{width}$ represents the width of the plateau –
    the region where all doses achieve maximal change in response from
    the zero-strength dose. It is restricted to be positive, and has a prior of:
    $$p_{width} ~ \sim \text{N}^+(\mu_{width}, \sigma_{width}^2)$$


6.  $w_{width}$ represents the width of the region for
    doses beyond the plateau, where response is returning to the
    zero-strength dose level. It is restricted to be positive, and has a prior of:
    $$w_{width} ~ \sim \text{N}^+(\mu_{w}, \sigma_{w}^2)$$

7.  $\alpha$ determines the rate of change of the dose response curve for
    doses below the plateau. Values less than $1$ indicate a rapid initial
    change from baseline, with slower change as the plateau is reached.
    Values greater than $1$ indicate a slow initial change from baseline,
    with more rapid change as the plateau is reached. To help avoid
    identifiability issues, $\alpha$ is restricted to be between
    $10^{-1}$ and $10^{1}$. $\alpha$'s prior is:
    $$\alpha \sim \text{LN}^*(\mu_\alpha, \sigma_\alpha^2)$$
    where $\text{LN}^*()$ represents the lognormal distribution with
    truncation constraints at $10^{-1}$ and $10^{1}$.

8.  $\beta$ determines the rate of change of the dose response curve for
    doses beyond the plateau. Values less than $1$ indicate a slow initial
    change from the plateau level, with more rapid change subsequently.
    Values greater than $1$ indicate a rapid initial change from the
    plateau level, with slower change subsequently. To help avoid
    identifiability issues, $\beta$ is restricted to be between
    $10^{-1}$ and $10^{1}$. The prior on $\beta$ is:
    $$\beta \sim \text{LN}^*(\mu_\beta, \sigma_\beta^2)$$

The U-shaped model has seven free parameters, and thus it is inadvisable
to use such a model for dose-finding trial designs that contain only a
few dose levels. When a small number of doses are available, it may help
to constrain the values of $\alpha$ and $\beta$ by utilizing small standard
deviations in the priors.

<img src="coreUGattachments/CoreUserGuide/media/image25.emf"
style="width:4.63571in;height:4.63571in" />

### Plateau Model

The plateau model is a special case of the U-shaped model, in which
$p_{width}=\infty$. That is, there is no return to
baseline for high doses. This model eliminates three parameters from the U-Shaped model,
since $p_{width}$, $w_{width}$, and $\beta$ are not used. See the figure below to see an example dose response
model for the plateau model that mimics the beginning of the U-Shaped
model above.

<img src="coreUGattachments/CoreUserGuide/media/image26.emf"
style="width:4.70513in;height:4.70714in" />

### 3 Parameter Exponential Logistic (Dichotomous Only)

The 3-parameter exponential logistic model has the following structure:

$$\theta_d = a_1 + a_2 \nu_d^{a_3}$$

Where $\nu_d$ is the effective dose strength of dose $d$. This
is a logistic model for the dichotomous endpoint because
$\theta_d$ is the log odds ratio of the probability of the
response, $P_d$ at dose $d$.

The exponent parameter $\alpha_3$ allows the change in slope at the
lower end of the curve to be asymmetric in comparison to the change in
curve at the upper end of the curve.

The priors for the parameters are:

$$a_1\sim \text{N}(\Lambda_1, \lambda_1^2)$$
$$a_2\sim \text{N}(\Lambda_2, \lambda_2^2)$$
$$a_3\sim \text{N}^+(\Lambda_3, \lambda_3^2)$$
The interpretations of the parameters defining this model are:

$a_1$
: the dose response for a dose with strength 0

$a_2$
: the slope associated with the exponentiated dose strength

$a_3$
: a shape parameter modifying the effective dose strength through exponentiation.

The figure below shows an example of two different 3-parameter
exponential logistic model fits. Notably, the fit shown in green has
an $a_3$ parameter greater than $1$, which leads to faster
increases of the response rate model as the effective dose strength increases.

<img src="coreUGattachments/CoreUserGuide/media/image27.png"
style="width:3.424in;height:2.28267in" />

### Hierarchical Model

Like the Independent Dose Model, the hierarchical model treats the arms
as if they are an unordered set of treatments, but unlike the
Independent Dose Model the Hierarchical Model shares information between
doses included in the model. The hierarchical model is a way of
encouraging the estimates of arm responses to be similar, and the degree
of this encouragement can be controlled using parameters of the prior
distribution. The hierarchical model assumes that the arm parameters are
drawn from the same normal distribution:

$$\theta_d \sim \text{N}(\mu, \tau^2)$$

Where $d$ is the set of doses included in the model. The prior
distributions for $\mu$ and $\tau^2$ are

$\mu \sim \text{N}(\Lambda_\mu, \lambda_\mu^2)$

and

$$\tau^{2}\sim \text{IG}\left(\frac{\tau_{n}}{2},\frac{\tau_{\mu}^{2}\tau_{n}}{2} \right)$$

where $\tau_\mu$ is a central value for $\tau$, and
$\tau_n$ is the prior weight. $\tau^2$ governs the
amount of information shared between doses in the model. The estimates
of the dose parameters can be encouraged to be close together by
choosing a small value for $\tau_\mu$ and a large value for
$\tau_n$. [See here](../../../../concepts/facts/InverseGammaDistribution.qmd "Widget for assessing how the center and weight parameters in FACTS impact the inverse gamma distribution.") for a tool to help understand the inverse gamma distribution specified by center and weight parameters.

The control arm can be included in the hierarchical model if it is
desired to encourage the experimental arms to be similar to the control
arm, or excluded from it if the experimental arms are encouraged to be
similar to each other but not necessarily to the control arm. This is not true with time-to-event data, when the control arm can only be
excluded from the hierarchical model.

### Linear Model

The linear model uses a strong assumption of exact linearity between the
responses and the dose strengths. (For dichotomous models, the linearity
is on the log-odds scale, and the linearity is on the log hazard ratio
scale for TTE models.) The model is

$$\theta_d=\alpha+\beta\nu_d$$
for all doses $d$ in the model. Both $\alpha$ and $\beta$ are given normal prior
distributions:

$$\alpha \sim \text{N}(\Lambda_\alpha, \lambda_\alpha^2)$$
$$\beta \sim \text{N}(\Lambda_\beta, \lambda_\beta^2)$$

The linear model is appropriate when there is strong scientific
knowledge that the shape of the response curve should be linear as a
function of dose strength. Note that if an arm has a dose strength
between two other arms in the model, its response will always be
estimated to be between the responses of those two arms. As a result,
the linear model may not be appropriate in combination with certain
decision quantities such as [Pr(Max)]{.fake-code-block}, because some doses are guaranteed
to have probability zero of being the best arm. If the control arm is
included in the linear model and it has the smallest dose value, then
all experimental arms will always have exactly the same probability of
being better than control.

We recommend creating scenarios where the true parameters deviate from
linearity and simulating them to test the robustness of a design based
on the linear model.

::: {.callout-tip}
## Note

For dichotomous models, the linear model replaces the 2-parameter
logistic model available in FACTS versions before 6.4. The two models
are identical.
:::

### Hierarchical Linear Model

A more flexible version of the linear model is the hierarchical linear
model. This model encourages the arm responses to have a linear-like
shape, but also includes a term allowing parameters to deviate from
linearity when appropriate. The arm parameters satisfy the relationship

$$\theta_d = \alpha + \beta \nu_d + \zeta_d$$
where the $\alpha$ and $\beta$ parameters are as in the linear model, with prior
distributions

$$\alpha \sim \text{N}(\Lambda_\alpha, \lambda_\alpha^2)$$
$$\beta \sim \text{N}(\Lambda_\beta, \lambda_\beta^2)$$
and the $\zeta_d$ parameters are the deviations from linearity.
These deviations are assumed to have a normal distribution constrained
so that their average value is zero:

$$\zeta_d \sim \text{N}(0, \tau^2) \text{ with } \sum_d\zeta_d=0.$$

The prior distribution for $\tau^2$ is

$$\tau^{2}\sim\text{IG}\left(\frac{\tau_{n}}{2},\frac{\tau_{\mu}^{2}\tau_{n}}{2} \right)$$

If $\tau^2$ is small, which can be encouraged by choosing
$\tau_\mu$ to be small and $\tau_n$ to be large, then
the dose parameter estimates will lie close to a line. [See here](../../../../concepts/facts/InverseGammaDistribution.qmd "Widget for assessing how the center and weight parameters in FACTS impact the inverse gamma distribution.") for help understanding FACTS's parameterization of the inverse gamma distribution.

The hierarchical linear model is a good choice when, for example, one
expects a linear relationship between the dose strengths and the
response but one is not prepared to assume exact linearity. It can also
be a good way of encouraging, but not requiring, monotonicity in the
response.

## 2D Treatment Dose Response Models

If on the Study > Treatment Arms tab, the “Use 2D treatment arm
model” option has been checked, the user may either use any of the 1D
Dose Response options described above, or may use a dose response model
specifically modelling the two dosing dimensions.

If using one of the 1D dose response models, the effective dose strength
$\nu_d$ is as specified by the user on the “Select doses to be
used in the trial” tab. These calculated dose levels are forced to be
distinct values, and this results in a 1D ordering of the combinations.

There are also three 2D Dose Response models that can be used:

1.  2D Continuous Factorial Model

2.  2D Discrete Factorial model

3.  2D NDLM

These are described in the next sections.

The 2D dose response models work with a slightly different notation to
accommodate that treatments are defined as the combination of two
factors. Rather than $\theta_i$ being the estimated mean of the
dose response estimate for dose $i$, the estimated dose response for the
treatment created from row factor level $r$ and column factor level $c$
is denoted $\theta_{rc}$. $Y_{rc}$ denotes the mean
of the observed data in the cell.

In the continuous case, the likelihood for the data is,

$$Y_{rc} \sim \text{N}(\theta_{rc}, \sigma^2)$$
$$\sigma^{2}\sim\text{IG}\left(\frac{\sigma_{n}}{2},\frac{\sigma_{\mu}^{2}\sigma_{n}}{2} \right)$$

Where the form of $\theta_{rc}$ varies based on dose response
model selection.

Similarly, in the dichotomous case,

$$Y_{rc} \sim \text{Bernoulli}(P_{rc})$$
$$P_{rc} = \frac{e^{\theta_{rc}}}{1 + e^{\theta_{rc}}}$$

### 2D Continuous Factorial Model

The 2D Continuous Factorial Model fits a regression on the dose response
that uses the numeric dose strength as a continuous valued covariate.
The dose response (with $\eta_r$ and $\zeta_c$ denoting
dose strength of the row level and column level, respectively) is
modeled as:

$$\theta_{rc} = \alpha_0 + \alpha_1 \zeta_c + \alpha_2 \eta_r + \alpha_3\zeta_c \eta_r$$
With priors

$$\alpha_0 \sim \text{N}(\mu_0, \sigma_0^2)$$
$$\alpha_1 \sim \text{N}(\mu_1, \sigma_1^2)$$
$$\alpha_2 \sim \text{N}(\mu_2, \sigma_2^2)$$
$$\alpha_3 \sim \text{N}(\mu_3, \sigma_3^2)$$

Then, $\alpha_0$ is the response at the control combination,
$\alpha_1$ is the linear coefficient of the response to the column
factor strengths $\zeta_c$, and $\alpha_2$ is the linear
coefficient of the response to the row factor strengths
$\eta_r$.

The user has the option to simplify the model and exclude the
interaction term $\alpha_3$, which is the coefficient of the
product of the two factor strengths.

Note that despite being called “continuous” this dose response model
applies to all endpoints. It’s called continuous because the
coefficients act on the dose strengths as continuous measures of
strength.

<img src="coreUGattachments/CoreUserGuide/media/image29.png"
style="width:5.50007in;height:4.17635in"
alt="A screenshot of a social media post Description automatically generated" />

### 2D Discrete Factorial Model

The 2D Discrete Factorial Model fits a regression on the dose response
that uses the dose strength as a factor covariate rather than a numeric
dose strength. Each dose is an independent level within each row or
column factor, so any dose is equidistant from all other doses

$$\theta_{rc} = \alpha + \gamma_r + \beta_c$$

With priors

$$\alpha \sim \text{N}(\mu_\alpha, \sigma_\alpha^2)$$

$$\beta_c \sim \text{N}(\mu_{\beta_c}, \sigma_{\beta_c}^2)$$
$$\gamma_r \sim \text{N}(\mu_{\gamma_r}, \sigma_{\gamma_r}^2)$$


The parameters associated with lowest level of each factor,
$\gamma_0$ and $\beta_0$, are constrained to be $0$.

<img src="coreUGattachments/CoreUserGuide/media/image30.png"
style="width:5.51472in;height:4.35172in"
alt="A screenshot of a social media post Description automatically generated" />

### 2D NDLM

The 2-D NDLM Model takes the idea of smoothing across doses that was
used in the standard NDLM model, but smooths across row factors and
column factors separately.

#### The Base Model, with Control Included

The treatment effect for the combination of level $r$ in the row factor
and level $c$ in the column factor is denoted as $\theta_{rc}$,
and $Y_{rc}$ is the observed data in that cell. The
borrowing parameters are denoted as $\phi$ for the row factor smoothing,
and $\tau$ for the column factor smoothing. The dose strengths are denoted
as $\nu_r$ for the row factors, and $\omega_c$ for the
column factors. Let $\Delta \nu_r = \nu_r - \nu_{r-1}$ and $\Delta \omega_c = \omega_c - \omega_{c-1}$ (for $r>0$ and $c>0$). For notational convenience at the grid edge,
let $\theta_{-1, c} = 0$, $\theta_{r,-1}$, $\Delta\nu_0\equiv\infty$, and $\Delta\omega_0\equiv\infty$.

The 2-D NDLM Model ***with control included in the model*** can then be
specified as:

$$\theta_{0,0} \sim \text{N}(\mu_0, \tau_0^2)$$
$$\theta_{rc} \sim \text{N}(\mu_{rc}, \tau_{rc}^2)$$
where

$$\tau_{rc}^{2} = \left( \frac{1}{\mathrm{\Delta}\nu_{r}\phi^{2}} + \frac{1}{\mathrm{\Delta}\omega_{c}\tau^{2}} \right)^{- 1}$$

$$\mu_{rc} = \tau_{rc}^{2}\left( \frac{\theta_{r - 1,c}}{\mathrm{\Delta}\nu_{r}\phi^{2}} + \frac{\theta_{r,c - 1}}{\mathrm{\Delta}\omega_{c}\tau^{2}} \right)$$

with priors

$$\tau^{2}\sim\text{IG}\left( \frac{\tau_{n}}{2},\frac{\tau_{\mu}^{2}\tau_{n}}{2} \right)$$

$$\phi^{2}\sim\text{IG}\left( \frac{\phi_{n}}{2},\frac{\phi_{\mu}^{2}\phi_{n}}{2} \right)$$

Note: that not all combinations of $r$ and $c$ will have data associated
with them. However, FACTS will still evaluate the prior at all
combinations. Thus, in the situation depicted below where the grayed-out
point indicates a treatment combination with no allocation,
$\theta_{1,2}$ is not modeled conditioned only on $\theta_{1,1}$. $\theta_{0,1}$ also informs on $\theta_{1,2}$ via $\theta_{0,2}$.

<img src="coreUGattachments/CoreUserGuide/media/image31.png"
style="width:2.1in;height:2.1in"
alt="Calendar Description automatically generated" />

#### Fix smoothing ratio for row factor and column factor

Optionally, the user might want to specify the row factor borrowing as a
function of the column factor borrowing:

$$\phi\equiv k \cdot \tau$$
where $k$ is a user-specified constant. This can be performed by
selecting “Fixed smoothing ratio” in the $\phi^2$ prior
specification area.

####  Control not in model, no zero-level doses

If neither treatment arm allows zero-level doses (e.g. like one would
expect if the column factor represented doses and the row factor
represented frequency), then a prior for the lowest dose would instead
be needed as input from the GUI:

$$\theta_{1,1} \sim \text{N}(\mu_1, \tau_1^2)$$

<img src="coreUGattachments/CoreUserGuide/media/image32.png"
style="width:5.47053in;height:4.31685in"
alt="A screenshot of a social media post Description automatically generated" />



## Baseline Adjusted Dose Response Models (Continuous Only)

If a baseline endpoint is simulated, the user has the option of adding a
linear covariate effect to the dose response model. If the chosen
dose response model states that for dose $d$,
$$Y\sim \text{N}(\theta_d, \sigma^2)$$,
then the distribution including the baseline adjustment term is
$$Y\sim \text{N}(\theta_d+\beta Z, \sigma^2)$$.
where $Z$ is the standardized baseline value $\left(Z=\frac{X-\bar X}{s_x}\right)$, and $\beta$ is an estimated parameter that is distinct from any parameter called $\beta$ within the dose response model.

The baseline adjustment model uses a normal prior for $\beta$ for which the user
enters a mean and standard deviation.

:::{.callout-tip}
## Technical Note

The VSR based simulation of baseline is more general than the
model adjustment for baseline (this is to allow baseline to be incorporated in a
number of different ways). This means that the parameters entered in the
VSR (for example the $\beta$ parameter) will not always match the corresponding parameter estimated in the dose response model.

As a simple example, suppose we enter into the VSR response $Y\sim \text{N}(\mu_Y, \sigma_Y^2)$, baseline $X\sim \text{N}(\mu_X, \sigma_X^2)$, and use the baseline adjustment ($c$, $s$, and $\beta_{VSR}$ are user inputs) so the actual simulated response $Y$ is $Y^{'} = Y + \beta_{VSR}\frac{X - c}{s}$.

If the values of $c$ and $s$ used in the baseline adjustment are the same as the mean $\mu_X$ and standard deviation $\sigma_X$ of the simulated baseline, then the $\beta_{VSR}$ will converge to the estimated $\beta$ parameter as the sample size of the study increases. If $c\ne\mu_X$ or $s\ne\sigma_X$, then the estimated $\beta$ will not converge to the $\beta_{VSR}$ used in data simulation. Additionally, if the baseline values are simulated from a truncated normal distribution, then the estimated $\beta$ will not converge to $\beta_{VSR}$.
:::

## Control and Comparator Priors

For [most](## "All continuous and dichotomous models except the U-shaped and Plateau models.") dose response models the control arm can be modeled either as part of the dose response model or separately. If it is modeled separately, it may have a simple user
specified Normal prior or a “historical” prior. A historical prior in
FACTS is a hierarchical prior that models the response on control as
coming from a distribution that also contains trial outcomes from external historical studies.

<img src="coreUGattachments/CoreUserGuide/media/image33.png"
style="width:5.7284in;height:4.23754in" />

The active comparator is always modeled separately, and as with a
control arm modeled separately, it can be modeled with a user
specified Normal prior or a “historical” prior.

When a user specified Normal prior is used, the user specifies the mean
and standard deviation of the prior normal distribution for
$\theta_0$. This is useful if the control response is not thought
to be consistent with the model being used to model the study doses –
for instance if using an NDLM and there might be a sharp step in
response from control to the lowest dose.

If “historical” prior is selected for either the control arm or an
Active Comparator arm, an “Augmented Priors” tab is created.

## Inverse-gamma priors

FACTS uses inverse-gamma priors for parameters of variance – these are
conjugate and allow efficient computation and avoid problems of
convergence.
[Andrew Gelman’s 2006 paper](#gelman "Prior distributions for variance parameters in hierarchical models, Andrew Gelman, Bayesian Analysis 2006, 1, Number 3 pp 515-533.") however notes a potential problem with this model, the problem is specifically

-   When updating the estimate of the variance of a hierarchical
    model parameter there will be typically relative few actual data
    observations (e.g. relatively few historic studies for estimating
    the variance of the hyper parameter in a Bayesian Augmented
    Control model for the response on a control arm, and relatively few
    observations of the change in response from one dose to the next
    when using an NDLM dose-response model).

-   The conventional ‘non-informative’ gamma-prior of IG(0.001, 0.001)
    has an effect when the observed variance is small, of over-shrinking
    the posterior estimate of variance.

In FACTS this possibility arises in the dose response models in the
context of priors for the $\tau$ parameter for the NDLM models and the $a_4$ parameter for the hierarchical logistic dose response
model, where the number of observations is the number of doses or dose
intervals.

To avoid the problem reported by Gelman we recommend using a weakly
informative prior. Using the settings that control how the inverse-gamma
distribution is parameterized (Settings > Options > Gamma
Distribution Parameters), use the ‘center and weight’ options and use a
weight of 1, with a ‘reasonable’ expectation for the upper limit
difference in the values being modeled entered as the center for the
SD. The inverse gamma distribution is the prior of the variance, so the center parameter being specified is more correctly an expectation of the square
root of the mean of the variance.

:::{.callout-tip}
## Help with Inverse Gamma specification

We have created a tool that can help visualize the inverse gamma distribution and relate the center/weight parameterization, which FACTS uses by default, to the alpha/beta parameterization that is common.

[See here.](../../../../concepts/facts/InverseGammaDistribution.qmd)
:::

This ‘reasonable’ upper limit for the difference is a value that a
clinical team will usually have an intuition about: for the largest
change in mean response from one dose to another for instance, it is
(often)[## "If the trial has doses that are more closely spaced than usual, a
smaller figure can be used."] reasonable to assume that the upper limit for the expected
change in response from one dose to the next is the ‘expected difference
in effect size’ that might have been used to power the trial in a
conventional setting.

In the dose response setting the smoothing parameters like $\tau$ in the NDLM are nuisance parameters in the sense that they are necessary to estimate in order to get appropriate estimates of the dose response values, but are rarely of inferential interest on their own. This can ease some of the undue burden of setting a prior on $\tau$. If it [gives estimates of the dose responses that look like they should](## "This is subjective. The Per Sim: Response and Subject Alloc plot in FACTS can help assess this fit."), then it is generally good enough.

## Handling Missing Data

FACTS allows a “pre-processing” step before fitting the dose response model that helps with handling of data that is missing due to dropouts. If a subject has incomplete data due to a dropout, the
user may specify all dropouts have their unknown final endpoint treated
as known with the following options:

1.  BOCF (continuous only, requires baseline be simulated) – All
    dropouts are assumed to have a final endpoint equal to their
    observed baseline value.

2.  LOCF (continuous or dichotomous, requires longitudinal data be
    present) – All dropouts are assumed to have a final endpoint equal
    to their last observed visit value. If no post-baseline visits are
    available, but the subject has a baseline visit value, then the
    baseline values is carried forward to their final endpoint.

3.  Missing is failure (dichotomous only) – All dropouts are assumed to
    be failures (which may be coded as 0 or 1 depending on whether a
    response is considered a success).

Subjects who are imputed in this pre-processing step have final endpoint
values known and used for the purposes of estimating the dose response
curve. However, these pre-processed final endpoint values are not used in
the updating on the longitudinal model, which is based only on observed
visit data.

If the user does not specify one of the dropout imputation methods
specified above, the dropout subjects and incomplete subjects (subjects
who have not reached their final endpoint but are still continuing in
the study) will have their final endpoints multiply imputed using
Bayesian Multiple Imputation, described in the [Longitudinal Modeling](#longitudinalModeling) section.

Generally, patients with “no data” do not affect the posterior
distribution, and thus are omitted from the analysis. However, one must
take into account a subject can have no visit data but still have “data”
based on these dropout imputation methods. For example, if one selects
“missing as failure” and a patient drops out before any visit data is
recorded, then the subject still supplies information through the
dropout imputation (similarly for BOCF). However, if LOCF is selected
and no visit data is available, there remains no information on the
subject to be used for the LOCF dropout imputation, and thus these
subjects are omitted from the analysis. All subjects are
included if they either 1) have some visit data available, or 2) are
dropouts before visit 1 with sufficient information to impute their
final endpoint with this pre-processing step.

### Time-to-Event Missingness

For a time-to-event endpoint, as is the convention in this endpoint,
subjects are observed up to the time they have an event (or the end of
follow up), so no subject drops out after having had an event. Subjects
that do drop-out are included in the analysis as not having had an event
up to the last observation of the subject.

# Augmented Priors (Historical Prior)

If a Bayesian Augmentation is used for either the Control or Active
Comparator arm then the user specifies:

-   The sufficient statistics to be included from each historic study,
    these are:

    -   Continuous: the mean response and the SD of the response of the
        control or active comparator arm and the number of subjects
        observed.

    -   Dichotomous: the observed number of responders and the number of
        subjects observed in the study.

    -   Time-to-Event: the number of events and the amount of exposure
        within each bin in the piecewise model.

The information from the historical studies can be ‘down-weighted’ by
decreasing the effective information in the sample size. For continuous,
this can be done by decreasing the sample size by a percentage. For
dichotomous, both the number of responders and the number of subjects
would be decreased by a percentage. For time-to-event, multiplying the
number of events and the exposure by the same fraction will reduce the
information in the study without changing the reported hazard rate.

<img src="coreUGattachments/CoreUserGuide/media/image34.png"
style="width:6.5in;height:4.62723in" />

The hierarchical models for the control or active comparator rates are
very similar across the endpoints. They are briefly described below.

:::{.panel-tabset}
## Continuous Endpoints

The model used for incorporating data from previous trials is as
follows:

$$\theta_{0t} \sim \text{N}(\mu_0, \tau_0^2) \text{ for } \tau=0,1,2,\ldots,T$$
where $\theta_{0t}$ is the mean for the control arm in trial $t$
($t=0$ for the current trial and $t=1,2,\ldots,T$ for previous trials).
A user needs to specify appropriate priors for the hyper-parameters:

$$\mu_0 \sim \text{N}(m_0, \sigma_0^2)$$
$$\tau_0^2 \sim \text{IG}(a_0, b_0)$$

## Dichotomous Endpoints

The model used for incorporating data from previous trials is as
follows:

$$\theta_{0t} \sim \text{N}(\mu_0, \tau_0^2) \text{ for } \tau=0,1,2,\ldots,T$$
where $\theta_{0t}$ is the log-odds for the control arm in trial
$t$ ($t=0$ for the current trial; $t=1,2,\ldots,T$ for previous
trials). A user needs to specify appropriate priors for the hyper-parameters:

$$\mu_0 \sim \text{N}(m_0, \sigma_0^2)$$
$$\tau_0^2 \sim \text{IG}(a_0, b_0)$$

The prior distributions for the hyper-parameters of the hierarchical
model. The hyper-parameters are the mean and standard deviation of
Normal distribution for the log hazard ratios of the event rates of the
historical studies and the current study.

## Time-to-Event Endpoints

The model used for incorporating data from previous trials is as
follows:

$$\lambda_{st}=\lambda_s \exp(\gamma_t) \text{ for } t=0,1,2,\ldots,T$$
where $\lambda_{st}$ is the hazard rate for the control arm in
segment $s$ ($s=1,2,\ldots,S$) for previous trial $t$
($t=1,2,\ldots,T$) and $\lambda_{s0}$ is the hazard rate for the
current control arm in segment $s$; $\lambda_s$ is a base hazard
for segment $s$; and $\gamma_t$ is the log hazard ratio between
that base rate and the $\lambda_{st}$ values.

The following hierarchical model is used

$$\gamma_t \sim \text{N}(\mu_\gamma, \tau_\gamma^2) \text{ for } t=0,1,2,\ldots,T$$
Users specify priors for the hyper-parameters:

$$\mu_\gamma \sim \text{N}(m_\gamma, t_\gamma^2)$$
$$\tau^2 \sim \text{IG}(a_\gamma, b_\gamma)$$

The formulation above is not identifiable as changes in
$\lambda_s$ can be compensated for by changes in the
$\gamma_t$ values (thus, one can use different combination of
$\lambda_s$ and $\gamma_t$ but acquire the same set of
values $\lambda_{st}$ and thus the same likelihood. To avoid this
difficulty, we use the above formulation but fix $\gamma_0 = 0$. In
addition to preserving the identifiability of the structure, this
constraint allows $\lambda_s$ to have the interpretation of being
the hazard rate for the current control arm, and thus the prior on
$\lambda_s$ from the main dose response may be used as the prior
for $\lambda_s$.
:::

## Setting Priors for Hierarchical Model Hyper Parameters

Unless the intent is to add information that is not included in the historic studies, the hyper parameters can and should be set so that they are ‘weak’ priors, centered on the expected values.

In this case the following would be reasonable:

- Set the prior mean value for Mu as the mean of the mean responses on the control arm in the historic studies

- Set the prior SD for Mu equal to the average SD of response divided by the square root of the number of studies.

- Set the center for tau to the same value as the prior SD for Mu.

- Set the weight for tau to be < 1.

One can traverse the spectrum from ‘complete pooling of data’ to ‘completely separate analyses’ through the prior for tau. If the weight of the prior for tau is small (relative to the number of studies), then (unless set to a very extreme value) the mean of the prior for tau will have little impact and the degree of borrowing will depend on the observed data.

To give some prior preference towards pooling or separate analysis the **weight** for tau has to be large (relative to the number of historic studies) – to have a design that is like pooling all the historic studies the mean for tau needs to be small (say 10% or less of the value suggested above). For there to be no borrowing from the historic studies the value for tau needs to be large (say 10x or more the value suggested above).

The best way to understand the impact of the priors is try different values and run simulations.

## Bayesian Augmented Control (BAC) Example:

It is easiest to study the impact of BAC in a simplified version of the
trial being designed, for example use a 2 arm fixed design with ‘no
model’, no adaptation, no longitudinal modeling, etc.

For instance, in a continuous setting with an expected sample size of 50
on control, a mean response of 5 and an SD in the response of 2, looking
at the effect of having 4 prior, similarly sized studies:

<table>
<caption><p>External Study Sufficient Statistics</p></caption>
<colgroup>
<col style="width: 24%" />
<col style="width: 22%" />
<col style="width: 30%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Number of subjects</th>
<th>Mean Response</th>
<th>SD of Response</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Study 1</td>
<td>50</td>
<td>4.76</td>
<td>2</td>
</tr>
<tr class="even">
<td>Study 2</td>
<td>50</td>
<td>4.93</td>
<td>2</td>
</tr>
<tr class="odd">
<td>Study 3</td>
<td>50</td>
<td>5.07</td>
<td>2</td>
</tr>
<tr class="even">
<td>Study 4</td>
<td>50</td>
<td>5.24</td>
<td>2</td>
</tr>
</tbody>
</table>

For simplicity we have just varied the means of the studies – they are
all consistent with a normally distributed estimate, with mean of 5 and
a standard error of $\frac{2}{\sqrt{50}}$.

By simulating a fixed trial with a sample size of 100 and fixed 1:1
allocation between control and a study arm we can easily observe the
effects of a BAC on the control arm. Creating 6 VSR profiles with sigma
= 2, and the same mean response for control and the study arm in each
profile of: 4.53, 4.76, 5.07, 5.24 and 5.47 we can look at the bias of
the final estimate and the shrinkage in the SD of the estimate that
could be brought about by using BAC:

<table>
<caption><p>Quick simulation study of how the hierarchical model for BAC effects estimates of the control rate under different true control rate scenarios.</p></caption>
<colgroup>
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 16%" />
<col style="width: 15%" />
<col style="width: 13%" />
<col style="width: 10%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>True Mean</th>
<th>Raw mean</th>
<th>Raw SD</th>
<th>Estimate inc BAC</th>
<th>SD inc BAC</th>
<th>Bias</th>
<th>Effective additional Subjects</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4.53</td>
<td>4.55</td>
<td>0.28</td>
<td>4.64</td>
<td>0.255</td>
<td>2.1%</td>
<td>11.1</td>
</tr>
<tr class="even">
<td>4.76</td>
<td>4.78</td>
<td>0.28</td>
<td>4.83</td>
<td>0.250</td>
<td>1.0%</td>
<td>13.5</td>
</tr>
<tr class="odd">
<td>4.93</td>
<td>4.95</td>
<td>0.28</td>
<td>4.96</td>
<td>0.248</td>
<td>0.2%</td>
<td>14.3</td>
</tr>
<tr class="even">
<td>5.07</td>
<td>5.09</td>
<td>0.28</td>
<td>5.07</td>
<td>0.248</td>
<td>-0.4%</td>
<td>14.2</td>
</tr>
<tr class="odd">
<td>5.24</td>
<td>5.26</td>
<td>0.28</td>
<td>5.20</td>
<td>0.250</td>
<td>-1.1%</td>
<td>13.2</td>
</tr>
<tr class="even">
<td>5.46</td>
<td>5.49</td>
<td>0.28</td>
<td>5.38</td>
<td>0.255</td>
<td>-1.9%</td>
<td>10.7</td>
</tr>
</tbody>
</table>

Note it is important to use the column “SD Mean resp 1” from the
simulations file to see the SD of the estimate and not use the SD of the
estimates in the summary table.

The small shrinkage in the SD is more impressive when viewed in terms of
how many additional subjects would have to be recruited to achieve the
same shrinkage in the raw estimate. These are useful savings for small
additional bias.

The “effective additional subjects” was calculated:
$$\left( \frac{\text{True sigma}}{\text{AVG(SD Mean resp)}} \right)^{2} - \left( \frac{\text{True sigma}}{\text{AVG(SE Mean Raw Response)}} \right)^{2}$$
where in this example $\text{True sigma}$ was 2.

# Frequentist Analysis

On the Frequentist Analysis tab the user can specify that some standard
frequentist analyses be performed at trial analyses. The
frequentist analysis tab is completely separate from, and independent of,
any p-value QOIs that have been defined. The analyses specified on this tab cannot be used for simulated trial decisions - they are for storing in output only.

Each specified analysis can be conducted using a variety of ways of handling missingness. Select all ways of handling missingness that are desired:

-   Missing data replaced by last observation carried forward (LOCF)

-   Missing data replaced by baseline observation carried forward
    (BOCF). This is only available if the endpoint is continuous and
    Baseline is being simulated.

-   Missing data is ignored (a “per-protocol” analysis).

-   Missing data is treated as a failure. This is only available if the
    endpoint is dichotomous.

If the trial has interim analyses, then for the simulations for which
frequentist weeks files are to be output (specified on the Simulation
tab) the standard frequentist analyses will be performed. If the trial
has p-values QOIs, those QOIs are calculated every interim in all
simulations.

Having the frequentist analysis include [Dunnett's](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.") adjusted p-values is a separate option (that applies to all the analysis type requested)
because of the significant run-time overhead this can entail. [Dunnett's](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.")
adjustment is available for continuous and dichotomous frequentist
analyses.

The frequentist analysis tabs for the continuous and dichotomous engines
also have trend tests, and allow the user to specify contrast
coefficients to conduct those tests.

Note that the reported frequentist estimates of the treatment effect
take the specified direction of response on the Study tab (whether a
response indicates subject improving or worsening) into account. They
are adjusted so that a treatment that is estimated to be better than the
control always has a positive treatment effect.

<img src="coreUGattachments/CoreUserGuide/media/image35.png"
style="width:4.51964in;height:3.24521in" />

## Continuous Endpoints

At the end of each simulated trial the following frequentist values will
be calculated for each missingness handling method checked:

1.  Using unadjusted dose-placebo comparisons based on a two-sample
    t-test calculate the:

    1.  test statistic,

    2.  (p-value)[## "1-sided p-value is reported in all the frequentist results. This is done in order to be consistent with comparisons with 1-sided α-values elsewhere."],

    3.  confidence interval for the mean difference,

    4.  for each dose its marginal probabilities of significance (the
        number of times it was significantly separated from placebo
        independent of whether any other doses were) .

2.  If selected, using [Dunnett](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.")-adjusted dose-placebo comparisons based on a two-sample t-test calculate the:

    1.  test statistic,

    2.  p-value,

    3.  confidence interval for the mean difference

    4.  for each dose its marginal probabilities of significance (the
        number of times it was significantly separated from placebo
        independent of whether any other doses were) .

3.  Using the general trend test calculate the t-test statistic and
    p-value using user supplied contrast coefficients.

If neither placebo nor an active comparator are simulated, a difference from
0 is assessed for each dose arm and [Dunnett](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.")-adjusted calculations are not carried out.

If high values of the endpoint are good, P-values are calculated using a one-sided t-test testing
$$H_0: \mu_T < \mu_C$$
against
$$H_1: \mu_T \ge \mu_C$$
with $\mu_T$ being the true treatment response mean and $\mu_C$ being the true control response mean. If low values of the endpoint are good, then the signs of the hypotheses are flipped.

## Dichotomous Endpoints

At the end of each simulated trial the following frequentist values will
be calculated for each missingness handling method checked:

1.  Using the methodology described by [Agresti](#agresti "Agresti, A. 2002.  Categorical Data Analysis. Second Edition. Wiley."), [Mee](#mee "Mee, R. W. 1984. Confidence bounds for the difference between two probabilities. Biometrics. 40, 1175-1176.") and [Nurminem](#nurminem "Nurminen, M. 1986. Confidence intervals for the ratio and difference of two binomial proportions. Biometrics. 42, 675-676.") for comparing the difference of proportions:

    1.  test statistic,

    2.  p-value,

    3.  95% confidence interval for the difference in proportions,

    4.  marginal probabilities of significance.

2.  If checked, using [Dunnett](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.")-adjusted dose-placebo comparisons for comparing the
    difference of proportions:

    1.  test statistic,

    2.  p-value,

    3.  95% confidence interval for the difference in proportions,

    4.  marginal probabilities of significance.

3.  Using the general trend test calculate the t-test statistic and
    p-value using user supplied contrast coefficients.

P-values are calculated by user choice either using a normal
approximation (see previous section on p-value calculations for
continuous endpoints) or a Fisher’s exact test, which is a conservative
test to test the null hypothesis that the outcome is independent of the
treatment assignment. For the normality approximation to have good
asymptotic coverage, the expected number of successes and failures under
the null hypothesis should be greater or equal to 5 for both groups. In
cases where this is not guaranteed, Fisher’s exact test is recommended
when strict type 1 error control is required.

If neither placebo nor active comparator are specified, a difference from 0
is assessed for each dose arm and [Dunnett](#dunnett "Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. Journal of the American Statistical Association. 1955; 50: 1096-1121.")-adjusted calculations are not carried out.

## Time-to-Event Frequentist Analysis

For each simulated trial, the following frequentist analyses will be
performed:

1.  Dose-placebo comparisons based on log-rank test and the Cox
    proportional hazards model. Summaries include:

    1.  The log-rank and Wilcoxon test statistics and the corresponding
        p-values,

    2.  Estimated hazard ratio and its confidence interval from Cox
        model,

    3.  For each dose its marginal probabilities of significance (the
        number of times it was significantly separated from placebo
        independent of whether any other doses were, based on the hazard
        ratio inference from the Cox model).

2.  Median survival times based on the Kaplan-Meier method.

3.  For the predictor, descriptive statistics are computes – a 7 number
    summary for a continuous predictor, percentages for the dichotomous
    predictor, and median time, average hazard, and a cox model with
    predictor as covariate are computed for a time to event predictor.

The following variations for adjusting the alpha level for multiple
comparisons should be available using the user-specified 1-sided alpha:
unadjusted, and Bonferonni adjusted.

<a name="longitudinalModeling"/>

# Longitudinal Modeling

The continuous and dichotomous endpoints provide the ability to use
longitudinal models to utilize data from incomplete subject’s observed
early endpoint values. These subjects may be those that have dropped
out, or subjects at an interim that have not had the opportunity to
complete their follow-up.

To perform any longitudinal modeling, ‘Use longitudinal modeling’ must
be checked on the Study > Study Info tab, and the subject visit
schedule must be defined.

To include dropouts in the longitudinal analysis, on the Design >
Dose Response tab select “Bayesian multiple imputation from post
baseline.”

## Multiple Imputation

Unless a deterministic method is used such as LOCF or BOCF, longitudinal
models inform the dose response estimates by using the longitudinal
model to stochastically impute subjects’ final endpoint data when it’s
not been directly observed.

First, data from subjects with both intermediate and final observations is
used to estimate the parameters of whatever longitudinal imputation model has been selected.

<img src="coreUGattachments/CoreUserGuide/media/image36.png"
style="width:3.33848in;height:1.92in" />

Subjects with missing final data have final data sampled from the
posterior distribution of the longitudinal model given the
subjects most recent intermediate visit (or in some models, all their
available intermediate visit data). Subjects with no final or
intermediate data have final data sampled from the posterior
distribution of the dose response model given the dose arm the subject
was allocated to.

<img src="coreUGattachments/CoreUserGuide/media/image37.png"
style="width:3.336in;height:1.91857in" />

Once every randomized subject has either a real known final endpoint or an imputed final endpoint, the dose response model is re-estimated using that *complete* dataset.

<img src="coreUGattachments/CoreUserGuide/media/image38.png"
style="width:3.352in;height:2.10985in" />

This impute-then-fit-dose-response process is built into the MCMC estimation sampling loop. Each time we draw a new set of parameters from the longitudinal model they are used to impute the final endpoint of incomplete subjects. These subjects then inform the dose response model. Then we get a new sample from the longitudinal model and so on. The longitudinal models, with [one exception](## "The ITP model uses the current state of the dose response models in its imputation."), are not conditioned on the dose response models.

The missing final endpoint values are imputed with the
uncertainty in the longitudinal model, and, as a result, the dose response model is estimated including both the uncertainty in the longitudinal model and the usual uncertainty in its parameters.

Above, it says "fit the dose response model," as a step in the iterative process. The idea is that once you create a dataset through imputation you should update the dose response model MCMC chain until it converges. By default only 1 step is taken on the dose response model MCMC chain. It is safer, especially if doing a real analysis, to allow the dose response model parameter chains to converge slightly before imputing the missing data again. This can be done on the MCMC settings control on the Simulation tab and setting the “Samples per Imputation” parameter. As a
rough guide, if it at some early interims > 5% of the data being
analyzed will be imputed, a value in the range 2 to 10 is recommended to
avoid underestimating the uncertainty. A higher number should be used
the greater the proportion of imputed data.

A similar procedure is used when imputing event times based on a
predictor endpoint in FACTS Core Time-to-Event.

:::{.callout-tip}

## Computational Note

FACTS is not fitting a joint Bayesian model of the longitudinal
and dose response models. This would require a full MCMC fit of one
model for every MCMC step of the other. Thus, if taking 2,500 samples,
we would require a total of 2,500<sup>2</sup> samples. This would make
running simulations with longitudinal model prohibitively
expensive.

:::

## How many longitudinal models?

When specifying a longitudinal model in FACTS, the user must clarify
whether the longitudinal model is shared across all doses, or if
different arms are to estimate different functions connecting early
endpoints to the final endpoint.

The options that may be selected for the number of model instances are:

-   “Single model for all arms” – just one set of longitudinal
    parameters is estimated using all the subject data regardless of
    their treatment.

-   “Model control separately” – two sets of longitudinal parameters are
    estimated, one using data from just those subjects on the control
    arm, and the other using all the subject data from all the other
    arms.

-   “Model comparator separately” (only available if there is an active
    comparator arm) - two sets of longitudinal parameters are estimated,
    one using data from just those subjects on the active comparator
    arm, and the other using all the subject data from all the other
    arms (control and active treatments).

-   “Model control and comparator separately” (only available if there
    is an active comparator arm) - three sets of longitudinal parameters
    are estimated, one using data from just those subjects on the active
    comparator arm, one using data from just those subjects on the
    control arm, and the third using all the subject data from all the
    other arms.

-   “Model all arms separately” – a set of longitudinal parameters is
    independently estimated separately for each arm, each set estimated
    just using the data available from the subjects on that arm.

The fewer models used, the more data is pooled, the more precisely the
model parameters can be estimated, and the more informative the
intermediate data can be. Pooling data assumes that subjects in
different groups have the same longitudinal profile, although they may
still have different magnitude or probability of response (estimated by
Dose Response Model).

If the response profile could be different on different treatment arms –
for example, if subjects on control recover constantly over time, but
those on an effective dose of the study drug recover very quickly early
on, and more gradually later in their follow-up - a longitudinal model
based on pooled data would tend to over-estimate the final endpoint of
subjects on an effective dose of the study drug when only their early
data was available. In a case like that, it would be better to use
separate models. If the rapid early response was likely on all study
drug treatment arms, then possibly use “model control separately.” If,
however, the rapid response will only be seen on some study drug arms
then use “model all arms separately”.

In addition to declaring how many longitudinal models should be
estimated, when there is more than one model instance the user must
declare if the prior distributions should be specified differently
across each model instance or not. The options that may be selected for
prior specification (for all LMs but linear regression) are:

1.  Same priors across all model instances

 -  Each instance of the model has identical prior distributions on
    its parameters. Estimation is still performed independently on
    each model instance.

2.  Specify priors per model instance

 -  Each instance of the model has its own priors that may vary
    across instances.

The linear regression longitudinal model has a different parameter set
estimated for each visit, so its prior specification rules are slightly
different. See the linear regression section below for specifics about
prior specification specific to this model.

## Longitudinal Models for a Continuous Endpoint

### LOCF (Last Observation Carried Forward)

The simplest possible longitudinal model. If {$y_{it}$} is
the set of observed responses from early visits, and $y_{i t_m}$ is the last observed value of
$y_{i t}$, then the LOCF model for the final endpoint $Y_i$ is

$$Y_i\mid \{y_{it}\} = y_{it_m}$$

In the continuous engine $t_m$ can be any earlier observed
visit including the baseline value.

### Linear Regression

:::{.callout-tip}
### Shiny App

The following shiny application for a tool that helps visualize and set priors for the linear regression longitudinal model.

[See here.](../../../../concepts/facts/LinearRegressionLMPriors.qmd)
:::

The linear regression model fits a simple linear model from the data at
each visit with the final visit

$$Y_i \mid y_{it} \sim \alpha_t + \beta_t y_{it} + \text{N}(0,\lambda_t^2)$$

The parameter $\alpha_t$ is the intercept of the model for visit
t, and the parameter $\beta_t$ is a multiplicative modifier
(slope) of the response observed longitudinal at visit $t$ to adjust the
prediction of the final endpoint.

Imputation of the final endpoint value for a subject using the linear
regression longitudinal model is based on only the latest observed
visit’s endpoint value.

The default setting of “Same priors across all model instances and
visits,” implies that each parameter $\alpha$, *β*, and *λ* have the same
prior for all *t*. Estimation of those parameters is still done
independently for each model instance. The one prior across all model
instance are formulated as:

$$\alpha_t \sim \text{N}(\alpha_\mu, \alpha_\sigma^2)$$
$$\beta_t \sim \text{N}(\beta_\mu, \beta_\sigma^2)$$
$$\lambda_{t}^{2} \sim \text{IG}\left( \frac{\lambda_{n}}{2},\frac{\lambda_{\mu}^{2}\lambda_{n}}{2} \right)$$

The above prior formulation may not be desirable if specifying priors
that are not extremely diffuse – especially on the $\beta$ parameters.
Instead, selecting “Specify priors per visit across all model
instances,” will share the prior specification across all instances of
the model, but allows for different priors to be put on the parameters
associated with each visit. The user inputted prior parameters are now
subscripted with $t$ to denote the visit they correspond to. These
priors apply to all model instances:

$$\alpha_t \sim \text{N}(\alpha_{\mu_t}, \alpha_{\sigma_t}^2)$$
$$\beta_t \sim \text{N}(\beta_{\mu_t}, \beta_{\sigma_t}^2)$$
$$\lambda_{t}^{2} \sim \text{IG}\left( \frac{\lambda_{n_t}}{2},\frac{\lambda_{\mu_t}^{2}\lambda_{n_t}}{2} \right)$$

It is also possible to specify priors “Per model instance and visit,” in
which every visit has separate priors, and those differing priors vary
across model instances. This is the most flexible prior specification
method. The user inputted prior parameters are now subscripted by both
*t* for visit and *i* for model instance.

$$\alpha_{ti} \sim \text{N}(\alpha_{\mu_{ti}}, \alpha_{\sigma_{ti}}^2)$$
$$\beta_{ti} \sim \text{N}(\beta_{\mu_{ti}}, \beta_{\sigma_{ti}}^2)$$
$$\lambda_{ti}^{2} \sim \text{IG}\left( \frac{\lambda_{n_{ti}}}{2},\frac{\lambda_{\mu_{ti}}^{2}\lambda_{n_{ti}}}{2} \right)$$

A potential starting place for non-informative prior values would be

$\alpha$
: mean of 0, SD $\ge$ largest expected response

$\beta$
: mean of either 0 or $\frac{\text{final visit time}}{\text{early visit time}}$, SD $\ge$ largest expected ratio of final visit to first visit

$\lambda$
: mean of expected SD of the endpoint (‘sigma’), weight of 1. The variability of the prediction from the longitudinal model (based on an observed intermediate response) should be less than that based solely on the treatment allocation, thus this is a weakly pessimistic prior on the effectiveness of the longitudinal model that should be quickly overwhelmed by the data.

This model is easy to understand and can be used even if there is only
one visit, but doesn’t get more powerful if there are more visits. Its
power will depend on the degree of correlation between the intermediate
visit response and the final response.

### Time Course Hierarchical

The Time Course Hierarchical models the relationship between subjects’
early responses and their final response. It incorporates a per-subject
offset from the mean response and a scaling factor from each visit to
the final endpoint, but with no model of the change from visit-to-visit.

The response at the $t_{th}$ visit for the $i^{th}$ subject, having been randomized to the
$d^{th}$ dose is modeled as:

$$y_{it} \sim e^{\alpha_t}(\theta_d + \delta_i) + \text{N}(0, \lambda_t^2)$$

The imputed final response (visit $T$$) for the $i^{th}$
subject, having been randomized to the $d^{th}$ dose is
modeled as:

$$Y_{iT} \sim \theta_d + \delta_i + \text{N}(0, \lambda_T^2)$$

(i.e. $\alpha_T$ is 0).

The model parameters can be interpreted as follows:

$\theta_d$
: the estimated mean response at the final visit in
dose $d$ from the dose response model.

$\delta_i$
: the estimated patient level random effect around the mean final response ($\theta_d$) for the dose $d$ that patient $i$ is randomized to.


$\alpha_t$
: a scaling parameter that determines the proportion of the final response that is observable at visit $t$. A value of $\alpha_t=0$ indicates that the expected value of early visit $t$ is equal to the estimated final visit mean $\theta_d$. A value of $\alpha_t= −0.69315$ indicates that the expected value of early visit $t$ is 50% of the estimated final visit mean $\theta_d$.

$\lambda_t^2$
: the variance of the endpoint around the estimated mean response at visit $t$.

The prior for $\alpha_t$ is a normal distribution with a user
specified the mean and standard deviation:

$$\alpha_t \sim \text{N}(\alpha_\mu, \alpha_\sigma^2)$$

The prior for the $\delta_i$ terms is a normal distribution with
a mean of 0 and variance *τ*<sup>2</sup>.

$$\delta_i \sim \text{N}(0, \tau^2)$$

$\tau^2$ is estimated as part of the longitudinal model, and has
an inverse gamma prior distribution with prior central value
$\tau_\mu$ and weight (in terms of “equivalent number of
observations”) $\tau_n$:

$$\tau^{2} \sim \text{IG}\left( \frac{\tau_{n}}{2},\\\frac{\tau_{\mu}^{2}\tau_{n}}{2} \right)$$

The prior for the $\lambda_t^2$ terms is an inverse
gamma distribution with prior central value $\lambda_\mu$ and weight
(in terms of “equivalent number of observations”) $\lambda_n$:

$$\lambda_{t}^{2}\sim\text{IG}\left( \frac{\lambda_{n}}{2},\frac{\lambda_{\mu}^{2}\lambda_{n}}{2} \right)$$

A reasonable starting place for prior values would be

$\alpha_t$
: mean of -2, SD of 2, … so the prior ~70% interval
for $\alpha_t$ is between -4 and 0 (+/1 1 SD) and thus the
prior 70% interval for $e^{\alpha_t}$ to be between
0.02 and 1.

$\tau$
: mean set to the expected SD of the endpoint (‘sigma’), with a
weight of 1.

$\lambda_t$
: mean set to the expected SD of the endpoint
(‘sigma’), with weight of 1.

We would expect $\tau^2 + \lambda^2 \approx \sigma^2$,
thus to specify a prior mean of $\sigma$ for each with a weight of 1 is a
weakly pessimistic prior that should be quickly overwhelmed by the
data.

This model is useful if there is thought to be a significant per-subject
component to the response that should be manifest at the intermediate
visits, and sufficient visits for the per-subject component to be
estimated.

### Kernel Density

The Kernel Density Model longitudinal model is a non-parametric
re-sampling approach that is ideal for circumstances where the
relationship between the interim time and the final endpoint is not
known or not canonical.

The procedure is as follows. Assume an interim value for patient $i$ at
time $t$, $Y_{it}$. Patient $i$ does not have an observed
final endpoint at time $T$, so one is to be imputed. Let $(X_{1t},X_{1T}), \ldots, (X_{nt}, X_{nT})$
be the set of values for the previous subjects for whom there exists an
interim value $X_{*t}$ and final value $X_{*T}$.

To impute a value of $Y_{iT}$ given $Y_{it}$, a
pair $(X_{kt},X_{kT})$ is selected with
probability based on the pair’s time $t$ visit response’s proximity to
the observed $Y_{it}$:

$$\Pr\left(\text{Selecting}\left( X_{kt},\\X_{kT} \right) \right) = \frac{\exp\left( - \frac{1}{2h_{X_{t}}^{2}}\left( Y_{it} - X_{kt} \right)^{2} \right)}{\sum_{k = 1}^{n}{\exp\left( - \frac{1}{2h_{X_{t}}^{2}}\left( Y_{it} - X_{kt} \right)^{2} \right)}}$$

Then, a value of $Y_{iT}$ is imputed from the following
distribution, which uses the selected pair’s final endpoint response $X_{kT}$:

$$Y_{iT} \sim \text{N}(X_{kT}, h_{X_T}^2)$$

The bandwidths $h_{X_t}$ and $h_{X_T}$ are selected based on the criterion
given by Scott (1992). That is,

$$h_{X_{j}} = \sigma_{X_{j}} \left( 1 - \rho^{2} \right)^{\frac{5}{12}} \left( 1 + \frac{\rho^{2}}{2} \right)^{- \frac{1}{6}}{\\n}^{- \frac{1}{6}}\text{   for } j = t \text{ and } T$$

where $\sigma_{X_j}$ is the standard deviation of the
observed responses at time $j$, $n$ is the number of pairs
$(X_{*t},X_{*T})$ that were chosen between, and
$\rho$  is the correlation coefficient between $X_t$ and
$X_T$ in the pairs $(X_{1t},X_{1T}), \ldots, (X_{nt}, X_{nT})$.

The Kernel Density model does not take prior distributions as input. So
long as each early visit has more subjects with an observed early
response and final response than the value entered in “Minimum number of participants with an early visit and final visit needed to estimate kernel bandwidths for that early visit:” then this algorithm runs without regard for user
input.

If any visit has fewer subjects with early data and final data than the specified minimum number of participants, then instead of
calculating the values of $h_{X_t}$ or $h_{X_T}$ the input values of "Fixed kernel bandwidth $h_x$:" and
"Fixed kernel bandwidth $h_y$:" are used.

For $h_x$ and $h_y$, possible starting values are the expected SD of the endpoint ('sigma'). The default value for the minimum number of subjects with complete early and final visits is 6, but this value can be set to anything greater than 0 that the user desires.

The Kernel Density model is effective and flexible with no model
assumptions, but its computational overhead is large. Simulations may
take $\sim 10$ times longer to run, and having no prior model there has to be
‘in trial’ data before it can come into play.

### ITP

The ITP (Integrated Two-component Prediction) model fits an observation
for patient $i$ on dose $d$ at visit $t$ as:

$$y_{idt} = \left( \theta_{d} + s_{id} + \epsilon_{idt} \right)\left( \frac{1 - \text{exp}\left( kx_{idt} \right)}{1 - \text{exp}(kX)} \right)$$

where
$$\epsilon_{idt} \sim \text{N}(0, \lamnbda^2)$$
$$\s_{id} \sim \text{N}(0, \tau^2)$$

and $\theta_d$ is the mean estimate of the final endpoint for
dose d using all complete and partial data and assuming an independent
dose response model on the doses. $s_{id}$ is a subject
specific random effect, $k$ is a shape parameter,
$x_{idt}$ is the time $y_{idt}$ is observed,
$X$ is the time to final endpoint, and each $\epsilon_{idt}$ is a
residual error.

The ITP model is similar to the Time Course Hierarchical above in that
both models estimate subject specific component of the response. The
biggest difference between the two is that in the ITP models the response
changes over time as a parametric function based on the parameter $k$,
rather than having a separately estimated $e^{\alpha_t}$ for each visit.

The shape parameter $k$ determines the rate at which the final
endpoint’s eventual effect is observed during a subject’s follow-up. A
value of $k=0$ indicates that the proportion of effect observed moves
linearly with time. A value of $k<0$ means that the eventual final
effect is observed earlier in follow-up and plateaus off as time moves
towards the final endpoint. A value of $k>0$ indicates that less of
the total final endpoint effect is observed early in follow up, but as
time approaches the final endpoint time the proportion of the effect
observed increases rapidly. Values of $k$ less than 0 tend to be more
common than values of $k$ greater than 0. See the figure below for a
visualization of the different response-over-time curves that can be
estimated with the ITP model.

<img src="coreUGattachments/CoreUserGuide/media/image39.png"
style="width:3.26636in;height:2.104in" />

The priors for the parameters in the ITP model are:
$$k \sim \text{N}(\mu_k, \sigma_k^2)$$
$$\theta_d \sim \text{N}(\mu_{\theta_d}, \sigma_{\theta_d}^2)$$
$$\tau^2 \sim \text{IG}\left( \frac{\tau_{n}}{2},\frac{\tau_{\mu}^{2}\tau_{n}}{2} \right)$$
$$\lambda^2 \sim \text{IG}\left( \frac{\lambda_{n}}{2},\frac{\lambda_{\mu}^{2}\lambda_{n}}{2} \right)$$

A reasonable starting place for prior values would be:

$\theta_d$
: mean of the expected mean improvement from baseline
on the Control arm, with SD greater than or equal to the expected
treatment effect size.

$k$
: a mean of 0 and SD of 2, if subject improvement is expected to be
rapid and then diminishing, the prior mean might be -0.5 or -1, if
subject improvement is expected to be initially slow or non-existent
then increasing, the prior mean might be 0.5 or 1.

$\tau$
: mean set to the expected SD of the endpoint (“sigma”), with a
weight of 1.

$\lambda$
: mean set to the expected SD of the endpoint (“sigma”), with a
weight of 1.

The ITP model implies that the variance of the observations shrinks
towards 0 with the mean (so early visits have reduced expected responses
and variances). The ITP model may result in biased estimates of
$\theta_d$ and/or the variance terms $\tau^2$ and
$\lambda^2$ if the mean-variance relationship assumed by the ITP
model is not present in the observed data. If the model assumptions are
correct this can be a very effective longitudinal model.

## Longitudinal Models for a Dichotomous Endpoint

### LOCF (Last Observation Carried Forward)

The simplest possible longitudinal model. If {$y_{it}$} is
the set of observed responses from early visits, and
$y_{i t_m}$  is the last observed value of
$y_{it}$, then the LOCF model for the final endpoint
$Y_i$ is

$$Y_i \mid \{y_{it}} = y_{i t_m}$$

### Beta Binomial

The Beta Binomial longitudinal model imputes a patient’s final endpoint
given their most recent observed response.

The final endpoint response $Y_i$ is modeled as:

$$Y_i \sim \text{Bernoulli}(\pi_{t y_{it}})$$

where $\pi_{t y_{it}}$ is the probability that a
patient is a response at the final endpoint given its early observed
endpoint at time $t$ is $y_{it}$,

$$\pi_{t y_{it}} = \Pr(Y_i = 1 \mid y_{it}) \sim \text{Beta}(\alpha_{t {y_it}}, \beta_{t y_{it}})$$

We use the set cardinality operator $\mid \ldots \mid$ to obtain the posterior
distributions of $\alpha_t$ and $\beta_t$ as:

$$\alpha_{t0} = \alpha_{\mu 0} + \left| Y_i = 1, y_{it} = 0 \right| $$
$$\alpha_{t1} = \alpha_{\mu 1} + \left| Y_i = 0, y_{it} = 0 \right| $$
$$\beta_{t0} = \beta_{\mu 0} + \left| Y_i = 1, y_{it} = 1 \right| $$
$$\beta_{t1} = \beta_{\mu 1} + \left| Y_i = 0, y_{it} = 1 \right| $$

i.e. a prior value $(\alpha_{\mu 0}, \alpha_{\mu 1}, \beta_{\mu 0}, \beta_{\mu 1})$
plus the number of subjects for which the final response is known to be
1 for $\alpha_{tx}$ (or 0 for $\beta_{tx}$) and the
response at time $t$ is $x$.

The $\alpha_{tx}$ and $\beta_{tx}$ parameters are
independently estimated using only patients in their model instance, and
may or not have identical priors $\alpha_{\mu *}$ and
$\beta_{\mu *}$ depending on the Model Priors selection in FACTS. A
common non-informative prior for the $\pi_{t0}$ and
$\pi_{t1}$ parameters is $\text{Beta}(1,1)$.

### Logistic regression

The Logistic regression longitudinal model works similarly to the Beta
Binomial imputation model. The difference is in the method of modeling
the probability of a transition from an interim visit to the final visit
$\Pr(Y_i = 1 \mid y_{it})$. Like the Beta Binomial
model, the logistic regression model imputes a patient’s final endpoint
given their most recent observed response.

The final endpoint response $Y_i$ is modeled as:

$$Y_i \sim \text{Bernoulli}(\pi_{t y_{it}})$$

where $\pi_{t y_{it}}$ is the probability of a
response at the final endpoint time given that its early observed
endpoint at time $t$$ is $y_{it}$. Then, we define the
parameter

$$\theta_{ty_{it}} = \text{logit}\left( \pi_{ty_{it}} \right) = \log\left( \frac{\pi_{ty_{it}}}{1 - \pi_{ty_{it}}} \right)$$.

The priors on $\theta_{t0}$ and $\theta{t1}$ are:

$$\theta_{t0} \sim \text{N}(\mu_0, \sigma_0^2)$$
$$\theta_{t1} \sim \text{N}(\mu_1, \sigma_1^2)$$

The model computes the posterior distribution of $\theta_{t0}$ and $\theta_{t1}$ using all patients on arms belonging to the model
instance that have observed endpoint values at time $t$ and the final
endpoint time $T$.

The priors on $\theta_{t0}$ and $\theta_{t1}$ may be shared
across model instances and/or visits depending on the selection made in
the Model Priors section of the FACTS Logistic regression Longitudinal
model page.

A possible starting place for non-informative priors in this model would
be: $\mu=0$, $\sigma=2$. A weakly informative set of priors that an early
response makes a final response more likely could be
$$\theta_{t0} \sim \text{N}(-.75, 1.25^2)$$ and
$$\theta_{t1} \sim \text{N}(0.75, 1.25^2)$$

### Restricted Markov Model (Absorbing Markov Chain)

:::{.callout-tip}
### Using the Restricted Markov Model

The restricted markov model is special in the sense that it can be used if and only if the "Use longitudinal modeling" check box is checked, the "Enable Special Longitudinal Options" check box is checked, and "Use restriced Markov model" is selected. When these conditions are met the Virtual Subject Response tab changes and the Design > Longitudinal tab only has the Restricted Markov option.
:::

The Absorbing Markov Chain model assumes that at each visit a subject is
in one of three states – responder (1), stable (S), or failure (0). The
responder and failure states are absorbing, meaning that once a patient
has entered one of those states they must remain in that state for the
remainder of the visits. Patients in the stable state may move to a
responder or a failure in subsequent visits.

Unlike most other longitudinal models in FACTS, the Restricted Markov
Model estimates the probability of a result at a visit based on the
visit right before it, rather than predicting directly to the final
endpoint from the early visit.

$$\Pr(y_{it} = n \mid y_{i, t-1} = S) \sim \text{Dirichlet}(\{\alpha_{0,t}, \alpha_{1,t}\, \alpha_{S,t}\}) \text{ for } t\ge 2$$

Where n can be 0, 1, or S, denoting the probability of going to the Fail
state, the Responder state, or the Stable state at visit $t$ from the
Stable state at visit $t-1$. $t$ must be greater than or equal to $2$,
because we do not impute the first visit – a subject missing visit 1 and
all visits after does not contribute to the longitudinal model or dose
response model.

The priors for the $\alpha$ parameters are specified in terms of the prior
number of transitions from Stable at $t-1$ to each different state at
time $t$. For example, if the prior value for the parameter
$\alpha_{1,3}$ is $2$, we are putting apriori information into the
Dirichlet distribution suggesting that $2$ patients transitioned from
Stable at visit 2 to Responders at visit 3. Specified priors can either
be common or different across model instances based on user
specification.

The parameters defining the posterior distribution of the state
probabilities are available in closed form as:

$$\alpha_{0,t} = \gamma_{0,t} + \left|y_{it}=0, y_{i, t-1} = S\right|$$
$$\alpha_{S,t} = \gamma_{S,t} + \left|y_{it}=S, y_{i, t-1} = S\right|$$
$$\alpha_{1,t} = \gamma_{1,t} + \left|y_{it}=1, y_{i, t-1} = S\right|$$

To create a dichotomous endpoint, the user specifies in the `Study > Study Info > Design Options` section whether patients remaining in a
stable state at the final visit should be dichotomized as responders or
failures.

### Dichotomous Endpoint: Dichotomized Continuous Longitudinal Model

The user may select (on the Study tab) to assume that the dichotomous
final endpoint is generated by observing continuous longitudinal data and
then dichotomizing the final endpoint based on whether it is greater than
or less than a provided threshold. If the user selects this option, then they
may select any of the continuous longitudinal models specified in
the [Continuous Longitudinal Models section](longitudinalmodels/continuous.qmd). The engine will impute
incomplete subjects according to the continuous model, resulting in a
continuous imputed final endpoint. The imputed dichotomous final
endpoint is then simply the dichotomized version of the continuous
imputation.

All priors and methods are identical to the continuous longitudinal
models mentioned above.

## Time-to-Event Predictor Models

For all predictors ($Z$) for time-to-event endpoints, the engine estimates
both a marginal distribution (normal mean and variance for continuous,
probability of response for dichotomous, and a piecewise exponential
hazard model for time to event predictors) and a working model relating
the predictor to the final endpoint. The marginal distribution is used
to impute predictors for subjects lacking an observed predictor value
and may also be used for stopping (see section on stopping). The working
model is used to impute final endpoints for subjects lacking a final
endpoint. For subject missing both a predictor and final endpoint, the
predictor is imputed first from its marginal distribution, and then the
final endpoint is imputed conditionally on the predictor $Z$.

#### Continuous Predictor

Within each dose (including control and active comparator), the marginal
distribution of $Z$ is a normal distribution with mean $\theta_{Zd}$ and standard deviation $\sigma_Z$. The
standard deviation is common across the doses, but the means
$\theta_{Zd}$ are allowed to vary across the same range of dose
response models as the final endpoint (NDLM, Logistic, etc.). The prior
specification for these predictor dose response models is identical in
structure to the final endpoint, although the user selects a separate
set of parameter values. The dose response for the predictor does not
need to match the dose response for the final endpoint.

The working model assumes the final event time T is related to the
predictor $Z$ by assuming $T\mid Z \sim \text{Exp}(\lambda_d e^{\beta Z})$, where
$\lambda_d$ varies by dose and has separate priors
$\lambda_d \sim \text{Gamma}(\alpha_d, \beta_d)$
for each dose. The coefficient in the exponent $\beta$ (no subscript) is
constant across doses with prior $\beta \sim \text{N}(m, s)$. 

#### Dichotomous Predictor

A dichotomous predictor is handled similarly to a continuous predictor,
with a marginal distribution having a predictor dose response model.
However, in this case the predictor dose response relates the log-odds
rather than the probability of response itself. The working model for
dichotomous is identical to the working model for a continuous
predictor, with $$T\mid Z \sim \text{Exp}(\lambda_d e^{\beta Z})$$. In this
situation the working model is simpler to understand, as
$$T\mid (Z=0) \sim \Exp(\lambda_d)$$ and $$T \mid (Z=1) \sim \text{Exp}(\lambda_{d\beta})$$.

#### Time to Event Predictor

The time to event predictor is qualitatively different than the
continuous or dichotomous predictors. Instead of the predictor adjusting
the hazard, the time to event predictor is viewed as an offset. The
final endpoint is viewed as a sum of a predictor time $Z_1$ and a post-predictor time $Z_2$, where $Z_1$ and
$Z_2$ are independent random variables and the final endpoint
is thus $Z_1 + Z_2$.

For the working model, $Z_1 \sim \text{PWExp}(\lambda_{1s}*\theta_{1d})$ and $Z_2 \sim \text{Exp}(\lambda_{2d})$,
with priors $\theta_{1s} \sim \text{Gamma}(\alpha_{1s}, \beta_{1s})$,
$\theta_{2d} \sim \text{Gamma}(\alpha_{2d}, \beta_{2d})$
(with $Z_1$’s control hazard model potentially being piecewise
exponential). For imputation, a subject missing both the biomarker and
final endpoint times has both $Z_1$ and $Z_2$
imputed, with the final endpoint imputed as the sum. For a subject with
a predictor time but no final endpoint, $Z_2$ is imputed and
added to the observed predictor time to impute the final endpoint.

# Allocation

The options on allocation tab depend on whether an adaptive or
non-adaptive design has been selected on the ‘Study > Study Info’
tab, and if adaptive whether subjects are recruited sequentially or in
cohorts.

If the design is non-adaptive then the only allocation option is blocked
with fixed allocation ratios.

If the design is adaptive with Continuous or Deterministic recruitment,
then there are 4 allocation options.

-   Fixed allocation – Subjects are randomized in blocks with fixed
    allocation ratios that do not change at interim analyses. This
    allocation strategy can still be useful in an adaptive design when
    paired with early stopping.

-   Arm dropping – which uses fixed allocation combined with the ability
    to drop under-performing treatment arms at any interim.

-   Adaptive Allocation - dose response adaptive allocation. At every
    interim the randomization probabilities are modified based on the
    specified adaptive allocation targets.

-   Deterministic Allocation – subjects are assigned treatments in an
    order specified in an external file that is imported into FACTS.
    This can be used to create a flexible randomization scheme in
    non-standard simulation scenarios.

If the design is adaptive with cohort recruitment then there are 3
allocation options, all of which can be combined with early stopping:

-   Fixed allocation – subjects are block allocated to treatments. The
    block distribution can be specified differently for the first cohort
    and the subsequent cohorts.

-   Adaptive Allocation - dose response adaptive allocation, in which at
    every interim the randomization probabilities are modified based on
    the provided adaptive allocation targets.

-   Adaptive Allocation – “best dose selection” after every interim the
    randomization for the next cohort is between the control and the
    dose that best meets the ‘target’ dose criteria.

The details of specifying each type of adaptive design are described
below, in each case specifying the “Interim Frequency” is the same, and
this facility is described in subsection: 10

## Non-adaptive designs

If the design is non-adaptive, then on this tab the user simply
specifies the fixed allocation ratio to use between all the treatment
arms for the duration of the study. The allocation is implemented using
a blocking scheme – the block size is the sum of the allocation ratios
entered and each arm is given the number of slots in the block
corresponding to its allocation ratio. Consequently, the values entered
must be integers.

<img src="coreUGattachments/CoreUserGuide/media/image40.png"
style="width:4.26584in;height:2.38513in" />

## Fixed Allocation

If allocation is to be fixed, then on this tab the fixed allocation
ratios and block size are specified. For each arm in the study
allocation ratios are entered as for fixed designs, and allocation uses
a block size that is the sum of the ratios. Fixed allocation works
identically to the non-adaptive design randomization. The difference is
that this Fixed allocation can be performed concurrently with interim
analyses being performed.

<img src="coreUGattachments/CoreUserGuide/media/image41.png"
style="width:4.17452in;height:2.8332in" />

## Arm Dropping

Adaptive arm dropping trials allow accruing data to inform the adaptive
design that an arm, or a set of arms, can be dropped, meaning they no
longer have subjects randomized to them. FACTS Core supports designs in
which some number of arms that are clearly ineffective can be dropped.
Designs where at an interim one or more arms are selected to be
continued and all other arms are dropped can be simulated using FACTS
Staged Designs.

<img src="coreUGattachments/CoreUserGuide/media/image42.png"
style="width:6.925in;height:4.92978in" />

### Randomization Ratio and Blocking

In the Randomization Ratio and Blocking section of the Allocation tab the
user inputs the components of randomization blocks that enroll from the onset
of the study and until any arm is dropped. These blocks work like the Fixed
Allocation tab blocks.

Once an arm is dropped the "Upon arm drop:" option in the Setup section of
the allocation tab will determine how randomization proceeds.

### Arm Dropping Criteria

The user may select any combination of Bayesian “Per Dose” QOIs: Posterior
Probabilities or Predictive Probabilities, or a Target Dose QOI. For each
criteria selected (at least one must be selected), the user supplies a
direction of comparison and probability threshold. If at any interim, any arm
meets the specified arm dropping criteria, then that arm becomes a
candidate for dropping. After all candidates are identified for
dropping, the Setup rules determine which, if any, of
the candidates will be dropped.

### Setup

A variety of rules are specified in the Setup section of the Allocation tab.

#### Max number of arms that can be dropped during the study
The maximum number of arms that can be dropped – this can be set
to from 0 to the number of active arms, the Control and Active
Comparator are never dropped. This figure is the ‘maximum number
of arms that can be dropped over the duration of the trial’. At
any interim any number between 0 and the ‘maximum number of arms
can be dropped as long as this limit is not exceeded on the
overall number of arms that would have been dropped.

If the maximum number of arms that can be dropped is set to the number
of study drug arms, then if all arms are dropped this is recorded as
an “early termination for futility”.

#### Prune from lowest/highest dose
Arm dropping can be restricted to ‘pruning from the lowest dose’
and/or ‘pruning from the highest dose’. If ‘prune from the lowest
dose’ is selected, then until the lowest dose meets the arm dropping
criteria the second lowest dose cannot be dropped and so on. At an
interim when the lowest dose **does** meet the arm dropping
criteria, and the second dose does too, then the second dose can be
dropped as well (and third and fourth …), it does not have to wait
until the next interim. Pruning from the highest dose does the same thing,
except that no dose can be dropped unless every larger dose will also be
dropped.

If no pruning is selected or pruning from both ends is selected,
then the user must select to prioritize the higher or lower dose to
cover the instance when more doses are candidates for dropping than are
allowed to drop by the maximum threshold. If “Prioritize dropping the lowest
dose” is selected then when forced to choose between two or more
doses to drop, the one with the lowest effective dose strength will
be dropped.

#### Upon arm drop
Finally, specify what is to be done with the unused subjects
that would have been allocated to an arm that has now been dropped. There
are three options:

Maintain study size, maintain combined block size of treatments
: Subjects that would have been allocated to arms that have been
dropped are now randomized between all the remaining active
treatment arms (excluding control and active comparator) using their
original allocation ratios. For example, if the allocation ratio was
2:1:1:1 between Control and D1, D2 and D3, then after dropping D1
the block size remains size 5 (2 to Control and 1 to each of D2 and
D3) with the $5^{th}$ slot being allocated 1:1 between the
remaining two study arms D2 & D3. Thus, the overall proportion of
subjects on Control (and Active Comparator) is maintained.

Maintain study size, reduce combined block size of treatments
: Subjects that would have been allocated to any arms that have been
dropped are randomized between all remaining arms (including
control) using a reduced total block size. Thus, if the allocation
ratio was 2:1:1 between Control and D1 and D2, then after dropping
D1 the block size drops to total size three: 2 to Control and 1 to
D2.

Decrease the study size, reduce combined block size of treatments
: Subjects that would have been allocated to any arms that have been
dropped are no longer recruited, and the total sample size of the
study is reduced; the remaining subjects are recruited using a
reduced block. If interims are specified by number of subjects
recruited, these interim sizes are reduced proportionally. As an
example of this, if an arm that gets 20% of the randomization is
dropped when there are 100 subjects enrolled, then if the next
interim is scheduled at 200 subjects enrolled, it will actually be
performed at 180 subjects enrolled.

## Adaptive Allocation

In adaptive allocation, the relative probabilities of assigning each of
the doses to a subject may change throughout the trial. The adaptive
allocation method that FACTS supports is one in which the allocation ratio
is modified at each interim to increase the allocation to doses that
have the preferred characteristics.

An adaptive allocation design has two phases: the “burn-in” before any
adaptation takes place and the “adaptive phase”. The burn-in lasts until
the first interim occurs, which is defined on the [Interims](design/interims.qmd) tab. The
adaptive phase lasts until the early stopping criteria are met or the
maximum sample size is reached.

<img src="coreUGattachments/CoreUserGuide/media/image43.png"
style="width:5.68039in;height:4.04189in" />

The table on the left hand side of the Adaptive Allocation tab
allows for the specification of the allocation ratio during the burn-in
period in the "Pre First Interim Allocation Ratio" column. The randomization
during the burn-in period works exactly like Fixed Allocation with the provided
randomization block.

When the first interim analysis is reached the allocation strategy changes
from fixed to adaptive, and the "Pre First Interim Allocation Ratio" column
is ignored. When randomizing adaptively the total block size is specified
below the table in the "Post first interim block size:" entry. The "Fix Alloc."
column check boxes should be checked for any dose that should not be adaptively
randomized to, but should get [a fixed proportion of the randomization
block](## "This is commonly done for the control arm and the active comparator in a trial that is randomizing adaptively."). When an arm is checked for having a fixed allocation its block contribution within the total block size must be specified. The sum of the fixed allocations must be smaller than the "Post first interim block size."

The slots that are in the block, but are not allocated to a fixed treatment are
probabilistically assigned to one of the non-fixed treatments.

If the Control arm is not given a fixed allocation, it is allocated to
with an adaptive probability that is equal to the treatment
arm with the highest weight.

#### Adaptive Allocation Targets

Adaptive allocation probabilities can be driven by any combination of posterior probabilities, predictive probabilities, conditional powers, and target probability QOIs. The allocation ratio of each arm is a proportional to a function of the QOIs - simply, arms with higher QOIs have higher allocation probabilities.

The way that the allocation probabilities are calculated is as follows.

Suppose that $M$ QOIs are specified in the "Adaptive Allocation Targets" table.
Call them $W^{(1)}, \ldots, W^{(M)}$, and to index the $m^{th}$ QOI's value for
dose $d$ we use the notation $W_d^{(m)}$. Then, the first step in calculating
the allocation weights is to convert the QOIs $W$ to the allocation targets $V$.

Any adaptive allocation QOI $W_d^{(m)}$ that has "Weight For:" of Probability
has an allocation target of $$V_d^{(m)} = \left(W_d^{(m)}\right)^\gamma,$$ and
if it has "Weight For:" of Information it has an
allocation target of $$V_d^{(m)} = \left[ \sqrt{\frac{W_d^{(m)} \text{Var} \left( \theta_{d} \right)}{n_{d} + 1}} \right]^{\gamma}$$ where [$\gamma$ is the value provided in "Raise allocation to power ($\gamma$)" in FACTS](## "Note that it's not the 'Relative Weight' provided when the Adaptive Allocation Target is specified."),
$n_d$ is the number of subjects on enrolled on dose $d$, and
$\text{Var}(\theta_d)$ is the variance of the dose-response model mean
estimate for arm $d$.

Then, when each dose has an allocation target $V_d$ for each of the $M$ QOIs,
the allocation weights are combined in a linear combination to get the
allocation weight. With $M$ allocation targets and Relative Weights $\omega_m$
provided when the allocation QOIs were specified:
$$\Omega_d = \sum_{m=1}^M (\omega_m V_{d}^{(m)})$$

Once each dose has an allocation weight $\Omega_d$, the set of $\Omega$'s is
renormalized so that they add to they add to the proportion of slots in the
block that will be allocated to adaptively. These renormalized values are
the adaptive randomization probabilities. If, after renormalization,
any of the doses have a randomization probability less than the value entered in
"Allocation probability set to zero for values less than:", then the dose with
the lowest randomization probability is set to 0 and the other randomization
probabilities are renormalized again to sum to the proportion of slots in the
block that will be allocated to adaptively. This process iterates until
all doses have randomization probabilities above the threshold or only one
non-fixed dose is being randomized to.

### Tips, Tricks, and Intuition

#### Weighting for Probability vs Information

When weighting for probability, the allocation target is derived simply from
the value of the QOI. Weighting for probability is ‘more aggressive’ in adapting in
pursuit of the target. The risk with a probability-based weighting
is never allocating again to an arm where the initial data is so
poor that its initial probability of being the target is so low it
is never allocated to again. So, it is appropriate when the
available sample size is small (and risks must be taken), the number
of study arms is small (so it is unlikely an arm will not be
allocated to again), or the allocation during the burn-in is large
and evenly distributed (so that having unrepresentative data is very
unlikely).

When weighting for information, the allocation target uses the value of the QOI,
but is adjusted by the current variance of the dose response estimate and the number
of subjects already allocated to the dose – an estimate of the
additional information that would result from adding one more
subject to that arm. Weighing for information will tend to spread the allocation around
the most likely target, reducing the risk of never learning that
dose is better than its initial data. If a dose response model is
being used, allocation to doses around the target dose will
contribute to the accuracy of the estimate of response on the target
dose, but compared to the probability weighting will tend to result
in fewer subjects allocated to the actual target dose.

Whichever weighing rule is selected the adaptation can be further
‘sharpened’ or ‘softened’ by adjusting the power to which the
allocation probability is raised. Setting the power to 0.5 will
significantly soften the allocation, setting it to 2 will
significantly sharpen it.

#### Fixed Allocation Target
Any Bayesian “Per Dose” QOI, or “Target Dose” QOI can be used to
determine the adaptive allocation weights. In addition there is the
option to use a static weighting – this is of course not adaptive!
What it does do is allow an adaptive allocation to be combined with a
“guaranteed minimum allocation” (see example discussion below). If a
Static target is included, a small table is displayed the
specification of the ratio of the division of the static weight
between the study arms that are being adaptively allocated to.

#### Non-fixed Control Adaptive Allocation

If the control allocation ratio is not fixed when performing adaptive
allocation, then the control arm gets a randomization ratio that targets
matching the number of control subjects to the active arm with the most
subjects.

To derive the control allocation rate, let $V_d$ for
$d=1, 2, \ldots, D$ be the allocation probabilities for each of the $D$
non-control dose arms (these may be obtained by calculating based on the
probability or information criteria as described above or the fixed
allocation proportion with respect to the block size), and let
$n_d$ for $d=1, 2, \ldots, D$ be the number of subjects
allocated to those arms.

Then, the allocation target for the control arm $V_0$ is:

$$V_{0} = \min\left\{\sum_{d = 1}^{D}{V_{d}\frac{(n_{d} + 1)}{(n_{0} + 1)}}, \;\; \text{max} \{V_{1},V_{2},\ldots,\\V_{D}\} \right\}$$

Following fixing this control rate, the allocation probabilities for the
non-fixed doses are renormalized to add up to the total non-fixed
probability.

#### Zero Out Allocation Probabilities

If, at the end of the adaptive allocation probability calculation, any
adaptively allocated arms have a randomization probability smaller than
the value provided for “Allocation probability set to zero for values
less than:”, then the allocation probabilities are adjusted.

To adjust the probabilities, first the arm with the smallest
randomization probability is given a fixed allocation rate of 0. Then,
the remaining adaptively allocated arms have their allocation
probabilities re-normalized to sum to the probability remaining after
all fixed doses have been allocated. Then, if any doses remain below the
zero-out threshold, then this process is repeated. Continue the
repetition as necessary. If none of the allocation probabilities are
below the threshold, then the allocation probabilities are set.

If all non-fixed doses drop due to being below the threshold, then the probability
is reallocated to the fixed doses. This can only happen if the fixed doses take up
a large enough proportion of the slots in the block.

### Adaptive Allocation Calculation Examples

#### Simple Response Adaptive Randomization Example

Suppose you have a control and 3 active doses. The control arm is
fixed to randomize $3/10$ slots in every block and a fixed 20% probability must be
placed on the first active dose (dose A). So, the control arm gets 30%
of the total allocation probability and dose A gets 20% of the total
allocation. Suppose the target QOIs $V_d$ for the three
active doses are 0.20, 0.20, and 0.60 and that we’re using probability
weighting with weight 1 for all of them. The second two doses are the only doses with
unknown randomization probabilities, so they split the amount of
non-fixed allocation probability proportionally based on their
$V_d$.

The allocation probability of the second active dose is
$$\left( 1 - (0.3 + 0.2) \right)*\left( \frac{0.2}{0.2 + 0.6} \right)$$
and the allocation probability of the third active dose is
$$\left( 1 - (0.3 + 0.2) \right)*\left( \frac{0.6}{0.2 + 0.6} \right)$$

Thus, the final allocation probabilities for the control and the three
active doses are: (0.3, 0.2, 0.125, 0.375).

If any of the adaptively allocated probabilities are less than a user
specified minimum (“Allocation probability set to zero for values less
than....” in the GUI) then these probabilities would be set to zero and
the resulting probability is reallocated among the non-fixed probability
doses. If all non-fixed doses drop at this point, then the probability
is reallocated to the fixed doses.

#### Using Static Weighting Example: Ensuring a minimum allocation to all arms

This example uses the static allocation QOI to allow adaptive randomization,
but with a guaranteed minimum allocation of 10% to each of 5 study
arms. Let us say that we want the adaptive allocation to be evenly
divided between targeting an EDq and MED target.

As there are 5 study arms, allocating 10% each amounts to 50% of the
total. Thus, we could specify weights of 1 to the MED target, 1 to the
EDq target, and 2 to the Static target, so the Static target gets 50% of
the total weighting.

However, this ignores the possible allocation to a Control arm. What we
have achieved above either allocates 10% to each study arm if there is
no Control arm, or if there is a Control arm, allocates 10% of the
subjects *allocated to the study arms*, not 10% of the overall.

Let us say that in addition to the 5 study arms you have a Control that
we want to have 20% allocation and we want each study arm to have at
least 10% of the overall allocation. The simplest way to specify this is
to set a “Post first interim block size” of 10 and that 2 slots are
allocated to Control. This leave 8 slots to be allocated across the treatment
arms. We don't want to allocate 1 out of 10 slots in the block to
each using fixed allocation, because that 10% is all they would get. We
need to allow them to be allocated to adaptively and use the Static
target to ensure they get a minimum of 10%. We can achieve this by
giving the static allocation a weight of 5 and divide a weight of 3
between the MED and EDq targets, so they get a weight of 1.5 each. Then,
set all of the "Static Weights" to be 1 (any non-zero value would work) for each
active arm.

By doing this, the static allocation targets make up $5\text{ arms }*\text{ weight of }5 = 25$ parts of
the weighted average and the MED and EDq combine to make up $5\text{ arms }*\text{ weight of }3 = 15$ parts.
So, if an arm has an MED of 0 and EDq of 0 it is still guaranteed to get at least
$\frac{5}{(25 + 15)}*(1 - 0.2) = 0.1$ of the total randomization.

#### Using Static Weighting Example: Ensuring a minimum allocation to top dose

Imagine an early Phase II Proof-of-Concept study that wants to compare 3
doses of the study drug (low, medium and high) to Control, and
adaptively allocates to the dose with the maximum response. The study
team also wish to ensure a minimum allocation to the top dose as they
have a strong prior that it will have the maximum effect. They want to
minimize the risk that the adaptive allocation avoids the top dose
because of randomly poor results on that dose early on in the trial.

The optimal allocation to control in a multi armed study is approximately
$\sqrt{N}:1:1:\ldots$ where N is the number of study arms and in
$\sqrt{3}:1:1:1$ the proportion on control would be 37%. Here we've
rounded down to 30% rather than up to 40% to reflect a typical clinical
teams desire to get more data on their study drug than Control.

After the first interim, we specify allocation to be in blocks of 10
with the Control allocated to 3 slots in each block, the "Relative Weight:" on the
Static target is 2 and the "Relative Weight:" on the Pr(Max) target is 5. So 50% of
the allocation is adaptive targeting the dose with the Maximum response
and 20% is fixed and allocated to the top dose by specifying that the
static weighting "Ratio" is split 0:0:1 in the "Static Weights" table.

<img src="coreUGattachments/CoreUserGuide/media/image44.png"
style="width:3.97826in;height:2.53209in" />

## Deterministic Allocation

The deterministic allocation option allows for the treatment assignments
of subjects accrued into the simulated trials to be assigned without
randomness based on an uploaded file. The file providing the assignments
should be a comma separated .dat file with 2 columns, but no column
labels.

The first column should be an increasing, unique column of numbers from
1 to at least the maximum number of subjects that can be enrolled in the
study. The second column should be the set of treatment assignments in
order from 1 to the number of patient IDs. The treatment assignments are
used in row order – they do not use the subject ID order. So, the first
subject accrued in the study is given the assignment indicated by the
first row, second column value, the second subject accrued in the study
is given the assignment indicated by the second row, second column
value, and so on.

The treatment assignments column in the .dat file should always have a
minimum value of 1 and a maximum of the number of total arms in the
study. If there is a control arm in the study, then treatment assignment
1 corresponds to a control arm randomization. If there is no control arm
in the study, then treatment assignment 1 in the .dat file corresponds
to a subject randomized to the lowest active dose.

<img src="coreUGattachments/CoreUserGuide/media/image46.png"
style="width:5.0296in;height:3.70171in" />

## Cohort recruitment – fixed allocation

If allocation is to be fixed the “Early Stopping Only” option should be
selected. Then on this tab the fixed allocation ratios and block size
are specified – for the first cohort and then for all subsequent
cohorts. In each case the user specifies the number of each arm to
allocate in each cohort and the sum must equal the size of the cohort.

<img src="coreUGattachments/CoreUserGuide/media/image47.png"
style="width:5.52636in;height:3.93229in" />

## Cohort recruitment – adaptive allocation

If allocation is to be adaptive the “Adaptive Allocation” option should
be selected and then under “Subsequent Cohort Allocation” the “Allocate
cohort by sampling from allocation vector” should be selected. Then on
this tab

-   The fixed allocation ratios and block size are specified for the
    first cohort. The user specifies the number of each arm to allocate
    and the sum must equal the size of the cohort.

-   The user then specifies

    -   The fixed allocation to control

    -   The QOIs to be used to calculate the allocation ratios for the
        remaining subjects in the cohort as for adaptive allocation (see
        Adaptive Allocation above).

<img src="coreUGattachments/CoreUserGuide/media/image48.png"
style="width:5.61615in;height:3.99618in" />

## Cohort recruitment – allocate to best dose

If allocation is to be adaptive, but at each cohort just Control and the
“best dose” are to be allocated to, the “Adaptive Allocation” option
should be selected and then under “Subsequent Cohort Allocation” the
“Allocate cohort to single treatment (plus control)” should be selected.
Then on this tab

-   The fixed allocation ratios and block size are specified for the
    first cohort. The user specifies the number of each arm to allocate
    and the sum must equal the size of the cohort.

-   The user then specifies

    -   The fixed allocation to control

    -   The QOIs to be used to calculate the allocation weighting to
        calculate for the non-Control arms, (see Adaptive Allocation
        above) and all the subjects not allocated to control will be
        allocated to this arm.

<img src="coreUGattachments/CoreUserGuide/media/image49.png"
style="width:5.5182in;height:3.92648in" />

#  Interims

Interim analyses allow for decision making throughout the lifecycle of
an adaptive trial in FACTS. Interim analyses can adjust allocation
probabilities, drop arms, or allow for early success/futility of the
trial. Interims can either be specified with calendar frequency –
occurring every specified number of weeks or specified to occur after a
specified amount of information has been collected.

## Interim Analysis Triggers

### Continuous and Dichotomous Endpoint

Information can be defined in terms of:

-   number of subjects that have been recruited

-   the number of subjects who have actually completed a specified visit

-   the number of subjects who have had the opportunity to complete a
    specified visit (includes drop-outs)

When specifying an interim analysis schedule, it can be done either based
on time or based on one of the information categories above.

If specifying interims based on time the first interim
analysis timing must be based on information,
and each subsequent interim is triggered after the provided amount of time
has elapsed. If accrual completes before the first interim threshold is reached,
and the first interim was defined in terms of the number of subjects
enrolled, then the interims by time start at full accrual. If the first
interim is defined in any other terms (by events, subjects complete or
subjects with opportunity to complete) then interims only start when
this is reached (which might be never).

If specifying interims based solely on information, the table
on the "Interims" tab determines when the analyses will be triggered.
Each interim is defined individually by the number of
patients/observations that have satisfied some criteria. If information is
If information is defined as Subjects Enrolled, then interim are triggered
immediately upon enrollment of the subject satisfying the criteria. If
information is defined as completers or opportunity to complete, then interims
are triggered immediately upon the visit being reached that satisfies the
specified criteria. Successive interims must be in terms of the same or
more observations at the same or later visit, and either Visit or Subject must
increase. Different types of information cannot be mixed to trigger interim analyses
except in using time to trigger interims after the first based on information.

<img src="coreUGattachments/CoreUserGuide/media/image50.png"
style="width:2.94314in;height:2.8326in"
alt="Graphical user interface Description automatically generated" />
<img src="coreUGattachments/CoreUserGuide/media/image51.png"
style="width:2.99611in;height:2.86372in"
alt="Graphical user interface Description automatically generated" />

If interims are governed by time, there is the
option as to whether interims should continue after full accrual, or
discontinue.

### Time-to-Event Endpoint

Information can be defined in terms of:

-   number of subjects that have been recruited

-   the number of subjects who have observed their predictor endpoint

-   the number of subjects who have had the opportunity to observe their
    predictor endpoint (includes drop-outs)

-   specified numbers of events observed

-   specified number of predictor events observed

Outside of the new types of information, the time-to-event triggers work
in exactly the same way that continuous and dichotomous triggers do.

## Subject Follow-up Options

Regardless of endpoint, the Interims tab contains options that control
the behaviour should a trial stop at an interim analysis. The options allows the
user to specify whether or not to complete the follow-up of subjects who
have been accrued, but have not had time to observe their final
endpoint.

The default options available for Subject Follow-Up are:

-   Continue follow-up if study stopped for early success

-   Continue follow-up if study stopped for early futility

If the check box corresponding to an interim decision is checked, then
at the time of an interim analysis decision - accrual will be stopped, all
subjects currently enrolled will be followed-up until they have had the
opportunity to observe their final endpoint, and then the final analysis
will be performed.

If the check box corresponding to an interim decision is not checked,
then at the time of an interim analysis decision - accrual is stopped, the
data is locked, and no follow-up on randomized patients is collected.
The interim dataset is the final dataset. [The final analysis is then
performed using the same data and model as was used for the interim
analysis.](## "This can result in success to futility flip-flops or futility
to success flip-flops, even if there is no additional follow-up, since the
final analysis criteria can be specified to be less strict than the interim
analysis threshold. If you do not follow up after a success, then a success
to futility flip-flop can be considered simply an early success.")

<img src="coreUGattachments/CoreUserGuide/media/image52.png"
style="width:2.96325in;height:0.70635in"
alt="Graphical user interface, text, application Description automatically generated" />

If the allocation method is selected as, “Arm Dropping,” then an
additional check box is provided in the Subject Follow-up Options box
asking whether the user would like to “Continue follow-up if arm
dropped.” If the box is checked, then subjects randomized to an arm that
is dropped before they have the opportunity to complete their follow-up
will have to opportunity to observe their final endpoint for subsequent
analyses. If the box is not checked then incomplete subjects on an arm
that is dropped will never have future endpoint values observed.

# Success/Futility Criteria

The Success/Futility Criteria tab is where users specify the decision rules
for determining study success or failure. The Final Analysis criteria always
exist, and should, in general, be specified for every simulated trial. If
simulating an adaptive trial, then interim analysis decision rules are also
specified here.

There are $7$ possible decisions that can be made in a FACTS Core design, each
with a numeric identifier that FACTS uses in the .csv output to denote
decisions:

1. Early Success
: Early success is achieved if and only if the trial meets the success condition
at an interim analysis, and does not meet the futility criteria at the final analysis.
The final analysis criteria must not be met whether or not subjects were selected
to follow-up after an early success decision.

2. Late Success
: Late success is achieved if and only if the trial enrolls to the maximum sample
size, collects full follow-up, and then meets the final analysis success criteria.

3. Late Futility
: Late success is achieved if and only if the trial enrolls to the maximum sample
size, collects full follow-up, and then meets the final analysis futility criteria.
Late futility is not automatically the complement of late success; the futility
rule must be specified as the complement of the success rule to make it true.

4. Early Futility
: Early futility is achieved if and only if the trial meets the futility condition
at an interim analysis, and does not meet the success criteria at the final analysis.
The final analysis criteria must not be met whether or not subjects were selected
to follow-up after an early success decision.

5. Success to futility flip-flop
: Success to futility flip-flop is achieved if and only if the trial meets the
success condition at an interim analysis, but meets the futility condition at the
final analysis. Success to futility flip-flops can be achieved whether or not
subjects are followed up after the early success decision.

6. Futility to success flip-flop
: Futility to success flip-flop is achieved if and only if the trial meets the
futility condition at an interim analysis, but meets the success condition at the
final analysis. Futility to success flip-flops can be achieved whether or not
subjects are followed up after the early futility decision.

7. Inconclusive
: Inconclusive is achieved if and only if the trial enrolls to the maximum sample
size, collects full follow-up, and then does not meet the final success or
final futility criteria.

Every simulated trial will result in exactly one of these decisions. Non-adaptive
trials will result in either Late Success, Late Futility, or Inconclusive. An
adaptive trial that does not stop at an interim analysis will result in Late
Success, Late Futility, or Inconclusive. An adaptive trial that stops enrolling for early
success at an interim analysis will end up as an Early Success or a Success to
Futility flip-flop. An adaptive trial that stops enrolling for early futility will result
in Early Futility or Futility to Success flip-flop.

## Final Evaluation

On the Final Evaluation tab, the user can specify rules for
judging the study for final futility or final success at its end.

The left column of the Final Evaluation tab contains the specification of the
trial final futility rule, and the right column contains the specification of
the final success rule.

To add a decision rule, click the "Add..." button within the appropriate column,
select a decision quantity QOI, a comparison inequality sign, and a threshold.
Final success and final futility criteria can each have multiple components to them,
and the selection at the bottom of the column called "Combine criteria using:"
dictates if success or futility should be declared if every single criteria is met
(AND) or if any criteria is met (OR).

The success and futility rules need not be complementary - there can be trials
that do not meet either criteria at the final analysis. These trials would be
considered inconclusive. It is allowable, although generally not recommended, to specify overlapping success and futility
rules. If a trial were to satisfy both the success and futility criteria at the
final analysis it would be considered a final futility.

The Final Evaluation Criteria are always applied at the time of the
final analysis, even if the study stops early for success or futility,
and whether or not “Continue follow-up if study stopped for success” or
“Continue follow-up if study stopped for futility” are selected. As a
result, setting final analysis criteria that are easier to satisfy than
interim analysis criteria may result in the labelling of trials that
meet the interim analysis criteria but do not satisfy the final analysis
criteria as flip-flops.

<img src="coreUGattachments/CoreUserGuide/media/image54.png"
style="width:5.48603in;height:4.10627in"
alt="A screenshot of a social media post Description automatically generated" />

## Interim Analysis Criteria

On the success/futility criteria of a design with "Enable adaptive features"
checked on the Study > Study Info page, the user can specify rules for
judging the study for futility or success at an interim and at the final
evaluation. If the trial has no interims there will be just a tab for
the Final Evaluation criteria. If the trial has interim analyses, then
there can be tabs that define different early success/futility criteria
at the different interims.

At the top of the main tab is a control to allow tabs to be created for
different interims. In a newly created adaptive design, FACTS will
create a tab for interim 1.

If early success/futility criteria are specified for an interim, then
they will be taken to apply to all subsequent interims until the next
one for which criteria are supplied, then those criteria will apply
until the next interim for which criteria are applied and so on. There will
be no stopping for success or futility until the first interim for which
early stopping criteria have been defined.

It is possible to specify overlapping early success and early futility criteria at
an interim, but it is not possible to stop
for both success and futility at the same time. The FACTS design
engines will stop the trial but the user should not rely on one or
other outcome taking precedence. It is not considered good practice
to have success and futility rules that could both be true, so FACTS
does not [guarantee a “tie break” rule](## "As of FACTS 7.1 early futility will
be the result if both early success and early futility criteria are met.").

In the output files there are columns labeled “Success <QOI>”
and “Futile <QOI>” indicating whether any decision criteria
became true, and “Success Combined” and “Futile Combined” indicating
if all the criteria for a success or futility determination have
been met. If both sets of criteria are met simultaneously, FACTS
will only flag one of “Success Combined” and “Futile Combined” as
being met, corresponding to how the outcome of the trial has been
flagged.

<img src="coreUGattachments/CoreUserGuide/media/image53.png"
style="width:5.52058in;height:4.13213in"
alt="A screenshot of a social media post Description automatically generated" />

Having created a tab to define the early success/futility criteria at an
interim, the user has options to delete the tab or to copy criteria that
have already been entered on another tab before editing them.

The user specifies:

-   Whether early stopping for futility or success is to be allowed by
    selecting the “Futility criteria” or “Success criteria”

-   The stopping rules are specified, by selecting the QOI to be tested,
    the direction of the comparison and the threshold value for the
    criteria to be met, as in the Final Evaluation tab above.

-   The user can specify whether, if multiple criteria have been
    specified, they all need to be meet (Criteria combined using “AND”)
    or only any one of them needs to be met (Criteria combined using
    “OR”). These are specified independently for stopping for success
    and for futility.

-   If stopping is allowed, the user specifies what the minimum amount
    of information (using the information type specified on the Interims
    tab). This can be specified both in overall terms and information on
    specific doses. If multiple minimum-information criteria are
    specified they must all be met for the QOI criteria to be evaluated.
